[{"url": "https://www.ijcai.org/Abstract/13/010", "abstract": "The growth of social media and on-line social networks has opened up a set of fascinating new challenges and directions for researchers in both computing and the social sciences, and an active interface is growing between these areas. We discuss a set of basic questions that arise in the design and analysis of systems supporting on-line social interactions, focusing on two main issues: the role of network structure in the dynamics of social media sites, and the analysis of textual data as a way to study properties of on-line social interaction.", "title": "Computational Perspectives on Social Phenomena at Global Scales"}, {"url": "https://www.ijcai.org/Abstract/13/011", "abstract": "There has been an increasing interest in applying biological principles to the design and control of robots. Unlike industrial robots that are programmed to execute a rather limited number of tasks, the new generation of bio-inspired robots is expected to display a wide range of behaviours in unpredictable environments, as well as to interact safely and smoothly with human co-workers. In this article, we put forward some of the properties that will characterize these new robots: soft materials, flexible and stretchable sensors, modular and efficient actuators, self-organization and distributed control. We introduce a number of design principles; in particular, we try to comprehend the novel design space that now includes soft materials and requires a completely different way of thinking about control. We also introduce a recent case study of developing a complex humanoid robot, discuss the lessons learned and speculate about future challenges and perspectives.", "title": "Soft Robotics: The Next Generation of Intelligent Machines"}, {"url": "https://www.ijcai.org/Abstract/13/012", "abstract": "The frequency and intensity of natural disasters have significantly increased over the past decades and this trend is predicted to continue. Natural disasters have dramatic impacts on human lives and on the socio-economic welfare of entire regions. They are identified as one of the major risks of the East Asia and Pacific region, which represents 85 percents of all people affected since 2007. Moreover, this exposure will likely double by 2050 due to rapid urbanization and climate change. Dramatic events such as Hurricane Katrina and the Tohoku tsunami have also emphasized the strong need for optimization tools in preparing, mitigating, responding, and recovering from disasters, complementing the role of situational awareness that had been the focus in the past. This talk presents some of the progress accomplished in the last 5 years and deployed to assist the response to hurricanes such as Irene and Sandy. It describes the computational challenges ahead in optimization, simulation, and the modeling of complex inter-dependent infrastructures, and sketches the disaster management platform built at NICTA.", "title": "Computational Disaster Management"}, {"url": "https://www.ijcai.org/Abstract/13/014", "abstract": "We consider the problem of updating  a multi-agent system with a set of conditional norms. A norm comes into effect when its condition becomes true, and imposes either an obligation or a prohibition on an agent which remains in force until a state satisfying a deadline condition is reached. If the norm is violated, a sanction is imposed on the agent. We define a notion of a normative update of a multi-agent system by a set of conditional norms, and study the problem of checking whether the agent(s) can bring about a state satisfying a property without incurring a specified number of sanctions.", "title": "Reasoning about Normative Update"}, {"url": "https://www.ijcai.org/Abstract/13/015", "abstract": "Dynamic epistemic logic (DEL) provides a very expressive framework for multi-agent planning that can deal with nondeterminism, partial observability, sensing actions, and arbitrary nesting of beliefs about other agents' beliefs. However, as we show in this paper, this expressiveness comes at a price. The planning framework is undecidable, even if we allow only purely epistemic actions (actions that change only beliefs, not ontic facts). Undecidability holds already in the S5 setting with at least 2 agents, and even with 1 agent in S4. It shows that multi-agent planning is robustly undecidable if we assume that agents can reason with an arbitrary nesting of beliefs about beliefs. We also prove a corollary showing undecidability of the DEL model checking problem with the star operator on actions (iteration).", "title": "Undecidability in Epistemic Planning"}, {"url": "https://www.ijcai.org/Abstract/13/016", "abstract": "In social choice settings with strict preferences, random dictatorship rules were characterized by Gibbard [1977] as the only randomized social choice functions that satisfy strategyproofness and ex post efficiency. In the more general domain with indifferences, RSD (random serial dictatorship) rules are the well-known and perhaps only known generalization of random dictatorship. We present a new generalization of random dictatorship for indifferences called Maximal Recursive (MR) rule as an alternative to RSD. We show that MR is polynomial-time computable, weakly strategyproof with respect to stochastic dominance, and, in some respects, outperforms RSD on efficiency.", "title": "Maximal Recursive Rule: A New Social Decision Scheme"}, {"url": "https://www.ijcai.org/Abstract/13/0017.html", "abstract": null, "title": "Audit Games"}, {"url": "https://www.ijcai.org/Abstract/13/018", "abstract": "We focus on solving two-player zero-sum extensive-form games with perfect information and simultaneous moves. In these games, both players fully observe the current state of the game where they simultaneously make a move determining the next state of the game. We solve these games by a novel algorithm that relies on two components: (1) it iteratively solves the games that correspond to a single simultaneous move using a double-oracle method, and (2) it prunes the states of the game using bounds on the sub-game values obtained by the classical Alpha-Beta search on a serialized variant of the game. We experimentally evaluate our algorithm on the Goofspiel card game, a pursuit-evasion game, and randomly generated games. The results show that our novel algorithm typically provides significant running-time improvements and reduction in the number of evaluated nodes compared to the full search algorithm.", "title": "Using Double-Oracle Method and Serialized Alpha-Beta Search for Pruning in Simultaneous Move Games"}, {"url": "https://www.ijcai.org/Abstract/13/019", "abstract": "The cake cutting problem models the fair division of a heterogeneous good between multiple agents. Previous work assumes that each agent derives value only from its own piece. However, agents may also care about the pieces assigned to other agents; such externalities naturally arise in fair division settings. We extend the classical model to capture externalities, and generalize the classical fairness notions of proportionality and envy-freeness. Our technical results characterize the relationship between these generalized properties, establish the existence or nonexistence of fair allocations, and explore the computational feasibility of fairness in the face of externalities.", "title": "Externalities in Cake Cutting"}, {"url": "https://www.ijcai.org/Abstract/13/020", "abstract": "We investigate the problem of deciding whether a given preference profile is close to a nicely structured preference profile of a certain type, as for instance single-peaked, single-caved, single-crossing, value-restricted, best-restricted, worst-restricted, medium-restricted, or group-separable profiles. We measure this distance by the number of voters or alternatives that have to be deleted so as to reach a nicely structured profile. Our results classify all considered problem variants with respect to their computational complexity, and draw a clear line between computationally tractable (polynomial time solvable) and computationally intractable (NP-hard) questions.", "title": "Are There Any Nicely Structured Preference Profiles Nearby?"}, {"url": "https://www.ijcai.org/Abstract/13/021", "abstract": "Learning in automated negotiations, while useful, is hard because of the indirect way the target function can be observed and the limited amount of experience available to learn from. This paper proposes two novel opponent modeling techniques based on deep learning methods. Moreover, to improve the learning efficacy of negotiating agents, the second approach is also capable of transferring knowledge efficiently between negotiation tasks. Transfer is conducted by automatically mapping the source knowledge to the target in a rich feature space. Experiments show that using these techniques the proposed strategies outperform existing state-of-the-art agents in highly competitive and complex negotiation domains. Furthermore, the empirical game theoretic analysis reveals the robustness of the proposed strategies.", "title": "Conditional Restricted Boltzmann Machines for Negotiations in Highly Competitive and Complex Domains"}, {"url": "https://www.ijcai.org/Abstract/13/022", "abstract": "This paper is devoted to complexity results regarding specific measures of proximity to single-peakedness and single-crossingness, called \"single-peaked width\" [Cornaz et al., 2012] and \"single-crossing width\". Thanks to the use of the PQ-tree data structure [Booth and Lueker, 1976], we show that both problems are polynomial time solvable in the general case (while it was only known for single-peaked width and in the case of narcissistic preferences). Furthermore, we establish one of the first results (to our knowledge) concerning the effect of nearly single-peaked electorates on the complexity of an NP-hard voting system, namely we show the fixed-parameter tractability of Kemeny elections with respect to the parameters \"single-peaked width\" and \"single-crossing width\".", "title": "Kemeny Elections with Bounded Single-Peaked or Single-Crossing Width"}, {"url": "https://www.ijcai.org/Abstract/13/023", "abstract": "En-route charging stations allow electric vehicles to greatly extend their range. However, as a full charge takes a considerable amount of time, there may be significant  waiting times at peak hours.To address this problem, we propose a novel navigation system, which communicates its intentions (i.e., routing policies) to other drivers. Using these intentions, our system accurately predicts congestion at charging stations and suggests the most efficient route to its user. We achieve this by extending existing time-dependent stochastic routing algorithms to include the battery's state of charge and charging stations. Furthermore, we describe a novel technique for combining historical information with agent intentions to predict the queues at charging stations. Through simulations we show that our system leads to a significant increase in utility compared to existing approaches that do not explicitly model waiting times or use intentions, in some cases reducing waiting times by over 80% and achieving near-optimal overall journey times.", "title": "Intention-Aware Routing to Minimise Delays at Electric Vehicle Charging Stations"}, {"url": "https://www.ijcai.org/Abstract/13/024", "abstract": "Optimally solving decentralized partially observable Markov decision processes (Dec-POMDPs) is a hard combinatorial problem. Current algorithms search through the space of full histories for each agent. Because of the doubly exponential growth in the number of policies in this space as the planning horizon increases, these methods quickly become intractable. However, in real world problems, computing policies over the full history space is often unnecessary. True histories experienced by the agents often lie near a structured, low-dimensional manifold embedded into the history space. We show that by transforming a Dec-POMDP into a continuous-state MDP, we are able to find and exploit these low-dimensional representations. Using this novel transformation, we can then apply powerful techniques for solving POMDPs and continuous-state MDPs. By combining a general search algorithm and dimension reduction based on feature selection, we introduce a novel approach to optimally solve problems with significantly longer planning horizons than previous methods.", "title": "Optimally Solving Dec-POMDPs as Continuous-State MDPs"}, {"url": "https://www.ijcai.org/Abstract/13/025", "abstract": "Algorithms for stable marriage and related matching problems typically assume that full preference information is available. While the Gale-Shapley algorithm can be viewed as a means of eliciting preferences incrementally, it does not prescribe a general means for matching with incomplete information, nor is it designed to minimize elicitation. We propose the use of maximum regret to measure the (inverse) degree of stability of a matching with\u00a0 partial preferences; minimax regret to find matchings that are maximally stable in the\u00a0 presence of partial preferences; and heuristic elicitation schemes that use max regret to determine relevant preference queries. We show that several of our schemes find stable matchings while eliciting considerably less preference information than Gale-Shapley and are much more appropriate in settings where approximate stability is viable.", "title": "Elicitation and Approximately Stable Matching with Partial Preferences"}, {"url": "https://www.ijcai.org/Abstract/13/026", "abstract": "Coalition formation is a fundamental approach to multi-agent coordination. In this paper we address the specific problem of coalition structure generation, and focus on providing good-enough  solutions using a novel heuristic approach that is based on data clustering methods. In particular, we propose a hierarchical agglomerative clustering approach (C-Link), which uses a similarity criterion between coalitions based on the gain that the system achieves if two coalitions merge. We empirically evaluate C-Link on a synthetic benchmark data-set as well as in collective energy purchasing settings. Our results show that the C-link approach performs very well against an optimal benchmark based on Mixed-Integer Programming, achieving solutions which are in the worst case about 80% of the optimal (in the synthetic data-set), and 98% of the optimal (in the energy data-set). Thus we show that C-Link can return solutions for problems involving thousands of agents within minutes.", "title": "C-Link: A Hierarchical Clustering Approach to Large-Scale Near-Optimal Coalition Formation"}, {"url": "https://www.ijcai.org/Abstract/13/027", "abstract": "Control and manipulation are two of the most studied types of attacks on elections. In this paper, we study the complexity of control attacks on elections in which there are manipulators. We study both the case where the \"chair\" who is seeking to control the election is allied with the manipulators, and the case where the manipulators seek to thwart the chair. In the latter case, we see that the order of play substantially influences the complexity. We prove upper bounds, holding over every election system with a polynomial-time winner problem, for all standard control cases, and some of these bounds are at the second or third level of the polynomial hierarchy, and we provide matching lower bounds to prove these tight. Nonetheless, for important natural systems the complexity can be much lower. We prove that for approval and plurality elections, the complexity of even competitive clashes between a controller and manipulators falls far below those high bounds, even as low as polynomial time. Yet we for a Borda-voting case show that such clashes raise the complexity unless NP=coNP.", "title": "Control in the Presence of Manipulators: Cooperative and Competitive Cases"}, {"url": "https://www.ijcai.org/Abstract/13/028", "abstract": "When solving extensive-form games with large action spaces, typically significant abstraction is needed to make the problem manageable from a modeling or computational perspective. When this occurs, a procedure is needed to interpret actions of the opponent that fall outside of our abstraction (by mapping them to actions in our abstraction). This is called an action translation mapping. Prior action translation mappings have been based on heuristics without theoretical justification. We show that the prior mappings are highly exploitable and that most of them violate certain natural desiderata. We present a new mapping that satisfies these desiderata and has significantly lower exploitability than the prior mappings. Furthermore, we observe that the cost of this worst-case performance benefit (low exploitability) is not high in practice; our mapping performs competitively with the prior mappings against no-limit Texas Hold'em agents submitted to the 2012 Annual Computer Poker Competition. We also observe several paradoxes that can arise when performing action abstraction and translation; for example, we show that it is possible to improve performance by including suboptimal actions in our abstraction and excluding optimal actions.", "title": "Action Translation in Extensive-Form Games with Large Action Spaces: Axioms, Paradoxes, and the Pseudo-Harmonic Mapping"}, {"url": "https://www.ijcai.org/Abstract/13/029", "abstract": "We study trade networks with a tree structure, where a seller with a single indivisible good is connected to buyers, each with some value for the good, via a unique path of intermediaries. Agents in the tree make multiplicative revenue share offers to their parent nodes, who choose the best offer and offer part of it to their parent, and so on; the winning path is determined by who finally makes the highest offer to the seller. In this paper, we investigate how these revenue shares might be set via a natural bargaining process between agents on the tree, specifically, egalitarian bargaining between endpoints of each edge in the tree. We investigate the fixed point of this system of bargaining equations and prove various desirable for this solution concept, including (i) existence, (ii) uniqueness, (iii) efficiency, (iv) membership in the core, (v) strict monotonicity, (vi) polynomial-time computability to any given accuracy.Finally, we present numerical evidence that asynchronous dynamics with randomly ordered updates always converges to the fixed point, indicating that the fixed point shares might arise from decentralized bargaining amongst agents on the trade network.", "title": "Bargaining for Revenue Shares on Tree Trading Networks"}, {"url": "https://www.ijcai.org/Abstract/13/030", "abstract": "We consider the problem of equitably allocating a set of indivisible goods to n agents so as to maximize the utility of the least happy agent. Demko and Hill sowed in [Demko, S., and Hill, T. P. (1988). Equitable distribution of indivisible objects. Mathematical Social Sciences, 16(2), 145-158.] the existence of an allocation where every agent values his share at least Vn(\u03b1), which is a family of nonincreasing functions in a parameter \u03b1, defined as the maximum value assigned by an agent to a single good. A deterministic algorithm returning such an allocation in polynomial time was proposed in [Markakis, E., & Psomas, C. A. (2011). On worst-case allocations in the presence of indivisible goods. In Workshop Internet and Network Economics (pp. 278-289). Springer Berlin Heidelberg.]. Interestingly, Vn(\u03b1) is tight for some values of \u03b1, i.e. it is the best lower bound on the valuation of the least happy agent. However, it is not true for all values of \u03b1. We propose a family of functions Wn such that Wn(x) \u2265 Vn(x) for all x, and Wn(x) \u2265 Vn(x) for values of x where Vn(x) is not tight. The new functions Wn apply on a problem which generalizes the allocation of indivisible goods. It is to find a solution (base) in a matroid which is common to n agents. Our results are constructive, they are achieved by analyzing an extension of the algorithm of Markakis and Psomas.", "title": "A Matroid Approach to the Worst Case Allocation of Indivisible Goods"}, {"url": "https://www.ijcai.org/Abstract/13/031", "abstract": "The paper generalizes abstract argument games to cope with cases where proponent and opponent argue in front of an audience whose type is known only with uncertainty. The generalization, which makes use of basic tools from probability theory, is motivated by several examples and delivers a class of abstract argument games whose adequacy is proven robust against uncertainty.", "title": "Audience-Based Uncertainty in Abstract Argument Games"}, {"url": "https://www.ijcai.org/Abstract/13/032", "abstract": "Airline ticket purchase timing is a strategic problem that requires both historical data and domain knowledge to solve consistently. \u00a0Even with some historical information (often a feature of modern travel reservation web sites), it is difficult for consumers to make true cost-minimizing decisions. \u00a0To address this problem, we introduce an automated agent which is able to optimize purchase timing on behalf of customers and provide performance estimates of its computed action policy based on past performance. \u00a0We apply machine learning to recent ticket price quotes from many competing airlines for the target flight route. \u00a0Our novelty lies in extending this using a systematic feature extraction technique incorporating elementary user-provided domain knowledge that greatly enhances the performance of machine learning algorithms. \u00a0Using this technique, our agent achieves much closer to the optimal purchase policy than other proposed decision theoretic approaches for this domain.", "title": "Optimal Airline Ticket Purchasing Using Automated User-Guided Feature Selection"}, {"url": "https://www.ijcai.org/Abstract/13/033", "abstract": "We study probabilistic single-item second-price auctions where the item is characterized by a set of attributes. The auctioneer knows the actual instantiation of all the attributes, but he may choose to reveal only a subset of these attributes to the bidders. Our model is an abstraction of the following Ad auction scenario. The website (auctioneer) knows the demographic information of its impressions, and this information is in terms of a list of attributes (e.g., age, gender, country of location). The website may hide certain attributes from its advertisers (bidders) in order to create thicker market, which may lead to higher revenue. We study how to hide attributes in an optimal way. We show that it is NP-hard to compute the optimal attribute hiding scheme. We then derive a polynomial-time solvable upper bound on the optimal revenue. Finally, we propose two heuristic-based attribute hiding schemes. Experiments show that revenue achieved by these schemes is close to the upper bound.", "title": "Revenue Maximization via Hiding Item Attributes"}, {"url": "https://www.ijcai.org/Abstract/13/034", "abstract": "A strategy is used by a participant in a persuasion dialogue to select locutions most likely to achieve its objective of persuading its opponent. Such strategies often assume that the participant has a model of its opponents, which may be constructed on the basis of a participant's accumulated dialogue experience. However in most cases the fact that an agent's experience may encode additional information which if appropriately used could increase a strategy's efficiency, is neglected. In this work, we rely on an agent's experience to define a mechanism for augmenting an opponent model with information likely to be dialectally related to information already contained in it. Precise computation of this likelihood is exponential in the volume of related information. We thus describe and evaluate an approximate approach for computing these likelihoods based on Monte-Carlo simulation.", "title": "Opponent Modelling in Persuasion Dialogues"}, {"url": "https://www.ijcai.org/Abstract/13/035", "abstract": "We examine sequential equilibrium in the context of computational games [Halpern and Pass, 2011a], where agents are charged forcomputation. In such games, an agent can rationally choose to forget, so issues of imperfect recall arise. In this setting, we consider two notions of sequential equilibrium. One is an ex ante notion, where a player chooses his strategy before the game starts and is committed to it, but chooses it in such a way that it remains optimal even off the equilibrium path.The second is an interim notion, where a player can reconsider at each information set whether he is doing the \"right\" thing, and if not, can change his strategy. The two notions agree in games of perfect recall, but not in games of imperfect recall. Although the interim notion seems more appealing, in [Halpern and Pass,2011b] it is argued that there are some deep conceptual problems with it in standard games of imperfect recall. We show that the conceptual problems largely disappear in the computational setting. Moreover, in this setting, under natural assumptions, the two notions coincide.", "title": "Sequential Equilibrium in Computational Games"}, {"url": "https://www.ijcai.org/Abstract/13/036", "abstract": "When making a mistake, individuals can apologize to secure further cooperation, even if the apology is  costly. Similarly, individuals arrange commitments to guarantee that  an action such as a cooperative one is in the others' best interest, and thus will be carried out to avoid eventual penalties for commitment failure. Hence, both apology and commitment should go side by side in behavioral evolution. Here we provide a computational model showing that apologizing acts are rare in non-committed interactions,  especially whenever cooperation is very costly, and that arranging prior commitments can considerably increase the frequency of such behavior. In addition, we show that in both cases, with or without commitments, apology works only if it is sincere, i.e. costly enough. Most interestingly, our model predicts that individuals tend to use much costlier  apology in committed relationships than otherwise, because it helps better identify free-riders such as fake committers: \"commitments bring about  sincerity.\" Furthermore, we show that this strategy of apology supported by commitments outperforms  the famous existent strategies of the iterated Prisoner's Dilemma.", "title": "Why Is It So Hard to Say Sorry? Evolution of Apology with Commitments in the Iterated Prisoner\u2019s Dilemma"}, {"url": "https://www.ijcai.org/Abstract/13/037", "abstract": "Coordination in cooperative multiagent systems is an important problem in multiagent learning literature. In practical complex environments, the interactions between agents can be sparse, and each agent's interacting partners may change frequently and randomly. To this end, we investigate the multiagent coordination problems in cooperative environments under the social learning framework. We consider a large population of agents where each agent interacts with another agent randomly chosen from the population in each round. Each agent learns its policy through repeated interactions with the rest of agents via social learning. It is not clear a priori if all agents can learn a consistent optimal coordination policy in such a situation. We distinguish two types of learners: individual action learner and joint action learner. The learning performance of both learners are evaluated under a number of challenging cooperative games, and the influence of the information sharing degree on the learning performance is investigated as well.", "title": "The Dynamics of Reinforcement Social Learning in Cooperative Multiagent Systems"}, {"url": "https://www.ijcai.org/Abstract/13/038", "abstract": "Reputation is a crucial concept in dynamic multiagent environments.  Despite the large body of work on reputation systems, no metrics exist to directly and quantitatively evaluate and compare them.  We present a common conceptual interface for reputation systems and a set of four measurable desiderata that are broadly applicable across multiple domains.  These desiderata employ concepts from dynamical systems theory to measure how a reputation system reacts to a strategic agent attempting to maximize its own utility.  We study a diverse set of well-known reputation models from the literature in a moral hazard setting and identify a rich variety of characteristics that they support.", "title": "Macau: A Basis for Evaluating Reputation Systems"}, {"url": "https://www.ijcai.org/Abstract/13/039", "abstract": "Persuasion is a common social and economic activity. It usually arises when conflicting interests among agents exist, and one of the agents wishes to sway the opinions of others. This paper considers the problem of an automated agent that needs to influence the decision of a group of self-interested agents that must reach an agreement on a joint action. For example, consider an automated agent that aims to reduce the energy consumption of a nonresidential building, by convincing a group of people who share an office to agree on an economy mode of the air-conditioning and low light intensity. In this paper we present four problems that address issues of minimality and safety of the persuasion process. We discuss the relationships to similar problems from social choice, and show that if the agents are using Plurality or Veto as their voting rule all of our problems are in P. We also show that with k-Approval, Bucklin and Borda voting rules some problems become intractable. We thus present heuristics for efficient persuasion with Borda, and evaluate them through simulations.", "title": "How to Change a Group\u2019s Collective Decision?"}, {"url": "https://www.ijcai.org/Abstract/13/040", "abstract": "Sponsored search is an important monetization channel for search engines, in which an auction mechanism is used to select the ads shown to users and determine the prices charged from advertisers. There have been several pieces of work in the literature that investigate how to design an auction mechanism in order to optimize the revenue of the search engine. However, due to some unrealistic assumptions used, the practical values of these studies are not very clear. In this paper, we propose a novel game-theoretic machine learning} approach, which naturally combines machine learning and game theory, and learns the auction mechanism using a bilevel optimization framework. In particular, we first learn a Markov model from historical data to describe how advertisers change their bids in response to an auction mechanism, and then for any given auction mechanism, we use the learnt model to predict its corresponding future bid sequences. Next we learn the auction mechanism through empirical revenue maximization on the predicted bid sequences. We show that the empirical revenue will converge when the prediction period approaches infinity, and a Genetic Programming algorithm can effectively optimize this empirical revenue. Our experiments indicate that the proposed approach is able to produce a much more effective auction mechanism than several baselines.", "title": "A Game-Theoretic Machine Learning Approach for Revenue Maximization in Sponsored Search"}, {"url": "https://www.ijcai.org/Abstract/13/041", "abstract": "Many trust models have been proposed to evaluate seller trustworthiness  in multiagent e-marketplaces. Their performance varies highly depending  on environments where they are applied. However, it is challenging to  choose suitable models for environments where ground truth about seller  trustworthiness is unknown (called unknown environments). We  propose a novel framework to choose suitable trust models for unknown  environments, based on the intuition that if a model performs well in  one environment, it will do so in another similar environment.  Specifically, for an unknown environment, we identify a similar  simulated environment (with known ground truth) where the trust model  performing the best will be chosen as the suitable solution. Evaluation  results confirm the effectiveness of our framework in choosing suitable  trust models for different environments.", "title": "A Framework to Choose Trust Models for Different E-Marketplace Environments"}, {"url": "https://www.ijcai.org/Abstract/13/042", "abstract": "We study security games with multiple defenders. To achieve maximum security, defenders must perfectly synchronize their randomized allocations of resources. However, in real-life scenarios (such as protection of the port of Boston) this is not the case. Our goal is to quantify the loss incurred by miscoordination between defenders, both theoretically and empirically. We introduce two notions that capture this loss under different assumptions: the price of miscoordination, and the price of sequential commitment. Generally speaking, our theoretical bounds indicate that the loss may be extremely high in the worst case, while our simulations establish a smaller yet significant loss in practice.", "title": "Defender (Mis)Coordination in Security Games"}, {"url": "https://www.ijcai.org/Abstract/13/043", "abstract": "We consider a simple sequential allocation procedure for sharing indivisible items between agents in which agents take turns to pick items. Supposing additive utilities and independence between the agents, we show that the expected utility of each agent is computable in polynomial time. Using this result, we prove that the expected utilitarian social welfare is maximized when agents take alternate turns. We also argue that this mechanism remains optimal when agents behave strategically.", "title": "A Social Welfare Optimal Sequential Allocation Procedure"}, {"url": "https://www.ijcai.org/Abstract/13/044", "abstract": "This paper details the development and evaluation of AstonTAC, an energy broker that successfully participated in the 2012 Power Trading Agent Competition (Power TAC). AstonTAC buys electrical energy from the wholesale market and sells it in the retail market. The main focus of the paper is on the broker's bidding strategy in the wholesale market. In particular, it employs Markov Decision Processes (MDP) to purchase energy at low prices in a day-ahead power wholesale market, and keeps energy supply and demand balanced. Moreover, we explain how the agent uses Non-Homogeneous Hidden Markov Model (NHHMM) to forecast energy demand and price. An evaluation and analysis of the 2012 Power TAC finals show that AstonTAC is the only agent that can buy energy at low price in the wholesale market and keep energy imbalance low.", "title": "An Intelligent Broker Agent for Energy Trading: An MDP Approach"}, {"url": "https://www.ijcai.org/Abstract/13/045", "abstract": "All-pay auctions, a common mechanism for various human and agent interactions, suffers, like many other mechanisms, from the possibility of players' failure to participate in the auction. We model such failures and show how they affect the equilibrium state, revealing various properties, such as the lack of influence of the most-likely-to-participate player on the behavior of the other players. We perform this analysis with two scenarios: the sum-profit model, where the auctioneer obtains the sum of all submitted bids, and the max-profit model of crowdsourcing contests, where the auctioneer can only use the best submissions and thus obtains only the winning bid. Furthermore, we examine various methods of influencing the probability of participation such as the effects of misreporting one's own probability of participating, and how influencing another player's participation chances (e.g., sabotage) changes the player's strategy.", "title": "Agent Failures in All-Pay Auctions"}, {"url": "https://www.ijcai.org/Abstract/13/046", "abstract": "Linearly solvable Markov Decision Process (MDP) models are a powerful subclass of problems with a simple structure that allow the policy to be written directly in terms of the uncontrolled (passive) dynamics of the environment and the goals of the agent. However, there have been no learning algorithms for this class of models. In this research, we develop a robust learning approach to linearly solvable MDPs. To exploit the simple solution for general problems, we show how to construct passive dynamics from any transition matrix, use Bayesian updating to estimate the model parameters and apply approximate and efficient Bayesian exploration to speed learning. In addition, we reduce the computational cost of learning using intermittent Bayesian updating and policy solving. We also gave a polynomial theoretical time complexity bound for the convergence of our learning algorithm, and demonstrate a linear bound for the subclass of the reinforcement learning problems with the property that the transition error depends only on the agent itself. Test results for our algorithm in a grid world are presented, comparing our algorithm with the BEB algorithm. The results showed that our algorithm learned more than the BEB algorithm without losing convergence speed, so that the advantage of our algorithm increased as the environment got more complex. We also showed that our algorithm's performance is more stable after convergence. Finally, we show how to apply our approach to the Cellular Telephones problem by defining the passive dynamics.", "title": "Efficient Learning in Linearly Solvable MDP Models"}, {"url": "https://www.ijcai.org/Abstract/13/047", "abstract": "Conventionally, the questions on a test are assumed to be kept secret from test takers until the test. \u00a0However, for tests that are taken on a large scale, particularly asynchronously, this is very hard to achieve. For example, example TOEFL iBT and driver's license test\u00a0questions are easily found online. \u00a0This also appears likely to become an issue for Massive Open Online Courses (MOOCs). In this paper, we take the loss of confidentiality as a fact. \u00a0Even so, not all hope is lost as the test taker can memorize only a limited set of questions' answers, and the tester can randomize which questions appear on the test. \u00a0We model this as a Stackelberg game, where the tester commits to a mixed strategy and the follower responds. \u00a0We provide an exponential-size linear program formulation, prove several NP-hardness results, and give efficient algorithms for special cases.", "title": "Game-Theoretic Question Selection for Tests"}, {"url": "https://www.ijcai.org/Abstract/13/048", "abstract": "Multi-winner social choice considers the problem of selecting a slate\u00a0of K options to realize some social objective. It has found application in the construction of political legislatures and committees, product recommendation, and related problems, and has recently attracted attention from a computational perspective. We address the multi-winner problem when facing incomplete voter preferences, using the notion of minimax regret to determine a\u00a0robust slate of options in the presence of preference uncertainty. We analyze the\u00a0complexity of this problem and develop new exact and greedy robust\u00a0optimization algorithms for its solution. Using these techniques,\u00a0we also develop preference elicitation heuristics which, in practice, allow us to find near-optimal slates with\u00a0considerable savings in the preference information required vis-a-vis complete votes.", "title": "Multi-Winner Social Choice with Incomplete Preferences"}, {"url": "https://www.ijcai.org/Abstract/13/049", "abstract": "Security is a critical concern around the world. Since resources for security are always limited, lots of interest have arisen in using game theory to handle security resource allocation problems. However, most of the existing work does not address adequately how a defender chooses his optimal strategy in a game with absent, inaccurate, uncertain, and even ambiguous strategy profiles' payoffs. To address this issue, we propose a general framework of security games under ambiguities based on Dempster-Shafer theory and the ambiguity aversion principle of minimax regret. Then, we reveal some properties of this framework. Also, we present two methods to reduce the influence of complete ignorance. Our investigation shows that this new framework is better in handling security resource allocation problems under ambiguities.", "title": "An Ambiguity Aversion Framework of Security Game under Ambiguities"}, {"url": "https://www.ijcai.org/Abstract/13/050", "abstract": "Team formation is a critical step in deploying a multi-agent team. In some scenarios, agents coordinate by voting continuously. When forming such teams, should we focus on the diversity of the team or on the strength of each member? Can a team of diverse (and weak) agents outperform a uniform team of strong agents? We propose a new model to address these questions. Our key contributions include: (i) we show that a diverse team can overcome a uniform team and we give the necessary conditions for it to happen; (ii) we present optimal voting rules for a diverse team; (iii) we perform synthetic experiments that demonstrate that both diversity and strength contribute to the performance of a team; (iv) we show experiments that demonstrate the usefulness of our model in one of the most difficult challenges for Artificial Intelligence: Computer Go.", "title": "Multi-Agent Team Formation: Diversity Beats Strength?"}, {"url": "https://www.ijcai.org/Abstract/13/051", "abstract": "Schulze voting is a recently introduced voting system enjoying unusual popularity and a high degree of real-world use, with users including the Wikimedia foundation, several branches of the Pirate Party, and MTV.  It is a Condorcet voting system that determines the winners of an election using information about paths in a graph representation of the election.  We resolve the complexity of many electoral control cases for Schulze voting.  We find that it falls short of the best known voting systems in terms of control resistance, demonstrating vulnerabilities of concern to some prospective users of the system.", "title": "Control Complexity of Schulze Voting"}, {"url": "https://www.ijcai.org/Abstract/13/052", "abstract": "We study a recently developed centrality metric to identify key players in terrorist organisations due to Lindelauf et al. [2011]. This metric, which involves computation of the Shapley value for connectivity games on graphs proposed by Amer and Gimenez [2004], was shown to produce substantially better results than previously used standard centralities. In this paper, we present the first computational analysis of this class of coalitional games, and propose two algorithms for computing Lindelauf et al.'s centrality metric. Our first algorithm is exact, and runs in time linear by number of connected subgraphs in the network. As shown in the numerical simulations, our algorithm identifies key players in the WTC 9/11 terrorist network, constructed of 36 members and 125 links, in less than 40 minutes. In contrast, a general-purpose Shapley value algorithm would require weeks to solve this problem. Our second algorithm is approximate and can be used to study much larger networks.", "title": "Computational Analysis of Connectivity Games with Applications to the Investigation of Terrorist Networks"}, {"url": "https://www.ijcai.org/Abstract/13/053", "abstract": "Optimal decentralized decision making in a team of cooperative agents as formalized by decentralized POMDPs is a notoriously hard problem.  A major obstacle is that the agents do not have access to a sufficient statistic during execution, which means that they need to base their actions on their histories of observations. A consequence is that even during off-line planning the choice of decision rules for different stages is tightly interwoven: decisions of earlier stages affect how to act optimally at later stages, and the optimal value function for a stage is known to have a dependence on the decisions made up to that point. This paper makes a contribution to the theory of decentralized POMDPs by showing how this dependence on the 'past joint policy' can be replaced by a sufficient statistic. These results are extended to the case of k-step delayed communication. The paper investigates the practical implications, as well as the effectiveness of a new pruning technique for MAA* methods, in a number of benchmark problems and discusses future avenues of research opened by these contributions.", "title": "Sufficient Plan-Time Statistics for Decentralized POMDPs"}, {"url": "https://www.ijcai.org/Abstract/13/054", "abstract": "Top-k voting is an especially natural form of partial vote elicitation in which only length k prefixes of rankings are elicited. We analyze the ability of top-k vote elicitation to correctly determine true winners, with high probability, given probabilistic models of voter preferences and candidate availability. We provide bounds on the minimal value of k required to determine the correct winner under the plurality and Borda voting rules, considering both worst-case preference profiles and profiles drawn from the impartial culture and Mallows probabilistic models. We also derive conditions under which the special case of zero-elicitation (i.e., k = 0) produces the correct winner. We provide empirical results that confirm the value of top-k voting.", "title": "Efficient Vote Elicitation under Candidate Uncertainty"}, {"url": "https://www.ijcai.org/Abstract/13/055", "abstract": "The impossibility results in judgement aggregation show a clash between fair aggregation procedures and rational collective outcomes. In this paper, we are interested in analysing the notion of rational outcome by proposing a proof-theoretical understanding of collective rationality. In particular, we use the analysis of proofs and inferences provided by linear logic in order to define a fine-grained notion of group reasoning that allows for studying collective rationality with respect to a number of logics. We analyse the well-known paradoxes in judgement aggregation and we pinpoint the reasoning steps that trigger the inconsistencies. Moreover, we extend the map of possibility and impossibility results in judgement aggregation by discussing the case of substructural logics. In particular, we show that there exist fragments of linear logic for which general possibility results can be obtained.", "title": "A Proof-Theoretical View of Collective Rationality"}, {"url": "https://www.ijcai.org/Abstract/13/056", "abstract": "We introduce a new representation scheme for coalitional games, called coalition-flow networks (CF-NETs), where the formation of effective coalitions in a task-based setting is reduced to the problem of directing flow through a network. We show that our representation is intuitive, fully expressive, and captures certain patterns in a significantly more concise manner compared to the conventional approach. Furthermore, our representation has the flexibility to express various classes of games, such as characteristic function games, coalitional games with overlapping coalitions, and coalitional games with agent types. As such, to the best of our knowledge, CF-NETs is the first representation that allows for switching conveniently and efficiently between overlapping/non-overlapping coalitions, with/without agent types. We demonstrate the efficiency of our scheme on the coalition structure generation problem, where near-optimal solutions for large instances can be found in a matter of seconds.", "title": "Coalitional Games via Network Flows"}, {"url": "https://www.ijcai.org/Abstract/13/057", "abstract": "This paper deals with the issue of strategic argumentation in the setting of Dung-style abstract argumentation theory. Such reasoning takes place through the use of opponent models\u2014recursive representations of an agent's knowledge and beliefs regarding the opponent's knowledge. Using such models, we present three approaches to reasoning. The first directly utilises the opponent model to identify the best move to advance in a dialogue. The second extends our basic approach through the use of quantitative uncertainty over the opponent's model. The final extension introduces virtual arguments into the opponent's reasoning process. Such arguments are unknown to the agent, but presumed to exist and interact with known arguments. They are therefore used to add a primitive notion of risk to the agent's reasoning. We have implemented our models and we have performed an empirical analysis that shows that this added expressivity improves the performance of an agent in a dialogue.", "title": "Opponent Models with Uncertainty for Strategic Argumentation"}, {"url": "https://www.ijcai.org/Abstract/13/058", "abstract": "We study the problem of designing efficient auctions where bidders have interdependent values; i.e., values that depend on the signals of other agents. We consider a contingent bid model in which agents can explicitly condition the value of their bids on the bids submitted by others. In particular, we adopt a linear contingent bidding model for single minded combinatorial auctions (CAs), in which submitted bids are linear combinations of bids received from others. We extend the existing state of the art, by identifying constraints on the interesting bundles and contingency weights reported by the agents which allow the efficient second priced, fixed point bids auction to be implemented in single minded CAs. Moreover, for domains in which the required single crossing condition fails (which characterizes when efficient, IC auctions are possible), we design a two-stage mechanism in which a subset of agents (''experts\") are allocated first, using their reports to allocate the remaining items to the other agents.", "title": "Efficient Interdependent Value Combinatorial Auctions with Single Minded Bidders"}, {"url": "https://www.ijcai.org/Abstract/13/059", "abstract": "Despite recent successful real-world deployments of Stackelberg Security Games (SSGs), scale-up remains a fundamental challenge in this field. The latest techniques do not scale-up to domains where multiple defenders must coordinate time-dependent joint activities. To address this challenge, this paper presents two branch-and-price algorithms for solving SSGs, SMARTO and SMARTH, with three novel features: (i) a column-generation approach that uses an ordered network of nodes (determined by solving the traveling salesman problem) to generate individual defender strategies; (ii) exploitation of iterative reward shaping of multiple coordinating defender units to generate coordinated strategies; (iii) generation of tighter upper-bounds for pruning by solving security games that only abide by key scheduling constraints. We provide extensive experimental results and formal analyses.", "title": "Efficiently Solving Joint Activity Based Security Games"}, {"url": "https://www.ijcai.org/Abstract/13/060", "abstract": "We study the complexity of (approximate) winner determination under Monroe's and Chamberlin-Courant's multiwinner voting rules, where we focus on the total (dis)satisfaction of the voters (the utilitarian case) or the (dis)satisfaction of the worst-off voter (the egalitarian case). We show good approximation algorithms for the satisfaction-based utilitarian cases, and inapproximability results for the remaining settings.", "title": "Fully Proportional Representation as Resource Allocation: Approximability Results"}, {"url": "https://www.ijcai.org/Abstract/13/061", "abstract": "We present a bimodal method for online planning in partially observable multiagent settings as formalized by a finitely-nested interactive partially observable Markov decision process (I-POMDP). An agent planning in an environment shared with another updates beliefs both over the physical state and the other agents' models. In problems where we do not observe other's action explicitly but must infer it from sensing its effect on the state, observations are more informative about the other when the belief over the state space has reduced uncertainty. For typical, uncertain initial beliefs, we model the agent as if it were acting alone and utilize fast online planning for POMDPs. Subsequently, the agent switches to online planning in multiagent settings. We maintain tight lower and upper bounds at each step, and switch over when the difference between them reduces to less than \u03b5.", "title": "Bimodal Switching for Online Planning in Multiagent Settings"}, {"url": "https://www.ijcai.org/Abstract/13/062", "abstract": "We consider the mechanism design problem for agents with single-peaked preferences over multi-dimensional domains when multiple alternatives can be chosen. Facility location and committee selection are classic embodiments of this problem. We propose a class of percentile mechanisms, a form of generalized median mechanisms, that are strategy-proof, and derive worst-case approximation ratios for social cost and maximum load for L1 and L2 cost models. More importantly, we propose a sample-based framework for optimizing the choice of percentiles relative to any prior distribution over preferences, while maintaining strategy-proofness. Our empirical investigations, using social cost and maximum load as objectives, demonstrate the viability of this approach and the value of such optimized mechanisms vis-a-vis mechanisms derived through worst-case analysis.", "title": "Analysis and Optimization of Multi-Dimensional Percentile Mechanisms"}, {"url": "https://www.ijcai.org/Abstract/13/063", "abstract": "Single-peakedness is one of the most commonly used domain restrictions in social choice. However, the extent to which agent preferences are single-peaked in practice, and the extent to which recent proposals for approximate single-peakedness can further help explain voter preferences, is unclear.  In this article, we assess the ability of both single-dimensional and multi-dimensional approximations to explain preference profiles drawn from several real-world elections. We develop a simple branch-and-bound algorithm that finds multi-dimensional, single-peaked axes that best fit a given profile, and which works with several forms of approximation.  Empirical results on two election data sets show  that preferences in these elections are far from single-peaked in any one-dimensional space, but are nearly single-peaked in two dimensions. Our algorithms are reasonably efficient in practice, and also show excellent anytime performance.", "title": "Multi-Dimensional Single-Peaked Consistency and Its Approximations"}, {"url": "https://www.ijcai.org/Abstract/13/064", "abstract": "We propose a new representation for coalitional games, called the coalitional skill vector model, where there is a set of skills in the system, and each agent has a skill vector \u2014 a vector consisting of values that reflect the agents' level in different skills. Furthermore, there is a set of goals, each with requirements expressed in terms of the minimum skill level necessary to achieve the goal. Agents can form coalitions to aggregate their skills, and achieve goals otherwise unachievable. We show that this representation is fully expressive, that is, it can represent any characteristic function game. We also show that, for some interesting classes of games, our representation is significantly more compact than the classical representation, and facilitates the development of efficient algorithms to solve the coalition structure generation problem, as well as the problem of computing the core and/or the least core. We also demonstrate that by using the coalitional skill vector representation, our solver can handle up to 500 agents.", "title": "An Efficient Vector-Based Representation for Coalitional Games"}, {"url": "https://www.ijcai.org/Abstract/13/065", "abstract": "In Boolean games players exercise control over propositional variables and strive to achieve a goal formula whose realization might require the opponents' cooperation. Recently, a theory of incentive engineering for such games has been devised, where an external authority steers the outcome of the game towards certain desirable properties consistent with players' goals, by imposing a taxation mechanism on the players that makes the outcomes that do not comply with those properties less appealing to them. The present contribution stems from a complementary perspective and studies, instead, how boolean games can be transformed from inside, rather than from outside, by endowing players with the possibility of sacrificing a part of their payoff received at a certain outcome in order to convince other players to play a certain strategy. What we call here endogenous boolean games (EBGs) boils down to enriching the framework of boolean games with the machinery of side payments coming from game theory. We analyze equilibria in EBGs, showing the preconditions needed for desirable outcomes to be achieved without external intervention. Finally, making use of taxation mechanism, we show how to transform an EBG in such a way that desirable outcomes can be realized independently of side payments.", "title": "Endogenous Boolean Games"}, {"url": "https://www.ijcai.org/Abstract/13/066", "abstract": "We address two significant drawbacks of state-of-the-art solvers of decentralized POMDPs (DEC-POMDPs): the reliance on complete knowledge of the model and limited scalability as the complexity of the domain grows.  We extend a recently proposed approach for solving DEC-POMDPs via a reduction to the maximum likelihood problem, which in turn can be solved using EM.  We introduce a model-free version of this approach that employs Monte-Carlo EM(MCEM).  While a naive implementation of MCEM is inadequate in multi-agent settings, we introduce several improvements in sampling that produce high-quality results on a variety of DEC-POMDP benchmarks, including large problems with thousands of agents.", "title": "Monte-Carlo Expectation Maximization for Dec-POMDPs"}, {"url": "https://www.ijcai.org/Abstract/13/067", "abstract": "To improve the current real-world deployments of Stackelberg security games (SSGs), it is critical now to efficiently incorporate models of adversary bounded rationality in large-scale SSGs. Unfortunately, previously proposed branch-and-price approaches fail to scale-up given the non-convexity of such models, as we show with a \u00a0realization called COCOMO. \u00a0Therefore, we next present a novel cutting-plane algorithm called BLADE to scale-up SSGs with complex adversary models, with three key novelties: (i) an efficient scalable separation oracle to generate deep cuts; (ii) a heuristic that uses gradient to further improve the cuts; (iii) techniques for quality-efficiency tradeoff.", "title": "Scaling-Up Security Games with Boundedly Rational Adversaries: A Cutting-Plane Approach"}, {"url": "https://www.ijcai.org/Abstract/13/068", "abstract": "The Decentralized Partially Observable Markov Decision Process (Dec-POMDP) is a powerful model for multi-agent planning under uncertainty, but its applicability is hindered by its high complexity -- solving Dec-POMDPs optimally is NEXP-hard. Recently, Kumar et al. introduced the Value Factorization (VF) framework, which exploits decomposable value functions that can be factored into subfunctions. This framework has been shown to be a generalization of several models that leverage sparse agent interactions such as TI-Dec-MDPs, ND-POMDPs and TD-POMDPs. Existing algorithms for these models assume that the interaction graph of the problem is given. In this paper, we introduce three algorithms to automatically generate interaction graphs for models within the VF framework and establish lower and upper bounds on the expected reward of an optimal joint policy. We illustrate experimentally the benefits of these techniques for sensor placement in a decentralized tracking application.", "title": "Automated Generation of Interaction Graphs for Value-Factored Decentralized POMDPs"}, {"url": "https://www.ijcai.org/Abstract/13/069", "abstract": "Trust is an important mechanism enabling agents to self-police open and dynamic multi-agent systems (ODMASs). Trusters evaluate the reputation of trustees based on their past observed performance, and use this information to guide their future interaction decisions. Existing trust models tend to concentrate trusters' interactions on a small number of highly reputable trustees to minimize risk exposure. When a trustee's servicing capacity is limited, such an approach may cause long delays for trusters and subsequently damage the reputation of trustees. To mitigate this problem, we propose a reputation management approach for trustee agents based on distributed constraint optimization. It helps a trustee to make situation-aware decisions on which incoming requests to serve and prevent the resulting reputation score from being affected by factors out of the trustee's control. The approach is evaluated through theoretical analysis and within a simulated, highly dynamic multi-agent environment. The results show that it can achieve close to optimally efficient utilization of the trustee agents' collective capacity in an ODMAS, promotes fair treatment of trustee agents based on their behavior, and significantly outperforms related work in enhancing social welfare.", "title": "A Reputation Management Approach for Resource Constrained Trustee Agents"}, {"url": "https://www.ijcai.org/Abstract/13/070", "abstract": "We study the complexity of electing a committee under several variants of the Chamberlin\u2013Courant rule when the voters' preferences are single-peaked on a tree. We first show that this problem is easy for the egalitarian, or \"minimax\" version of this problem, for arbitrary trees and misrepresentation functions. For the standard (utilitarian) version of this problem we provide an algorithm for an arbitrary misrepresentation function whose running time is polynomial in the input size as long as the number of leaves of the underlying tree is bounded by a constant. On the other hand, we prove that our problem remains computationally hard on trees that have bounded degree, diameter or path width. Finally, we show how to modify Trick's [1989] algorithm to check whether an election is single-peaked on a tree whose number of leaves does not exceed a given parameter.", "title": "Multiwinner Elections under Preferences that Are Single-Peaked on a Tree"}, {"url": "https://www.ijcai.org/Abstract/13/071", "abstract": "Weighted voting games (WVGs) model decision making bodies such as parliaments and councils. In such settings, it is often important to provide a measure of the influence a player has on the vote. Two highly popular such measures are the Shapley-Shubik power index, and the Banzhaf power index. Given a power measure, proportional representation is the property of having players' voting power proportional to the number of parliament seats they receive. Approximate proportional representation (w.r.t. the Banzhaf power index) can be ensured by changing the number of parliament seats each party receives; this is known as Penrose's square root method. However, a discrepancy between player weights and parliament seats is often undesirable or unfeasible; a simpler way of achieving approximate proportional representation is by changing the quota, i.e. the number of votes required in order to pass a bill.   It is known that a player's Shapley-Shubik power index is proportional to his weight when one chooses a quota at random; that is, when taking a random quota, proportional representation holds in expectation. In our work, we show that not only does proportional representation hold in expectation, it also holds for many quotas. We do so by providing bounds on the variance of the Shapley value when the quota is chosen at random, assuming certain weight distributions. We further explore the case where weights are sampled from i.i.d. binomial distributions; for this case, we show good bounds on an important parameter governing the behavior of the variance, as well as substantiating our claims with empirical analysis.", "title": "On Random Quotas and Proportional Representation in Weighted Voting Games"}, {"url": "https://www.ijcai.org/Abstract/13/073", "abstract": "Motivated by considerations in quantum mechanics, we introduce the class of robust constraint satisfaction problems in which the question is whether every partial assignment of a certain length can be extended to a solution, provided the partial assignment does not violate any of the constraints of the given instance. We explore the  complexity of specific robust colorability and robust satisfiability problems, and show that they are NP-complete. We then use these results to establish the computational intractability of detecting local hidden-variable models in quantum mechanics.", "title": "Robust Constraint Satisfaction and Local Hidden Variables in Quantum Mechanics"}, {"url": "https://www.ijcai.org/Abstract/13/074", "abstract": "Since the first principles of Knowledge Compilation (KC), most of the work has been focused in finding a good compilation target language in terms of compromises between compactness and expressiveness. The central idea remained unchanged in the last fifteen years: an off-line, very hard, stage, allows to \"compile\" the initial theory in order to guarantee (theoretically) an efficient on-line stage, on a set of predefined queries and operations. \u00a0We propose a new \"Just-in-Time\" approach for KC. Here, any Knowledge Base (KB) will be immediately available for queries, and the effort spent on past queries will be partly amortized for future ones. \u00a0To guarantee efficient answers, we rely on the tremendous progresses made in the practical solving of SAT and incremental SAT applicative problems. Even if each query may be theoretically hard, we \u00a0show that our approach outperforms previous KC approaches on the set of classical problems used in the field, and allows to handle problems that are out of the scope of current approaches.", "title": "Just-in-Time Compilation of Knowledge Bases"}, {"url": "https://www.ijcai.org/Abstract/13/075", "abstract": "Constraint programming techniques are widely used to model and solve interactive decision problems, and especially configuration problems. In this type of application, the configurable product is described by means of a set of constraints bearing on the configuration variables. The user interactively solves the CSP by assigning the variables according to her preferences. The system then has to keep the domains of the other variables consistent with these choices. Since maintaining the global inverse consistency of the domains is not tractable, the domains are instead filtered according to some level of local consistency, e.g. arc-consistency. The present paper aims at offering a more convenient interaction by providing the user with possible alternative values for the already assigned variables, i.e. values that could replace the current ones without leading to a constraint violation. We thus present the new concept of alternative domains in a (possibly) partially assigned CSP. We propose a propagation algorithm that computes all the alternative domains in a single step. Its worst case complexity is comparable to the one of the naive algorithm that would run a full propagation for each variable, but its experimental efficiency is better", "title": "Maintaining Alternative Values in Constraint-Based Configuration"}, {"url": "https://www.ijcai.org/Abstract/13/076", "abstract": "In this paper, we propose the first heuristic approach for the vertex separator problem (VSP), based on Breakout Local Search (BLS). BLS is a recent meta-heuristic that follows the general framework of the popular Iterated Local Search (ILS) with a particular focus\u00a0 on the perturbation strategy. Based on some relevant information on search history, it tries to introduce the most suitable degree of diversification by determining adaptively the number and type of moves for the next perturbation phase. The proposed heuristic is highly competitive with the exact state-of-art approaches from the literature on the current VSP benchmark. Moreover, we present for the first time computational results for a set of large graphs with up to 3000 vertices, which constitutes a new challenging benchmark for VSP approaches.", "title": "Breakout Local Search for the Vertex Separator Problem"}, {"url": "https://www.ijcai.org/Abstract/13/077", "abstract": "Constraint satisfaction problems may be nearly tractable. For instance, most of the relations in a problem might belong to a tractable language. We introduce a method to take advantage of this fact by computing a backdoor to this tractable language. The method can be applied to many tractable classes for which the membership test is itself tractable. We introduce therefore two polynomial membership testing algorithms, to check if a language is closed under a majority or conservative Mal'tsev polymorphism, respectively. Then we show that computing a minimal backdoor for such classes is fixed parameter tractable (FPT) if the tractable subset of relations is given, and W[2]-complete otherwise. Finally, we report experimental results on the XCSP benchmark set. We identified a few promising problem classes where problems were nearly closed under a majority polymorphism and small backdoors could be computed.", "title": "Detecting and Exploiting Subproblem Tractability"}, {"url": "https://www.ijcai.org/Abstract/13/078", "abstract": "We learn constraint networks by asking the user partial queries. That is, we ask the user to classify assignments to subsets of the variables as positive or negative. We provide an algorithm that, given a negative example, focuses onto a constraint of the target network in a number of queries logarithmic in the size of the example. We give information theoretic lower bounds for learning some simple classes of constraint networks and show that our generic algorithm is optimal in some cases. Finally we evaluate our algorithm on some benchmarks.", "title": "Constraint Acquisition via Partial Queries"}, {"url": "https://www.ijcai.org/Abstract/13/079", "abstract": "Determining the complexity of perfect information trick-taking card games is a long standing open problem. This question is worth addressing not only because of the popularity of these games among human players, e.g., Double Dummy Bridge, but also because of its practical importance as a building block in state-of-the-art playing engines for Contract Bridge, Skat, Hearts, and Spades. We define a general class of perfect information two-player trick-taking card games dealing with arbitrary numbers of hands, suits, and suit lengths. We investigate the complexity of determining the winner in various fragments of this game class. Our main result is a proof of PSPACE-completeness for a fragment with bounded number of hands, through a reduction from Generalized Geography. Combining our results with W\u00e4stlund's tractability results gives further insight in the complexity landscape of trick-taking card games.", "title": "On the Complexity of Trick-Taking Card Games"}, {"url": "https://www.ijcai.org/Abstract/13/080", "abstract": "It is widely acknowledged that stochastic local search (SLS) algorithms can efficiently find models of satisfiable formulae for the Boolean Satisfiability (SAT) problem. There has been much interest in studying SLS algorithms on random k-SAT instances. Compared to random 3-SAT instances which have special statistical properties rendering them easy to solve, random k-SAT instances with long clauses are similar to structured ones and remain very difficult. This paper is devoted to efficient SLS algorithms for random k-SAT instances with long clauses. By combining a novel variable property subscore with the commonly used property score, we design a scoring function named comprehensive score, which is utilized to develop a new SLS algorithm called CScoreSAT. The experiments show that CScoreSAT outperforms state-of-the-art SLS solvers, including the winners of recent SAT competitions, by one to two orders of magnitudes on large random 5-SAT and 7-SAT instances. In addition, CScoreSAT significantly outperforms its competitors on random k-SAT instances for each k = 4, 5, 6, 7 from SAT Challenge 2012, which indicates its robustness.", "title": "Comprehensive Score: Towards Efficient Local Search for SAT with Long Clauses"}, {"url": "https://www.ijcai.org/Abstract/13/082", "abstract": "We investigate the computational complexity of two global constraints, CUMULATIVE and INTERDISTANCE. These are key constraints in modeling and solving scheduling problems. Enforcing domain consistency on both is NP-hard. However, restricted versions of these constraints are often sufficient in practice. Some examples include scheduling problems with a large number of similar tasks, or tasks sparsely distributed over time. Another example is runway sequencing problems in air-traffic control, where landing periods have a regular pattern. Such cases can be characterized in terms of structural restrictions on the constraints. We identify a number of such structural restrictions and investigate how they impact the computational complexity of propagating these global constraints. In particular, we prove that such restrictions often make propagation tractable.", "title": "A Tree-Based Tabu Search Algorithm for the Manpower Allocation Problem with Time Windows and Job-Teaming Constraints"}, {"url": "https://www.ijcai.org/Abstract/13/082", "abstract": "We investigate the computational complexity of two global constraints, CUMULATIVE and INTERDISTANCE. These are key constraints in modeling and solving scheduling problems. Enforcing domain consistency on both is NP-hard. However, restricted versions of these constraints are often sufficient in practice. Some examples include scheduling problems with a large number of similar tasks, or tasks sparsely distributed over time. Another example is runway sequencing problems in air-traffic control, where landing periods have a regular pattern. Such cases can be characterized in terms of structural restrictions on the constraints. We identify a number of such structural restrictions and investigate how they impact the computational complexity of propagating these global constraints. In particular, we prove that such restrictions often make propagation tractable.", "title": "On the Complexity of Global Scheduling Constraints under Structural Restrictions"}, {"url": "https://www.ijcai.org/Abstract/13/083", "abstract": "There are many complex combinatorial problems which involve searching for an undirected graph satisfying a certain property. These problems are often highly challenging because of the large number of isomorphic representations of a possible solution. In this paper we introduce novel, effective and compact, symmetry breaking constraints for undirected graph search. While incomplete, these prove highly beneficial in pruning the search for a graph. We illustrate the application of symmetry breaking in graph representation to resolve several open instances in extremal graph theory.", "title": "Breaking Symmetries in Graph Representation"}, {"url": "https://www.ijcai.org/Abstract/13/084", "abstract": "A variable elimination rule allows the polynomial-time identification of certain variables whose elimination does not affect the satisfiability of an instance. Variable elimination in the constraint satisfaction problem (CSP) can be used in preprocessing or during search to reduce search space size. We show that there are essentially just four variable elimination rules defined by forbidding generic sub-instances, known as irreducible patterns, in arc-consistent CSP instances. One of these rules is the Broken Triangle Property, whereas the other three are novel.", "title": "Variable Elimination in Binary CSP via Forbidden Patterns"}, {"url": "https://www.ijcai.org/Abstract/13/085", "abstract": "Intensification and diversification are the key factors that control the performance of stochastic local search in satisfiability (SAT). Recently, Novelty Walk has become a popular method for improving diversification of the search and so has been integrated in many well-known SAT solvers such as TNM and gNovelty+. In this paper, we introduce new heuristics to improve the effectiveness of Novelty Walk in terms of reducing search stagnation. In particular, we use weights (based on statistical information collected during the search) to focus the diversification phase onto specific areas of interest. With a given probability, we select the most frequently unsatisfied clause instead of a totally random one as Novelty Walk does. Amongst all the variables appearing in the selected clause, we then select the least flipped variable for the next move. Our experimental results show that the new weight-enhanced diversification method significantly improves the performance of gNovelty$^+$ and thus outperforms other local search SAT solvers on a wide range of structured and random satisfiability benchmarks.", "title": "Weight-Enhanced Diversification in Stochastic Local Search for Satisfiability"}, {"url": "https://www.ijcai.org/Abstract/13/086", "abstract": "Abduction has been extensively studied in propositional logic because of its many applications in artificial intelligence. However, its intrinsic complexity has been a limitation to the implementation of abductive reasoning tools in more expressive logics. We have devised such a tool in ground flat equational logic, in which literals are equations or disequations between constants. Our tool is based on the computation of prime implicates. It uses a relaxed paramodulation calculus, designed to generate all prime implicates of a formula, together with a carefully defined data structure storing the implicates and able to efficiently detect, and remove, redundancies. In addition to a detailed description of this method, we present an analysis of some experimental results.", "title": "An Approach to Abductive Reasoning in Equational Logic"}, {"url": "https://www.ijcai.org/Abstract/13/087", "abstract": "Multiobjective Dynamic Programming (MODP) is a general problem solving method used to determine the set of Pareto-optimal solutions in optimization problems involving discrete decision variables and multiple objectives. It applies to combinatorial problems in which Pareto-optimality of a solution extends to all its sub-solutions (Bellman principle). In this paper we focus on the determination of the preferred tradeoffs in the Pareto set where preference is measured by a Choquet integral. This model provides high descriptive possibilities but the associated preferences generally do not meet the Bellman principle, thus preventing any straightforward adaptation of MODP. To overcome this difficulty, we introduce here a general family of dominance rules enabling an early pruning of some Pareto-optimal sub-solutions that cannot lead to a Choquet optimum. Within this family, we identify the most efficient dominance rules and show how they can be incorporated into a MODP algorithm. Then we report numerical tests showing the actual efficiency of this approach to find Choquet-optimal tradeoffs in multiobjective knapsack problems.", "title": "Dominance Rules for the Choquet Integral in Multiobjective Dynamic Programming"}, {"url": "https://www.ijcai.org/Abstract/13/089", "abstract": "This paper is about transforming constraint networks to accommodate additional constraints in specific ways. The focus is on two intertwined issues. First, we investigate how partial solutions to an initial network can be preserved from the potential impact of additional constraints. Second, we study how more permissive constraints, which are intended to enlarge the set of solutions, can be accommodated in a constraint network. These two problems are studied in the general case and the light is shed on their relationship. A case study is then investigated where a more permissive additional constraint is taken into account through a form of network relaxation, while some previous partial solutions are preserved at the same time.", "title": "Constraint Satisfaction and Fair Multi-Objective Optimization Problems: Foundations, Complexity, and Islands of Tractability"}, {"url": "https://www.ijcai.org/Abstract/13/089", "abstract": "This paper is about transforming constraint networks to accommodate additional constraints in specific ways. The focus is on two intertwined issues. First, we investigate how partial solutions to an initial network can be preserved from the potential impact of additional constraints. Second, we study how more permissive constraints, which are intended to enlarge the set of solutions, can be accommodated in a constraint network. These two problems are studied in the general case and the light is shed on their relationship. A case study is then investigated where a more permissive additional constraint is taken into account through a form of network relaxation, while some previous partial solutions are preserved at the same time.", "title": "Preserving Partial Solutions while Relaxing Constraint Networks"}, {"url": "https://www.ijcai.org/Abstract/13/092", "abstract": "Constraint propagation is one of the key techniques in constraint programming, and a large body of work has built up around it. Special-purpose constraint propagation algorithms frequently make implicit use of short supports \u2014 by examining a subset of the variables, they can infer support (a justification that a variable-value pair still forms part of a solution to the constraint) for all other variables and values and save substantial work. Recently short supports have been used in general purpose propagators, and (when the constraint is amenable to short supports) speed ups of more than three orders of magnitude have been demonstrated. In this paper we present ShortSTR2, a development of the Simple Tabular Reduction algorithm STR2+. We show that ShortSTR2 is complementary to the existing algorithms ShortGAC and HaggisGAC that exploit short supports, while being much simpler. When a constraint is amenable to short supports, the short support set can be exponentially smaller than the full-length support set. Therefore ShortSTR2 can efficiently propagate many constraints that STR2+ cannot even load into memory. We also show that ShortSTR2 can be combined with a simple algorithm to identify short supports from full-length supports, to provide a superior drop-in replacement for STR2+.", "title": "Sufficiency-Based Selection Strategy for MCTS"}, {"url": "https://www.ijcai.org/Abstract/13/091", "abstract": "This paper presents a new DCOP algorithm calledDeQED (Decomposition with Quadratic Encoding to Decentralize). DeQED is based on the Divide-and-Coordinate (DaC) framework, where the agents repeatedly solve their updated local subproblems (the divide stage) and exchange coordination information that causes them to update their local sub-problems (the coordinate stage). Unlike other DaC-based DCOP algorithms, DeQED does not essentially increase the complexity of local subproblems and allows agents to avoid exchanging (primal) variable values in the coordinate stage. Our experimental results show that DeQED significantly outperformed other incomplete DCOP algorithms for both random and structured instances.", "title": "DeQED: An Efficient Divide-and-Coordinate Algorithm for DCOP"}, {"url": "https://www.ijcai.org/Abstract/13/092", "abstract": "Constraint propagation is one of the key techniques in constraint programming, and a large body of work has built up around it. Special-purpose constraint propagation algorithms frequently make implicit use of short supports \u2014 by examining a subset of the variables, they can infer support (a justification that a variable-value pair still forms part of a solution to the constraint) for all other variables and values and save substantial work. Recently short supports have been used in general purpose propagators, and (when the constraint is amenable to short supports) speed ups of more than three orders of magnitude have been demonstrated. In this paper we present ShortSTR2, a development of the Simple Tabular Reduction algorithm STR2+. We show that ShortSTR2 is complementary to the existing algorithms ShortGAC and HaggisGAC that exploit short supports, while being much simpler. When a constraint is amenable to short supports, the short support set can be exponentially smaller than the full-length support set. Therefore ShortSTR2 can efficiently propagate many constraints that STR2+ cannot even load into memory. We also show that ShortSTR2 can be combined with a simple algorithm to identify short supports from full-length supports, to provide a superior drop-in replacement for STR2+.", "title": "Extending Simple Tabular Reduction with Short Supports"}, {"url": "https://www.ijcai.org/Abstract/13/093", "abstract": "This paper introduces Monte Carlo *-Minimax Search (MCMS), a Monte Carlo search algorithm for turned-based, stochastic, two-player, zero-sum games of perfect information. The algorithm is designed for the class of of densely stochastic games; that is, games where one would rarely expect to sample the same successor state multiple times at any particular chance node. Our approach combines sparse sampling techniques from MDP planning with classic pruning techniques developed for adversarial expectimax planning. We compare and contrast our algorithm to the traditional *-Minimax approaches, as well as MCTS enhanced with the Double Progressive Widening, on four games: Pig, EinStein Wurfelt Nicht!, Can't Stop, and Ra. Our results show that MCMS can be competitive with enhanced MCTS variants in some domains, while consistently outperforming the equivalent classic approaches given the same amount of thinking time.", "title": "Monte Carlo *-Minimax Search"}, {"url": "https://www.ijcai.org/Abstract/13/094", "abstract": "We present the first polynomial time construction procedure for generating graceful double-wheel graphs. A graph is graceful if its vertices can be labeled with distinct integer values from {0, ..., e}, where e is the number of edges, such that each edge has a unique value corresponding to the absolute difference of its endpoints. Graceful graphs have a range of practical application domains, including in radio astronomy, X-ray crystallography, cryptography, and experimental design. Various families of graphs have been proven to be graceful, while others have only been conjectured to be. In particular, it has been conjectured that so-called double-wheel graphs are graceful. A double-wheel graph consists of two cycles of N nodes connected to a common hub. We prove this conjecture by providing the first construction for graceful double-wheel graphs, for any N > 3, using a framework that combines streamlined constraint reasoning with insights from human computation. We also use this framework to provide a polynomial time construction for diagonally ordered magic squares.", "title": "Double-Wheel Graphs Are Graceful"}, {"url": "https://www.ijcai.org/Abstract/13/095", "abstract": "This paper provides algorithms for predicting the size of the Expanded Search Tree (EST) of Depth-first Branch and Bound algorithms (DFBnB) for optimization tasks. The prediction algorithm is implemented and evaluated in the context of solving combinatorial optimization problems over graphical models such as Bayesian and Markov networks. Our methods extend to DFBnB the approaches provided by Knuth-Chen schemes that were designed and applied for predicting the EST size of backtracking search algorithms. Our empirical results demonstrate good predictions which are superior to competing schemes.", "title": "Predicting the Size of Depth-First Branch and Bound Search Trees"}, {"url": "https://www.ijcai.org/Abstract/13/096", "abstract": "This paper addresses the Target-Value Search (TVS) problem, which is the problem of finding a path between two nodes in a graph whose cost is as close as possible to a given target value, T. This problem has been previously addressed only for directed acyclic graphs. In this work we develop the theory required to solve this problem optimally for any type of graphs. We modify traditional heuristic search algorithms for this setting, and propose a novel bidirectional search algorithm that is specifically suited for TVS. The benefits of this bidirectional search algorithm are discussed both theoretically and experimentally on several domains.", "title": "Target-Value Search Revisited"}, {"url": "https://www.ijcai.org/Abstract/13/097", "abstract": "Different solution approaches for combinatorial problems often exhibit incomparable performance that depends on the concrete problem instance to be solved. Algorithm portfolios aim to combine the strengths of multiple algorithmic approaches by training a classifier that selects or schedules solvers dependent on the given instance. We devise a new classifier that selects solvers based on a cost-sensitive hierarchical clustering model. Experimental results on SAT and MaxSAT show that the new method outperforms the most effective portfolio builders to date.", "title": "Algorithm Portfolios Based on Cost-Sensitive Hierarchical Clustering"}, {"url": "https://www.ijcai.org/Abstract/13/098", "abstract": "A set of constraints that cannot be simultaneously satisfied is over-constrained. Minimal relaxations and minimal explanations for over-constrained problems find many practical uses. For Boolean formulas, minimal relaxations of over-constrained problems are referred to as Minimal Correction Subsets (MCSes). MCSes find many applications, including the enumeration of MUSes. Existing approaches for computing MCSes either use a Maximum Satisfiability (MaxSAT) solver or iterative calls to a Boolean Satisfiability (SAT) solver. This paper shows that existing algorithms for MCS computation can be inefficient, and so inadequate, in certain practical settings. To address this problem, this paper develops a number of novel techniques for improving the performance of existing MCS computation algorithms. More importantly, the paper proposes a novel algorithm for computing MCSes. Both the techniques and the algorithm are evaluated empirically on representative problem instances, and are shown to yield the most efficient and robust solutions for MCS computation.", "title": "On Computing Minimal Correction Subsets"}, {"url": "https://www.ijcai.org/Abstract/13/099", "abstract": "The number partitioning problem seeks to divide a set of n numbers across k distinct subsets so as to minimize the sum of the largest partition. In this work, we develop a new optimal algorithm for multi-way number partitioning. A critical observation motivating our methodology is that a globally optimal k-way partition may be recursively constructed by obtaining suboptimal solutions to subproblems of size k \u2013 1. We introduce a new principle of optimality that provides necessary and sufficient conditions for this construction, and use it to strengthen the relationship between sequential decompositions by enforcing upper and lower bounds on intermediate solutions. We also demonstrate how to further prune unpromising partial assignments by detecting and eliminating dominated solutions. Our approach outperforms the previous state-of-the-art by up to four orders of magnitude, reducing average runtime on the largest benchmarks from several hours to less than a second.", "title": "Search Strategies for Optimal Multi-Way Number Partitioning"}, {"url": "https://www.ijcai.org/Abstract/13/100", "abstract": "The FOCUS constraint expresses the notion that solutions are concentrated. In practice, this constraint suffers from the rigidity of its semantics. To tackle this issue, we propose three generalizations of the FOCUS constraint. We provide for each one a complete filtering algorithm as well as discussing decompositions.", "title": "Three Generalizations of the FOCUS Constraint"}, {"url": "https://www.ijcai.org/Abstract/13/101", "abstract": "Constructing a strong heuristic function is a central problem in heuristic search. A common approach is to combine a number of heuristics by maximizing over the values from each. If a limit is placed on this number, then a subset selection problem arises. We treat this as an optimization problem, and proceed by translating a natural loss function into a submodular and monotonic utility function under which greedy selection is guaranteed to be near-optimal. We then extend this approach with a sampling scheme that retains provable optimality. Our empirical results show large improvements over existing methods, and give new insight into building heuristics for directed domains.", "title": "Subset Selection of Search Heuristics"}, {"url": "https://www.ijcai.org/Abstract/13/102", "abstract": "Graphical models are one of the most prominent frameworks to model complex systems and efficiently query them. Their underlying algebraic properties are captured by a valuation structure that, most usually, is a semiring. Depending on the semiring of choice, we can capture probabilistic models, constraint networks, cost networks, etc. In this paper we address the partitioning problem which occurs in many approximation techniques such as mini-bucket elimination and join-graph propagation algorithms. Roughly speaking, subject to complexity bounds, the algorithm needs to find a partition of a set of factors such that best approximates the whole set. While this problem has been addressed in the past in a particular case, we present here a general description. Furthermore, we also propose a\u00a0 general partitioning scheme. Our proposal is general in the sense that it is\u00a0 presented in terms of a generic semiring with the only additional requirements of a division operation and a refinement of its order. The proposed algorithm\u00a0 instantiates to the particular task of computing the probability of evidence, but also applies directly to other important reasoning tasks. We demonstrate its good empirical behaviour on the problem of computing the most probable explanation.", "title": "Semiring-Based Mini-Bucket Partitioning Schemes"}, {"url": "https://www.ijcai.org/Abstract/13/103", "abstract": "The bin-packing problem is to partition a multiset of n numbers into as few bins of capacity C as possible, such that the sum of the numbers in each bin does not exceed C. We compare two existing algorithms for solving this problem: bin completion (BC) and branch-and-cut-and-price (BCP). We show experimentally that the problem difficulty and dominant algorithm are a function of n, the precision of the input elements and the number of bins in an optimal solution. We describe three improvements to BC which result in a speedup of up to five orders of magnitude as compared to the original BC algorithm. While the current belief is that BCP is the dominant bin-packing algorithm, we show that improved BC is up to five orders of magnitude faster than a state-of-the-art BCP algorithm on problems with relatively few bins. We then explore a closely related problem, the number-partitioning problem, and show that an algorithm based on improved bin packing is up to three orders of magnitude faster than a BCP solver called DIMM which claims to be state of the art. Finally, we show how to use number partitioning to generate difficult bin-packing instances.", "title": "Improved Bin Completion for Optimal Bin Packing and Number Partitioning"}, {"url": "https://www.ijcai.org/Abstract/13/104", "abstract": "There are many hard shortest-path search problems that cannot be solved, because best-first search runs out of memory space and depth-first search runs out of time. We propose Forward Perimeter Search (FPS), a heuristic search with controlled use of memory. It builds a perimeter around the root node and tests each perimeter node for a shortest path to the goal. The perimeter is adaptively extended towards the goal during the search process. We show that FPS expands in random 24-puzzles 50% fewer nodes than BF-IDA* while requiring several orders of magnitude less memory. Additionally, we present a hard problem instance of the 24-puzzle that needs at least 140 moves to solve; i.e. 26 more moves than the previously published hardest instance.", "title": "Forward Perimeter Search with Controlled Use of Memory"}, {"url": "https://www.ijcai.org/Abstract/13/105", "abstract": "Recent research on external-memory search has shown that disks can be effectively used as secondary storage when performing large breadth-first searches. We introduce the Write-Minimizing Breadth-First Search (WMBFS) algorithm which is designed to minimizethe number of writes performed in an external-memory BFS. WMBFS is also designed to store the results of the BFS for later use.We present the results of a BFS on a single-agent version of Chinese Checkers and the Rubik's Cube edge cubes, state spaces with about 1 trillion states each. In evaluating against a comparable approach, WMBFS reduces the I/O for the Chinese Checkers domain by over an order of magnitude. In Rubik's cube, in addition to reducing I/O, the search is also 3.5 times faster. Analysis of the results suggests the machine and state-space properties necessary for WMBFS to perform well.", "title": "Minimizing Writes in Parallel External Memory Search"}, {"url": "https://www.ijcai.org/Abstract/13/106", "abstract": "The obvious way to use several admissible heuristics in A* is to take their maximum. In this paper we aim to reduce the time spent on computing heuristics. We discuss Lazy A*, a variant of A* where heuristics are evaluated lazily: only when they are essential to a decision to be made in the A* search process. We present a new rational meta-reasoning based scheme, Rational Lazy A*, which decides whether to compute the more expensive heuristics at all, based on a myopic value of information estimate. Both methods are examined theoretically. Empirical evaluation on several domains supports the theoretical results, and shows that Lazy A* and Rational Lazy A* are state-of-the-art heuristic combination methods.", "title": "Towards Rational Deployment of Multiple Heuristics in A*"}, {"url": "https://www.ijcai.org/Abstract/13/107", "abstract": "Nowadays, Nearest Neighbor Search becomes more and more important when facing the challenge of big data. Traditionally, to solve this problem, researchers mainly focus on building effective data structures such as hierarchical k-means tree or using hashing methods to accelerate the query process. In this paper, we propose a novel unified approximate nearest neighbor search scheme to combine the advantages of both the effective data structure and the fast Hamming distance computation in hashing methods. In this way, the searching procedure can be further accelerated. Computational complexity analysis and extensive experiments have demonstrated the effectiveness of our proposed scheme.", "title": "A Unified Approximate Nearest Neighbor Search Scheme by Combining Data Structure and Hashing"}, {"url": "https://www.ijcai.org/Abstract/13/109", "abstract": "This work is motivated by the following concern. Suppose we have a game exhibiting multiple Nash equilibria, with little to distinguish them except that one of them can be verified while the others cannot. That is, one of these equilibria carries sufficient information that, if this is the outcome, then the players can tell that an equilibrium has been played. This provides an argument for this equilibrium being played, instead of the alternatives. Verifiability can thus serve to make an equilibrium a focal point in the game. We formalise and investigate this concept using a model of Boolean games with incomplete information. We define and investigate three increasingly strong types of verifiable equilibria, characterise the complexity of checking these, and show how checking their existence can be captured in a variant of modal epistemic logic.", "title": "Verifiable Equilibria in Boolean Games"}, {"url": "https://www.ijcai.org/Abstract/13/110", "abstract": "The Interval Algebra (IA) and a subset of the Region Connection Calculus (RCC), namely RCC-8, are the dominant Artificial Intelligence approaches for representing and reasoning about qualitative temporal and topological relations respectively. Such qualitative information can be formulated as a Qualitative Constraint Network (QCN). In this paper, we focus on the minimal labeling problem (MLP) and we propose an algorithm to efficiently derive all the feasible base relations of a QCN. Our algorithm considers chordal QCNs and a new form of partial consistency. Further, the proposed algorithm uses tractable subclasses of relations having a specific patchwork property for which closure under weak composition implies the consistency of the input QCN. Experimentations with QCNs of IA and RCC-8 show the importance and efficiency of this new approach.", "title": "Efficient Approach to Solve the Minimal Labeling Problem of Temporal and Spatial Qualitative Constraints"}, {"url": "https://www.ijcai.org/Abstract/13/111", "abstract": "Knowledge base exchange is an important problem in the area of data exchange and knowledge representation, where one is interested in exchanging information between a source and a target knowledge base connected through a mapping. In this paper, we study this fundamental problem for knowledge bases and mappings expressed in OWL 2 QL, the profile of OWL 2 based on the description logic DL-LiteR. More specifically, we consider the problem of computing universal solutions, identified as one of the most desirable translations to be materialized, and the problem of computing UCQ- representations, which optimally capture in a target TBox the information that can be extracted from a source TBox and a mapping by means of unions of conjunctive queries. For the former we provide a novel automata-theoretic technique, and complexity results that range from NP to EXPTIME, while for the latter we show NLOGSPACE-completeness.", "title": "Exchanging OWL 2 QL Knowledge Bases"}, {"url": "https://www.ijcai.org/Abstract/13/112", "abstract": "Our aim is to investigate ontology-based data access over temporal data with validity time and ontologies capable of temporal conceptual modelling. To this end, we design a temporal description logic, TQL, that extends the standard ontology language OWL 2 QL, provides basic means for temporal conceptual modelling and ensures first-order rewritability of conjunctive queries for suitably defined data instances with validity time.", "title": "Temporal Description Logic for Ontology-Based Data Access"}, {"url": "https://www.ijcai.org/Abstract/13/113", "abstract": "Recently there has been an increasing interest in incorporating \"intensional\" functions in answer set programming. Intensional functions are those whose values can be described by other functions and predicates, rather than being pre-defined as in the standard answer set programming. We demonstrate that the functional stable model semantics plays an important role in the framework of \"Answer Set Programming Modulo Theories (ASPMT)\" \u2014 a tight integration of answer set programming and satisfiability modulo theories, under which existing integration approaches can be viewed as special cases where the role of functions is limited. We show that \"tight\" ASPMT programs can be translated into SMT instances, which is similar to the known relationship between ASP and SAT.", "title": "Functional Stable Model Semantics and Answer Set Programming Modulo Theories"}, {"url": "https://www.ijcai.org/Abstract/13/114", "abstract": "Artifact-Centric Systems are a novel paradigm in service-oriented computing. In the present contribution we show that model checking bounded, non-uniform artifact-centric systems is undecidable. We provide a partial model checking procedure for artifact-centric systems against the universal fragment of a first-order version of the logic CTL. We obtain this result by introducing a counterpart semantics and developing an abstraction methodology operating on these structures. This enables us to generate finite abstractions of infinite artifact-centric systems, hence perform verification on abstract models.", "title": "Decidability of Model Checking Non-Uniform Artifact-Centric Quantified Interpreted Systems"}, {"url": "https://www.ijcai.org/Abstract/13/115", "abstract": "Among the many approaches for reasoning about degrees of belief in the presence of noisy sensing and acting, the logical account proposed by Bacchus, Halpern, and Levesque is perhaps the most expressive. While their formalism is quite general, it is restricted to fluents whose values are drawn from discrete countable domains, as opposed to the continuous domains seen in many robotic applications. In this paper, we show how this limitation in their approach can be lifted. By dealing seamlessly with both discrete distributions and continuous densities within a rich theory of action, we provide a very general logical specification of how belief should change after acting and sensing in complex noisy domains.", "title": "Reasoning about Continuous Uncertainty in the Situation Calculus"}, {"url": "https://www.ijcai.org/Abstract/13/116", "abstract": "We extend hybrid possibilistic conditioning to deal with inputs consisting of a set of triplets composed of propositional formulas, the level at which the formulas should be accepted, and the way in which their models should be revised. We characterize such conditioning using elementary operations on possibility distributions. We then solve a difficult issue that concerns the syntactic computation of the revision of possibilistic knowledge bases, made of weighted formulas, using hybrid conditioning. An important result is that there is no extra computational cost in using hybrid possibilistic conditioning and in particular the size of the revised possibilistic base is polynomial with respect to the size of the initial base and the input.", "title": "Syntactic Computation of Hybrid Possibilistic Conditioning under Uncertain Inputs"}, {"url": "https://www.ijcai.org/Abstract/13/117", "abstract": "A notion of quantified conditional logics is provided that includes quantification over individual and propositional variables. The former is supported with respect to constant and variable domain semantics. In addition, a sound and complete embedding of this framework in classical higher-order logic is presented. Using prominent examples from the literature it is demonstrated how this embedding enables effective automation of reasoning within (object-level) and about (meta-level) quantified conditional logics with off-the-shelf higher-order theorem provers and model finders.", "title": "Automating Quantified Conditional Logics in HOL"}, {"url": "https://www.ijcai.org/Abstract/13/118", "abstract": "One of the most advanced approaches to querying data in the presence of ontologies is to make use of relational database systems, rewriting the original query and the ontology into a new query that is formulated in SQL or, equivalently, in first-order logic (FO). For ontologies written in many standard description logics (DLs), however, such FO-rewritings are not guaranteed to exist. We study FO-rewritings and their existence for a basic class of queries and for ontologies formulated in Horn DLs such as Horn-SHI and EL. Our results include characterizations of the existence of FO-rewritings, tight complexity bounds for deciding whether an FO-rewriting exists (ExpTime and PSpace), and tight bounds on the (worst-case) size of FO-rewritings, when presented as a union of conjunctive queries.", "title": "First-Order Rewritability of Atomic Queries in Horn Description Logics"}, {"url": "https://www.ijcai.org/Abstract/13/119", "abstract": "Conjunctive regular path queries are an expressive extension of the well-known class of conjunctive queries and have been extensively studied in the database community. Somewhat surprisingly, there has been little work aimed at using such queries in the context of description logic (DL) knowledge bases, and all existing results target expressive DLs, even though lightweight DLs are considered better-suited for data-intensive applications. This paper aims to bridge this gap by providing algorithms and tight complexity bounds for answering two-way conjunctive regular path queries over DL knowledge bases formulated in lightweight DLs of the DL-Lite and EL families.", "title": "Conjunctive Regular Path Queries in Lightweight Description Logics"}, {"url": "https://www.ijcai.org/Abstract/13/120", "abstract": "It is a classic result in database theory that conjunctive query (CQ) answering, which is NP-complete in general, is feasible in polynomial time when restricted to acyclic queries. Subsequent results identified more general structural properties of CQs (like bounded treewidth) which ensure tractable query evaluation. In this paper, we lift these tractability results to knowledge bases formulated in the lightweight description logics DL-Lite and ELH. The proof exploits known properties of query matches in these logics and involves a query-dependent modification of the data. To obtain a more practical approach, we propose a concrete polynomial-time algorithm for answering acyclic CQs based on rewriting queries into datalog programs. A preliminary evaluation suggests the interest of our approach for handling large acyclic CQs.", "title": "Tractable Queries for Lightweight Description Logics"}, {"url": "https://www.ijcai.org/Abstract/13/121", "abstract": "A robust system for ontology-based data access should provide meaningful answers to queries even when the data conflicts with the ontology. This can be accomplished by adopting an inconsistency-tolerant semantics, with the consistent query answering (CQA) semantics being the most prominent example. Unfortunately, query answering under the CQA semantics has been shown to be computationally intractable, even when extremely simple ontology languages are considered. In this paper, we address this problem by proposing two new families of inconsistency-tolerant semantics which approximate the CQA semantics from above and from below and converge to it in the limit. We study the data complexity of conjunctive query answering under these new semantics, and show a general tractability result for all known first-order rewritable ontology languages. We also analyze the combined complexity of query answering for ontology languages of the DL-Lite family.", "title": "Tractable Approximations of Consistent Query Answering for Robust Ontology-Based Data Access"}, {"url": "https://www.ijcai.org/Abstract/13/122", "abstract": "We provide both a semantic interpretation and logical (inferential) characterization of the Markov principle that underlies the main action theories in AI. This principle will be shown to constitute a nonmonotonic assumption that justifies the actual restrictions on action descriptions in these theories, as well as constraints on allowable queries. It will be shown also that the well-known regression principle is a consequence of the Markov assumption, and it is valid also for non-deterministic domains.", "title": "The Markov Assumption: Formalization and Impact"}, {"url": "https://www.ijcai.org/Abstract/13/123", "abstract": "The Description Logic EL is used to formulate several large biomedical ontologies. Fuzzy extensions of EL can express the vagueness inherent in many biomedical concepts. We study the reasoning problem of deciding positive subsumption in fuzzy EL with semantics based on general t-norms. We show that the complexity of this problem depends on the specific t-norm chosen. More precisely, if the t-norm has zero divisors, then the problem is co-NP-hard; otherwise, it can be decided in polynomial time. We also show that the best subsumption degree cannot be computed in polynomial time if the t-norm contains the \u0141ukasiewicz t-norm.", "title": "Positive Subsumption in Fuzzy EL with General t-Norms"}, {"url": "https://www.ijcai.org/Abstract/13/124", "abstract": "We study the complexity of conjunctive query answering under (weakly-)(frontier-)guarded disjunctive existential rules, i.e., existential rules extended with disjunction, and their main subclasses, linear rules and inclusion dependencies (IDs). Our main result states that conjunctive query answering under a fixed set of disjunctive IDs is 2EXPTIME-hard. This quite surprising result together with a 2EXPTIME upper bound for weakly-frontier-guarded disjunctive rules, obtained by exploiting recent results on guarded negation first-order logic, gives us a complete picture of the computational complexity of our problem. We also consider a natural subclass of disjunctive IDs, namely\u00a0frontier-one (only one variable is propagated), for which the combined complexity decreases to EXPTIME. Finally, we show that frontier-guarded rules, combined with negative constraints, are strictly more expressive than DL-LiteHbool, one of the most expressive languages of the DL-Lite family. We also show that query answering under this DL is 2EXPTIME-complete in combined complexity.", "title": "The Impact of Disjunction on Query Answering under Guarded-Based Existential Rules"}, {"url": "https://www.ijcai.org/Abstract/13/125", "abstract": "We present various new concepts and results related to abstract dialectical frameworks (ADFs), a powerful generalization of Dung's argumentation frameworks (AFs). In particular, we show how the existing definitions of stable and preferred semantics which are restricted to the subcase of so-called bipolar ADFs can be improved and generalized to arbitrary frameworks. Furthermore, we introduce preference handling methods for ADFs, allowing for both reasoning with and about preferences. Finally, we present an implementation based on an encoding in answer set programming.", "title": "Abstract Dialectical Frameworks Revisited"}, {"url": "https://www.ijcai.org/Abstract/13/126", "abstract": "Description Logic Knowledge and Action Bases (KABs) have been recently introduced as a mechanism that provides a semantically rich representation of the information on the domain of interest in terms of a DL KB and a set of actions to change such information over time, possibly introducing new objects. In this setting, decidability of verification of sophisticated temporal properties over KABs, expressed in a variant of first-order mu-calculus, has been shown. However, the established framework treats inconsistency in a simplistic way, by rejecting inconsistent states produced through action execution. We address this problem by showing how inconsistency handling based on the notion of repairs can be integrated into KABs, resorting to inconsistency-tolerant semantics. In this setting, we establish decidability and complexity of verification.", "title": "Verification of Inconsistency-Aware Knowledge and Action Bases"}, {"url": "https://www.ijcai.org/Abstract/13/127", "abstract": "Wastl introduced for first time a tableaux-like method based on an inference system for deriving all minimal keys from a relational schema. He introduced two inference rules and built an automated method over them.In this work we tackle the key finding problem with a tableaux method, but we will use two inference rules inspired by the Simplification Logic for Functional Dependencies. Wastl's method requires the input to be a set of functional dependencies with atomic right hand sides. Therefore, it is necessary to apply fragmentation rule with the consequent increasing of the input.The main novelty of our rules is that they deal with generalized formulas, avoiding the fragmentation needed in the former tableaux. Finally we illustrate the advantages of our new tableaux method with an experiment.", "title": "Automated Reasoning to Infer All Minimal Keys"}, {"url": "https://www.ijcai.org/Abstract/13/128", "abstract": "Many AI-related reasoning problems are based on the problem of satisfiability (SAT). While SAT itself becomes easy when restricting the structure of the formulas in a certain way, this is not guaranteed for more involved reasoning problems. In this work, we focus on reasoning tasks in the areas of belief revision and logic-based abduction and show that in some cases the restriction to Krom formulas (i.e., formulas in CNF where clauses have at most two literals) decreases the complexity, while in others it does not. We thus also consider additional restrictions to Krom formulas towards a better identification of the tractability frontier of such problems.", "title": "Do Hard SAT-Related Reasoning Tasks Become Easier in the Krom Fragment?"}, {"url": "https://www.ijcai.org/Abstract/13/129", "abstract": "Rewriting-based approaches for answering queries over an OWL 2 DL ontology have so far been developed mainly for Horn fragments of OWL 2 DL. In this paper, we study the possibilities of answering queries over non-Horn ontologies using datalog rewritings. We prove that this is impossible in general even for very simple ontology languages, and even if PTIME = NP. Furthermore, we present a resolution-based procedure for SHI ontologies that, in case it terminates, produces a datalog rewriting of the ontology. We also show that our procedure necessarily terminates on DL-LiteBoolH,+ ontologies \u2014 an extension of OWL 2 QL with transitive roles and Boolean connectives.", "title": "Computing Datalog Rewritings Beyond Horn Ontologies"}, {"url": "https://www.ijcai.org/Abstract/13/130", "abstract": "We present a new approach to token-level causal reasoning that we call Sequences Of Mechanisms (SoMs), which models causality as a dynamic sequence of active mechanisms that chain together to propagate causal influence through time. We motivate this approach by using examples from AI and robotics and show why existing approaches are inadequate. We present an algorithm for causal reasoning based on SoMs, which takes as input a knowledge base of first-order mechanisms and a set of observations, and it hypothesizes which mechanisms are active at what time. We show empirically that our algorithm produces plausible causal explanations of simulated observations generated from a causal model. We argue that the SoMs approach is qualitatively closer to the human causal reasoning process, for example, it will only include relevant variables in explanations. We present new insights about causal reasoning that become apparent with this view. One such insight is that observation and manipulation do not commute in causal models, a fact which we show to be a generalization of the Equilibration-Manipulation Commutability of [Dash(2005)].", "title": "Sequences of Mechanisms for Causal Reasoning in Artificial Intelligence"}, {"url": "https://www.ijcai.org/Abstract/13/131", "abstract": "We define the class of e-bounded theories in the epistemic situation calculus, where the number of fluent atoms that the agent thinks may be true is bounded by a constant. Such theories can still have an infinite domain and an infinite set of states. We show that for them verification of an expressive class of first-order mu-calculus temporal epistemic properties is decidable. We also show that if the agent's knowledge in the initial situation is e-bounded and the objective part of an action theory maintains boundedness, then the entire epistemic theory is e-bounded.", "title": "Bounded Epistemic Situation Calculus Theories"}, {"url": "https://www.ijcai.org/Abstract/13/132", "abstract": "In this paper we look into the assumption of interpreting LTL over finite traces. In particular we show that LTLf, i.e., LTL under this assumption, is less expressive than what might appear at first sight, and that at essentially no computational cost one can make a significant increase in expressiveness while maintaining the same intuitiveness of LTLf. Indeed, we propose a logic, LDLf for Linear Dynamic Logic over finite traces, which borrows the syntax from Propositional Dynamic Logic PDL, but is interpreted over finite traces. Satisfiability, validity and logical implication (as well as model checking) for LDLf are PSPACE-complete as for LTLf (and LTL).", "title": "Linear Temporal Logic and Linear Dynamic Logic on Finite Traces"}, {"url": "https://www.ijcai.org/Abstract/13/133", "abstract": null, "title": "A Formal Account of Nondeterministic and Failed Actions"}, {"url": "https://www.ijcai.org/Abstract/13/134", "abstract": "Nonmonotonic Description Logic (DL) programs support rule-based reasoning on top of Description Logic ontologies, using a well-defined query interface. However, the interaction of the rules and the ontology may cause inconsistency such that no answer set (i.e. model) exists. We thus consider repairing DL-programs, i.e., changing formulas to obtain consistency. Viewing the data part of the ontology as the source of inconsistency, we define program repairs and repair answer sets based on changes to it. We analyze the complexity of the notion, and we extend an algorithm for evaluating DL-programs to compute repair answer sets, under optional selection of preferred repairs. The extension involves a generalized ontology repair problem, in which the entailment and non-entailment of sets of queries with updates to the ontology must be achieved. While this is intractable in general, we identify for the Description Logic DL-Lite_A some tractable classes of preferred repairs that are useful in practice.", "title": "Data Repair of Inconsistent DL-Programs"}, {"url": "https://www.ijcai.org/Abstract/13/135", "abstract": "The knowledge compilation map introduced by Darwiche and Marquis takes advantage of a number of concepts (mainly queries, transformations, expressiveness, and succinctness) to compare the relative adequacy of representation languages to some AI problems. However, the framework is limited to the comparison of languages that are interpreted in a homogeneous way (formulae are interpreted as Boolean functions). This prevents one from comparing, on a formal basis, languages that are close in essence, such as OBDD, MDD, and ADD. To fill the gap, we present a generalized framework into which comparing formally heterogeneous representation languages becomes feasible. In particular, we explain how the key notions of queries and transformations, expressiveness, and succinctness can be lifted to the generalized setting.", "title": "Towards a Knowledge Compilation Map for Heterogeneous Representation Languages"}, {"url": "https://www.ijcai.org/Abstract/13/136", "abstract": "Existing languages in the valued decision diagrams (VDDs) family, including ADD, AADD, and those of the SLDD family, prove to be valuable target languages for compiling multivariate functions. However, their efficiency is directly related to the size of the compiled formulae.In practice, the existence of canonical forms may have a major impact on the size of the compiled VDDs. While efficient normalization procedures have been pointed out for ADD and AADD, the canonicity issue for SLDD formulae has not been addressed so far. In this paper, the SLDD family is revisited. We modify the algebraic requirements imposed on the valuation structure so as to ensure tractable conditioning, optimization and normalization for some languages of the revisited SLDD family. We show that AADD is captured by this family. Finally, we compare the spatial efficiency of some languages of this family, from both the theoretical side and the practical side.", "title": "Semiring Labelled Decision Diagrams, Revisited: Canonicity and Spatial Efficiency Issues"}, {"url": "https://www.ijcai.org/Abstract/13/137", "abstract": "We study a logical system FQHT that is appropriate for reasoning about nonmonotonic theories with intensional functions as treated in the approach of Bartholomew and Lee (2012). We provide a logical semantics, a Gentzen style proof theory and establish completeness results. The adequacy of the approach is demonstrated by showing that it captures the Bartholemew/Lee semantics and satisfies a strong equivalence property.", "title": "FQHT: The Logic of Stable Models for Logic Programs with Intensional Functions"}, {"url": "https://www.ijcai.org/Abstract/13/138", "abstract": "Probabilistic abstract argumentation combines Dung's abstract argumentation framework with probability theory in order to model uncertainty in argumentation. In this setting, we address the fundamental problem of computing the probability that a set of arguments is an extension according to a given semantics. We focus on the most popular semantics (i.e., admissible, stable, complete, grounded, preferred, ideal), and show the following dichotomy result: computing the probability that a set of arguments is an extension is either PTIME or FP#P-complete depending on the semantics adopted. Our PTIME results are particularly interesting, as they hold for some semantics for which no polynomial-time technique was known so far.", "title": "On the Complexity of Probabilistic Abstract Argumentation"}, {"url": "https://www.ijcai.org/Abstract/13/139", "abstract": "Entities in two-dimensional space are often approximated using rectangles that are parallel to the two axes that define the space, so-called minimum-bounding rectangles (MBRs). MBRs are popular in Computer Vision and other areas as they are easy to obtain and easy to represent. In the area of Qualitative Spatial Reasoning, many different spatial representations are based on MBRs. Surprisingly, there has been no such representation proposed for general rectangles, i.e., rectangles that can have any angle, nor for general solid rectangles (GSRs) that cannot penetrate each other. GSRs are often used in computer graphics and computer games, such as Angry Birds, where they form the building blocks of more complicated structures. In order to represent and reason about these structures, we need a spatial representation that allows us to use GSRs as the basic spatial entities. In this paper we develop and analyze a qualitative spatial representation for GSRs. We apply our representation and the corresponding reasoning methods to solve a very interesting practical problem: Assuming we want to detect GSRs in computer games, but computer vision can only detect MBRs. How can we infer the GSRs from the given MBRs? We evaluate our solution and test its usefulness in a real gaming scenario.", "title": "Representation and Reasoning about General Solid Rectangles"}, {"url": "https://www.ijcai.org/Abstract/13/140", "abstract": "We introduce a new approach to disjunctive ASP solving that aims at an equitable interplay between \"generating\" and \"testing\" solver units. To this end, we develop novel characterizations of answer sets and unfounded sets allowing for a bidirectional dynamic information exchange between solver units for orthogonal tasks. This results in the new multi-threaded disjunctive ASP solver claspD-2, greatly improving the performance of existing systems.", "title": "Advanced Conflict-Driven Disjunctive Answer Set Solving"}, {"url": "https://www.ijcai.org/Abstract/13/141", "abstract": "A novel contextual logic is presented that combines features of both multi context systems and logics of context. Broadly, contextual logics are those with a formal notion of context \u2014 knowledge that is true only under specific assumptions. Multi-context systems use discrete logistic systems as individual contexts, related by meta-level rules, whereas logics of context partition a single knowledge base into contexts, related using object-level rules. The contextual logic presented here is strongly-local, in that knowledge and inference is discrete for individual contexts, but which are nevertheless part of a single logistic system that relates contexts at the object-level, so combining advantages of both. A deductive system of contextual inference and a possible-worlds based semantics is given, with formal results including soundness and completeness, and a number of properties are examined.", "title": "A Strongly-Local Contextual Logic"}, {"url": "https://www.ijcai.org/Abstract/13/142", "abstract": "While function symbols are widely acknowledged as an important feature in logic programming, they make common inference tasks undecidable. To cope with this problem, recent research has focused on identifying classes of logic programs imposing restrictions on the use of function symbols, but guaranteeing decidability of common inference tasks. This has led to several criteria, called termination criteria, providing sufficient conditions for a program to have finitely many stable models, each of finite size.This paper introduces the new class of bounded programs which guarantees the aforementioned property and strictly includes the classes of programs determined by current termination criteria. Different results on the correctness, the expressiveness, and the complexity of the class of bounded programs are presented.", "title": "Bounded Programs: A New Decidable Class of Logic Programs with Function Symbols"}, {"url": "https://www.ijcai.org/Abstract/13/143", "abstract": "Iterated games are well-known in the game theory literature. We study iterated Boolean games. These are games in which players repeatedly choose truth values for Boolean variables they have control over. Our model of iterated Boolean games assumes that players have goals given by formulae of Linear Temporal Logic (LTL), a formalism for expressing properties of state sequences. In order to model the strategies that players use in such games, we use a finite state machine model. After introducing and formally defining iterated Boolean games, we investigate the computational complexity of their associated game-theoretic decision problems as well as semantic conditions characterising classes of LTL properties that are preserved by pure strategy Nash equilibria whenever they exist.", "title": "Iterated Boolean Games"}, {"url": "https://www.ijcai.org/Abstract/13/144", "abstract": "We consider the problem of how enormous databases of \"common sense\" knowledge can be both learned and utilized in reasoning in a computationally efficient manner. We propose that this is possible if the learning only occurs implicitly, i.e., without generating an explicit representation. We show that it is feasible to invoke such implicitly learned knowledge in essentially all natural tractable reasoning problems. This implicit learning also turns out to be provably robust to occasional counterexamples, as appropriate for such common sense knowledge.", "title": "Implicit Learning of Common Sense for Reasoning"}, {"url": "https://www.ijcai.org/Abstract/13/145", "abstract": "Counting the models of a propositional formula is a key issue for a number of AI problems, but few propositional languages offer the possibility to count models efficiently. In order to fill the gap, we introduce the language EADT of (extended) affine decision trees. An extended affine decision tree simply is a tree with affine decision nodes and some specific decomposable conjunction or disjunction nodes. Unlike standard decision trees, the decision nodes of an EADT formula are not labeled by variables but by affine clauses. We study EADT, and several subsets of it along the lines of the knowledge compilation map. We also describe a CNF-to-EADT compiler and present some experimental results. Those results show that the EADT compilation-based approach is competitive with (and in some cases is able to outperform) the model counter Cachet and the d-DNNF compilation-based approach to model counting.", "title": "Knowledge Compilation for Model Counting: Affine Decision Trees"}, {"url": "https://www.ijcai.org/Abstract/13/146", "abstract": "The goal of this paper is a systematic parameterized complexity analysis of different variants of propositional STRIPS planning. We identify several natural problem parameters and study all possible combinations of 9 parameters in 6 different settings. These settings arise, for instance, from the distinction if negative effects of actions are allowed or not. We provide a complete picture by establishing for each case either paraNP-hardness (i.e., the parameter combination does not help) or W[t]-completeness with t \u2208 1,2 (i.e., fixed-parameter intractability), or FPT (i.e., fixed-parameter tractability).", "title": "Parameterized Complexity of Optimal Planning: A Detailed Map"}, {"url": "https://www.ijcai.org/Abstract/13/147", "abstract": "Fuzzy description logics (DLs) serve as a tool to handle vagueness in real-world knowledge. There is particular interest in logics implementing Lukasiewicz semantics, which has a number of favourable properties. Current decision procedures for Lukasiewicz fuzzy DLs work by reduction to exponentially large mixed integer programming problems. Here, we present a decision method that stays closer to logical syntax, a labelled tableau algorithm for Lukasiewicz Fuzzy ALC that calls only on (pure) linear programming, and this only to decide atomic clashes. The algorithm realizes the best known complexity bound, NEXPTIME. Our language features a novel style of fuzzy ABoxes that work with comparisons of truth degrees rather than explicit numerical bounds.", "title": "Syntactic Labelled Tableaux for \u0141ukasiewicz Fuzzy ALC"}, {"url": "https://www.ijcai.org/Abstract/13/148", "abstract": "There are not very many existing logics of belief which have both a perspicuous semantics and are computationally attractive. An exception is the logic SL, proposed by Liu, Lakemeyer, and Levesque, which allows for a decidable and often even tractable form of reasoning. While the language is first-order and hence quite expressive, it still has a number of shortcomings. For one, beliefs about beliefs are not addressed at all. For another, the names of individuals are rigid, that is, their identity is assumed to be known. In this paper, we show how both shortcomings can be overcome by suitably extending the language and its semantics. Among other things, we show that determining the beliefs of a certain kind of fully introspective knowledge bases is decidable and that unknown individuals in the knowledge base can be accommodated in a decidable manner as well.", "title": "Decidable Reasoning in a Logic of Limited Belief with Introspection and Unknown Individuals"}, {"url": "https://www.ijcai.org/Abstract/13/149", "abstract": "Relative direction information is very commonly used. Observers typically describe their environment by specifying the relative directions in which they see other objects or other people from their point of view. Or they receive navigation instructions with respect to their point of view, for example, turn left at the next intersection. However, it is surprisingly hard to integrate relative direction information obtained from different observers, and to reconstruct a model of the environment or the locations of the observers based on this information. Despite intensive research, there is currently no algorithm that can effectively integrate this information: this problem is NP-hard, but not known to be in NP, even if we only use left and right relations. In this paper we present a novel qualitative representation, StarVars, that can solve these problems. It is an extension of the STAR calculus [Renz and Mitra, 2004]) by a VARiable interpretation of the orientation of observers. We show that reasoning in StarVars is in NP and present the first algorithm that allows us to effectively integrate relative direction information from different observers.", "title": "StarVars\u2014Effective Reasoning about Relative Directions"}, {"url": "https://www.ijcai.org/Abstract/13/150", "abstract": "The action description languages B and C have significant common core. Nevertheless, some expressive possibilities of B are difficult or impossible to simulate in C, and the other way around. The main advantage of B is that it allows the user to give Prolog-style recursive definitions, which is important in applications. On the other hand, B solves the frame problem by incorporating the commonsense law of inertia in its semantics, which makes it difficult to talk about fluents whose behavior is described by defaults other than inertia. In C and in its extension C+, the inertia assumption is expressed by axioms that the user is free to include or not to include, and other defaults can be postulated as well. This paper defines a new action description language, called BC, that combines the attractive features of B and C. Examples of formalizing commonsense domains discussed in the paper illustrate the expressive capabilities of BC and the use of answer set solvers for the automation of reasoning about actions described in this language.", "title": "Action Language BC: Preliminary Report"}, {"url": "https://www.ijcai.org/Abstract/13/151", "abstract": "Answer Set Programming Modulo Theories (ASPMT) is a new framework of tight integration of answer set programming (ASP) and satisfiability modulo theories (SMT). Similar to the relationship between first-order logic and SMT, it is based on a recent proposal of the functional stable model semantics by fixing interpretations of background theories. Analogously to a known relationship between ASP and SAT, \"tight\" ASPMT programs can be translated into SMT instances. We demonstrate the usefulness of ASPMT by enhancing action language C+ to handle continuous changes as well as discrete changes. We reformulate the semantics of C+ in terms of ASPMT, and show that SMT solvers can be used to compute the language. We also show how the language can represent cumulative effects on continuous resources.", "title": "Answer Set Programming Modulo Theories and Reasoning about Continuous Changes"}, {"url": "https://www.ijcai.org/Abstract/13/152", "abstract": "In dynamic systems, state constraints are formulas that hold in every reachable state. It has been shown that state constraints can be used to greatly reduce the planning search space. They are also useful in program verification. In this paper, we propose a sound but incomplete method for automatic verification and discovery of state constraints for a class of action theories that include many planning benchmarks. Our method is formulated in the situation calculus, theoretically based on Skolemization and Herbrand Theorem, and implemented with SAT solvers. Basically, we verify a state constraint by strengthening it in a novel and smart way so that it becomes a state invariant. We experimented with the blocks world, logistics and satellite domains, and the results showed that, almost all known state constraints can be verified in a reasonable amount of time, and meanwhile succinct and intuitive related state constraints are discovered.", "title": "Reasoning about State Constraints in the Situation Calculus"}, {"url": "https://www.ijcai.org/Abstract/13/153", "abstract": "G\u00f6del's proof of his famous first incompleteness theorem (G1) has quite understandably long been a tantalizing target for those wanting to engineer impressively intelligent computational systems. After all, in establishing G1, G\u00f6del did something that by any metric must be classified as stunningly intelligent. We observe that it has long been understood that there is some sort of analogical relationship between the Liar Paradox (LP) and G1, and that G\u00f6del himself appreciated and exploited the relationship. Yet the exact nature of the relationship has hitherto not been uncovered, by which we mean that the following question has not been answered: Given a description of LP,and the suspicion that it may somehow be used by a suitably programmed computing machine to find a proof of the incompleteness of Peano Arithmetic, can such a machine, provided this description as input, produce as output a complete and verifiably correct proof of G1? In this paper, we summarize engineering that entails an affirmative answer to this question. Our approach uses what we call analogico-deductive reasoning (ADR), which combines analogical and deductive reasoning to produce a full deductive proof of G1 from LP. Our engineering uses a form of ADR based on our META-R system, and a connection between the Liar Sentence in LP and G\u00f6del's Fixed Point Lemma, from which G1 follows quickly.", "title": "Analogico-Deductive Generation of G\u00f6del\u2019s First Incompleteness Theorem from the Liar Paradox"}, {"url": "https://www.ijcai.org/Abstract/13/154", "abstract": "We define a family of epistemic extensions of Halpern-Shoham logic for reasoning about temporal-epistemic properties of multi-agent systems. We exemplify their use and study the complexity of their model checking problem. We show a range of results ranging from PTIME to PSPACE-hard depending on the logic considered.", "title": "An Epistemic Halpern\u2013Shoham Logic"}, {"url": "https://www.ijcai.org/Abstract/13/155", "abstract": null, "title": "Preference-Based Query Answering in Datalog+/\u2013 Ontologies"}, {"url": "https://www.ijcai.org/Abstract/13/156", "abstract": "When answering queries in the presence of ontologies, adopting the closed world assumption for some predicates easily results in intractability. We analyze this situation on the level of individual ontologies formulated in the description logics DL-Lite and EL and show that in all cases where answering CQs with (open and) closed predicates is tractable, it coincides with answering CQs with all predicates assumed open. In this sense, CQ answering with closed predicates in inherently intractable. Our analysis also yields a dichotomy between AC0 and coNP for CQ answering in DL-Lite and a dichotomy between PTime and coNP for EL. Interestingly, the situation is less dramatic in the more expressive description logic ELI, where we find ontologies for which CQ answering is in PTime, but does not coincide with CQ answering where all predicates are open.", "title": "Ontology-Based Data Access with Closed Predicates Is Inherently Intractable (Sometimes)"}, {"url": "https://www.ijcai.org/Abstract/13/157", "abstract": "In this work, we consider function-free existential rules extended with nonmonotonic negation under a stable model semantics. We present new acyclicity and stratification conditions that identify a large class of rule sets having finite, unique stable models, and we show how the addition of constraints on the input facts can further extend this class. Checking these conditions is computationally feasible, and we provide tight complexity bounds. Finally, we demonstrate how these new methods allowed us to solve relevant reasoning problems over a real-world knowledge base from biochemistry using an off-the-shelf answer set programming engine.", "title": "Computing Stable Models for Nonmonotonic Existential Rules"}, {"url": "https://www.ijcai.org/Abstract/13/158", "abstract": "Diagnosis, i.e., the identification of root causes for failing or unexpected system behavior, is an important task in practice. Within the last three decades, many different AI-based solutions for solving the diagnosis problem have been presented and have been gaining in attraction. This leaves us with the question of which algorithm to prefer in a certain situation. In this paper we contribute to answering this question. In particular, we compare two classes of diagnosis algorithms. One class exploits conflicts in their search, i.e., sets of system components whose correct behavior contradicts given observations. The other class ignores conflicts and derives diagnoses from observations and the underlying model directly. In our study we use different reasoning engines ranging from an optimized Horn-clause theorem prover to general SAT and constraint solvers. Thus we also address the question whether publicly available general reasoning engines can be used for an efficient diagnosis.", "title": "The Route to Success \u2013 A Performance Comparison of Diagnosis Algorithms"}, {"url": "https://www.ijcai.org/Abstract/13/159", "abstract": "Abductive reasoning (or Abduction, for short) is among the most fundamental AI reasoning methods, with a broad range of applications, including fault diagnosis, belief revision, and automated planning. Unfortunately, Abduction is of high computational complexity; even propositional Abduction is \u03a32P-complete and thus harder than NP and coNP. This complexity barrier rules out the existence of a polynomial transformation to propositional satisfiability (SAT). In this work we use structural properties of the Abduction instance to break this complexity barrier. We utilize the problem structure in terms of small backdoor sets. We present fixed-parameter tractable transformations from Abduction to SAT, which make the power of today's SAT solvers available to Abduction.", "title": "Backdoors to Abduction"}, {"url": "https://www.ijcai.org/Abstract/13/160", "abstract": "Product defects and rework efforts due to flawed specifications represent major issues for a project's performance, so that there is a high motivation for providing effective means that assist designers in assessing and ensuring a specification's quality. Recent research in the context of formal specifications, e.g. on coverage and vacuity, offers important means to tackle related issues. In the currently underrepresented research direction of diagnostic reasoning on a specification, we propose a scenario-based diagnosis at a specification's operator level using weak or strong fault models. Drawing on efficient SAT encodings, we show in this paper how to achieve that effectively for specifications in LTL. Our experimental results illustrate our approach's validity and attractiveness.", "title": "Behavioral Diagnosis of LTL Specifications at Operator Level"}, {"url": "https://www.ijcai.org/Abstract/13/161", "abstract": "We analyze the foundations of cyclic causal models for discrete variables, and compare structural equation models (SEMs) to an alternative semantics as the equilibrium (stationary) distribution of a Markov chain. We show under general conditions, discrete cyclic SEMs cannot have independent noise; even in the simplest case, cyclic structural equation models imply constraints on the noise. We give a formalization of an alternative Markov chain equilibrium semantics which requires not only the causal graph, but also a sample order. We show how the resulting equilibrium is a function of the sample ordering, both theoretically and empirically.", "title": "Cyclic Causal Models with Discrete Variables: Markov Chain Equilibrium Semantics and Sample Ordering"}, {"url": "https://www.ijcai.org/Abstract/13/162", "abstract": "Parameterized linear systems allow for modelling and reasoning over classes of polyhedra. Collections of squares, rectangles, polytopes, and so on, can readily be defined by means of linear systems with parameters. In this paper, we investigate the problem of learning a parameterized linear system whose class of polyhedra includes a given set of example polyhedral sets and it is minimal.", "title": "Learning from Polyhedral Sets"}, {"url": "https://www.ijcai.org/Abstract/13/163", "abstract": "Vast amounts of video data are available on the web and are being generated daily using surveillance cameras or other sources. Being able to efficiently analyse and process this data is essential for a number of different applications. We want to be able to efficiently detect activities in these videos or be able to extract and store essential information contained in these videos for future use and easy search and access. Cohn et al. (2012) proposed a comprehensive representation of spatial features that can be efficiently extracted from video and used for these purposes. In this paper, we present a modified version of this approach that is equally efficient and allows us to extract spatial information with much higher accuracy than previously possible. We present efficient algorithms both for extracting and storing spatial information from video, as well as for processing this information in order to obtain useful spatial features. We evaluate our approach and demonstrate that the extracted spatial information is considerably more accurate than that obtained from existing approaches.", "title": "Efficient Extraction and Representation of Spatial Information from Video Data"}, {"url": "https://www.ijcai.org/Abstract/13/164", "abstract": "RCC5 is an important and well-known calculus for representing and reasoning about mereological relations. Among many other applications, it is pivotal in the formalization of commonsense reasoning about natural categories. In particular, it allows for a qualitative representation of conceptual spaces in the sense of Gardenfors. To further the role of RCC5 as a vehicle for conceptual reasoning, in this paper we combine RCC5 relations with information about betweenness of regions. The resulting calculus allows us to express, for instance, that some part (but not all) of region B is between regions A and C. We show how consistency can be decided in polynomial time for atomic networks, even when regions are required to be convex. From an application perspective, the ability to express betweenness information allows us to use RCC5 as a basis for interpolative reasoning, while the restriction to convex regions ensures that all consistent networks can be faithfully represented as a conceptual space.", "title": "Combining RCC5 Relations with Betweenness Information"}, {"url": "https://www.ijcai.org/Abstract/13/165", "abstract": "Default reasoning and interpolation are two important forms of commonsense rule-based reasoning. The former allows us to draw conclusions from incompletely specified states, by making assumptions on normality, whereas the latter allows us to draw conclusions from states that are not explicitly covered by any of the available rules. Although both approaches have received considerable attention in the literature, it is at present not well understood how they can be combined to draw reasonable conclusions from incompletely specified states and incomplete rule bases. In this paper, we introduce an inference system for interpolating default rules, based on a geometric semantics in which normality is related to spatial density and interpolation is related to geometric betweenness. We view default rules and information on the betweenness of natural categories as particular types of constraints on qualitative representations of G\u00e4rdenfors conceptual spaces. We propose an axiomatization, extending the well-known System P, and show its soundness and completeness w.r.t. the proposed semantics. Subsequently, we explore how our extension of preferential reasoning can be further refined by adapting two classical approaches for handling the irrelevance problem in default reasoning: rational closure and conditional entailment.", "title": "Interpolative Reasoning with Default Rules"}, {"url": "https://www.ijcai.org/Abstract/13/166", "abstract": "Update semantics for Answer-Set Programming assign models to sequences of answer-set programs which result from the iterative process of updating programs by programs. Each program in the sequence represents an update of the preceding ones. One of the enduring problems in this context is state condensing, or the problem of determining a single logic program that faithfully represents the sequence of programs. Such logic program should 1) be written in the same alphabet, 2) have the same stable models, and 3) be equivalent to the sequence of programs when subject to further updates. It has been known for more than a decade that update semantics easily lead to non-minimal stable models, so an update sequence cannot be represented by a single non-disjunctive program. On the other hand, more expressive classes of programs were never considered, mainly because it was not clear how they could be updated further. In this paper we solve the state condensing problem for two foundational rule update semantics, using nested logic programs. Furthermore, we also show that disjunctive programs with default negation in the head can be used for the same purpose.", "title": "On Condensing a Sequence of Updates in Answer-Set Programming"}, {"url": "https://www.ijcai.org/Abstract/13/167", "abstract": "Nominal schemas have recently been introduced as a new approach for the integration of DL-safe rules into the Description Logic framework. The efficient processing of knowledge bases with nominal schemas remains, however, challenging. We address this by extending the well-known optimisation of absorption as well as the standard tableau calculus to directly handle the (absorbed) nominal schema axioms. We implement the resulting extension of standard tableau calculi in a novel reasoning system and we integrate further optimisations. In our empirical evaluation, we show the effect of these optimisations and we find that the proposed approach performs well even when compared to other DL reasoners with dedicated rule support.", "title": "Nominal Schema Absorption"}, {"url": "https://www.ijcai.org/Abstract/13/168", "abstract": "Qualitative representations of spatial knowledge have been widely studied and a variety of frameworks are used to express relationships between static regions. Dynamic regions present a much greater challenge, but are important in practical applications such as describing crowds of people moving over time. Previous work has analysed changes as regions merge and split and as new regions are created and existing ones disappear. We present a novel framework for the qualitative description of spatial regions based on two levels of granularity. Introducing granularity yields significantly more informative qualitative descriptions than are available from a single level of detail. The formal model represents a region, which may have multiple components, as a bipartite graph where the nodes are the components of the region at a fine level of detail and at a coarse level. The edges of the graph model the way that a component in the coarse view can be made up of parts of components at the more detailed level. We show that all graphs of this form (except for some degenerate cases) can be realized as regions in a discrete space of pixels, and we develop a theory of relations between these graphs to model the dynamic behaviour of regions.", "title": "Granular Description of Qualitative Change"}, {"url": "https://www.ijcai.org/Abstract/13/169", "abstract": "Answer set programming is the most appreciated framework for non-monotonic reasoning. Stable model semantics, as the semantics behind this success, has been subject to many extensions. The two main such extensions are equilibrium models and FLP semantics. Despite their very interesting foundations, they both have two problems: they cannot guarantee either minimality, or rationality of their intended models. That is, both these semantics allow models in which some atoms are self-justified (i.e., the only possible reason for including those atoms in the model are those atoms themselves). Present paper extends stable model semantics to the full propositional language while guaranteeing both properties above. Our extension is called supported because it guarantees the existence of non-circular justifications for all atoms in a supported model. These goals are achieved through a form of completion in intuitionistic logic. We also discuss how supported models relate to other semantics for non-monotonic reasoning such as equilibrium models. Finally, we discuss the complexity of reasoning about supported models and show that the complexity of brave/cautious reasoning in supported semantics remains as before, i.e., the rationality property comes for no additional cost.", "title": "A Rational Extension of Stable Model Semantics to the Full Propositional Language"}, {"url": "https://www.ijcai.org/Abstract/13/170", "abstract": "Querying large databases while taking ontologies into account is currently a very active domain research. In this paper, we consider ontologies described by existential rules (also known as Datalog+/-), a framework that generalizes lightweight description logics. A common approach is to rewrite a conjunctive query w.r.t an ontology into a union of conjunctive queries (UCQ) which can be directly evaluated against a database. However, the practicability of this approach is questionable due to 1) the weak expressivity of classes for which efficient rewriters have been implemented 2) the large size of optimal rewritings using UCQ. We propose to use semi-conjunctive queries (SCQ), which are a restricted form of positive existential formulas, and compute sound and complete rewritings, which are union of SCQ (USCQ). A novel algorithm for query rewriting, Compact, is presented. It computes sound and complete rewritings for large classes of ontologies. First experiments show that USCQ are both efficiently computable and more efficiently evaluable than their equivalent UCQ.", "title": "Compact Rewritings for Existential Rules"}, {"url": "https://www.ijcai.org/Abstract/13/171", "abstract": "Projection in the situation calculus refers to answering queries about the future evolutions of the modeled domain, while progression refers to updating the logical representation of the initial state so that it reflects the changes due to an executed action. In the general case projection is not decidable and progression may require second-order logic. In this paper we focus on a recent result about the decidability of projection and use it to drive results for the problem of progression. In particular we contribute with the following: (i) a major result showing that for a large class of intuitive action theories with bounded unknowns a first-order progression always exists and can be computed; (ii) a comprehensive classification of the known classes that can be progressed in first-order; (iii) a novel account of nondeterministic actions in the situation calculus.", "title": "A Classification of First-Order Progressable Action Theories in Situation Calculus"}, {"url": "https://www.ijcai.org/Abstract/13/172", "abstract": "In this paper, we provide a new axiomatization of the event-model-based Dynamic Epistemic Logic, based on the completeness proof method proposed in [Wang and Cao, 2013]. This axiomatization does not use any of the standard reduction axioms, but naturally captures the essence of the update product. We demonstrate the use of our new axiomatization and the corresponding proof techniques by three sets of results: characterization theorems of the update operations, representation theorems of the DEL-generatable epistemic temporal structures given a fixed event model, and a complete axiomatization of DEL on models with protocols.", "title": "An Alternative Axiomatization of DEL and Its Applications"}, {"url": "https://www.ijcai.org/Abstract/13/173", "abstract": "In his seminal work [Plaza, 1989], Plaza proposed the public announcement logic (PAL), which is considered as the pilot logic in the field of dynamic epistemic logic. In the same paper, Plaza also introduced an interesting \"know-value\" operator Kv and listed a few valid formulas of PAL+Kv. However, it is unknown that whether these formulas, on top of the axioms for PAL, completely axiomatize PAL+Kv. In this paper, we first give a negative answer to this open problem. Moreover, we generalize the Kv operator and show that in the setting of PAL, replacing the Kv operator with its generalized version does not increase the expressive power of the resulting logic. This suggests that we can simply use the more flexible generalization instead of the original PAL+Kv. As the main result, we give a complete proof system for PAL plus the generalized operator based on a complete axiomatization of epistemic logic with the same operator in the single-agent setting.", "title": "Knowing That, Knowing What, and Public Communication: Public Announcement Logic with Kv Operators"}, {"url": "https://www.ijcai.org/Abstract/13/174", "abstract": "Subset space logics have been introduced and studied as a framework for reasoning about a notion of effort in epistemic logic. The seminal Subset Space Logic (SSL) by Moss and Parikh modeled a single agent, and most work in this area has focused on different extensions of the language, or different model classes resulting from restrictions on subset spaces, while still keeping the single-agent assumption. In this paper we argue that the few existing attempts at multi-agent versions of SSL are unsatisfactory, and propose a new multi-agent subset space logic which is a natural extension of single-agent SSL. The main results are a sound and complete axiomatization of this logic, as well as an alternative and equivalent relational semantics.", "title": "Multi-Agent Subset Space Logic"}, {"url": "https://www.ijcai.org/Abstract/13/175", "abstract": "A new semantic forgetting for answer set programs (ASP), called SM-forgetting, is proposed in the paper.It distinguishes itself from the others in that it preserves not only skeptical and credulous consequences on unforgotten variables, but also strong equivalence \u2014 forgetting same variables in strongly equivalent logic programs has strongly equivalent results. The forgetting presents a positive answer to Gabbay, Pearce and Valverde's open question \u2014 if ASP has uniform interpolation property. We also investigate some properties, algorithm and computational complexities for the forgetting. It shows that computing the forgetting result is generally intractable even for Horn logic programs.", "title": "Forgetting for Answer Set Programs Revisited"}, {"url": "https://www.ijcai.org/Abstract/13/176", "abstract": "Many formalisms discussed in the literature on qualitative spatial reasoning are designed for expressing static spatial constraints only. However, dynamic situations arise in virtually all applications of these formalisms, which makes it necessary to study variants and extensions involving change. This paper presents a study on the computational complexity of qualitative change. More precisely, we discuss the reasoning task of finding a solution to a temporal sequence of static reasoning problems where this sequence is subject to additional transition constraints. Our focus is primarily on smoothness and continuity constraints: we show how such transitions can be defined as relations and expressed within qualitative constraint formalisms. Our results demonstrate that for point-based constraint formalisms the interesting fragments become NP-complete in the presence of continuity constraints, even if the satisfiability problem of its static descriptions is tractable.", "title": "Transition Constraints: A Study on the Computational Complexity of Qualitative Change"}, {"url": "https://www.ijcai.org/Abstract/13/177", "abstract": "The behavior composition problem involves the automatic synthesis of a controller able to \"realize\" (i.e., implement) a desired target behavior specification by suitably coordinating a set of already available behaviors. While the problem has been thoroughly studied, one open issue has resisted a principled solution: if the target specification is not fully realizable, is there a way to realize it \"at best\"? In this paper we answer positively, by showing that there exists a unique supremal realizable target behavior satisfying the specification. More importantly we give an effective procedure to compute such a target. Then, we introduce exogenous events, and show that the supremal can again be computed, though this time, into two variants, depending on the ability to observe such events.", "title": "Supremal Realizability of Behaviors with Uncontrollable Exogenous Events"}, {"url": "https://www.ijcai.org/Abstract/13/178", "abstract": "The task of explanatory diagnosis conjectures actions to explain observations.This is a common task in real life and an essential ability of intelligent agents.It becomes more complicated in multi-agent scenarios, since agents' actions may be partially observable to other agents, and observations might involve agents' knowledge about the world or other agents' knowledge or even common knowledge of a group of agents.For example, we might want to explain the observation that p does not hold,but Ann believes p, or the observation that Ann, Bob, and Carl commonly believe p. In this paper, we formalize the multi-agent explanatory diagnosis task in the framework of dynamic epistemic logic, where Kripke models of actions are used to represent agents' partial observability of actions. Since this task is undecidable in general, we identify important decidable fragments via techniques of reducing the potentially infinite search spaces to finite ones of epistemic states or action sequences.", "title": "Multi-Agent Epistemic Explanatory Diagnosis via Reasoning about Actions"}, {"url": "https://www.ijcai.org/Abstract/13/179", "abstract": "In the area of Description Logics the least common subsumer (lcs) and the most specific concept (msc) are inferences that generalize a set of concepts or an individual, respectively, into a single concept. If computed w.r.t. a general EL-TBox neither the lcs nor the msc need to exist. So far in this setting no exact conditions for the existence of lcs- or msc-concepts are known. This paper provides necessary and sufficient conditions for the existence of these two kinds of concepts. For the lcs of a fixed number of concepts and the msc we show decidability of the existence in PTime and polynomial bounds on the maximal role-depth of the lcs- and msc-concepts. This bound allows to compute the lcs and the msc, respectively.", "title": "Most Specific Generalizations w.r.t. General EL-TBoxes"}, {"url": "https://www.ijcai.org/Abstract/13/180", "abstract": "In this paper, the fixed point semantics developed in [Lobo et al., 1992] is generalized to disjunctive logic programs with default negation and over arbitrary structures, and proved to coincide with the stable model semantics. By using the tool of ultraproducts, a preservation theorem, which asserts that a disjunctive logic program without default negation is bounded with respect to the proposed semantics if and only if it has a first-order equivalent, is then obtained. For the disjunctive logic programs with default negation, a sufficient condition assuring the first-order expressibility is also proposed.", "title": "First-Order Expressibility and Boundedness of Disjunctive Logic Programs"}, {"url": "https://www.ijcai.org/Abstract/13/181", "abstract": "In the AGM framework [Alchourron and Makinson, 1985], a revision function can be defined directly through constructions like systems of spheres, epistemic entrenchment, etc., or indirectly through a contraction operation via the Levi identity. A recent trend is to construct AGM style contraction and revision functions that operate under Horn logic. A direct construction of Horn revision is given in [Delgrande and Peppas, 2011]. However, it is unknown whether Horn revision can be defined indirectly from Horn contraction. In this paper, we address this problem by obtaining a model-based Horn revision through the model-based Horn contraction studied in [Zhuang and Pagnucco, 2012]. Our result shows that, under proper restrictions, Horn revision is definable through Horn contraction via the Levi identity.", "title": "Definability of Horn Revision from Horn Contraction"}, {"url": "https://www.ijcai.org/Abstract/13/183", "abstract": "Bayesian approaches to preference learning using Gaussian Processes(GPs) are attractive due to their ability to explicitly model uncertainty in users' latent utility functions; unfortunately existing techniques have cubic time complexity in the number of users, which renders this approach intractable for collaborative preference learning over a large user base. Exploiting the observation that user populations often decompose into communities of shared preferences, we model user preferences as an infinite Dirichlet Process (DP) mixture of communities and learn (a) the expected number of preference communities represented in the data, (b) a GP-based preference model over items tailored to each community, and (c) the mixture weights representing each user's fraction of community membership. This results in a learning and inference process that scales linearly in the number of users rather than cubicly and additionally provides the ability to analyze individual community preferences and their associated members. We evaluate our approach on a variety of preference data sources including Amazon Mechanical Turk showing that our method is more scalable and as accurate as previous GP-based preference learning work.", "title": "Learning Community-Based Preferences via Dirichlet Process Mixtures of Gaussian Processes"}, {"url": "https://www.ijcai.org/Abstract/13/184", "abstract": "We present a novel approach for multilabel classification based on an ensemble of Bayesian networks. The class variables are connected by a tree; each model of the ensemble uses a different class as root of the tree. We assume the features to be conditionally independent given the classes, thus generalizing the naive Bayes assumption to the multiclass case. This assumption allows us to optimally identify the correlations between classes and features; such correlations are moreover shared across all models of the ensemble. Inferences are drawn from the ensemble via logarithmic opinion pooling. To minimize Hamming loss, we compute the marginal probability of the classes by running standard inference on each Bayesian network in the ensemble, and then pooling the inferences. To instead minimize the subset 0/1 loss, we pool the joint distributions of each model and cast the problem as a MAP inference in the corresponding graphical model. Experiments show that the approach is competitive with state-of-the-art methods for multilabel classification.", "title": "An Ensemble of Bayesian Networks for Multilabel Classification"}, {"url": "https://www.ijcai.org/Abstract/13/185", "abstract": "With information about the world implicitly embedded in complex, high-dimensional neural population responses, the brain must perform some sort of statistical inference on a large scale to form hypotheses about the state of the environment. This ability is, in part, acquired after birth and often with very little feedback to guide learning. This is a very difficult learning problem considering the little information about the meaning of neural responses available at birth. In this paper, we address the question of how the brain might solve this problem: We present an unsupervised artificial neural network algorithm which takes from the self-organizing map (SOM) algorithm the ability to learn a latent variable model from its input. We extend the SOM algorithm so it learns about the distribution of noise in the input and computes probability density functions over the latent variables. The algorithm represents these probability density functions using population codes. This is done with very few assumptions about the distribution of noise. Our simulations indicate that our algorithm can learn to perform similar to a maximum likelihood estimator with the added benefit of requiring no a-priori knowledge about the input and computing not only best hypotheses, but also probabilities for alternatives.", "title": "Self-Organized Neural Learning of Statistical Inference from High-Dimensional Data"}, {"url": "https://www.ijcai.org/Abstract/13/186", "abstract": "We present a study regarding basic level of concepts in conceptual categorization. The basic level of concepts is an important phenomenon studied in the psychology of concepts. We propose to utilize this phenomenon in formal concept analysis to select important formal concepts. Such selection is critical because, as is well known, the number of all concepts extracted from data is usually large. We review and formalize the main existing psychological approaches to basic level which are presented only informally and are not related to any particular formal model of concepts in the psychological literature. We argue and demonstrate by examples that basic level concepts may be regarded as interesting, informative formal concepts from a user viewpoint. Interestingly, our formalization and experiments reveal previously unknown relationships between the existing approaches to basic level. Thus,we argue that a formalization of basic level in the framework of formal concept analysis is beneficial for the psychological investigations themselves because it helps put them on a solid, formal ground.", "title": "Basic Level in Formal Concept Analysis: Interesting Concepts and Psychological Ramifications"}, {"url": "https://www.ijcai.org/Abstract/13/187", "abstract": "In this paper, we propose a novel robust and pragmatic feature selection approach. Unlike those sparse learning based feature selection methods which tackle the approximate problem by imposing sparsity regularization in the objective function, the proposed method only has one l2,1-norm loss term with an explicit l2,0-Norm equality constraint. An efficient algorithm based on augmented Lagrangian method will be derived to solve the above constrained optimization problem to find out the stable local solution. Extensive experiments on four biological datasets show that although our proposed model is not a convex problem, it outperforms the approximate convex counterparts and state of the art feature selection methods evaluated in terms of classification accuracy by two popular classifiers. What is more, since the regularization parameter of our method has the explicit meaning, i.e. the number of feature selected, it avoids the burden of tuning the parameter, making it a pragmatic feature selection method.", "title": "Exact Top-k Feature Selection via l2,0-Norm Constraint"}, {"url": "https://www.ijcai.org/Abstract/13/188", "abstract": "Pose variation is one of the challenging factors for face recognition. In this paper, we propose a novel cross-pose face recognition method named as Regularized Latent Least Square Regression (RLLSR). The basic assumption is that the images captured under different poses of one person can be viewed as pose-specific transforms of a single ideal object. We treat the observed images as regressor, the ideal object as response, and then formulate this assumption in the least square regression framework, so as to learn the multiple pose-specific transforms. Specifically, we incorporate some prior knowledge as two regularization terms into the least square approach: 1) the smoothness regularization, as the transforms for nearby poses should not differ too much; 2) the local consistency constraint, as the distribution of the latent ideal objects should preserve the geometric structure of the observed image space. We develop an alternating algorithm to simultaneously solve for the ideal objects of the training individuals and a set of pose-specific transforms. The experimental results on the Multi-PIE dataset demonstrate the effectiveness of the proposed method and superiority over the previous methods.", "title": "Regularized Latent Least Square Regression for Cross Pose Face Recognition"}, {"url": "https://www.ijcai.org/Abstract/13/189", "abstract": "Tensors are increasingly common in several areas such as data mining, computer graphics, and computer vision. Tensor clustering is a fundamental tool for data analysis and pattern discovery. However, there usually exist outlying data points in real world datasets, which will reduce the performance of clustering. This motivates us to develop a tensor clustering algorithm that is robust to the outliers. In this paper, we propose an algorithm of Robust Tensor Clustering (RTC). The RTC firstly finds a lower rank approximation of the original tensor data using a L1 norm optimization function. Because the L1 norm doesn't exaggerate the effect of outliers compared with L2 norm, the minimization of the L1 norm approximation function makes RTC robust to outliers. Then we compute the HOSVD decomposition of this approximate tensor to obtain the final clustering results. Different from the traditional algorithm solving the approximation function with a greedy strategy, we utilize a non-greedy strategy to obtain a better solution. Experiments demonstrate that RTC has better performance than the state-of-the-art algorithms and is more robust to outliers.", "title": "Robust Tensor Clustering with Non-Greedy Maximization"}, {"url": "https://www.ijcai.org/Abstract/13/190", "abstract": "The ability to cluster high-dimensional categorical data is essential for many machine learning applications such as bioinfomatics. Currently, central clustering of categorical data is a difficult problem due to the lack of a geometrically interpretable definition of a cluster center. In this paper, we propose a novel kernel-density-based definition using a Bayes-type probability estimator. Then, a new algorithm called k-centers is proposed for central clustering of categorical data, incorporating a new feature weighting scheme by which each attribute is automatically assigned with a weight measuring its individual contribution for the clusters. Experimental results on real-world data show outstanding performance of the proposed algorithm, especially in recognizing the biological patterns in DNA sequences.", "title": "Central Clustering of Categorical Data with Automated Feature Weighting"}, {"url": "https://www.ijcai.org/Abstract/13/191", "abstract": "In this paper, we propose a general dimensionality reduction method for data generated from a very broad family of distributions and nonlinear functions based on the generalized linear model, called Generalized Linear Principal Component Analysis (GLPCA). Data of different domains often have very different structures. These data can be modeled by different distributions and reconstruction functions. For example, real valued data can be modeled by the Gaussian distribution with a linear reconstruction function, whereas binary valued data may be more appropriately modeled by the Bernoulli distribution with a logit or probit function. Based on general linear models, we propose a unified framework for extracting features from data of different domains. A general optimization algorithm based on natural gradient ascent on distribution manifold is proposed for obtaining the maximum likelihood solutions. We also present some specific algorithms derived from this framework to deal with specific data modeling problems such as document modeling. Experimental results of these algorithms on several data sets are shown for the validation of GLPCA.", "title": "Dimensionality Reduction with Generalized Linear Models"}, {"url": "https://www.ijcai.org/Abstract/13/192", "abstract": "Relational topic models have shown promise on analyzing document network structures and discovering latent topic representations. This paper presents three extensions: 1) unlike the common link likelihood with a diagonal weight matrix that allows the-same-topic interactions only, we generalize it to use a full weight matrix that captures all pairwise topic interactions and is applicable to asymmetric networks; 2) instead of doing standard Bayesian inference, we perform regularized Bayesian inference with a regularization parameter to deal with the imbalanced link structure issue in common real networks; and 3) instead of doing variational approximation with strict mean-field assumptions, we present a collapsed Gibbs sampling algorithm for the generalized relational topic models without making restricting assumptions. Experimental results demonstrate the significance of these extensions on improving the prediction performance, and the time efficiency can be dramatically improved with a simple fast approximation method.", "title": "Generalized Relational Topic Models with Data Augmentation"}, {"url": "https://www.ijcai.org/Abstract/13/193", "abstract": "A serious and ubiquitous issue in machine learning is the lack of sufficient training data in a domain of interest. Domain adaptation isan effective approach to dealing with this problem by transferring information or models learned from related, albeit distinct, domains to the target domain. We develop a novel domain adaptation method for text document classification under the framework of Non-negative Matrix Factorization. Two key ideas of our method are to construct a latent topic space where a topic is decomposed into common words shared by all domains and words specific to individual domains, and then to establish associations between words in different domains through the common words as a bridge for knowledge transfer. The correspondence between cross-domain topics leads to more coherent distributions of source and target domains in the new representation while preserving the predictive power. Our new method outperformed several state-of-the-art domain adaptation methods on several benchmark datasets.", "title": "Domain Adaptation with Topical Correspondence Learning"}, {"url": "https://www.ijcai.org/Abstract/13/194", "abstract": "Most of the algorithms for inverse reinforcement learning (IRL) assume that the reward function is a linear function of the pre-defined state and action features. However, it is often difficult to manually specify the set of features that can make the true reward function representable as a linear function. We propose a Bayesian nonparametric approach to identifying useful composite features for learning the reward function. The composite features are assumed to be the logical conjunctions of the predefined atomic features so that we can represent the reward function as a linear function of the composite features. We empirically show that our approach is able to learn composite features that capture important aspects of the reward function on synthetic domains, and predict taxi drivers' behaviour with high accuracy on a real GPS trace dataset.", "title": "Bayesian Nonparametric Feature Construction for Inverse Reinforcement Learning"}, {"url": "https://www.ijcai.org/Abstract/13/195", "abstract": "In many problem settings, for example on graph domains, online learning algorithms on streams of data need to respect strict time constraints dictated by the throughput on which the data arrive. When only a limited amount of memory (budget) is available, a learning algorithm will eventually need to discard some of the information used to represent the current solution, thus negatively affecting its classification performance. More importantly, the overhead due to budget management may significantly increase the computational burden of the learning algorithm. In this paper we present a novel approach inspired by the Passive Aggressive and the Lossy Counting algorithms. Our algorithm uses a fast procedure for deleting the less influential features. Moreover, it is able to estimate the weighted frequency of each feature and use it for prediction.", "title": "A Lossy Counting Based Approach for Learning on Streams of Graphs on a Budget"}, {"url": "https://www.ijcai.org/Abstract/13/196", "abstract": "Suppose a learner is faced with a domain of problems about which it knows nearly nothing. It does not know the distribution of problems, the space of solutions is not smooth, and the reward signal is uninformative, providing perhaps a few bits of information but not enough to steer the learner effectively. How can such a learner ever get off the ground? A common intuition is that if the solutions to these problems share a common structure, and the learner can solve some simple problems by brute force, it should be able to extract useful components from these solutions and, by composing them, explore the solution space more efficiently. Here, we formalize this intuition, where the solution space is that of typed functional programs and the gained information is stored as a stochastic grammar over programs. We propose an iterative procedure for exploring such spaces: in the first step of each iteration, the learner explores a finite subset of the domain, guided by a stochastic grammar; in the second step, the learner compresses the successful solutions from the first step to estimate a new stochastic grammar. We test this procedure on symbolic regression and Boolean circuit learning and show that the learner discovers modular concepts for these domains. Whereas the learner is able to solve almost none of the posed problems in the procedure's first iteration, it rapidly becomes able to solve a large number by gaining abstract knowledge of the structure of the solution space.", "title": "Bootstrap Learning via Modular Concept Discovery"}, {"url": "https://www.ijcai.org/Abstract/13/197", "abstract": "Automatically identifying informative reviews is increasingly important given the rapid growth of user generated reviews on sites like Amazon and TripAdvisor. In this paper, we describe and evaluate techniques for identifying and recommending helpful product reviews using a combination of review features, including topical and sentiment information, mined from a review corpus.", "title": "Topic Extraction from Online Reviews for Classification and Recommendation"}, {"url": "https://www.ijcai.org/Abstract/13/198", "abstract": "Nonnegative Matrix Tri-factorization (NMTF) and its graph regularized extensions have been widely used for co-clustering task to group data points and features simultaneously. However existing methods are sensitive to noises and outliers which are because of the squared loss function is used to measure the quality of data reconstruction and graph regularization. In this paper, we extend GNMTF by introducing a sparse outlier matrix into the data reconstruction function and applying the l1 norm to measure graph dual regularization errors, which leads to a novel Robust Co-Clustering (RCC) method. Accordingly, RCC is expected to obtain a more faithful approximation to the data recovered from sparse outliers, and achieve robust regularization by reducing the regularization errors of unreliable graphs via '1norm. To solve the optimization problem of RCC, an alternating iterative algorithm is provided and its convergence is also proved. We also show the connection between the sparse outlier matrix in data reconstruction function and the robust Huber M-estimator. Experimental results on real-world data sets show that our RCC consistently outperforms the other algorithms in terms of clustering performance, which validates the effectiveness and robustness of the proposed approach.", "title": "Towards Robust Co-Clustering"}, {"url": "https://www.ijcai.org/Abstract/13/199", "abstract": "During the past decade, finite mixture modeling has become a well-established technique in data analysis and clustering. This paper focus on developing a variational inference framework to learn finite Beta-Liouville mixture models that have been proposed recently as an efficient way for proportional data clustering. In contrast to the conventional expectation maximization (EM) algorithm, commonly used for learning finite mixture models, the proposed algorithm has the advantages that it is more efficient from a computational point of view and by preventing over- and under-fitting problems. Moreover, the complexity of the mixture model (i.e. the number of components) can be determined automatically and simultaneously with the parameters estimation in a closed form as part of the Bayesian inference procedure. The merits of the proposed approach are shown using both artificial data sets and two interesting and challenging real applications namely dynamic textures clustering and facial expression recognition.", "title": "Learning Finite Beta-Liouville Mixture Models via Variational Bayes for Proportional Data Clustering"}, {"url": "https://www.ijcai.org/Abstract/13/200", "abstract": "Cepstral features have been widely used in audio applications. Domain knowledge has played an important role in designing different types of cepstral features proposed in the literature. In this paper, we present a novel approach for learning optimized cepstral features directly from audio data to better discriminate between different categories of signals in classification tasks. We employ multi-layer feedforward neural networks to model the cepstral feature extraction process. The network weights are initialized to replicate a reference cepstral feature like the mel frequency cepstral coefficient. We then propose a embedded approach that integrates feature learning with the training of a support vector machine (SVM) classifier. A single optimization problem is formulated where the feature and classifier variables are optimized simultaneously so as to refine the initial features and minimize the classification risk. Experimental results have demonstrated the effectiveness of the proposed feature learning approach, outperforming competing methods by a large margin on benchmark data.", "title": "Optimizing Cepstral Features for Audio Classification"}, {"url": "https://www.ijcai.org/Abstract/13/201", "abstract": "Most studies were devoted to the design of efficient algorithms and the evaluation and application on diverse ranking problems, whereas few work has been paid to the theoretical studies on ranking learnability. In this paper, we study the relation between uniform convergence, stability and learnability of ranking. In contrast to supervised learning where the learnability is equivalent to uniform convergence, we show that the ranking uniform convergence is sufficient but not necessary for ranking learnability with AERM, and we further present a sufficient condition for ranking uniform convergence with respect to bipartite ranking loss. Considering the ranking uniform convergence being unnecessary for ranking learnability, we prove that the ranking average stability is a necessary and sufficient condition for ranking learnability.", "title": "Uniform Convergence, Stability and Learnability for Ranking Problems"}, {"url": "https://www.ijcai.org/Abstract/13/202", "abstract": "Many information gathering problems require determining the set of points, for which an unknown function takes value above or below some given threshold level. We formalize this task as a classification problem with sequential measurements, where the unknown function is modeled as a sample from a Gaussian process (GP). We propose LSE, an algorithm that guides both sampling and classification based on GP-derived confidence bounds, and provide theoretical guarantees about its sample complexity. Furthermore, we extend LSE and its theory to two more natural settings: (1) where the threshold level is implicitly defined as a percentage of the (unknown) maximum of the target function and (2) where samples are selected in batches. We evaluate the effectiveness of our proposed methods on two problems of practical interest, namely autonomous monitoring of algal populations in a lake environment and geolocating network latency.", "title": "Active Learning for Level Set Estimation"}, {"url": "https://www.ijcai.org/Abstract/13/203", "abstract": "Creating descriptors for trajectories has many applications in robotics/human motion analysis and video copy detection. Here, we propose a novel descriptor for 2D trajectories: Histogram of Oriented Displacements (HOD). Each displacement in the trajectory votes with its length in a histogram of orientation angles. 3D trajectories are described by the HOD of their three projections. We use HOD to describe the 3D trajectories of body joints to recognize human actions, which is a challenging machine vision task, with applications in human-robot/machine interaction, interactive entertainment, multimedia information retrieval, and surveillance. The descriptor is fixed-length, scale-invariant and speed-invariant. Experiments on MSR-Action3D and HDM05 datasets show that the descriptor outperforms the state-of-the-art when using off-the-shelf classification tools.", "title": "Histogram of Oriented Displacements (HOD): Describing Trajectories of Human Joints for Action Recognition"}, {"url": "https://www.ijcai.org/Abstract/13/204", "abstract": "We propose a multi-prototype-based algorithm for online learning of soft pairwise-preferences over labels. The algorithm learns soft label preferences via minimization of the proposed soft rank-loss measure, and can learn from total orders as well as from various types of partial orders. The soft pairwise preference algorithm outputs are further aggregated to produce a total label ranking prediction using a novel aggregation algorithm that outperforms existing aggregation solutions. Experiments on synthetic and real-world data demonstrate state-of-the-art performance of the proposed model.", "title": "Multi-Prototype Label Ranking with Novel Pairwise-to-Total-Rank Aggregation"}, {"url": "https://www.ijcai.org/Abstract/13/205", "abstract": "We introduce MiningZinc, a general framework for constraint-based pattern mining, one of the most popular tasks in data mining. MiningZinc consists of two key components: a language component and a toolchain component. The language allows for high-level and natural modeling of mining problems, such that MiningZinc models closely resemble definitions found in the data mining literature. It is inspired by the Zinc family of languages and systems and supports user-defined constraints and optimization criteria. The toolchain allows for finding solutions to the models. It ensures the solver independence of the language and supports both standard constraint solvers and specialized data mining systems. Automatic model transformations enable the efficient use of different solvers and systems. The combination of both components allows one to rapidly model constraint-based mining problems and execute these with a wide variety of methods. We demonstrate this experimentally for a number of well-known solvers and data mining tasks.", "title": "MiningZinc: A Modeling Language for Constraint-based Mining"}, {"url": "https://www.ijcai.org/Abstract/13/206", "abstract": "Multi-label classification is a critical problem in many areas of data analysis such as image labeling and text categorization. In this paper we propose a probabilistic multi-label classification model based on novel sparse feature learning. By employing an individual sparsity inducing \u21131-norm and a group sparsity inducing \u21132,1-norm, the proposed model has the capacity of capturing both label interdependencies and common predictive model structures. We formulate this sparse norm regularized learning problem as a non-smooth convex optimization problem, and develop a fast proximal gradient algorithm to solve it for an optimal solution. Our empirical study demonstrates the efficacy of the proposed method on a set of multi-label tasks given a limited number of labeled training instances.", "title": "Probabilistic Multi-Label Classification with Sparse Feature Learning"}, {"url": "https://www.ijcai.org/Abstract/13/207", "abstract": "Supervised feature selection determines feature relevance by evaluating feature's correlation with the classes. Joint minimization of a classifier's loss function and an l2,1-norm regularization has been shown to be effective for feature selection. However, the appropriate feature subset learned from different classifiers' loss function may be different. Less effort has been made on improving the performance of feature selection by the ensemble of different classifiers' criteria and take advantages of them. Furthermore, for the cases when only a few labeled data per class are available, over-fitting would be a potential problem and the performance of each classifier is restrained. In this paper, we add a joint l2,1-norm on multiple feature selection matrices to ensemble different classifiers' loss function into a joint optimization framework. This added co-regularization term has twofold role in enhancing the effect of regularization for each criterion and uncovering common irrelevant features. The problem of over-fitting can be alleviated and thus the performance of feature selection is improved. Extensive experiment on different data types demonstrates the effectiveness of our algorithm.", "title": "Co-Regularized Ensemble for Feature Selection"}, {"url": "https://www.ijcai.org/Abstract/13/208", "abstract": "Road traffic prediction is a critical component in modern smart transportation systems. It provides the basis for traffic management agencies to generate proactive traffic operation strategies for alleviating congestion. Existing work on near-term traffic prediction (forecasting horizons in the range of 5 minutes to 1 hour) relies on the past and current traffic conditions. However, once the forecasting horizon is beyond 1 hour, i.e., in longer-term traffic prediction, these techniques do not work well since additional factors other than the past and current traffic conditions start to play important roles. To address this problem, in this paper, for the first time, we examine whether it is possible to use the rich information in online social media to improve longer-term traffic prediction. To this end, we first analyze the correlation between traffic volume and tweet counts with various granularities. Then we propose an optimization framework to extract traffic indicators based on tweet semantics using a transformation matrix, and incorporate them into traffic prediction via linear regression. Experimental results using traffic and Twitter data originated from the San Francisco Bay area of California demonstrate the effectiveness of our proposed framework.", "title": "Improving Traffic Prediction with Tweet Semantics"}, {"url": "https://www.ijcai.org/Abstract/13/209", "abstract": "Recent advances in Bayesian reinforcement learning (BRL) have shown that Bayes-optimality is theoretically achievable by modeling the environment's latent dynamics using Flat-Dirichlet-Multinomial (FDM) prior. In self-interested multi-agent environments, the transition dynamics are mainly controlled by the other agent's stochastic behavior for which FDM's independence and modeling assumptions do not hold. As a result, FDM does not allow the other agent's behavior to be generalized across different states nor specified using prior domain knowledge. To overcome these practical limitations of FDM, we propose a generalization of BRL to integrate the general class of parametric models and model priors, thus allowing practitioners' domain knowledge to be exploited to produce a fine-grained and compact representation of the other agent's behavior. Empirical evaluation shows that our approach outperforms existing multi-agent reinforcement learning algorithms.", "title": "A General Framework for Interacting Bayes-Optimally with Self-Interested Agents Using Arbitrary Parametric Model and Model Prior"}, {"url": "https://www.ijcai.org/Abstract/13/210", "abstract": "With the rapid proliferation of social media, more and more people freely express their opinions (or comments) on news, products, and movies through online services such as forums, discussion groups, and microblogs. Those comments may be concerned with different aspects (topics) of the target Web document (e.g., a news page).It would be interesting to align the social comments to the corresponding subtopics contained in the Web document. In this paper, we propose a novel framework that is able to automatically detect the subtopics from a given Web document, and also align the associated social comments with the detected subtopics. This provides a new view of the Web standard document and its associated user generated content through topics, which facilitates the readers to quickly focus on those hot topics or grasp topics that they are interested in. Extensive experiments show that our proposed framework significantly outperforms the existing state-of-the-art methods in social content alignment.", "title": "What Users Care About: A Framework for Social Content Alignment"}, {"url": "https://www.ijcai.org/Abstract/13/211", "abstract": "Side information is highly useful in the learning of a nonparametric kernel matrix. However, this often leads to an expensive semidefinite program (SDP). In recent years, a umber of dedicated solvers have been proposed. Though much better than off-the-shelf SDP solvers, they still cannot scale to large data sets. In this paper, we propose a novel solver based on the alternating direction method of multipliers (ADMM). The key idea is to use a low-rank decomposition of the kernel matrix K = VT U, with the constraint that V = U. The resultant optimization problem, though non-convex, has favorable convergence properties and can be efficiently solved without requiring eigen-decomposition in each iteration. Experimental results on a number of real-world data sets demonstrate that the proposed method is as accurate as directly solving the SDP, but can be one to two orders of magnitude faster.", "title": "Efficient Kernel Learning from Side Information Using ADMM"}, {"url": "https://www.ijcai.org/Abstract/13/212", "abstract": "In many real world scenarios, active learning methods are used to select the most informative points for labeling to reduce the expensive human action. One direction for active learning is selecting the most representative points, ie., selecting the points that other points can be approximated by linear combination of the selected points. However, these methods fails to consider the local geometrical information of the data space. In this paper, we propose a novel framework named Active Learning via Neighborhood Reconstruction (ALNR) by taking into account the locality information directly during the selection. Specifically, for the linear reconstruction of target point, the nearer neighbors should have a greater effect and the selected points distant from the target point should be penalized severely. We further develop an efficient two-stage iterative procedure to solve the final optimization problem. Our empirical study shows encouraging results of the proposed algorithms in comparison to other state-of-the-art active learning algorithms on both synthetic and real visual data sets.", "title": "Active Learning via Neighborhood Reconstruction"}, {"url": "https://www.ijcai.org/Abstract/13/213", "abstract": "Hash function learning has been recently received more and more attentions in fast search for large scale data. However, existing popular learning based hashing methods are batch-based learning models and thus incur large scale computational problem for learning an optimal model on a large scale of labelled data and cannot handle data which comes sequentially. In this paper, we address the problem by developing an online hashing learning algorithm to get hashing model accommodate to each new pair of data. At the same time the new updated hash model is penalized by the last learned model in order to retain important information learned in previous rounds. We also derive a tight bound for the cumulative loss of our proposed online learning algorithm. The experimental results demonstrate superiority of the proposed online hashing model on searching both metric distance neighbors and semantical similar neighbors in the experiments.", "title": "Online Hashing"}, {"url": "https://www.ijcai.org/Abstract/13/214", "abstract": "In traditional topic models such as LDA, a word is generated by choosing a topic from a collection. However, existing topic models do not identify different types of topics in a document, such as topics that represent the content and topics that represent the sentiment. In this paper, our goal is to discover such different types of topics, if they exist. We represent our model as several parallel topic models (called topic factors), where each word is generated from topics from these factors jointly. Since the latent membership of the word is now a vector, the learning algorithms become challenging. We show that using a variational approximation still allows us to keep the algorithm tractable. Our experiments over several datasets show that our approach consistently outperforms many classic topic models while also discovering fewer, more meaningful, topics.", "title": "Discovering Different Types of Topics: Factored Topic Models"}, {"url": "https://www.ijcai.org/Abstract/13/215", "abstract": "We study Bayesian reinforcement learning (RL) as a solution of the exploration-exploitation dilemma. As full Bayesian planning is intractable except for special cases, previous work has proposed several approximation methods. However, these were often computationally expensive or limited to Dirichlet priors. In this paper, we propose a new algorithm that is fast and of polynomial time for near Bayesian optimal policy with any prior distributions that are not greatly misspecified. Perhaps even more interestingly, the proposed algorithm can naturally avoid being misled by incorrect beliefs, while effectively utilizing useful parts of prior information. It can work well even when an utterly misspecified prior is assigned. In that case, the algorithm will follow PAC-MDP behavior instead, if an existing PAC-MDP algorithm does so. The proposed algorithm naturally outperformed other algorithms compared with it on a standard benchmark problem.", "title": "Prior-Free Exploration Bonus for and beyond Near Bayes-Optimal Behavior"}, {"url": "https://www.ijcai.org/Abstract/13/216", "abstract": "Large-scale observational datasets are prevalent in many areas of research, including biomedical informatics, computational social science, and finance. However, our ability to use these data for decision-making lags behind our ability to collect and mine them. One reason for this is the lack of methods for inferring the causal impact of rare events. In cases such as the monitoring of continuous data streams from intensive care patients, social media, or finance, though, rare events may in fact be the most important ones \u2014 signaling critical changes in a patient's status or trading volume. While prior data mining approaches can identify or predict rare events, they cannot determine their impact, and probabilistic causal inference methods fail to handle inference with infrequent events. Instead, we develop a new approach to finding the causal impact of rare events that leverages the large amount of data available to infer a model of a system's functioning and evaluates how rare events explain deviations from usual behavior. Using simulated data, we evaluate the approach and compare it against others, demonstrating that it can accurately infer the effects of rare events.", "title": "Causal Inference with Rare Events in Large-Scale Time-Series Data"}, {"url": "https://www.ijcai.org/Abstract/13/217", "abstract": "We investigate an interactive teaching scenario, where a human teaches a robot symbols which abstract the geometric properties of objects. There are multiple motivations for this scenario: First, state-of-the-art methods for relational reinforcement learning demonstrate that we can learn and employ strongly generalizing abstract models with great success for goal-directed object manipulation. However, these methods rely on given grounded action and state symbols and raise the classical question: Where do the symbols come from? Second, existing research on learning from human-robot interaction has focused mostly on the motion level (e.g., imitation learning). However, if the goal of teaching is to enable the robot to autonomously solve sequential manipulation tasks in a goal-directed manner, the human should have the possibility to teach the relevant abstractions to describe the task and let the robot eventually leverage powerful relational RL methods. In this paper we formalize human-robot teaching of grounded symbols as an active learning problem, where the robot actively generates pick-and-place geometric situations that maximize its information gain about the symbol to be learned. We demonstrate that the learned symbols can be used by a robot in a relational RL framework to learn probabilistic relational rules and use them to solve object manipulation tasks in a goal-directed manner.", "title": "Active Learning for Teaching a Robot Grounded Relational Symbols"}, {"url": "https://www.ijcai.org/Abstract/13/218", "abstract": "Thresholding a measure in conditional independence (CI) tests using a fixed value enables learning and removing edges as part of learning a Bayesian network structure. However, the learned structure is sensitive to the threshold that is commonly selected: 1) arbitrarily; 2) irrespective of characteristics of the domain; and 3) fixed for all CI tests. We analyze the impact on mutual information \u2013 a CI measure \u2013 of factors, such as sample size, degree of variable dependence, and variables' cardinalities. Following, we suggest to adaptively threshold individual tests based on the factors. We show that adaptive thresholds better distinguish between pairs of dependent variables and pairs of independent variables and enable learning structures more accurately and quickly than when using fixed thresholds.", "title": "Adaptive Thresholding in Structure Learning of a Bayesian Network"}, {"url": "https://www.ijcai.org/Abstract/13/219", "abstract": "This paper presents a specialised Bayesian model for analysing the covariance of data that are observed in the form of matrices, which is particularly suitable for images. Compared to existing general-purpose covariance learning techniques, we exploit the fact that the variables are organised as an array with two sets of ordered indexes, which induces innate relationship between the variables. Specifically, we adopt a factorised structure for the covariance matrix. The covariance of two variables is represented by the product of the covariance of the two corresponding rows and that of the two columns. The factors, i.e. the row-wise and column-wise covariance matrices are estimated by Bayesian inference with sparse priors. Empirical study has been conducted on image analysis. The model first learns correlations between the rows and columns in an image plane. Then the correlations between individual pixels can be inferred by their locations. This scheme utilises the structural information of an image, and benefits the analysis when the data are damaged or insufficient.", "title": "A Bayesian Factorised Covariance Model for Image Analysis"}, {"url": "https://www.ijcai.org/Abstract/13/220", "abstract": "Graph based semi-supervised learning (GSSL) plays an important role in machine learning systems. The most crucial step in GSSL is graph construction. Although several interesting graph construction methods have been proposed in recent years, how to construct an effective graph is still an open problem. In this paper, we develop a novel approach to constructing graph, which is based on low-rank coding and $b$-matching constraint. By virtue of recent advances in low-rank subspace recovery theory, compact encoding using low-rank representation coefficients allows us to obtain a robust similarity metric between all pairs of samples. Meanwhile, the $b$-matching constraint helps in obtaining a sparse and balanced graph, which benefits label propagation in GSSL. We build a joint optimization model to learn low-rank codes and balanced graph simultaneously. After using a graph re-weighting strategy, we present a semi-supervised learning algorithm by incorporating our sparse and balanced graph with Gaussian harmonic function (GHF). Experimental results on the Extended YaleB, PIE, ORL and USPS databases demonstrate that our graph outperforms several state-of-the-art graphs, especially when the labeled samples are very scarce.", "title": "Low-Rank Coding with b-Matching Constraint for Semi-Supervised Classification"}, {"url": "https://www.ijcai.org/Abstract/13/221", "abstract": "Multi-label classification, where each instance is assigned to multiple categories, is a prevalent problem in data analysis. However, annotations of multi-label instances are typically more time-consuming or expensive to obtain than annotations of single-label instances. Though active learning has been widely studied on reducing labeling effort for single-label problems, current research on multi-label active learning remains in a preliminary state. In this paper, we first propose two novel multi-label active learning strategies, a max-margin prediction uncertainty strategy and a label cardinality inconsistency strategy, and then integrate them into an adaptive framework of multi-label active learning. Our empirical results on multiple multi-label data sets demonstrate the efficacy of the proposed active instance selection strategies and the integrated active learning approach.", "title": "Active Learning with Multi-Label SVM Classification"}, {"url": "https://www.ijcai.org/Abstract/13/222", "abstract": "Graph clustering has received growing attention in recent years as an important analytical technique, both due to the prevalence of graph data, and the usefulness of graph structures for exploiting intrinsic data characteristics.However, as graph data grows in scale, it becomes increasingly more challenging to identify clusters. In this paper we propose an efficient clustering algorithm for large-scale graph data using spectral methods. The key idea is to repeatedly generate a small number of \"supernodes\" connected to the regular nodes, in order to compress the original graph into a sparse bipartite graph. By clustering the bipartite graph using spectral methods, we are able to greatly improve efficiency without losing considerable clustering power. Extensive experiments show the effectiveness and efficiency of our approach.", "title": "Large-Scale Spectral Clustering on Graphs"}, {"url": "https://www.ijcai.org/Abstract/13/223", "abstract": "Recently, the low-cost Microsoft Kinect sensor, which can capture real-time high-resolution RGB and depth visual information, has attracted increasing attentions for a wide range of applications in computer vision. Existing techniques extract hand-tuned features from the RGB and the depth data separately and heuristically fuse them, which would not fully exploit the complementarity of both data sources. In this paper, we introduce an adaptive learning methodology to automatically extract (holistic) spatio-temporal features, simultaneously fusing the RGB and depth information, from RGBD video data for visual recognition tasks. We address this as an optimization problem using our proposed restricted graph-based genetic programming (RGGP) approach, in which a group of primitive 3D operators are first randomly assembled as graph-based combinations and then evolved generation by generation by evaluating on a set of RGBD video samples. Finally the best-performed combination is selected as the (near-)optimal representation for a pre-defined task. The proposed method is systematically evaluated on a new hand gesture dataset, SKIG, that we collected ourselves and the public MSRDailyActivity3D dataset, respectively. Extensive experimental results show that our approach leads to significant advantages compared with state-of-the-art handcrafted and machine-learned features.", "title": "Learning Discriminative Representations from RGB-D Video Data"}, {"url": "https://www.ijcai.org/Abstract/13/224", "abstract": "We present online nested expectation maximization for model-free reinforcement learning in a POMDP. The algorithm evaluates the policy only in the current learning episode, discarding the episode after the evaluation and memorizing the sufficient statistic, from which the policy is computed in closed-form. As a result, the online algorithm has a time complexity O(n) and a memory complexity O(1), compared to O(n2) and O(n) for the corresponding batch-mode algorithm, where $n$ is the number of learning episodes. The online algorithm, which has a provable convergence, is demonstrated on five benchmark POMDP problems.", "title": "Online Expectation Maximization for Reinforcement Learning in POMDPs"}, {"url": "https://www.ijcai.org/Abstract/13/225", "abstract": "We present a novel unsupervised data analysis method, Multi-feature Information Bottleneck (MfIB), which is an extension of the Information Bottleneck (IB). In comparison with the original IB, the proposed MfIB method can analyze the data simultaneously from multiple feature variables, which characterize the data from multiple cues. To verify the effectiveness of MfIB, we apply the corresponding MfIB algorithm to unsupervised image categorization. In our experiments, by taking into account multiple types of features, such as local shape, color and texture, the MfIB algorithm is found to be consistently superior to the original IB algorithm which takes only one source of features into consideration. Besides, the performance of MfIB algorithm is also superior to the state-of-the-art unsupervised image categorization methods.", "title": "The Multi-Feature Information Bottleneck with Application to Unsupervised Image Categorization"}, {"url": "https://www.ijcai.org/Abstract/13/226", "abstract": "Canonical correlation analysis (CCA) is a useful technique for measuring relationship between two sets of vector data. For paired tensor data sets, we propose a multilinear CCA (MCCA) method. Unlike existing multilinear variations of CCA, MCCA extracts uncorrelated features under two architectures while maximizing paired correlations. Through a pair of tensor-to-vector projections, one architecture enforces zero-correlation within each set while the other enforces zero-correlation between different pairs of the two sets. We take a successive and iterative approach to solve the problem. Experiments on matching faces of different poses show that MCCA outperforms CCA and 2D-CCA, while using much fewer features. In addition, the fusion of two architectures leads to performance improvement, indicating complementary information.", "title": "Learning Canonical Correlations of Paired Tensor Sets Via Tensor-to-Vector Projection"}, {"url": "https://www.ijcai.org/Abstract/13/227", "abstract": "This paper presents a novel semantic regularized matrix factorization method for learning descriptive visual bag-of-words (BOW) representation. Although very influential in image classification, the traditional visual BOW representation has one distinct drawback. That is, for efficiency purposes, this visual representation is often generated by directly clustering the low-level visual feature vectors extracted from local key points or regions, without considering the high-level semantics of images. In other words, this visual representation still suffers from the semantic gap and may lead to significant performance degradation in more challenging tasks (e.g., classification of community-contributed images with large intra-class variations). To overcome this drawback, we develop a semantic regularized matrix factorization method for learning descriptive visual BOW representation by adding Laplacian regularization defined with the tags (easy to access although noisy) of community-contributed images into matrix factorization. Experimental results on two benchmark datasets show the promising performance of the proposed method.", "title": "Learning Descriptive Visual Representation by Semantic Regularized Matrix Factorization"}, {"url": "https://www.ijcai.org/Abstract/13/228", "abstract": "In this paper, we propose a new classification framework for image matrices. The approach is realized by learning two groups of classification vectors for each dimension of the image matrices. One novelty is that we utilize compound regression models in the learning process, which endows the algorithm increased degree of freedom. On top of that, we extend the two-dimensional classification method to a semi-supervised classifier which leverages both labeled and unlabeled data. A fast iterative solution is then proposed to solve the objective function. The proposed method is evaluated by several different applications. The experimental results show that our method outperforms several classification approaches. In addition, we observe that our method attains respectable classification performance even when only few labeled training samples are provided. This advantage is especially desirable for real-world problems since precisely annotated images are scarce.", "title": "Thinking of Images as What They Are: Compound Matrix Regression for Image Classification"}, {"url": "https://www.ijcai.org/Abstract/13/229", "abstract": "Eliciting user preferences constitutes a major step towards developing recommender systems and decision support tools. Assuming that preferences are ceteris paribus allows for their concise representation as Conditional Preference Networks (CP-nets). This work presents the first empirical investigation of an algorithm for reliably and efficiently learning CP-nets in a manner that is minimally intrusive. At the same time, it introduces a novel process for efficiently reasoning with (the learned) preferences.", "title": "An Empirical Investigation of Ceteris Paribus Learnability"}, {"url": "https://www.ijcai.org/Abstract/13/230", "abstract": "The problem of detecting the direction of time in vector Autoregressive (VAR) processes using statistical techniques is considered. By analogy to causal AR(1) processes with non-Gaussian noise, we conjecture that the distribution of the time reversed residuals of a linear VAR model is closer to a Gaussian than the distribution of actual residuals in the forward direction. Experiments with simulated data illustrate the validity of the conjecture. Based on these results, we design a decision rule for detecting the direction of VAR processes. The correct direction in time (forward) is the one in which the residuals of the time series are less Gaussian. A series of experiments illustrate the superior results of the proposed rule when compared with other methods based on independence tests.", "title": "Statistical Tests for the Detection of the Arrow of Time in Vector Autoregressive Models"}, {"url": "https://www.ijcai.org/Abstract/13/231", "abstract": "In recent years Predicate Invention has been under-explored within Inductive Logic Programming due to difficulties in formulating efficient search mechanisms. However, a recent paper demonstrated that both predicate invention and the learning of recursion can be efficiently implemented for regular and context-free grammars, by way of abduction with respect to a meta-interpreter. New predicate symbols are introduced as constants representing existentially quantified higher-order variables. In this paper we generalise the approach of Meta-Interpretive Learning (MIL) to that of learning higher-order dyadic datalog programs. We show that with an infinite signature the higher-order dyadic datalog class H22 has universal Turing expressivity though H22 is decidable given a finite signature. Additionally we show that Knuth-Bendix ordering of the hypothesis space together with logarithmic clause bounding allows our Dyadic MIL implementation MetagolD to PAC-learn minimal cardinality H22 definitions. This result is consistent with our experiments which indicate that MetagolD efficiently learns compact H22 definitions involving predicate invention for robotic strategies and higher-order concepts in the NELL language learning domain.", "title": "Meta-Interpretive Learning of Higher-Order Dyadic Datalog: Predicate Invention Revisited"}, {"url": "https://www.ijcai.org/Abstract/13/232", "abstract": "This paper studies the problem of image annotation in a multi-modal setting where both visual and textual information are available. We propose Multi-modal Multi-instance Multi-label Latent Dirichlet Allocation (M3LDA), where the model consists of a visual-label part, a textual-label part and a label-topic part. The basic idea is that the topic decided by the visual information and the topic decided by the textual information should be consistent, leading to the correct label assignment. Particularly, M3LDA is able to annotate image regions, thus provides a promising way to understand the relation between input patterns and output semantics. Experiments on Corel5K and ImageCLEF validate the effectiveness of the proposed method.", "title": "Multi-Modal Image Annotation with Multi-Instance Multi-Label LDA"}, {"url": "https://www.ijcai.org/Abstract/13/233", "abstract": "The semi-supervised learning usually only predict labels for unlabeled data appearing in training data, and cannot effectively predict labels for testing data never appearing in training set. To handle this out-of-sample problem, many inductive methods make a constraint such that the predicted label matrix should be exactly equal to a linear model. In practice, this constraint is too rigid to capture the manifold structure of data. Motivated by this deficiency, we relax the rigid linear embedding constraint and propose to use an elastic embedding constraint on the predicted label matrix such that the manifold structure can be better explored. To solve our new objective and also a more general optimization problem, we study a novel adaptive loss with efficient optimization algorithm. Our new adaptive loss minimization method takes the advantages of both L1 norm and L2 norm, and is robust to the data outlier under Laplacian distribution and can efficiently learn the normal data under Gaussian distribution. Experiments have been performed on image classification tasks and our approach outperforms other state-of-the-art methods.", "title": "Adaptive Loss Minimization for Semi-Supervised Elastic Embedding"}, {"url": "https://www.ijcai.org/Abstract/13/234", "abstract": "Labeling training data is quite time-consuming but essential for supervised learning models. To solve this problem, the active learning has been studied and applied to select the informative and representative data points for labeling. However, during the early stage of experiments, only a small number (or none) of labeled data points exist, thus the most representative samples should be selected first. In this paper, we propose a novel robust active learning method to handle the early stage experimental design problem and select the most representative data points. Selecting the representative samples is an NP-hard problem, thus we employ the structured sparsity-inducing norm to relax the objective to an efficient convex formulation. Meanwhile, the robust sparse representation loss function is utilized to reduce the effect of outliers. A new efficient optimization algorithm is introduced to solve our non-smooth objective with low computational cost and proved global convergence. Empirical results on both single-label and multi-label classification benchmark data sets show the promising results of our method.", "title": "Early Active Learning via Robust Representation and Structured Sparsity"}, {"url": "https://www.ijcai.org/Abstract/13/235", "abstract": "We present a new sampling approach to Bayesian learning of the Bayesian network structure. Like some earlier sampling methods, we sample linear orders on nodes rather than directed acyclic graphs (DAGs). The key difference is that we replace the usual Markov chain Monte Carlo (MCMC) method by the method of annealed importance sampling (AIS). We show that AIS is not only competitive to MCMC in exploring the posterior, but also superior to MCMC in two ways: it enables easy and efficient parallelization, due to the independence of the samples, and lower-bounding of the marginal likelihood of the model with good probabilistic guarantees. We also provide a principled way to correct the bias due to order-based sampling, by implementing a fast algorithm for counting the linear extensions of a given partial order.", "title": "Annealed Importance Sampling for Structure Learning in Bayesian Networks"}, {"url": "https://www.ijcai.org/Abstract/13/236", "abstract": "Recent years have witnessed an increasing number of applications involving data with structural dependency and graph representations. For these applications, it is very common that their class distribution is imbalanced with minority samples being only a small portion of the population. Such imbalanced class distributions impose significant challenges to the learning algorithms. This problem is further complicated with the presence of noise or outliers in the graph data. In this paper, we propose an imbalanced graph boosting algorithm, igBoost, that progressively selects informative subgraph patterns from imbalanced graph data for learning. To handle class imbalance, we take class distributions into consideration to assign different weight values to graphs. The distance of each graph to its class center is also considered to adjust the weight to reduce the impact of noisy graph data. The weight values are integrated into the iterative subgraph feature selection and margin learning process to achieve maximum benefits. Experiments on real-world graph data with different degrees of class imbalance and noise demonstrate the algorithm performance.", "title": "Graph Classification with Imbalanced Class Distributions and Noise"}, {"url": "https://www.ijcai.org/Abstract/13/237", "abstract": "Bayesian treatment of matrix factorization has been successfully applied to the problem of collaborative prediction,where unknown ratings are determined by the predictive distribution, inferring posterior distributions over user and item factor matrices that are used to approximate the user-item matrix as their product. In practice, however, Bayesian matrix factorization suffers from cold-start problems, where inferences are required for users or items about which a sufficient number of ratings are not gathered. In this paper we present a method for Bayesian matrix factorization with side information, to handle cold-start problems. To this end, we place Gaussian-Wishart priors on mean vectors and precision matrices of Gaussianuser and item factor matrices, such that mean of each prior distribution is regressed on corresponding side information. We develop variational inference algorithms to approximately compute posterior distributions over user and item factor matrices.In addition, we provide Bayesian Cram\u00e9r-Rao Bound for our model, showing that the hierarchical Bayesian matrix factorization with side information improves the reconstruction over the standard Bayesian matrix factorization where the side information is not used. Experiments on MovieLens data demonstrate the useful behavior of our model in the case of cold-start problems.", "title": "Hierarchical Bayesian Matrix Factorization with Side Information"}, {"url": "https://www.ijcai.org/Abstract/13/238", "abstract": "In this paper, we address the column-based low-rank matrix approximation problem using a novel parallel approach. Our approach is based on the divide-and-combine idea. We first perform column selection on submatrices of an original data matrix in parallel, and then combine the selected columns into the final output. Our approach enjoys a theoretical relative-error upper bound. In addition, our column-based low-rank approximation partitions data in a deterministic way and makes no assumptions about matrix coherence. Compared with other traditional methods, our approach is scalable on large-scale matrices. Finally, experiments on both simulated and real world data show that our approach is both efficient and effective.", "title": "A Scalable Approach to Column-Based Low-Rank Matrix Approximation"}, {"url": "https://www.ijcai.org/Abstract/13/239", "abstract": "Multiple task learning (MTL) is becoming popular due to its theoretical advances and empirical successes. The key idea of MTL is to explore the hidden relationships among multiple tasks to enhance learning performance. Recently, many MTL algorithms have been developed and applied to various problems such as feature selection and kernel learning. However, most existing methods highly relied on certain assumptions of the task relationships. For instance, several works assumed that there is a major task group and several outlier tasks, and used a decomposition approach to identify the group structure and outlier tasks simultaneously. In this paper, we adopt a more general formulation for MTL without making specific structure assumptions. Instead of performing model decomposition, we directly impose an elastic-net regularization with a mixture of the structure and outlier penalties and formulate the objective as an unconstrained convex problem. To derive the optimal solution efficiently, we propose to use an Iteratively Reweighted Least Square (IRLS) method with a preconditioned conjugate gradient, which is computationally affordable for high dimensional data. Extensive experiments are conducted over both synthetic and real data, and comparisons with several state-of-the-art algorithms clearly show the superior performance of the proposed method.", "title": "Multiple Task Learning Using Iteratively Reweighted Least Square"}, {"url": "https://www.ijcai.org/Abstract/13/240", "abstract": "Active learning has been extensively studied and shown to be useful in solving real problems. The typical setting of traditional active learning methods is querying labels from an oracle. This is only possible if an expert exists, which may not be the case in many real world applications. In this paper, we focus on designing easier questions that can be answered by a non-expert. These questions poll relative information as opposed to absolute information and can be even generated from side-information. We propose an active learning approach that queries the ordering of the importance of an instance's neighbors rather than its label. We explore our approach on real datasets and make several interesting discoveries including that querying neighborhood information can be an effective question to ask and sometimes can even yield better performance than querying labels.", "title": "Active Learning from Relative Queries"}, {"url": "https://www.ijcai.org/Abstract/13/241", "abstract": "A new unsupervised feature selection method, i.e., Robust Unsupervised Feature Selection (RUFS), is proposed. Unlike traditional unsupervised feature selection methods, pseudo cluster labels are learned via local learning regularized robust nonnegative matrix factorization. During the label learning process, feature selection is performed simultaneously by robust joint l2, 1 norms minimization. Since RUFS utilizes l2, 1 norm minimization on processes of both label learning and feature learning, outliers and noise could be effectively handled and redundant or noisy features could be effectively reduced. Our method adopts the advantages of robust nonnegative matrix factorization, local learning, and robust feature learning. In order to make RUFS be scalable, we design a (projected) limited-memory BFGS based iterative algorithm to efficiently solve the optimization problem of RUFS in terms of both memory consumption and computation complexity. Experimental results on different benchmark real world datasets show the promising performance of RUFS over the state-of-the-arts.", "title": "Robust Unsupervised Feature Selection"}, {"url": "https://www.ijcai.org/Abstract/13/242", "abstract": "We present an embedding of stochastic optimal control problems, of the so called path integral form, into reproducing kernel Hilbert spaces. Using consistent, sample based estimates of the embedding leads to a model-free, non-parametric approach for calculation of an approximate solution to the control problem. This formulation admits a decomposition of the problem into an invariant and task dependent component. Consequently, we make much more efficient use of the sample data compared to previous sample based approaches in this domain, e.g., by allowing sample re-use across tasks. Numerical examples on test problems, which illustrate the sample efficiency, are provided.", "title": "Path Integral Control by Reproducing Kernel Hilbert Space Embedding"}, {"url": "https://www.ijcai.org/Abstract/13/243", "abstract": "Multi-level logic synthesis is a problem of immense practical significance, and is a key to developing circuits that optimize a number of parameters, such as depth, energy dissipation, reliability, etc. The problem can be defined as the task of taking a collection of components from which one wants to synthesize a circuit that optimizes a particular objective function. This problem is computationally hard, and there are very few automated approaches for its solution. To solve this problem we propose an algorithm, called Circuit-Decomposition Engine (CDE), that is based on learning decision trees, and uses a greedy approach for function learning. We empirically demonstrate that CDE, when given a library of different component types, can learn the function of Disjunctive Normal Form (DNF) Boolean representations and synthesize circuit structure using the input library. We compare the structure of the synthesized circuits with that of well-known circuits using a range of circuit similarity metrics.", "title": "Machine-Learning-Based Circuit Synthesis"}, {"url": "https://www.ijcai.org/Abstract/13/244", "abstract": "Directed acyclic graphs can be used across many application domains. In this paper, we study a new pattern domain for supporting their analysis. Therefore, we propose the pattern language of weighted paths, primitive constraints that enable to specify their relevancy (e.g., frequency and compactness constraints), and algorithms that can compute the specified collections. It leads to a condensed representation setting whose efficiency and scalability are empirically studied.", "title": "Weighted Path as a Condensed Pattern in a Single Attributed DAG"}, {"url": "https://www.ijcai.org/Abstract/13/245", "abstract": "We propose a method for learning causal relations within high-dimensional tensor data as they are typically recorded in non-experimental databases. The method allows the simultaneous inclusion of numerous dimensions within the data analysis such as samples, time and domain variables construed as tensors. In such tensor data we exploit and integrate non-Gaussian models and tensor analytic algorithms in a novel way. We prove that we can determine simple causal relations independently of how complex the dimensionality of the data is. We rely on a statistical decomposition that flattens higher-dimensional data tensors into matrices. This decomposition preserves the causal information and is therefore suitable for structure learning of causal graphical models, where a causal relation can be generalised beyond dimension, for example, over all time points. Related methods either focus on a set of samples for instantaneous effects or look at one sample for effects at certain time points. We evaluate the resulting algorithm and discuss its performance both with synthetic and real-world data.", "title": "Multi-Dimensional Causal Discovery"}, {"url": "https://www.ijcai.org/Abstract/13/246", "abstract": "Predictive methods are becoming increasingly popular for representing world knowledge in autonomous agents.A recently introduced predictive method that shows particular promise is the General Value Function (GVF), which is more flexible than previous predictive methods and can more readily capture regularities in the agent's sensorimotor stream.The goal of the current paper is to investigate the ability of these GVFs (also called \"forecasts\") to capture such regularities.We generate focused sets of forecasts and measure their capacity for generalization.We then compare the results with a closely related predictive method (PSRs) already shown to have good generalization abilities.Our results indicate that forecasts provide a substantial improvement in generalization, producing features that lead to better value-function approximation (when computed with linear function approximators) than PSRs and better generalization to as-yet-unseen parts of the state space.", "title": "Better Generalization with Forecasts"}, {"url": "https://www.ijcai.org/Abstract/13/247", "abstract": "The ever-growing literature in biomedicine makes it virtually impossible for individuals to grasp all the information relevant to their interests. Since even experts' knowledge is limited, important associations among key biomedical concepts may remain unnoticed in the flood of information. Discovering those hidden associations is called hypothesis discovery. This paper reports our approach to this problem taking advantage of a triangular chain of relations extracted from published knowledge. We consider such chains of relations as implicit rules to generate potential hypotheses. The generated hypotheses are then compared with newer knowledge for assessing their validity and, if validated, they are served as positive examples for learning a regression model to rank hypotheses. This framework, called supervised hypothesis discovery, is tested on real-world knowledge from the biomedical literature to demonstrate its effectiveness.", "title": "Supervised Hypothesis Discovery Using Syllogistic Patterns in the Biomedical Literature"}, {"url": "https://www.ijcai.org/Abstract/13/248", "abstract": "This paper studies the recovery guarantees of the models of minimizing ||X||\u2217 + 1/2a ||X||2F where X is a tensor and ||X||\u2217 and ||X||F are the trace and Frobenius norm of respectively. We show that they can efficiently recover low-rank tensors. In particular, they enjoy exact guarantees similar to those known for minimizing ||X||\u2217 under the conditions on the sensing operator such as its null-space property, restricted isometry property, or spherical section property. To recover a low-rank tensor X0, minimizing ||X||\u2217 + 1/2a ||X||2F returns the same solution as minimizing ||X||\u2217 almost whenever \u03b1 \u2265 10max ||X0(i)||2.", "title": "Guarantees of Augmented Trace Norm Models in Tensor Recovery"}, {"url": "https://www.ijcai.org/Abstract/13/249", "abstract": "Hartigan's method for k-means clustering holds several potential advantages compared to the classical and prevalent optimization heuristic known as Lloyd's algorithm. E.g., it was recently shown that the set of local minima of Hartigan's algorithm is a subset of those of Lloyd's method. We develop a closed-form expression that allows to establish Hartigan's method for k-means clustering with any Bregman divergence, and further strengthen the case of preferring Hartigan's algorithm over Lloyd's algorithm. Specifically, we characterize a range of problems with various noise levels of the inputs, for which any random partition represents a local minimum for Lloyd's algorithm, while Hartigan's algorithm easily converges to the correct solution. Extensive experiments on synthetic and real-world data further support our theoretical analysis.", "title": "Hartigan\u2019s K-Means Versus Lloyd\u2019s K-Means \u2014 Is It Time for a Change?"}, {"url": "https://www.ijcai.org/Abstract/13/250", "abstract": "Sequential anomaly detection is a challenging problem due to the one-class nature of the data (i.e., data is collected from only one class) and the temporal dependence in sequential data. We present One-Class Conditional Random Fields (OCCRF) for sequential anomaly detection that learn from a one-class dataset and capture the temporal dependence structure, in an unsupervised fashion. We propose a hinge loss in a regularized risk minimization framework that maximizes the margin between each sequence being classified as \"normal\" and \"abnormal.\" This allows our model to accept most (but not all) of the training data as normal, yet keeps the solution space tight. Experimental results on a number of real-world datasets show our model outperforming several baselines. We also report an exploratory study on detecting abnormal organizational behavior in enterprise social networks.", "title": "One-Class Conditional Random Fields for Sequential Anomaly Detection"}, {"url": "https://www.ijcai.org/Abstract/13/251", "abstract": "We propose to measure statistical dependence between two random variables by the mutual information dimension (MID), and present a scalable parameter-free estimation method for this task. Supported by sound dimension theory, our method gives an effective solution to the problem of detecting interesting relationships of variables in massive data, which is nowadays a heavily studied topic in many scientific disciplines. Different from classical Pearson's correlation coefficient, MID is zero if and only if two random variables are statistically independent and is translation and scaling invariant. We experimentally show superior performance of MID in detecting various types of relationships in the presence of noise data. Moreover, we illustrate that MID can be effectively used for feature selection in regression.", "title": "Measuring Statistical Dependence via the Mutual Information Dimension"}, {"url": "https://www.ijcai.org/Abstract/13/252", "abstract": "When doing learning from demonstration, it is often the case that the demonstrator provides corrective examples to fix errant behavior by the agent or robot. We present a set of algorithms which use this corrective data to identify and remove noisy examples in datasets which caused errant classifications, and ultimately errant behavior. The objective is to actually modify the source datasets rather than solely rely on the noise-insensitivity of the classification algorithm. This is particularly useful in the sparse datasets often found in learning from demonstration experiments. Our approach tries to distinguish between noisy misclassification and mere undersampling of the learning space. If errors are a result of misclassification, we potentially remove the responsible points and update the classifier. We demonstrate our method on UCI Machine Learning datasets at different levels of sparsity and noise, using decision trees, K-Nearest-Neighbor, and support vector machines.", "title": "Unlearning from Demonstration"}, {"url": "https://www.ijcai.org/Abstract/13/253", "abstract": "Maximum entropy discrimination (MED) is a general framework for discriminative estimation based on the well known maximum entropy principle, which embodies the Bayesian integration of prior information with large margin constraints on observations. It is a successful combination of maximum entropy learning and maximum margin learning, and can subsume support vector machines (SVMs) as a special case. In this paper, we present a multi-view maximum entropy discrimination framework that is an extension of MED to the scenario of learning with multiple feature sets. Different from existing approaches to exploiting multiple views, such as co-training style algorithms and co-regularization style algorithms, we propose a new method to make use of the distinct views where classification margins from these views are required to be identical. We give the general form of the solution to the multi-view maximum entropy discrimination, and provide an instantiation under a specific prior formulation which is analogical to a multi-view version of SVMs. Experimental results on real-world data sets show the effectiveness of the proposed multi-view maximum entropy discrimination approach.", "title": "Multi-View Maximum Entropy Discrimination"}, {"url": "https://www.ijcai.org/Abstract/13/254", "abstract": "Non-negative Matrix Factorization (NMF) is a traditional unsupervised machine learning technique for decomposing a matrix into a set of bases and coefficients under the non-negative constraint. NMF with sparse constraints is also known for extracting reasonable components from noisy data. However, NMF tends to give undesired results in the case of highly sparse data, because the information included in the data is insufficient to decompose. Our key idea is that we can ease this problem if complementary data are available that we could integrate into the estimation of the bases and coefficients. In this paper, we propose a novel matrix factorization method called Non-negative Multiple Matrix Factorization (NMMF), which utilizes complementary data as auxiliary matrices that share the row or column indices of the target matrix. The data sparse- ness is improved by decomposing the target and auxiliary matrices simultaneously, since auxiliary matrices provide information about the bases and coefficients. We formulate NMMF as a generalization of NMF, and then present a parameter estimation procedure derived from the multiplicative up- date rule. We examined NMMF in both synthetic and real data experiments. The effect of the auxiliary matrices appeared in the improved NMMF performance. We also confirmed that the bases that NMMF obtained from the real data were intuitive and reasonable thanks to the non-negative constraint.", "title": "Non-Negative Multiple Matrix Factorization"}, {"url": "https://www.ijcai.org/Abstract/13/255", "abstract": "This paper proposes a simple linear Bayesian approach to reinforcement learning. We show that with an appropriate basis, a Bayesian linear Gaussian model is sufficient for accurately estimating the system dynamics, and in particular when we allow for correlated noise. Policies are estimated by first sampling a transition model from the current posterior, and then performing approximate dynamic programming on the sampled model. This form of approximate Thompson sampling results in good exploration in unknown environments. The approach can also be seen as a Bayesian generalisation of least-squares policy iteration, where the empirical transition matrix is replaced with a sample from the posterior.", "title": "Linear Bayesian Reinforcement Learning"}, {"url": "https://www.ijcai.org/Abstract/13/256", "abstract": "Multi class problems are everywhere. Given an input the goal is to predict one of a few possible classes. Most previous work reduced learning to minimizing the empirical loss over some training set and an additional regularization term, prompting simple models or some other prior knowledge. Many learning regularizations promote sparsity, that is, small models or small number of features, as performed in group LASSO. Yet, such models do not always represent the classes well. In some problems, for each class, there is a small set of features that represents it well, yet the union of these sets is not small. We propose to use other regularizations that promote this type of sparsity, analyze the generalization property of such formulations, and show empirically that indeed, these regularizations not only perform well, but also promote such sparsity structure.", "title": "Multi Class Learning with Individual Sparsity"}, {"url": "https://www.ijcai.org/Abstract/13/257", "abstract": "The usual representation of quantitative data is to formalize it as an information table, which assumes the independence of attributes. In real-world data, attributes are more or less interacted and coupled via explicit or implicit relationships. Limited research has been conducted on analyzing such attribute interactions, which only describe a local picture of attribute couplings in an implicit way. This paper proposes a framework of the coupled attribute analysis to capture the global dependency of continuous attributes. Such global couplings integrate the intra-coupled interaction within an attribute (i.e. the correlations between attributes and their own powers) and inter-coupled interaction among different attributes (i.e. the correlations between attributes and the powers of others) to form a coupled representation for numerical objects by the Taylor-like expansion. This work makes one step forward towards explicitly addressing the global interactions of continuous attributes, verified by the applications in data structure analysis, data clustering, and data classification. Substantial experiments on 13 UCI data sets demonstrate that the coupled representation can effectively capture the global couplings of attributes and outperforms the traditional way, supported by statistical analysis.", "title": "Coupled Attribute Analysis on Numerical Data"}, {"url": "https://www.ijcai.org/Abstract/13/258", "abstract": "This paper proposes a novel algorithm for manifold alignment preserving global geometry. This approach constructs mapping functions that project data instances from different input domains to a new lower-dimensional space, simultaneously matching the instances in correspondence and preserving global distances between instances within the original domains. In contrast to previous approaches, which are largely based on preserving local geometry, the proposed approach is suited to applications where the global manifold geometry needs to be respected. We evaluate the effectiveness of our algorithm for transfer learning in two real-world cross-lingual information retrieval tasks.", "title": "Manifold Alignment Preserving Global Geometry"}, {"url": "https://www.ijcai.org/Abstract/13/259", "abstract": "In this work, we present a new framework for large scale online kernel classification, making kernel methods efficient and scalable for large-scale online learning tasks. Unlike the regular budget kernel online learning scheme that usually uses different strategies to bound the number of support vectors, our framework explores a functional approximation approach to approximating a kernel function/matrix in order to make the subsequent online learning task efficient and scalable. Specifically, we present two different online kernel machine learning algorithms: (i) the Fourier Online Gradient Descent (FOGD) algorithm that applies the random Fourier features for approximating kernel functions; and (ii) the Nystrom Online Gradient Descent (NOGD) algorithm that applies the Nystrom method to approximate large kernel matrices. We offer theoretical analysis of the proposed algorithms, and conduct experiments for large-scale online classification tasks with some data set of over 1 million instances. Our encouraging results validate the effectiveness and efficiency of the proposed algorithms, making them potentially more practical than the family of existing budget kernel online learning approaches.", "title": "Large Scale Online Kernel Classification"}, {"url": "https://www.ijcai.org/Abstract/13/260", "abstract": "Online feature selection with dynamic features has become an active research area in recent years. However, in some real-world applications such as image analysis and email spam filtering, features may arrive by groups. Existing online feature selection methods evaluate features individually, while existing group feature selection methods cannot handle online processing. Motivated by this, we formulate the online group feature selection problem, and propose a novel selection approach for this problem. Our proposed approach consists of two stages: online intra-group selection and online inter-group selection. In the intra-group selection, we use spectral analysis to select discriminative features in each group when it arrives. In the inter-group selection, we use Lasso to select a globally optimal subset of features. This 2-stage procedure continues until there are no more features to come or some predefined stopping conditions are met. Extensive experiments conducted on benchmark and real-world data sets demonstrate that our proposed approach outperforms other state-of-the-art online feature selection methods.", "title": "Online Group Feature Selection"}, {"url": "https://www.ijcai.org/Abstract/13/261", "abstract": "Motivated by the recent developments of nonconvex penalties in sparsity modeling, we propose a nonconvex optimization model for handing the low-rank matrix recovery problem. Different from the famous robust principal component analysis (RPCA), we suggest recovering low-rank and sparse matrices via a nonconvex loss function and a nonconvex penalty. The advantage of the nonconvex approach lies in its stronger robustness. To solve the model, we devise a majorization-minimization augmented Lagrange multiplier (MM-ALM) algorithm which finds the local optimal solutions of the proposed nonconvex model. We also provide an efficient strategy to speedup MM-ALM, which makes the running time comparable with the state-of-the-art algorithm of solving RPCA. Finally, empirical results demonstrate the superiority of our nonconvex approach over RPCA in terms of matrix recovery accuracy.", "title": "Nonconvex Relaxation Approaches to Robust Matrix Recovery"}, {"url": "https://www.ijcai.org/Abstract/13/262", "abstract": "The standard Gaussian process (GP) regression is often intractable when a data set is large or spatially nonstationary. In this paper, we address these challenging data properties by designing a novel K nearest neighbor based Kalman filter Gaussian process (KNN-KFGP) regression. Based on a state space model established by the KNN driven data grouping, our KNN-KFGP recursively filters out the latent function values in a computationally efficient and accurate Kalman filtering framework. Moreover, KNN allows each test point to find its strongly correlated local training subset, so our KNN-KFGP provides a suitable way to deal with spatial nonstationary problems. We evaluate the performance of our KNN-KFGP on several synthetic and real data sets to show its validity.", "title": "A KNN Based Kalman Filter Gaussian Process Regression"}, {"url": "https://www.ijcai.org/Abstract/13/263", "abstract": "Bayesian optimization techniques have been successfully applied to robotics, planning, sensor placement, recommendation, advertising, intelligent user interfaces and automatic algorithm configuration. Despite these successes, the approach is restricted to problems of moderate dimension, and several workshops on Bayesian optimization have identified its scaling to high dimensions as one of the holy grails of the field. In this paper, we introduce a novel random embedding idea to attack this problem. The resulting Random EMbedding Bayesian Optimization (REMBO) algorithm is very simple and applies to domains with both categorical and continuous variables. The experiments demonstrate that REMBO can effectively solve high-dimensional problems, including automatic parameter configuration of a popular mixed integer linear programming solver.", "title": "Bayesian Optimization in High Dimensions via Random Embeddings"}, {"url": "https://www.ijcai.org/Abstract/13/264", "abstract": "Recent years have seen a great interest in using deep architectures for feature learning from data. One drawback of the commonly used unsupervised deep feature learning methods is that for supervised or semi-supervised learning tasks, the information in the target variables are not used until the final stage when the classifier or regressor is trained on the learned features. This could lead to over-generalized features that are not competitive on the specific supervised or semi-supervised learning tasks. In this work, we describe a new learning method that combines deep feature learning on mixed labeled and unlabeled data sets. Specifically, we describe a weakly supervised learning method of a prior supervised convolutional stacked auto-encoders (PCSA), of which information in the target variables is represented probabilistically using a Gaussian Bernoulli restricted Boltzmann machine (RBM). We apply this method to the decoding problem of an ECoG based Brain Computer Interface (BCI) system. Our experimental results show that PCSA achieves significant improvement in decoding performance on benchmark data sets compared to the unsupervised feature learning as well as to the current state-of-the-art algorithms that are based on manually crafted features.", "title": "Deep Feature Learning Using Target Priors with Applications in ECoG Signal Decoding for BCI"}, {"url": "https://www.ijcai.org/Abstract/13/265", "abstract": "By always mapping data from lower dimensional space into higher or even infinite dimensional space, kernel k-means is able to organize data into groups when data of different clusters are not linearly separable. However, kernel k-means incurs the large scale computation due to the representation theorem, i.e. keeping an extremely large kernel matrix in memory when using popular Gaussian and spatial pyramid matching kernels, which largely limits its use for processing large scale data. Also, existing kernel clustering can be over fitted by outliers as well. In this paper, we introduce an Euler clustering, which can not only maintain the benefit of nonlinear modeling using kernel function but also significantly solve the large scale computational problem in kernel-based clustering. This is realized by incorporating Euler kernel. Euler kernel is relying on a nonlinear and robust cosine metric that is less sensitive to outliers. More important it intrinsically induces an empirical map which maps data onto a complex space of the same dimension. Euler clustering takes these advantages to measure the similarity between data in a robust way without increasing the dimensionality of data, and thus solves the large scale problem in kernel k-means. We evaluate Euler clustering and show its superiority against related methods on five publicly available datasets.", "title": "Euler Clustering"}, {"url": "https://www.ijcai.org/Abstract/13/266", "abstract": "Consensus clustering emerges as a promising solution to find cluster structures from data. As an efficient approach for consensus clustering, the K-means based method has garnered attention in the literature, but the existing research is still preliminary and fragmented. In this paper, we provide a systematic study on the framework of K-means-based Consensus Clustering (KCC). We first formulate the general definition of KCC, and then reveal a necessary and sufficient condition for utility functions that work for KCC, on both complete and incomplete basic partitionings. Experimental results on various real-world data sets demonstrate that KCC is highly efficient and is comparable to the state-of-the-art methods in terms of clustering quality. In addition, KCC shows high robustness to incomplete basic partitionings with substantial missing values.", "title": "A Theoretic Framework of K-Means-Based Consensus Clustering"}, {"url": "https://www.ijcai.org/Abstract/13/267", "abstract": "Multi-modal data is dramatically increasing with the fast growth of social media. Learning a good distance measure for data with multiple modalities is of vital importance for many applications, including retrieval, clustering, classification and recommendation. In this paper, we propose an effective and scalable multi-modal distance metric learning framework. Based on the multi-wing harmonium model, our method provides a principled way to embed data of arbitrary modalities into a single latent space, of which an optimal distance metric can be learned under proper supervision, i.e., by minimizing the distance between similar pairs whereas maximizing the distance between dissimilar pairs. The parameters are learned by jointly optimizing the data likelihood under the latent space model and the loss induced by distance supervision, thereby our method seeks a balance between explaining the data and providing an effective distance metric, which naturally avoids overfitting. We apply our general framework to text/image data and present empirical results on retrieval and classification to demonstrate the effectiveness and scalability.", "title": "Multi-Modal Distance Metric Learning"}, {"url": "https://www.ijcai.org/Abstract/13/268", "abstract": "Facing a large number of clustering solutions, cluster ensemble method provides an effective approach to aggregating them into a better one. In this paper, we propose a novel cluster ensemble method from probabilistic perspective. It assumes that each clustering solution is generated from a latent cluster model, under the control of two probabilistic parameters. Thus, the cluster ensemble problem is reformulated into an optimization problem of maximum likelihood. An EM-style algorithm is designed to solve this problem. It can determine the number of clusters automatically. Experimental results have shown that the proposed algorithm outperforms the state-of-the-art methods including EAC-AL, CSPA, HGPA, and MCLA. Furthermore, it has been shown that our algorithm is stable in the predicted numbers of clusters.", "title": "A Probabilistic Approach to Latent Cluster Analysis"}, {"url": "https://www.ijcai.org/Abstract/13/269", "abstract": "Hashing-based fast nearest neighbor search technique has attracted great attention in both research and industry areas recently.Many existing hashing approaches encode data with projection-based hash functions and represent each projected dimension by 1-bit.However, the dimensions with high variance hold large energy or information of data but treated equivalently as dimensions with low variance,which leads to a serious information loss.In this paper, we introduce a novel hashing algorithm called Harmonious Hashing which aims at learning hash functions with low information loss.Specifically, we learn a set of optimized projections to preserve the maximum cumulative energy and meet the constraint of equivalent variance on each dimension as much as possible.In this way, we could minimize the information loss after binarization. Despite the extreme simplicity, our method outperforms superiorly to many state-of-the-art hashing methods in large-scale and high-dimensional nearest neighbor search experiments.", "title": "Harmonious Hashing"}, {"url": "https://www.ijcai.org/Abstract/13/270", "abstract": "Change-point detection is the problem of finding abrupt changes in time-series, and it is attracting a lot of attention in the artificial intelligence and data mining communities. In this paper, we present a supervised learning based change-point detection approach in which we use the separability of past and future data at time t (they are labeled as +1 and -1) as plausibility of change-points. Based on this framework, we propose a detection measure called the additive Hilbert-Schmidt Independence Criterion (aHSIC), which is defined as the weighted sum of the HSIC scores between features and its corresponding binary labels. Here, the HSIC is a kernel-based independence measure. A novel aspect of the aHSIC score is that it can incorporate feature selection during its detection measure estimation. More specifically, we first select features that are responsible for an abrupt change by using a supervised approach, and then compute the aHSIC score by employing the selected features. Thus, compared with traditional detection measures, our approach tends to be robust as regards noise features, and so the aHSIC is suitable for a use with high-dimensional time-series change-point detection problems. We demonstrate that the proposed change-point detection method is promising through extensive experiments on synthetic data sets and a real-world human activity data set.", "title": "Change-Point Detection with Feature Selection in High-Dimensional Time-Series Data"}, {"url": "https://www.ijcai.org/Abstract/13/271", "abstract": "We study robust high-dimensional estimation of generalized linear models (GLMs); where a small number k of the n observations can be arbitrarily corrupted, and where the true parameter is high dimensional in the \"p > n\" regime, but only has a small number s of non-zero entries. There has been some recent work connecting robustness and sparsity, in the context of linear regression with corrupted observations, by using an explicitly modeled outlier response vector that is assumed to be sparse. Interestingly, we show, in the GLM setting, such explicit outlier response modeling can be performed in two distinct ways. For each of these two approaches, we give l2 error bounds for parameter estimation for general values of the tuple (n, p, s, k).", "title": "On Robust Estimation of High Dimensional Generalized Linear Models"}, {"url": "https://www.ijcai.org/Abstract/13/272", "abstract": "The Nystr\u00f6m method is a well known sampling based low-rank matrix approximation approach. It is usually considered to be originated from the numerical treatment of integral equations and eigende composition of matrices. In this paper, we present a novel point of view for the Nystr\u00f6m approximation. We show that theoretically the Nystr\u00f6m method can be regraded as a set of point-wise ordinary least square linear regressions of the kernel matrix, sharing the same design matrix. With the new interpretation, we are able to analyze the approximation quality based on the fulfillment of the homoscedasticity assumption and explain the success and deficiency of various sampling methods. We also empirically show that positively skewed explanatory variable distributions can lead to heteroscedasticity. Based on this discovery, we propose to use non-symmetric explanatory functions to improve the quality of the Nystr\u00f6m approximation with almost no extra computational cost. Experiments show that positively skewed datasets widely exist, and our method exhibits good improvements on these datasets.", "title": "Reduced Heteroscedasticity Linear Regression for Nystr\u00f6m Approximation"}, {"url": "https://www.ijcai.org/Abstract/13/273", "abstract": "We study to incorporate multiple views of data in a perceptive transfer learning framework and propose a Multi-view Discriminant Transfer (MDT) learning approach for domain adaptation. The main idea is to find the optimal discriminant weight vectors for each view such that the correlation between the two-view projected data is maximized, while both the domain discrepancy and the view disagreement are minimized simultaneously. Furthermore, we analyze MDT theoretically from discriminant analysis perspective to explain the condition and reason, under which the proposed method is not applicable. The analytical results allow us to investigate whether there exist within-view and/or between-view conflicts, and thus provides a deep insight into whether the transfer learning algorithm work properly or not in the view-based problems and the combined learning problem. Experiments show that MDT significantly outperforms the state-of-the-art baselines including some typical multi-view learning approaches in single- or cross-domain.", "title": "Multi-View Discriminant Transfer Learning"}, {"url": "https://www.ijcai.org/Abstract/13/274", "abstract": "Recent years have witnessed the growing popularity of hash function learning for large-scale data search.Although most existing hashing-based methods have been proven to obtain high accuracy, they are regarded as passive hashing and assume that the labelled points are provided in advance. In this paper, we consider updating a hashing model upon gradually increased labelled data in a fast response to users, called smart hashing update (SHU). In order to get a fast response to users, SHU aims to select a small set of hash functions to relearn and only updates the corresponding hash bits of all data points. More specifically, we put forward two selection methods for performing efficient and effective update. In order to reduce the response time for acquiring a stable hashing algorithm, we also propose an accelerated method in order to further reduce interactions between users and the computer. We evaluate our proposals on two benchmark data sets. Our experimental results show it is not necessary to update all hash bits in order to adapt the model to new input data, and meanwhile we obtain better or similar performance without sacrificing much accuracy against the batch mode update.", "title": "Smart Hashing Update for Fast Response"}, {"url": "https://www.ijcai.org/Abstract/13/275", "abstract": "Multi-Instance Multi-Label learning (MIML) deals with data objects that are represented by a bag of instances and associated with a set of class labels simultaneously. Previous studies typically assume that for every training example, all positive labels are tagged whereas the untagged labels are all negative. In many real applications such as image annotation, however, the learning problem often suffers from weak label; that is, users usually tag only a part of positive labels, and the untagged labels are not necessarily negative. In this paper, we propose the MIMLwel approach which works by assuming that highly relevant labels share some common instances, and the underlying class means of bags for each label are with a large margin. Experiments validate the effectiveness of MIMLwel in handling the weak label problem.", "title": "Multi-Instance Multi-Label Learning with Weak Label"}, {"url": "https://www.ijcai.org/Abstract/13/276", "abstract": "Determining protein function constitutes an exercise in integrating information derived from several heterogeneous high-throughput experiments. To utilize the information spread across multiple sources in a combined fashion, these data sources are transformed into kernels. Several protein function prediction methods follow a two-phased approach: they first optimize the weights on individual kernels to produce a composite kernel, and then train a classifier on the composite kernel. As such, these methods result in an optimal composite kernel, but not necessarily in an optimal classifier. On the other hand, some methods optimize the loss of binary classifiers, and learn weights for the different kernels iteratively. A protein has multiple functions, and each function can be viewed as a label. These methods solve the problem of optimizing weights on the input kernels for each of the labels. This is computationally expensive and ignores inter-label correlations. In this paper, we propose a method called Protein Function Prediction by Integrating Multiple Kernels(ProMK). ProMK iteratively optimizes the phases of learning optimal weights and reducing the empirical loss of a multi-label classifier for each of the labels simultaneously, using a combined objective function. ProMK can assign larger weights to smooth kernels and downgrade the weights on noisy kernels. We evaluate the ability of ProMK to predict the function of proteins using several standard benchmarks. We show that our approach performs better than previously proposed protein function prediction approaches that integrate data from multiple networks, and multi-label multiple kernel learning methods.", "title": "Protein Function Prediction by Integrating Multiple Kernels"}, {"url": "https://www.ijcai.org/Abstract/13/277", "abstract": "In this paper, we address the relation between domain differences and domain adaptation for dependency parsing. Our quantitative analyses showed that it is the inconsistent behavior of same features cross-domain, rather than word or feature coverage, that is the major cause of performances decrease of out-domain model. We further studied those ambiguous features in depth and found that the set of ambiguous features is small and has concentric distributions. Based on the analyses, we proposed a DA method. The DA method can automatically learn which features are ambiguous cross domain according to errors made by out-domain model on in-domain training data. Our method is also extended to utilize multiple out-domain models. The results of dependency parser adaptation from WSJ to Genia and Question bank showed that our method achieved significant improvements on small in-domain datasets where DA is mostly in need. Additionally, we achieved improvement on the published best results of CoNLL07 shared task on domain adaptation, which confirms the significance of our analyses and our method.", "title": "Learning Domain Differences Automatically for Dependency Parsing Adaptation"}, {"url": "https://www.ijcai.org/Abstract/13/278", "abstract": "Bag-of-Words approach has played an important role in recent works for image classification. In consideration of efficiency, most methods use k-means clustering to generate the codebook. The obtained codebooks often lose the cluster size and shape information with distortion errors and low discriminative power. Though some efforts have been made to optimize codebook in sparse coding, they usually incur higher computational cost. Moreover, they ignore the correlations between codes in the following coding stage, that leads to low discriminative power of the final representation. In this paper, we propose a bilevel visual words coding approach in consideration of representation ability, discriminative power and efficiency. In the bilevel codebook generation stage, k-means and an efficient spectral clustering are respectively run in each level by taking both class information and the shapes of each visual word cluster into account. To obtain discriminative representation in the coding stage, we design a certain localized coding rule with bilevel codebook to select local bases. To further achieve an efficient coding referring to this rule, an online method is proposed to efficiently learn a projection of local descriptor to the visual words in the codebook. After projection, coding can be efficiently completed by a low dimensional localized soft-assignment. Experimental results show that our proposed bilevel visual words coding approach outperforms the state-of-the-art approaches for image classification.", "title": "Bilevel Visual Words Coding for Image Classification"}, {"url": "https://www.ijcai.org/Abstract/13/279", "abstract": "We propose a novel approach to semantic segmentation using weakly supervised labels. In traditional fully supervised methods, superpixel labels are available for training; however, it is not easy to obtain enough labeled superpixels to learn a satisfying model for semantic segmentation. By contrast, only image-level labels are necessary in weakly supervised methods, which makes them more practical in real applications. In this paper we develop a new way of evaluating classification models for semantic segmentation given weekly supervised labels. For a certain category, provided the classification model parameter, we firstly learn the basis superpixels by sparse reconstruction, and then evaluate the parameters by measuring the reconstruction errors among negative and positive superpixels. Based on Gaussian Mixture Models, we use Iterative Merging Update (IMU) algorithm to obtain the best parameters for the classification models. Experimental results on two real-world datasets show that the proposed approach outperforms the existing weakly supervised methods, and it also competes with state-of-the-art fully supervised methods.", "title": "Sparse Reconstruction for Weakly Supervised Semantic Segmentation"}, {"url": "https://www.ijcai.org/Abstract/13/280", "abstract": "In this paper, we propose a locality-constrained and sparsity-encouraged manifold fitting approach, aiming at capturing the locally sparse manifold structure into neighborhood graph construction by exploiting a principled optimization model. The proposed model formulates neighborhood graph construction as a sparse coding problem with the locality constraint, therefore achieving simultaneous neighbor selection and edge weight optimization. The core idea underlying our model is to perform a sparse manifold fitting task for each data point so that close-by points lying on the same local manifold are automatically chosen to connect and meanwhile the connection weights are acquired by simple geometric reconstruction. We term the novel neighborhood graph generated by our proposed optimization model M-Fitted Graph since such a graph stems from sparse manifold fitting. To evaluate the robustness and effectiveness ofM -fitted graphs, we leverage graph-based semisupervised learning as the testbed. Extensive experiments carried out on six benchmark datasets validate that the proposed M -fitted graph is superior to state-of-the-art neighborhood graphs in terms of classification accuracy using popular graph-based semi-supervised learning methods.", "title": "Semi-Supervised Learning with Manifold Fitted Graphs"}, {"url": "https://www.ijcai.org/Abstract/13/281", "abstract": "Complex networks describe a wide range of systems in nature and society. To understand the complex networks, it is crucial to investigate their internal structure. In this paper, we propose an online community detection method for large complex networks, which make it possible to process networks edge-by-edge in a serial fashion. We investigate the generative mechanism of complex networks and propose a split mechanism based on the degree of the nodes to create new community. Our method has linear time complexity. The method has been applied to six real-world network datasets and the experimental results show that it is comparable to existing methods in modularity with much less running time.", "title": "Online Community Detection for Large Complex Networks"}, {"url": "https://www.ijcai.org/Abstract/13/282", "abstract": "In many applications, the data may be high dimensional, represented by multiple features, and associated with more than one labels. Embedding learning is an effective strategy for dimensionality reduction and for nearest neighbor search in massive datasets. We propose a novel method to seek compact embedding that allows efficient retrieval with incompletely-labeled multi-view data. Based on multi-graph Laplacian, we achieve the optimal combination of heterogeneous features to effectively describe data, which exploits the feature correlations between different views. We learn the embedding that preserves the neighborhood context in the original spaces, and obtain the complete labels simultaneously. Inter-label correlations are sufficiently leveraged in the proposed framework. Our goal is to find the maps from multiple input spaces to the compact embedding space and to the semantic concept space at the same time. There is semantic gap between the input multi-view feature spaces and the semantic concept space; and the compact embedding space can be looked on as the bridge between the above spaces. Experimental evaluation on three real-world datasets demonstrates the effectiveness of the proposed method.", "title": "Multi-View Embedding Learning for Incompletely Labeled Data"}, {"url": "https://www.ijcai.org/Abstract/13/283", "abstract": "Multi-task learning is a way of bringing inductive transfer studied in human learning to the machine learning community. A central issue in multi-task learning is to model the relationships between tasks appropriately and exploit them to aid the simultaneous learning of multiple tasks effectively. While some recent methods model and learn the task relationships from data automatically, only pairwise relationships can be represented by them. In this paper, we propose a new model, called Multi-Task High-Order relationship Learning (MTHOL), which extends in a novel way the use of pairwise task relationships to high-order task relationships. We first propose an alternative formulation of an existing multi-task learning method. Based on the new formulation, we propose a high-order generalization leading to a new prior for the model parameters of different tasks. We then propose a new probabilistic model for multi-task learning and validate it empirically on some benchmark datasets.", "title": "Learning High-Order Task Relationships in Multi-Task Learning"}, {"url": "https://www.ijcai.org/Abstract/13/284", "abstract": "In virtually all machine learning applications, hyper-parameter tuning is required to maximize predictive accuracy. Such tuning is computationally expensive, and the cost is further exacerbated by the need for multiple evaluations (via cross-validation or bootstrap) at each configuration setting to guarantee statistically significant results. This paper presents a simple, general technique for improving the efficiency of hyper-parameter tuning by minimizing the number of resampled evaluations at each configuration. We exploit the fact that train-test samples can easily be matched across candidate hyper-parameter configurations. This permits the use of paired hypothesis tests and power analysis that allow for statistically sound early elimination of suboptimal candidates to minimize the number of evaluations. Results on synthetic and real-world datasets demonstrate that our method improves over competitors for discrete parameter settings, and enhances state-of-the-art techniques for continuous parameter settings.", "title": "Lazy Paired Hyper-Parameter Tuning"}, {"url": "https://www.ijcai.org/Abstract/13/285", "abstract": "Error-correcting output codes (ECOC) are a successful technique to combine a set of binary classifiers for multi-class learning problems. However, in traditional ECOC framework, all the base classifiers are trained independently according to the defined ECOC matrix. In this paper, we reformulate the ECOC models from the perspective of multi-task learning, where the binary classifiers are learned in a common subspace of data. This novel model can be considered as an adaptive generalization of the traditional ECOC framework. It simultaneously optimizes the representation of data as well as the binary classifiers. More importantly, it builds a bridge between the ECOC framework and multi-task learning for multi-class learning problems. To deal with complex data, we also present the kernel extension of the proposed model. Extensive empirical study on 14 data sets from UCI machine learning repository and the USPS handwritten digits recognition application demonstrates the effectiveness and efficiency of our model.", "title": "Adaptive Error-Correcting Output Codes"}, {"url": "https://www.ijcai.org/Abstract/13/286", "abstract": "In classification problems,isotonic regression has been commonly used to map the prediction scores to posterior class probabilities. However, isotonic regression may suffer from overfitting, and the learned mapping is often discontinuous. Besides, current efforts mainly focus on the calibration of a single classifier. As different classifiers have different strengths, a combination of themcan lead to better performance. In this paper, we propose a novel probability calibration approach for such an ensemble of classifiers. We first construct isotonic constraints on the desired probabilities based on soft voting of the classifiers. Manifold information is also incorporated to combat overfitting and ensurefunction smoothness. Computationally,the extended isotonic regression model can be learned efficiently by a novel optimization algorithm based on the alternating direction method of multipliers (ADMM). Experimentson a number of real-world data sets demonstrate that the proposed approach consistently outperforms independent classifiers and other combinations of the classifiers' probabilities in terms of the Brier score and AUC.", "title": "Accurate Probability Calibration for Multiple Classifiers"}, {"url": "https://www.ijcai.org/Abstract/13/287", "abstract": "In low-rank and sparse matrix decomposition, the entries of the sparse part are often assumed to be i.i.d. sampled from a random distribution. But the structure of sparse part, as the central interest of many problems, has been rarely studied. One motivating problem is tracking multiple sparse object flows (motions) in video. We introduce \"shifted subspaces tracking (SST)\" to segment the motions and recover their trajectories by exploring the low-rank property of background and the shifted subspace property of each motion. SST is composed of two steps, background modeling and flow tracking. In step 1, we propose \"semi-soft GoDec\" to separate all the motions from the low-rank background L as a sparse outlier S. Its soft-thresholding in updating S significantly speeds up GoDec and facilitates the parameter tuning. In step 2, we update X as S obtained in step 1 and develop \"SST algorithm\" further decomposing X as X=\u2211i=1kL(i)\u25ca\u03c4(i)+S+G, wherein L(i) is a low-rank matrix storing the ith flow after transformation \u03c4(i). SST algorithm solves k sub-problems in sequel by alternating minimization, each of which recovers one L(i) and its \u03c4(i) by randomized method. Sparsity of L(i) and between-frame affinity are leveraged to save computations. We justify the effectiveness of SST on surveillance video sequences.", "title": "Shifted Subspaces Tracking on Sparse Outlier for Motion Segmentation"}, {"url": "https://www.ijcai.org/Abstract/13/288", "abstract": "Persistent homology is a mathematical tool from topological data analysis. It performs multi-scale analysis on a set of points and identifies clusters, holes, and voids therein. These latter topological structures complement standard feature representations, making persistent homology an attractive feature extractor for artificial intelligence. Research on persistent homology for AI is in its infancy, and is currently hindered by two issues: the lack of an accessible introduction to AI researchers, and the paucity of applications. In response, the first part of this paper presents a tutorial on persistent homology specifically aimed at a broader audience without sacrificing mathematical rigor. The second part contains one of the first applications of persistent homology to natural language processing. Specifically, our Similarity Filtration with Time Skeleton (SIFTS) algorithm identifies holes that can be interpreted as semantic \"tie-backs\" in a text document, providing a new document structure representation. We illustrate our algorithm on documents ranging from nursery rhymes to novels, and on a corpus with child and adolescent writings.", "title": "Persistent Homology: An Introduction and a New Text Representation for Natural Language Processing"}, {"url": "https://www.ijcai.org/Abstract/13/289", "abstract": "Cross-domain learning targets at leveraging the knowledge from source domains to train accurate models for the test data from target domains with different but related data distributions. To tackle the challenge of data distribution difference in terms of raw features, previous works proposed to mine high-level concepts (e.g., word clusters) across data domains, which shows to be more appropriate for classification. However, all these works assume that the same set of concepts are shared in the source and target domains in spite that some distinct concepts may exist only in one of the data domains. Thus, we need a general framework, which can incorporate both shared and distinct concepts, for cross-domain classification. To this end, we develop a probabilistic model, by which both the shared and distinct concepts can be learned by the EM process which optimizes the data likelihood. To validate the effectiveness of this model we intentionally construct the classification tasks where the distinct concepts exist in the data domains. The systematic experiments demonstrate the superiority of our model over all compared baselines, especially on those much more challenging tasks.", "title": "Concept Learning for Cross-Domain Text Classification: A General Probabilistic Framework"}, {"url": "https://www.ijcai.org/Abstract/13/291", "abstract": "Natural deduction, which is a method for establishing validity of propositional type arguments, helps develop important reasoning skills and is thus a key ingredient in a course on introductory logic. We present two core components, namely solution generation and practice problem generation, for enabling computer-aided education for this important subject domain. The key enabling technology is use of an offline-computed data-structure called Universal Proof Graph (UPG) that encodes all possible applications of inference rules over all small propositions abstracted using their bitvector-based truth-table representation. This allows an efficient forward search for solution generation. More interestingly, this allows generating fresh practice problems that have given solution characteristics by performing a backward search in UPG. We obtained around 300 natural deduction problems from various textbooks. Our solution generation procedure can solve many more problems than the traditional forward-chaining based procedure, while our problem generation procedure can efficiently generate several variants with desired characteristics.", "title": "Automatically Generating Problems and Solutions for Natural Deduction"}, {"url": "https://www.ijcai.org/Abstract/13/292", "abstract": "One challenge in making online education more effective is to develop automatic grading software that can provide meaningful feedback. This paper provides a solution to automatic grading of the standard computation-theory problem that asks a student to construct a deterministic finite automaton (DFA) from the given description of its language. We focus on how to assign partial grades for incorrect answers. Each student's answer is compared to the correct DFA using a hybrid of three techniques devised to capture different classes of errors. First, in an attempt to catch syntactic mistakes, we compute the edit distance between the two DFA descriptions. Second, we consider the entropy of the symmetric difference of the languages of the two DFAs, and compute a score that estimates the fraction of the number of strings on which the student answer is wrong. Our third technique is aimed at capturing mistakes in reading of the problem description. For this purpose, we consider a description language Mosel, which adds syntactic sugar to the classical Monadic Second Order Logic, and allows defining regular languages in a concise and natural way. We provide algorithms, along with optimizations, for transforming Mosel descriptions into DFAs and vice-versa. These allow us to compute the syntactic edit distance of the incorrect answer from the correct one in terms of their logical representations. We report an experimental study that evaluates hundreds of answers submitted by (real) students by comparing grades/feedback computed by our tool with human graders. Our conclusion is that the tool is able to assign partial grades in a meaningful way, and should be preferred over the human graders for both scalability and consistency.", "title": "Automated Grading of DFA Constructions"}, {"url": "https://www.ijcai.org/Abstract/13/293", "abstract": "It is indispensable for users to evaluate the trustworthiness of other users (referred to as advisors), to cope with possible misleading opinions provided by them. Advisors' misleading opinions may be induced by their dishonest, subjectivity difference with users, or both. Existing approaches do not well distinguish the two different causes. In this paper, we propose a novel probabilistic graphical trust model to separately consider these two factors, involving three types of latent variables: benevolence, integrity and competence of advisors, trust propensity of users, and subjectivity difference between users and advisors. Experimental results on real datasets demonstrate that our method advances state-of-the-art approaches to a large extent.", "title": "Misleading Opinions Provided by Advisors: Dishonesty or Subjectivity"}, {"url": "https://www.ijcai.org/Abstract/13/294", "abstract": "Constraint-based applications such as configurators, recommenders, and scheduling systems support users in complex decision making scenarios. Typically, these systems try to identify a solution that satisfies all articulated user requirements. If the requirements are inconsistent with the underlying constraint set, users have to be actively supported in finding a way out from the no solution could be found dilemma. In this paper we introduce techniques that support the calculation of personalized diagnoses for inconsistent constraint sets. These techniques significantly improve the diagnosis prediction quality compared to approaches based on the calculation of minimal cardinality diagnoses. In order to show the applicability of our approach we present the results of an empirical study and a corresponding performance analysis.", "title": "Personalized Diagnosis for Over-Constrained Problems"}, {"url": "https://www.ijcai.org/Abstract/13/296", "abstract": "On-line portfolio selection has been attracting increasing interests from artificial intelligence community in recent decades. Mean reversion, as one most frequent pattern in financial markets, plays an important role in some state-of-the-art strategies. Though successful in certain datasets, existing mean reversion strategies do not fully consider noises and outliers in the data, leading to estimation error and thus non-optimal portfolios, which results in poor performance in practice. To overcome the limitation, we propose to exploit the reversion phenomenon by robust $L_1$-median estimator, and design a novel on-line portfolio selection strategy named \"Robust Median Reversion\" (RMR), which makes optimal portfolios based on the improved reversion estimation. Empirical results on various real markets show that RMR can overcome the drawbacks of existing mean reversion algorithms and achieve significantly better results. Finally, RMR runs in linear time, and thus is suitable for large-scale trading applications.", "title": "A Brain-Computer Interface to a Plan-Based Narrative"}, {"url": "https://www.ijcai.org/Abstract/13/296", "abstract": "On-line portfolio selection has been attracting increasing interests from artificial intelligence community in recent decades. Mean reversion, as one most frequent pattern in financial markets, plays an important role in some state-of-the-art strategies. Though successful in certain datasets, existing mean reversion strategies do not fully consider noises and outliers in the data, leading to estimation error and thus non-optimal portfolios, which results in poor performance in practice. To overcome the limitation, we propose to exploit the reversion phenomenon by robust $L_1$-median estimator, and design a novel on-line portfolio selection strategy named \"Robust Median Reversion\" (RMR), which makes optimal portfolios based on the improved reversion estimation. Empirical results on various real markets show that RMR can overcome the drawbacks of existing mean reversion algorithms and achieve significantly better results. Finally, RMR runs in linear time, and thus is suitable for large-scale trading applications.", "title": "Robust Median Reversion Strategy for On-Line Portfolio Selection"}, {"url": "https://www.ijcai.org/Abstract/13/297", "abstract": "We put forward a cutoff technique for determining the number of agents that is sufficient to consider when checking temporal-epistemic specifications on a system of any size. We identify a special class of interleaved interpreted systems for which we give a parameterised semantics and an abstraction methodology. This enables us to overcome the significant limitations in expressivity present in the state-of-the-art. We present an implementation and discuss experimental results.", "title": "A Cutoff Technique for the Verification of Parameterised Interpreted Systems with Parameterised Environments"}, {"url": "https://www.ijcai.org/Abstract/13/298", "abstract": "Authoring tutorials for complex software applications is a time consuming process. It also highly depends on the tutorial designer's skill level and experience. This paper introduces an approach which automatically generates software tutorials using the digital artifacts produced by the users of a software program. We model this process as an optimal planning problem using software produced artifacts, software specifications and the human-computer interaction Keystroke-Level Model (KLM). We present TutorialPlan, an automated tutorial generator, which creates step-by-step text and image instructions from CAD drawings and helps users learn AutoCAD, a complex design and drafting software. In our tutorial generator, the optimal planning problem is represented and solved using DLV, a general Answer Set Programming (ASP) system. DLV offers a natural representation of both the problem and the heuristics needed to solve it efficiently. A user study shows that the tutorials generated by our system are comparable to those generated by experienced AutoCAD users.", "title": "TutorialPlan: Automated Tutorial Generation from CAD Drawings"}, {"url": "https://www.ijcai.org/Abstract/13/299", "abstract": "Automatic mathematical solution assessment checks the equivalence of mathematical expressions in the user answer and standard solution. It is a challenging problem as the semantics of mathematical expressions are highly symbolic and equivalent mathematical expressions can be expressed in different forms. In this paper, we propose an effective Probabilistic Equivalence Verification (PEV) approach for automatic mathematical solution assessment. The proposed PEV approach is a randomized method based on the probabilistic numerical equivalence testing of two mathematical expressions. It can avoid false negative errors completely while guaranteeing a small probability of false positive errors to occur. The performance results have shown that the proposed PEV approach has outperformed other popular techniques in Computer Algebra Systems such as Maple and Mathematica.", "title": "Probabilistic Equivalence Verification Approach for Automatic Mathematical Solution Assessment"}, {"url": "https://www.ijcai.org/Abstract/13/300", "abstract": "People's facial expressions, whether made consciously or subconsciously, continuously reveal their state of mind. This work proposes a method for predicting people's strategic decisions based on their facial expressions. We designed a new version of the centipede game that introduces an incentive for the human participant to hide her facial expressions. We recorded on video participants who played several games of our centipede version, and concurrently logged their decisions throughout the games. The video snippet of the participants' faces prior to their decisions is represented as a fixed-size vector by estimating the covariance matrix of key facial points which change over time. This vector serves as input to a classifier that is trained to predict the participant's decision. We compare several training techniques, all of which are designed to work with the imbalanced decisions typically made by the players of the game. Furthermore, we investigate adaptation of the trained model to each player individually, while taking into account the player's facial expressions in the previous games. The results show that our method outperforms standard SVM as well as humans in predicting subjects'strategic decisions. To the best of our knowledge, this is the first study to present a methodology for predicting people's strategic decisions when there is an incentive to hide facial expressions.", "title": "Predicting Human Strategic Decisions Using Facial Expressions"}, {"url": "https://www.ijcai.org/Abstract/13/300", "abstract": "People's facial expressions, whether made consciously or subconsciously, continuously reveal their state of mind. This work proposes a method for predicting people's strategic decisions based on their facial expressions. We designed a new version of the centipede game that introduces an incentive for the human participant to hide her facial expressions. We recorded on video participants who played several games of our centipede version, and concurrently logged their decisions throughout the games. The video snippet of the participants' faces prior to their decisions is represented as a fixed-size vector by estimating the covariance matrix of key facial points which change over time. This vector serves as input to a classifier that is trained to predict the participant's decision. We compare several training techniques, all of which are designed to work with the imbalanced decisions typically made by the players of the game. Furthermore, we investigate adaptation of the trained model to each player individually, while taking into account the player's facial expressions in the previous games. The results show that our method outperforms standard SVM as well as humans in predicting subjects'strategic decisions. To the best of our knowledge, this is the first study to present a methodology for predicting people's strategic decisions when there is an incentive to hide facial expressions.", "title": "Employing Batch Reinforcement Learning to Control Gene Regulation"}, {"url": "https://www.ijcai.org/Abstract/13/301", "abstract": "The goal of controlling a gene regulatory network (GRN) is to generate an intervention strategy, i.e., a control policy, such that by applying the policy the system will avoid undesirable states. In this work, we propose a method to control GRNs by using Batch Mode Reinforcement Learning (Batch RL). Our idea is based on the fact that time series gene expression data can actually be interpreted as a sequence of experience tuples collected from the environment. Existing studies on this control task try to infer a model using gene expression data and then calculate a control policy over the constructed model. However, we propose a method that can directly use the available gene expression data to obtain an approximated control policy for gene regulation that avoids the time consuming model building phase. Results show that we can obtain policies for gene regulation systems of several thousands of genes just in several seconds while existing solutions get stuck for even tens of genes. Interestingly, the reported results also show that our method produces policies that are almost as good as the ones generated by existing model dependent methods.", "title": "without Explicitly Constructing Gene Regulatory Networks"}, {"url": "https://www.ijcai.org/Abstract/13/302", "abstract": "Understanding the molecular mechanisms of life requires decoding the functions of the proteins in an organism. Various high-throughput experimental techniques have been developed to characterize biological systems at the genome scale. A fundamental challenge of the post-genomic era is to assign biological functions to all the proteins encoded by the genome using high-throughput biological data. To address this challenge, we propose a novel Laplacian Network Partitioning incorporating function category Correlations (LNPC) method to predict protein function on proteinprotein interaction (PPI) networks by optimizing a Laplacian based quotient objective function that seeks the optimal network configuration to maximize consistent function assignments over edges on the whole graph. Unlike the existing approaches that have no unique optimization solutions, our optimization problem has unique global solution by eigen-decomposition methods. The correlations among protein function categories are quantified and incorporated into a correlated protein affinity graph which is integrated into the PPI graph to significantly improve the protein function prediction accuracy. We apply our new method to the BioGRID dataset for the Saccharomyces Cerevisiae species using the MIPS annotation scheme. Our new method outperforms other related state-of-the-art approaches more than 63% by the average precision of function prediction and 53% by the average F1 score.", "title": "Protein Function Prediction via Laplacian Network Partitioning Incorporating Function Category Correlations"}, {"url": "https://www.ijcai.org/Abstract/13/304", "abstract": "Post-editing feedback provided by users of on-line translation services offers an excellent opportunity for automatic improvement of statistical machine translation (SMT) systems. However, feedback provided by casual users is very noisy, and must be automatically filtered in order to identify the potentially useful cases. We present a study on automatic feedback filtering in a real weblog collected from Reverso.net. We extend and re-annotate a training corpus, define an extended set of simple features and approach the problem as a binary classification task, experimenting with linear and kernel based classifiers and feature selection. Results on the feedback filtering task show a significant improvement over the majority class, but also a precision ceiling around 70-80%. This reflects the inherent difficulty of the problem and indicates that shallow features cannot fully capture the semantic nature of the problem. Despite the modest results on the filtering task, the classifiers are proven effective in an application-based evaluation. The incorporation of a filtered set of feedback instances selected from a larger corpus significantly improves the performance of a phrase-based SMT system, according to a set of standard evaluation metrics.", "title": "Identifying Useful Human Correction Feedback from an On-line Machine Translation Service"}, {"url": "https://www.ijcai.org/Abstract/13/305", "abstract": "The ability to recognize analogies is an important factor that is closely related to human intelligence. Verbal analogies have been used for evaluating both examinees at university entrance exams as well as algorithms for measuring relational similarity. However, relational similarity measures proposed so far are confined to measuring the similarity between pairs of words. Unfortunately, such pairwise approaches ignore the rich relational structure that exists in real-world knowledge bases containing millions of entities and semantic relations. We pro- pose a method to efficiently identify analogous entity tuples from a given entity-relation graph. First, we present an efficient approach for extract- ing potential analogous tuples from a given entity-relation graph. Second, to measure the structural similarity between two tuples, we propose two types of kernel functions: vertex-feature kernels, and edge-feature kernels. Moreover, we combine those kernels to construct composite kernels that simultaneously consider both vertex and edge features. Experimental results show that our proposed method accurately identifies analogous tuples and significantly outperforms a state-of-the-art pairwise relational similarity measure, extended to tuples.", "title": "Mining for Analogous Tuples from an Entity-Relation Graph"}, {"url": "https://www.ijcai.org/Abstract/13/306", "abstract": "Topic models have been widely used to identify topics in text corpora. It is also known that purely unsupervised models often result in topics that are not comprehensible in applications. In recent years, a number of knowledge-based models have been proposed, which allow the user to input prior knowledge of the domain to produce more coherent and meaningful topics. In this paper, we go one step further to study how the prior knowledge from other domains can be exploited to help topic modeling in the new domain. This problem setting is important from both the application and the learning perspectives because knowledge is inherently accumulative. We human beings gain knowledge gradually and use the old knowledge to help solve new problems. To achieve this objective, existing models have some major difficulties. In this paper, we propose a novel knowledge-based model, called MDK-LDA, which is capable of using prior knowledge from multiple domains. Our evaluation results will demonstrate its effectiveness.", "title": "Leveraging Multi-Domain Prior Knowledge in Topic Models"}, {"url": "https://www.ijcai.org/Abstract/13/307", "abstract": "Hashtags can be viewed as an indication to the context of the tweet or as the core idea expressed in the tweet. They provide valuable information for many applications, such as information retrieval, opinion mining, text classification, and so on. However, only a small number of microblogs are manually tagged. To address this problem, in this work, we propose a topical translation model for microblog hashtag suggestion. We assume that the content and hashtags of the tweet are talking about the same themes but written in different languages. Under the assumption, hashtag suggestion is modeled as a translation process from content to hashtags. Moreover, in order to cover the topic of tweets, the proposed model regards the translation probability to be topic-specific. It uses topic-specific word trigger to bridge the vocabulary gap between the words in tweets and hashtags, and discovers the topics of tweets by a topic model designed for microblogs. Experimental results on the dataset crawled from real world microblogging service demonstrate that the proposed method outperforms state-of-the-art methods.", "title": "Learning Topical Translation Model for Microblog Hashtag Suggestion"}, {"url": "https://www.ijcai.org/Abstract/13/308", "abstract": "Bracketing induction is the unsupervised learning of hierarchical constituents without labeling their syntactic categories such as verb phrase (VP) from natural raw sentences. Constituent Context Model (CCM) is an effective generative model for the bracketing induction, but the CCM computes probability of a constituent in a very straightforward way no matter how long this constituent is. Such method causes severe data sparse problem because long constituents are more unlikely to appear in test set. To overcome the data sparse problem, this paper proposes to define a non-parametric Bayesian prior distribution, namely the Pitman-Yor Process (PYP) prior, over constituents for constituent smoothing. The PYP prior functions as a back-off smoothing method through using a hierarchical smoothing scheme (HSS). Various kinds of HSS are proposed in this paper. We find that two kinds of HSS are effective, attaining or significantly improving the state-of-the-art performance of the bracketing induction evaluated on standard treebanks of various languages, while another kind of HSS, which is commonly used for smoothing sequences by n-gram Markovization, is not effective for improving the performance of the CCM.", "title": "Smoothing for Bracketing Induction"}, {"url": "https://www.ijcai.org/Abstract/13/309", "abstract": "Structured Web search incorporating data from structured sources into search engine results has attracted much attention from both academic and industrial communities. To understand user's intent, query structure interpretation is proposed to analyze the structure of queries in a query log and map query terms to the semantically relevant attributes of data sources in a target domain. Existing methods assume all queries should be classified to the target domain, and thus they are limited when interpreting queries from different domains in real query logs. To address the problem, we introduce a human-machine hybrid method by utilizing crowdsourcing platforms. Our method selects a small number of query terms and asks the crowdsourcing workers to interpret them, and then infers the interpretations based on the crowdsourcing results. To improve the performance, we propose an iterative probabilistic inference method based on a similarity graph of query terms, and select the most useful query terms for crowdsourcing by considering their domain-relevance and gained benefit. We evaluate our method on a real query log, and the experimental results show that our method outperforms the state-of-the-art method.", "title": "Crowdsourcing-Assisted Query Structure Interpretation"}, {"url": "https://www.ijcai.org/Abstract/13/310", "abstract": "In this paper, we investigate a very challenging task of automatically generating presentation slides for academic papers. The generated presentation slides can be used as drafts to help the presenters prepare their formal slides in a quicker way. A novel system called PPSGen is proposed to address this task. It first employs regression methods to learn the importance of the sentences in an academic paper, and then exploits the integer linear programming (ILP) method to generate well-structured slides by selecting and aligning key phrases and sentences. Evaluation results on a test set of 200 pairs of papers and slides collected on the web demonstrate that our proposed PPSGen system can generate slides with better quality. A user study is also illustrated to show that PPSGen has a few evident advantages over baseline methods.", "title": "PPSGen: Learning to Generate Presentation Slides for Academic Papers"}, {"url": "https://www.ijcai.org/Abstract/13/311", "abstract": "Coreference resolution is the problem of clustering mentions into entities and is very critical for natural language understanding. This paper studies the problem of coreference resolution in the context of the important domain of clinical text. Clinical text is unique because it requires significant use of domain knowledge to support coreference resolution. It also has specific discourse characteristics which impose several constraints on coreference decisions. We present a principled framework to incorporate knowledge-based constraints in the coreference model. We also show that different pronouns behave quite differently, necessitating the development of distinct ways for resolving different pronouns. Our methods result in significant performance improvements and we report the best results on a clinical corpora that has been used in coreference shared tasks. Moreover, for the first time, we report the results for end-to-end coreference resolution on this corpora.", "title": "End-to-End Coreference Resolution for Clinical Narratives"}, {"url": "https://www.ijcai.org/Abstract/13/312", "abstract": "Empty elements (EEs) play a critical role in Chinese syntactic, semantic and discourse analysis. Previous studies employ a language-independent sentence-level approach to EE recovery, by casting it as a linear tagging or structured parsing problem. In comparison, this paper proposes a clause-level hybrid approach to address specific problems in Chinese EE recovery, which recovers EEs in Chinese language from the clause perspective and integrates the advantages of both linear tagging and structured parsing. In particular, a comma disambiguation method is employed to improve syntactic parsing and help determine clauses in Chinese. In this way, the noise introduced by sentence-level syntactic parsing and multiple EEs in the same position of a linear sentence can be well addressed. Evaluation on Chinese Treebank 6.0 shows the significant performance improvement of our clause-level hybrid approach over the state-of-the-art sentence-level baselines, and its great impact on a state-of-the-art Chinese syntactic parser.", "title": "A Clause-Level Hybrid Approach to Chinese Empty Element Recovery"}, {"url": "https://www.ijcai.org/Abstract/13/313", "abstract": "Argument extraction is a challenging task in event extraction. However, most of previous studies focused on intra-sentence information and failed to extract inter-sentence arguments. This paper proposes a discourse-level joint model of argument identification and role determination to infer those inter-sentence arguments in a discourse. Moreover, to better represent the relationship among relevant event mentions and the relationship between an event mention and its arguments in a discourse, this paper introduces various kinds of corpus-based and discourse-based constraints in the joint model, either automatically learned or linguistically motivated. Evaluation on the ACE 2005 Chinese corpus justifies the effectiveness of our joint model over a strong baseline in Chinese argument extraction, in particular argument identification.", "title": "Joint Modeling of Argument Identification and Role Determination in Chinese Event Extraction with Discourse-Level Information"}, {"url": "https://www.ijcai.org/Abstract/13/314", "abstract": "In the literature, various approaches have been proposed to address the domain adaptation problem in sentiment classification (also called cross-domain sentiment classification). However, the adaptation performance normally much suffers when the data distributionsin the source and target domains differ significantly. In this paper, we suggest to perform active learning for cross-domain sentiment classification by actively selecting a smallamount of labeled data in the target domain. Accordingly, we propose an novel active learning approach for cross-domain sentiment classification. First, we train two individual classifiers, i.e., the source and target classifiers with the labeled data from the source and target respectively. Then, the two classifiers are employed to select informative samples with the selection strategy of QueryBy Committee (QBC). Third, the two classifier is combined to make the classification decision. Importantly, the two classifiers are trained by fully exploiting the unlabeled data in the target domain with the label propagation(LP) algorithm. Empirical studies demonstrate the effectiveness of our active learning approach for cross-domain sentiment classification over some strong baselines.", "title": "Active Learning for Cross-Domain Sentiment Classification"}, {"url": "https://www.ijcai.org/Abstract/13/315", "abstract": "Mining opinion targets from online reviews is an important and challenging task in opinion mining. This paper proposes a novel approach to extract opinion targets by using partial-supervised word alignment model (PSWAM). At first, we apply PSWAM in a monolingual scenario to mine opinion relations in sentences and estimate the associations between words. Then, a graph-based algorithm is exploited to estimate the confidence of each candidate, and the candidates with higher confidence will be extracted as the opinion targets. Compared with existing syntax-based methods, PSWAM can effectively avoid parsing errors when dealing with informal sentences in online reviews. Compared with the methods using alignment model, PSWAM can capture opinion relations more precisely through partial supervision from partial alignment links. Moreover, when estimating candidate confidence, we make penalties on higher-degree vertices in our graph-based algorithm in order to decrease the probability of the random walk running into the unrelated regions in the graph. As a result, some errors can be avoided. The experimental results on three data sets with different sizes and languages show that our approach outperforms state-of-the-art methods.", "title": "Opinion Target Extraction Using Partially-Supervised Word Alignment Model"}, {"url": "https://www.ijcai.org/Abstract/13/316", "abstract": "This paper is concerned with data selection for adapting language model (LM) in statistical machine translation (SMT), and aims to find the LM training sentences that are topic similar to the translation task. Although the traditional approaches have gained significant performance, they ignore the topic information and the distribution information of words when selecting similar training sentences. In this paper, we present two bilingual topic model (BLTM) (joint and coupled BLTM) based sentence representations for cross-lingual data selection. We map the data selection task into cross-lingual semantic representations that are language independent, then rank and select sentences in the target language LM training corpus for a sentence in the translation task by the semantics-based likelihood. The semantic representations are learned from the parallel corpus, with the assumption that the bilingual pair shares the same or similar distribution over semantic topics. Large-scale experimental results demonstrate that our approaches significantly outperform the state-of-the-art approaches on both LM perplexity and translation performance, respectively.", "title": "Joint and Coupled Bilingual Topic Model Based Sentence Representations for Language Model Adaptation"}, {"url": "https://www.ijcai.org/Abstract/13/317", "abstract": "In this paper we present an approach aimed at enriching the Open Information Extraction paradigm with semantic relation ontologization by integrating syntactic and semantic features into its workflow. To achieve this goal, we combine deep syntactic analysis and distributional semantics using a shortest path kernel method and soft clustering. The output of our system is a set of automatically discovered and ontologized semantic relations.", "title": "Integrating Syntactic and Semantic Analysis into the Open Information Extraction Paradigm"}, {"url": "https://www.ijcai.org/Abstract/13/318", "abstract": "This paper presents a reranking approach to combining constituent and dependency parsing, aimed at improving parsing performance on both sides. Most previous combination methods rely on complicated joint decoding to integrate graph- and transition-based dependency models. Instead, our approach makes use of a high-performance probabilistic context free grammar (PCFG) model to output k-best candidate constituent trees, and then a dependency parsing model to rerank the trees by their scores from both models, so as to get the most probable parse. Experimental results show that this reranking approach achieves the highest accuracy of constituent and dependency parsing on Chinese treebank (CTB5.1) and a comparable performance to the state of the art on English treebank (WSJ).", "title": "Combine Constituent and Dependency Parsing via Reranking"}, {"url": "https://www.ijcai.org/Abstract/13/319", "abstract": "Convolution tree kernels have been successfully applied to many language processing tasks for achieving state-of-the-art accuracy. Unfortunately, higher computational complexity of learning with kernels w.r.t. using explicit feature vectors makes them less attractive for large-scale data.In this paper, we study the latest approaches to solve such problems ranging from feature hashing to reverse kernel engineering and approximate cutting plane training with model compression. We derive a novel method that relies on reverse-kernel engineering together with an efficient kernel learning method. The approach gives the advantage of using tree kernels to automatically generate rich structured feature spaces and working in the linear space where learning and testing is fast. We experimented with training sets up to 4 million examples from Semantic Role Labeling. The results show that (i) the choice of correct structural features is essential and (ii) we can speed-up training from weeks to less than 20 minutes.", "title": "Fast Linearization of Tree Kernels over Large-Scale Data"}, {"url": "https://www.ijcai.org/Abstract/13/320", "abstract": "In question answering, answer extraction aims to pin-point the exact answer from passages. However, most previous methods perform such extraction on each passage separately, without considering clues provided in other passages. This paper presents a novel approach to extract answers by fully leveraging connections among different passages. Specially, extraction is performed on a PassageGraph which is built by adding links upon multiple passages. Different passages are connected by linking words with the same stem. We use the factor graph as our model for answer extraction. Experimental results on multiple QA datasets demonstrate that our method significantly improves the performance of answer extraction.", "title": "Answer Extraction from Passage Graph for Question Answering"}, {"url": "https://www.ijcai.org/Abstract/13/321", "abstract": "Due to the explosive growth of the Internet online reviews, we can easily collect a large amount of labeled reviews from different domains. But only some of them are beneficial for training a desired target-domain sentiment classifier. Therefore, it is important for us to identify those samples that are the most relevant to the target domain and use them as training data. To address this problem, a novel approach, based on instance selection and instance weighting via PU learning, is proposed. PU learning is used at first to learn an in-target-domain selector, which assigns an in-target-domain probability to each sample in the training set. For instance selection, the samples with higher in-target-domain probability are used as training data; For instance weighting, the calibrated in-target-domain probabilities are used as sampling weights for training an instance-weighted naive Bayes model, based on the principle of maximum weighted likelihood estimation. The experimental results prove the necessity and effectiveness of the approach, especially when the size of training data is large. It is also proved that the larger the Kullback-Leibler divergence between the training and test data is, the more effective the proposed approach will be.", "title": "Instance Selection and Instance Weighting for Cross-Domain Sentiment Classification via PU Learning"}, {"url": "https://www.ijcai.org/Abstract/13/322", "abstract": "Lexical cohesion arises from a chain of lexical items that establish links between sentences in a text. In this paper we propose three different models to capture lexical cohesion for document-level machine translation: (a) a direct reward model where translation hypotheses are rewarded whenever lexical cohesion devices occur in them, (b) a conditional probability model where the appropriateness of using lexical cohesion devices is measured, and (c) a mutual information trigger model where a lexical cohesion relation is considered as a trigger pair and the strength of the association between the trigger and the triggered item is estimated by mutual information. We integrate the three models into hierarchical phrase-based machine translation and evaluate their effectiveness on the NIST Chinese-English translation tasks with large-scale training data. Experiment results show that all three models can achieve substantial improvements over the baseline and that the mutual information trigger model performs better than the others.", "title": "Modeling Lexical Cohesion for Document-Level Machine Translation"}, {"url": "https://www.ijcai.org/Abstract/13/323", "abstract": "Previous text processing techniques focus on text itself while neglecting human reading process. Therefore they are limited in special applications. This paper proposes a text scanning mechanism for generating the dynamic impressions of words in text by simulating recall, association and forget processes during reading. Experiments show that the mechanism is suitable for multiple text processing applications.", "title": "A Text Scanning Mechanism Simulating Human Reading Process"}, {"url": "https://www.ijcai.org/Abstract/13/324", "abstract": "Part of the long lasting cultural heritage of China is the classical ancient Chinese poems which follow strict formats and complicated linguistic rules. Automatic Chinese poetry composition by programs is considered as a challenging problem in computational linguistics and requires high Artificial Intelligence assistance, and has not been well addressed. In this paper, we formulate the poetry composition task as an optimization problem based on a generative summarization framework under several constraints. Given the user specified writing intents, the system retrieves candidate terms out of a large poem corpus, and then orders these terms to fit into poetry formats, satisfying tonal and rhythm requirements. The optimization process under constraints is conducted via iterative term substitutions till convergence, and outputs the subset with the highest utility as the generated poem. For experiments, we perform generation on large datasets of 61,960 classic poems from Tang and Song Dynasty of China. A comprehensive evaluation, using both human judgments and ROUGE scores, has demonstrated the effectiveness of our proposed approach.", "title": "i, Poet: Automatic Chinese Poetry Composition through a Generative Summarization Framework under Constrained Optimization"}, {"url": "https://www.ijcai.org/Abstract/13/325", "abstract": "With the progress in machine translation, it becomes more subtle to develop the evaluation metric capturing the systems' differences in comparison to the human translations. In contrast to the current efforts in leveraging more linguistic information to depict translation quality, this paper takes the thread of combining language independent features for a robust solution to MT evaluation metric. To compete with finer granularity of modeling brought by linguistic features, the proposed method augments the word level metrics by a letter based calculation. An empirical study is then conducted over WMT data to train the metrics by ranking SVM. The results reveal that the integration of current language independent metrics can generate well enough performance for a variety of languages. Time-split data validation is promising as a better training setting, though the greedy strategy also works well.", "title": "Fusion of Word and Letter Based Metrics for Automatic MT Evaluation"}, {"url": "https://www.ijcai.org/Abstract/13/326", "abstract": "In statistical word alignment for machine translation, function words usually cause poor aligning performance because they do not have clear correspondence between different languages. This paper proposes a novel approach to improve word alignment by pruning alignments of function words from an existing alignment model with high precision and recall. Based on monolingual and bilingual frequency characteristics, a language-independent function word recognition algorithm is first proposed. Then a group of carefully defined syntactic structures combined with content word alignments are used for further function word alignment pruning. The experimental results show that the proposed approach improves both the quality of word alignment and the performance of statistical machine translation on Chinese-to-English, German-to-English and French-to-English language pairs.", "title": "Improving Function Word Alignment with Frequency and Syntactic Information"}, {"url": "https://www.ijcai.org/Abstract/13/327", "abstract": "Cross lingual entity linking means linking an entity mention in a background source document in one language with the corresponding real world entity in a knowledge base written in the other language. The key problem is to measure the similarity score between the context of the entity mention and the document of the candidate entity. This paper presents a general framework for doing cross lingual entity linking by leveraging a large scale and bilingual knowledge base, Wikipedia. We introduce a bilingual topic model that mining bilingual topic from this knowledge base with the assumption that the same Wikipedia concept documents of two different languages share the same semantic topic distribution. The extracted topics have two types of representation, with each type corresponding to one language. Thus both the context of the entity mention and the document of the candidate entity can be represented in a space using the same semantic topics. We use these topics to do cross lingual entity linking. Experimental results show that the proposed approach can obtain the competitive results compared with the state-of-art approach.", "title": "Cross Lingual Entity Linking with Bilingual Topic Model"}, {"url": "https://www.ijcai.org/Abstract/13/328", "abstract": "Keyword extraction attracts much attention for its significant role in various natural language processing tasks. While some existing methods for keyword extraction have considered using single type of semantic relatedness between words or inherent attributes of words, almost all of them ignore two important issues: 1) how to fuse multiple types of semantic relations between words into a uniform semantic measurement and automatically learn the weights of the edges between the words in the word graph of each document, and 2) how to integrate the relations between words and words' intrinsic features into a unified model. In this work, we tackle the two issues based on the supervised random walk model. We propose a supervised ranking based method for keyword extraction, which is called SEAFARER. It can not only automatically learn the weights of the edges in the unified graph of each document which includes multiple semantic relations but also combine the merits of semantic relations of edges and intrinsic attributes of nodes together. We conducted extensive experimental study on an established benchmark and the experimental results demonstrate that SEAFARER outperforms the state-of-the-art supervised and unsupervised methods.", "title": "Integrating Semantic Relatedness and Words\u2019 Intrinsic Features for Keyword Extraction"}, {"url": "https://www.ijcai.org/Abstract/13/329", "abstract": "We present partial-tree linearization, a generalized word ordering (i.e. ordering a set of input words into a grammatical and fluent sentence) task for text-to-text applications. Recent studies of word ordering can be categorized into either abstract word ordering (no input syntax except for POS) or tree linearization (input words are associated with a full unordered syntax tree). Partial-tree linearization covers the whole spectrum of input between these two extremes. By allowing POS and dependency relations to be associated with any subset of input words, partial-tree linearization is more practical for a dependency-based NLG pipeline, such as transfer-based MT and abstractive text summarization. In addition, a partial-tree linearizer can also perform abstract word ordering and full-tree linearization. Our system achieves the best published results on standard PTB evaluations of these tasks.", "title": "Partial-Tree Linearization: Generalized Word Ordering for Text Synthesis"}, {"url": "https://www.ijcai.org/Abstract/13/330", "abstract": "Community question answering (cQA), which provides a platform for people with diverse background to share information and knowledge, has become an increasingly popular research topic. In this paper, we focus on the task of question retrieval.The key problem of question retrieval is to measure the similarity between the queried questions and the historical questions which have been solved by other users. The traditional methods measure the similarity based on the bag-of-words(BOWs) representation. This representation neither captures dependencies between related words, nor handles synonyms or polysemous words. In this work, we first propose a way to build a concept thesaurus based on the semantic relations extracted from the world knowledge of Wikipedia. Then, we develop a unified framework to leverage these semantic relations in order to enhance the question similarity in the concept space. Experiments conducted on a real cQA data set show that with the help of Wikipedia thesaurus, the performance of question retrieval is improved as compared to the traditional methods.", "title": "Improving Question Retrieval in Community Question Answering Using World Knowledge"}, {"url": "https://www.ijcai.org/Abstract/13/331", "abstract": "Discriminative structured prediction models have been widely used in many natural language processing tasks, but it is challenging to apply the methods to semantic parsing. In this paper, by introducing hybrid tree as a latent structure variable to close the gap between the input sentences and output representations, we formulate semantic parsing as a structured prediction problem, based on the latent variable perceptron model incorporated with a tree edit-distance loss as optimization criterion. The proposed approach maintains the advantage of a discriminative model in accommodating flexible combination of features and naturally incorporates an efficient decoding algorithm in learning and inference. Furthermore, in order to enhance the efficiency and accuracy of inference, we design an effective approach based on vector space model to extract a smaller subset of relevant MR productions for test examples. Experimental results on publicly available corpus show that our approach significantly outperforms previous systems.", "title": "Efficient Latent Structural Perceptron with Hybrid Trees for Semantic Parsing"}, {"url": "https://www.ijcai.org/Abstract/13/333", "abstract": "Heuristic search with reachability-based heuristics is arguably the most successful paradigm in Automated Planning to date. In its earlier stages of development, heuristic search was proposed as both forward and backward search. Due to the disadvantages of backward search, in the last decade researchers focused mainly on forward search, and backward search was abandoned for the most part as a valid alternative. In the last years, important advancements regarding both the theoretical understanding and the performance of heuristic search have been achieved, applied mainly to forward search planners. In this work we revisit regression in planning with reachability-based heuristics, trying to extrapolate to backward search current lines of research that were not as well understood as they are now.", "title": "Revisiting Regression in Planning"}, {"url": "https://www.ijcai.org/Abstract/13/334", "abstract": "There are two major uses of abstraction in planning and search: refinement (where abstract solutions are extended into concrete solutions) and heuristics (where abstract solutions are used to compute heuristics for the original search space). These two approaches are usually viewed as unrelated in the literature. It is reasonable to believe, though, that they are related, since they are both intrinsically based on the structure of abstract search spaces. We take the first steps towards formally investigating their relationships, employing our recently introduced framework for analysing and comparing abstraction methods. By adding some mechanisms for expressing metric properties, we can capture concepts like admissibility and consistency of heuristics. We present an extensive study of how such metric properties relate to the properties in the original framework, revealing a number of connections between the refinement and heuristic approaches. This also provides new insights into, for example, Valtorta's theorem and spurious states.", "title": "Bridging the Gap Between Refinement and Heuristics in Abstraction"}, {"url": "https://www.ijcai.org/Abstract/13/335", "abstract": "Domain-independent optimal planning has seen important breakthroughs in recent years with the development of tractable and informative admissible heuristics, suitable for planners based on forward state-space search. These heuristics allow planners to optimally solve an important number of benchmark problems, including problems that are quite involved and difficult for the layman. In this paper we present a new admissible heuristic that is obtained from the state equation associated to the Petri-net representation of the planning problem. The new heuristic, that does not fall into one of the four standard classes, can be computed in polynomial time and is competitive with the current state of the art for optimal planning, as empirically demonstrated over a large number of problems, mainly because it often shows an improved quality-to-cost ratio. The new heuristic applies to SAS+ planning tasks with arbitrary non-negative action costs.", "title": "An Admissible Heuristic for SAS+ Planning Obtained from the State Equation"}, {"url": "https://www.ijcai.org/Abstract/13/336", "abstract": "Belief tracking is a basic problem in planning with sensing. While the problem is intractable, it has been recently shown that for both deterministic and non-deterministic systems expressed in compact form, it can be done in time and space that are exponential in the problem width. The width measures the maximum number of state variables that are all relevant to a given precondition or goal. In this work, we extend this result both theoretically and practically. First, we introduce an alternative decomposition scheme and algorithm with the same time complexity but different completeness guarantees, whose space complexity is much smaller: exponential in the causal width of the problem that measures the number of state variables that are causally relevant to a given precondition, goal, or observable. Second, we introduce a fast, meaningful, and powerful approximation that trades completeness by speed, and is both time and space exponential in the problem causal width. It is then shown empirically that the algorithm combined with simple heuristics yields state-of-the-art real-time performance in domains with high widths but low causal widths such as Minesweeper, Battleship, and Wumpus.", "title": "Causal Belief Decomposition for Planning with Sensing: Completeness Results and Practical Approximation"}, {"url": "https://www.ijcai.org/Abstract/13/337", "abstract": "The recent proliferation of smart-phones and other wearable devices has lead to a surge of new mobile applications. Partially observable Markov decision processes provide a natural framework to design applications that continuously make decisions based on noisy sensor measurements. However, given the limited battery life, there is a need to minimize the amount of online computation. This can be achieved by compiling a policy into a finite state controller since there is no need for belief monitoring or online search. In this paper, we propose a new branch and bound technique to search for a good controller. In contrast to many existing algorithms for controllers, our search technique is not subject to local optima. We also show how to reduce the amount of search by avoiding the enumeration of isomorphic controllers and by taking advantage of suitable upper and lower bounds. The approach is demonstrated on several benchmark problems as well as a smart-phone application to assist persons with Alzheimer's to wayfind.", "title": "Isomorph-Free Branch and Bound Search for Finite State Controllers"}, {"url": "https://www.ijcai.org/Abstract/13/338", "abstract": "Recently, several methods have been proposed for optimal delete-free planning. We present an incremental compilation approach that enables these methods to be applied to problems with conditional effects, which none of them support natively. With an h+ solver for problems with conditional effects in hand, we also consider adapting the h++ anytime lower bound function to use the more space-efficient PCce compilation. This avoids the memory limitation of the original h++ caused by its reliance on an exponential-space compilation. It also leads to improvements on some problems where memory is not an issue.", "title": "Optimal Delete-Relaxed (and Semi-Relaxed) Planning with Conditional Effects"}, {"url": "https://www.ijcai.org/Abstract/13/339", "abstract": "A key challenge in non-cooperative multi-agent systems is that of developing efficient planning algorithms for intelligent agents to interact and perform effectively among boundedly rational, self-interested agents (e.g., humans). The practicality of existing works addressing this challenge is being undermined due to either the restrictive assumptions of the other agents' behavior, the failure in accounting for their rationality, or the prohibitively expensive cost of modeling and predicting their intentions. To boost the practicality of research in this field, we investigate how intention prediction can be efficiently exploited and made practical in planning, thereby leading to efficient intention-aware planning frameworks capable of predicting the intentions of other agents and acting optimally with respect to their predicted intentions. We show that the performance losses incurred by the resulting planning policies are linearly bounded by the error of intention prediction. Empirical evaluations through a series of stochastic games demonstrate that our policies can achieve better and more robust performance than the state-of-the-art algorithms.", "title": "Interactive POMDP Lite: Towards Practical Planning to Predict and Exploit Intentions for Interacting with Self-Interested Agents"}, {"url": "https://www.ijcai.org/Abstract/13/340", "abstract": "The ability to understand the goals and plans of other agents is an important characteristic of intelligent behaviours in many contexts. One of the approaches used to endow agents with this capability is the weighted model counting approach. Given a plan library and a sequence of observations, this approach exhaustively enumerates plan execution models that are consistent with the observed behaviour. The probability that the agent might be pursuing a particular goal is then computed as a proportion of plan execution models satisfying the goal. The approach allows to recognize multiple interleaved plans, but suffers from a combinatorial explosion of plan execution models, which impedes its application to real-world domains. This paper presents a heuristic weighted model counting algorithm that limits the number of generated plan execution models in order to recognize goals quickly by computing their lower and upper bound likelihoods.", "title": "Controlling the Hypothesis Space in Probabilistic Plan Recognition"}, {"url": "https://www.ijcai.org/Abstract/13/341", "abstract": "Predictive models play a key role for inference and decision making in crowdsourcing. We present methods that can be used to guide the collection of data for enhancing the competency of such predictive models while using the models to provide a base crowdsourcing service. We focus on the challenge of ideally balancing the goals of collecting data over time for learning and for improving task performance with the cost of workers' contributions over the lifetime of the operation of a system. We introduce the use of distributions over a set of predictive models to represent uncertainty about the dynamics of the world. We employ a novel Monte Carlo algorithm to reason simultaneously about uncertainty about the world dynamics and the progression of task solution as workers are hired over time to optimize hiring decisions. We evaluate the methodology with experiments on a challenging citizen-science problem, demonstrating how it balances exploration and exploitation over the lifetime of a crowdsourcing system.", "title": "Lifelong Learning for Acquiring the Wisdom of the Crowd"}, {"url": "https://www.ijcai.org/Abstract/13/342", "abstract": "Real-world problems generally involve several antagonistic objectives, like quality and cost for design problems, or makespan and cost for planning problems. The only approaches to multiobjective AI Planning rely on metrics, that can incorporate several objectives in some linear combinations, and metric sensitive planners, that are able to give different plans for different metrics, and hence to eventually approximate the Pareto front of the multiobjective problem, i.e. the set of optimal trade-offs between the antagonistic objectives. Divide-and-Evolve (DaE) is an evolutionary planner that embeds a classical planner and feeds it with a sequence of subproblems of the problem at hand. Like all Evolutionary Algorithms, DaE can be turned into a Pareto-based multiobjective solver, even though using an embedded planner that is not metric sensitive. The Pareto-based multiobjective planner MO-DaE thus avoids the drawbacks of the aggregation method. Furthermore, using YAHSP as the embedded planner, it outperforms in many cases the metric-based approach using LPG metric sensitive planner, as witnessed by experimental results on original multiobjective benchmarks built upon IPC-2011 domains.", "title": "Pareto-Based Multiobjective AI Planning"}, {"url": "https://www.ijcai.org/Abstract/13/343", "abstract": "We propose a unified approach to plan execution and schedule dispatching that converts a plan, which has been augmented with temporal constraints, into a policy for dispatching. Our approach generalizes the original plan and temporal constraints so that the executor need only consider the subset of state that is relevant to successful execution of valid plan fragments. We can accommodate a variety of calamitous and serendipitous changes to the state of the world by supporting the seamless re-execution or omission of plan fragments, without the need for costly replanning. Our methodology for plan generalization and online dispatching is a novel combination of plan execution and schedule dispatching techniques. We demonstrate the effectiveness of our method through a prototype implementation and a series of experiments.", "title": "Flexible Execution of Partial Order Plans with Temporal Constraints"}, {"url": "https://www.ijcai.org/Abstract/13/344", "abstract": "Random walks have become a popular component of recent planning systems. The increased exploration is a valuable addition to more exploitative search methods such as Greedy Best First Search (GBFS). A number of successful planners which incorporate random walks have been built. The work presented here aims to exploit the experience gained from building those systems. It begins a systematic study of the design space and alternative choices for building such a system, and develops a new random walk planner from scratch, with careful experiments along the way. Four major insights are: 1. a high state evaluation frequency is usually superior to the endpoint-only evaluation used in earlier systems, 2. adjusting the restarting parameter according to the progress speed in the search space performs better than any fixed setting, 3. biasing the action selection towards preferred operators of only the current state is better than Monte Carlo Helpful Actions, which depend on the number of times an action has been a preferred operator in previous walks, and 4. even simple forms of random walk planning can compete with GBFS.", "title": "Towards a Second Generation Random Walk Planner: An Experimental Exploration"}, {"url": "https://www.ijcai.org/Abstract/13/345", "abstract": "We consider the problem of planning in environments where the state is fully observable, actions have non-deterministic effects, and plans must generate infinite state trajectories for achieving a large class of LTL goals. More formally, we focus on the control synthesis problem under the assumption that the LTL formula to be realized can be mapped into a deterministic B\u00fcchi automaton. We show that by assuming that action non-determinism is fair, namely that infinite executions of a non-deterministic action in the same state yield each possible successor state an infinite number of times, the (fair) synthesis problem can be reduced to a standard strong cyclic planning task over reachability goals. Since strong cyclic planners are built on top of efficient classical planners, the transformation reduces the non-deterministic, fully observable, temporally extended planning task into the solution of classical planning problems. A number of experiments are reported showing the potential benefits of this approach to synthesis in comparison with state-of-the-art symbolic methods.", "title": "Fair LTL Synthesis for Non-Deterministic Systems Using Strong Cyclic Planners"}, {"url": "https://www.ijcai.org/Abstract/13/346", "abstract": "A fault represents some erroneous operation of a system that could result from an action selection error or some abnormal condition. We formally define error models that characterize the likelihood of various faults and consider the problem of fault-tolerant planning, which optimizes performance given an error model. We show that factoring the possibility of errors significantly degrades the performance of stochastic planning algorithms such as LAO*, because the number of reachable states grows dramatically. We introduce an approach to plan for a bounded number of faults and analyze its theoretical properties. When combined with a continual planning paradigm, the k-fault-tolerant planning method can produce near-optimal performance, even when the number of faults exceeds the bound. Empirical results in two challenging domains confirm the effectiveness of the approach in handling different types of runtime errors.", "title": "Fault-Tolerant Planning under Uncertainty"}, {"url": "https://www.ijcai.org/Abstract/13/347", "abstract": "The iPDB procedure by Haslum et al. is the state-of-the-art method for computing additive abstraction heuristics for domain-independent planning. It performs a hill-climbing search in the space of pattern collections, combining information from multiple patterns in the so-called canonical heuristic. We show how stronger heuristic estimates can be obtained through linear programming. An experimental evaluation demonstrates the strength of the new technique on the IPC benchmark suite.", "title": "Getting the Most Out of Pattern Databases for Classical Planning"}, {"url": "https://www.ijcai.org/Abstract/13/348", "abstract": "We describe an approach to computing upper bounds on the lengths of solutions to reachability problems in transition systems. It is based on a decomposition of state-variable dependency graphs (causal graphs). Our approach is able to find practical upper bounds in a number of planning benchmarks. Computing the bounds is computationally cheap in practice, and in a number of benchmarks our algorithm runs in polynomial time in the number of actions and propositional variables that characterize the problem.", "title": "Computing Upper Bounds on Lengths of Transition Sequences"}, {"url": "https://www.ijcai.org/Abstract/13/349", "abstract": "Formulating knowledge for use in AI Planning engines is currently something of an ad-hoc process, where the skills of knowledge engineers and the tools they use may significantly influence the quality of the resulting planning application. There is little in the way of guidelines or standard procedures, however, for knowledge engineers to use when formulating knowledge into planning domain languages such as PDDL. This paper seeks to investigate this process using as a case study a road traffic accident management domain. Managing road accidents requires systematic, sound planning and coordination of resources to improve outcomes for accident victims. We have derived a set of requirements in consultation with stakeholders for the resource coordination part of managing accidents. We evaluate two separate knowledge engineering strategies for encoding the resulting planning domain from the set of requirements: (a) the traditional method of PDDL experts and text editor, and (b) a leading planning GUI with built in UML modelling tools. These strategies are evaluated using process and product metrics, where the domain model (the product) was tested extensively with a range of planning engines. The results give insights into the strengths and weaknesses of the approaches, highlight lessons learned regarding knowledge encoding, and point to important lines of research for knowledge engineering for planning.", "title": "Exploring Knowledge Engineering Strategies in Designing and Modelling a Road Traffic Accident Management Domain"}, {"url": "https://www.ijcai.org/Abstract/13/350", "abstract": "One drawback of Hierarchical Task Network (HTN) planning is the difficulty of providing complete domain knowledge, i.e., a complete and correct set of HTN methods for every task. To provide a principled way to overcome this difficulty, we define a simple formalism that extends classical planning to include problem decomposition using methods, and a planning algorithm based on this formalism. In our formalism, the methods specify ways to achieve goals (rather than tasks as in conventional HTN planning), and goals may be achieved even when no methods are available. Our planning algorithm, GoDeL (Goal Decomposition with Landmarks), is sound and complete irrespective of whether the domain knowledge (i.e., the set of methods given to the planner) is complete. By comparing GoDeL's performance with varying amounts of domain knowledge across three benchmark planning domains, we show experimentally that (1) GoDeL works correctly with partial planning knowledge, (2) GoDeL's performance improves as more planning knowledge is given, and (3) when given full domain knowledge, GoDeL matches the performance of a state-of-the-art hierarchical planner.", "title": "The GoDeL Planning System: A More Perfect Union of Domain-Independent and Hierarchical Planning"}, {"url": "https://www.ijcai.org/Abstract/13/351", "abstract": "AI planners have to compromise between the speed of the planning process and the quality of the generated plan. Anytime planners try to balance these objectives by finding plans of better quality over time, but current anytime planners often do not make effective use of increasing runtime beyond a certain limit. We present a new method of continuing plan improvement, that works by repeatedly decomposing a given plan into subplans and optimising each subplan locally. The decomposition exploits block-structured plan deordering to identify coherent subplans, which make sense to treat as units. This approach extends the \"anytime capability\" of current planners - to provide continuing plan quality improvement at any time scale.", "title": "Plan Quality Optimisation via Block Decomposition"}, {"url": "https://www.ijcai.org/Abstract/13/352", "abstract": "Symbolic PDBs and Merge-and-Shrink (M&S) are two approaches to derive admissible heuristics for optimal planning. We present a combination of these techniques, Symbolic Merge-and-Shrink (SM&S), which uses M&S abstractions as a relaxation criterion for a symbolic backward search. Empirical evaluation shows that SM&S has the strengths of both techniques deriving heuristics at least as good as the best of them for most domains.", "title": "Symbolic Merge-and-Shrink for Cost-Optimal Planning"}, {"url": "https://www.ijcai.org/Abstract/13/353", "abstract": "In this paper, we revisit the idea of splitting a planning problem into subproblems hopefully easier to solve with the help of landmark analysis. While this technique initially proposed in the first approaches related to landmarks has been outperformed by landmark-based heuristics, we believe that it is still a promising research direction. To this end, we propose a new method for problem splitting based on landmarks which has two advantages over the original technique: it is complete (if a solution exists, the algorithm finds it), and it uses the precedence relation over the landmarks in a more flexible way. We lay in this paper the foundations of a meta best-first search algorithm, which explores the landmark orderings to create subproblems and can use any embedded planner to solve subproblems. It opens up avenues for future research: among them are new heuristics for guiding the meta search towards the most promising orderings, different policies for generating subproblems, and influence of the embedded subplanner.", "title": "Problem Splitting Using Heuristic Search in Landmark Orderings"}, {"url": "https://www.ijcai.org/Abstract/13/354", "abstract": "The most successful recent approaches to partially observable Markov decision problem (POMDP) solving have largely been point-based approximation algorithms. These work by selecting a finite number of belief points, computing alpha-vectors for those points, and using the resulting policy everywhere. However, if during execution the belief state is far from the points, there is no guarantee that the policy will be good. This case occurs either when the points are chosen poorly or there are too few points to capture the whole optimal policy, for example in domains where there are many low probability transitions, such as faults or exogenous events. In this paper we explore the use of an on-line plan repair approach to overcome this difficulty. The idea is to split computation between off-line plan creation and, if necessary, on-line plan repair. We evaluate a variety of heuristics used to determine when plan repair might be useful, and then repair the plan by sampling a small number of additional belief points and recomputing the policy. We show in several domains that the approach is more effective than either off-line planning alone even with much more computation time, or a purely on-line planning based on forward search. We also show that the overhead of checking the heuristics is very small when replanning is unnecessary.", "title": "Run-Time Improvement of Point-Based POMDP Policies"}, {"url": "https://www.ijcai.org/Abstract/13/355", "abstract": "To tackle the potentially hard task of defining the reward function in a Markov Decision Process, we propose a new approach, based on Value Iteration, which interweaves the elicitation and optimization phases. We assume that rewards whose numeric values are unknown can only be ordered, and that a tutor is present to help comparing sequences of re- wards. We first show how the set of possible reward functions for a given preference relation can be rep- resented as a polytope. Then our algorithm, called Interactive Value Iteration, searches for an optimal policy while refining its knowledge about the pos- sible reward functions, by querying a tutor when necessary. We prove that the number of queries needed before finding an optimal policy is upper- bounded by a polynomial in the size of the problem, and we present experimental results which demon- strate that our approach is efficient in practice.", "title": "Interactive Value Iteration for Markov Decision Processes with Unknown Rewards"}, {"url": "https://www.ijcai.org/Abstract/13/356", "abstract": "In this paper we concentrate on finding a suitable metric to determine the flexibility of a Simple Temporal Problem (STP). After reviewing some flexibility metrics that have been proposed, we conclude that these metrics fail to capture the correlation between events specified in the STP, resulting in an overestimation of the available flexibility in the system. We propose to use an intuitively more acceptable flexibility metric based upon uncorrelated time-intervals for the allowed starting times of events in an STP. This metric is shown to be computable in low-polynomial time. As a byproduct of the flexibility computation, we get a decomposition of the STN almost for free: for every possible k-partitioning of the event space, a decomposition can be computed in O(k)-time. Even more importantly, we show that contrary to popular belief, such a decomposition does not affect the flexibility of the original STP.", "title": "Flexibility and Decoupling in the Simple Temporal Problem"}, {"url": "https://www.ijcai.org/Abstract/13/357", "abstract": "Over-constrained temporal problems are commonly encountered while operating autonomous and decision support systems. An intelligent system must learn a human's preference over a problem in order to generate preferred resolutions that minimize perturbation. We present the Best-first Conflict-Directed Relaxation (BCDR) algorithm for enumerating the best continuous relaxation for an over-constrained conditional temporal problem with controllable choices. BCDR reformulates such a problem by making its temporal constraints relaxable and solves the problem using a conflict-directed approach. It extends the Conflict-Directed A* (CD-A*) algorithm to conditional temporal problems, by first generalizing the conflict learning process to include all discrete variable assignments and continuous temporal constraints, and then by guiding the forward search away from known infeasible regions using conflict resolution. When evaluated empirically on a range of coordinated car sharing network problems, BCDR demonstrates a substantial improvement in performance and solution quality compared to previous conflict-directed approaches.", "title": "Continuously Relaxing Over-Constrained Conditional Temporal Problems through Generalized Conflict Learning and Resolution"}, {"url": "https://www.ijcai.org/Abstract/13/358", "abstract": "Recent advances in solutions to Hybrid MDPs with discrete and continuous state and action spaces have significantly extended the class of MDPs for which exact solutions can be derived, albeit at the expense of a restricted transition noise model. In this paper, we work around limitations of previous solutions by adopting a robust optimization approach in which Nature is allowed to adversarially determine transition noise within pre-specified confidence intervals. This allows one to derive an optimal policy with an arbitrary (user-specified) level of success probability and significantly extends the class of transition noise models for which Hybrid MDPs can be solved. This work also significantly extends results for the related \"chance-constrained\" approach in stochastic hybrid control to accommodate state-dependent noise. We demonstrate our approach working on a variety of hybrid MDPs taken from AI planning, operations research, and control theory, noting that this is the first time robust solutions with strong guarantees over all states have been automatically derived for such problems.", "title": "Robust Optimization for Hybrid MDPs with State-Dependent Noise"}, {"url": "https://www.ijcai.org/Abstract/13/359", "abstract": "There is increasing awareness in the planning community that the burden of specifying complete domain models is too high, which impedes the applicability of planning technology in many real-world domains. Although there have been many learning approaches that help automatically creating domain models, they all assume plan traces (training data) are correct. In this paper, we aim to remove this assumption, allowing plan traces to be with noise. Compared to collecting large amount of correct plan traces, it is much easier to collect noisy plan traces, e.g., we can directly exploit sensors to help collect noisy plan traces. We consider a novel solution for this challenge that can learn action models from noisy plan traces. We create a set of random variables to capture the possible correct plan traces behind the observed noisy ones, and build a graphical model to describe the physics of the domain. We then learn the parameters of the graphical model and acquire the domain model based on the learnt parameters. In the experiment, we empirically show that our approach is effective in several planning domains.", "title": "Action-Model Acquisition from Noisy Plan Traces"}, {"url": "https://www.ijcai.org/Abstract/13/360", "abstract": "Most existing work on learning planning models assumes that the entire model needs to be learned from scratch. A more realistic situation is that the planning agent has an incomplete model which it needs to refine through learning. In this paper we propose and evaluate a method for doing this. Our method takes as input an incomplete model (with missing preconditions and effects in the actions), as well as a set of plan traces that are known to be correct. It outputs a refined model that not only captures additional precondition/effect knowledge about the given actions, but also macro actions. We use a MAX-SAT framework for learning, where the constraints are derived from the executability of the given plan traces, as well as the preconditions/effects of the given incomplete model. Unlike traditional macro-action learners which use macros to increase the efficiency of planning (in the context of a complete model), our motivation for learning macros is to increase the accuracy (robustness) of the plans generated with the refined model. We demonstrate the effectiveness of our approach through a systematic empirical evaluation.", "title": "Refining Incomplete Planning Domain Models through Plan Traces"}, {"url": "https://www.ijcai.org/Abstract/13/362", "abstract": "Users may ask a service robot to accomplish various tasks so that the designer of the robot cannot program each of the tasks beforehand. As more and more open-source knowledge resources become available, it is worthwhile trying to make use of open-source knowledge resources for service robots. The challenge lies in the autonomous identification, acquisition and utilization of missing knowledge about a user task at hand. In this paper, the core problem is formalized and the complexity results of the main reasoning issues are provided. A mechanism for task planning with open-knowledge rules which are provided by non-experts in semi-structured natural language and thus generally underspecified are introduced. Techniques for translating the semi-structured knowledge from a large open-source knowledge base are also presented. Experiments showed a remarkable improvement of the system performance on a test set consisting of hundreds of user desires from the open-source knowledge base.", "title": "Handling Open Knowledge for Service Robots"}, {"url": "https://www.ijcai.org/Abstract/13/363", "abstract": "Human action recognition from videos is a challenging machine vision task with multiple important application domains, such as human-robot/machine interaction, interactive entertainment, multimedia information retrieval, and surveillance. In this paper, we present a novel approach to human action recognition from 3D skeleton sequences extracted from depth data. We use the covariance matrix for skeleton joint locations over time as a discriminative descriptor for a sequence. To encode the relationship between joint movement and time, we deploy multiple covariance matrices over sub-sequences in a hierarchical fashion. The descriptor has a fixed length that is independent from the length of the described sequence. Our experiments show that using the covariance descriptor with an off-the-shelf classification algorithm outperforms the state of the art in action recognition on multiple datasets, captured either via a Kinect-type sensor or a sophisticated motion capture system. We also include an evaluation on a novel large dataset using our own annotation.", "title": "Human Action Recognition Using a Temporal Hierarchy of Covariance Descriptors on 3D Joint Locations"}, {"url": "https://www.ijcai.org/Abstract/13/364", "abstract": "Dispersing a team of robots into an unknown and dangerous environment, such as a collapsed building, can provide information about structural damage and locations of survivors and help rescuers plan their actions. We propose a rolling dispersion algorithm, which makes use of a small number of robots and achieves full exploration. The robots disperse as much as possible while maintaining communication, and then advance as a group, leaving behind beacons to mark explored areas and provide a path back to the entrance. The novelty of this algorithm comes from the manner in which the robots continue their exploration as a group after reaching the maximum dispersion possible while staying in contact with each other. We use simulation to show that the algorithm works in multiple environments and for varying numbers of robots.", "title": "Rolling Dispersion for Robot Teams"}, {"url": "https://www.ijcai.org/Abstract/13/365", "abstract": "Localization of a mobile robot is crucial for autonomous navigation. Using laser scanners, this can be facilitated by the pairwise alignment of consecutive scans. In this paper, we are interested in improving this scan alignment in challenging natural environments. For this purpose, local descriptors are generally effective as they facilitate point matching. However, we show that in some natural environments, many of them are likely to be unreliable, which affects the accuracy and robustness of the results. Therefore, we propose to filter the unreliable descriptors as a prior step to alignment. Our approach uses a fast machine learning algorithm, trained on-the-fly under the positive and unlabeled learning paradigm without the need for human intervention. Our results show that the number of descriptors can be significantly reduced, while increasing the proportion of reliable ones, thus speeding up and improving the robustness of the scan alignment process.", "title": "Accelerated Robust Point Cloud Registration in Natural Environments through Positive and Unlabeled Learning"}, {"url": "https://www.ijcai.org/Abstract/13/366", "abstract": "We introduce a novel algorithm called Upper Confidence Weighted Learning (UCWL) for online multiclass learning from binary feedback. UCWL combines the Upper Confidence Bound (UCB) framework with the Soft Confidence Weighted (SCW) online learning scheme.\u00a0 UCWL achieves state of the art performance (especially on noisy and non-separable data) with low computational costs. Estimated confidence intervals are used for informed exploration, which enables faster learning than the uninformed exploration case or the case where exploration is not used.\u00a0 The targeted application setting is human-robot interaction (HRI), in which a robot is learning to classify its observations while a human teaches it by providing only binary feedback (e.g., right/wrong).\u00a0 Results in an HRI experiment, and with two benchmark datasets, show UCWL outperforms other algorithms in the online binary feedback setting, and surprisingly even sometimes beats state-of-the-art algorithms that get full feedback, while UCWL gets only binary feedback on the same data.", "title": "Upper Confidence Weighted Learning for Efficient Exploration in Multiclass Prediction with Binary Feedback"}, {"url": "https://www.ijcai.org/Abstract/13/367", "abstract": "Directing robot attention to recognise activities and to anticipate events like goal-directed actions is a crucial skill for human-robot interaction. Unfortunately, issues like intrinsic time constraints, the spatially distributed nature of the entailed information sources, and the existence of a multitude of unobservable states affecting the system, like latent intentions, have long rendered achievement of such skills a rather elusive goal. The problem tests the limits of current attention control systems. It requires an integrated solution for tracking, exploration and recognition, which traditionally have been seen as separate problems in active vision.We propose a probabilistic generative framework based on a mixture of Kalman filters and information gain maximisation that uses predictions in both recognition and attention-control. This framework can efficiently use the observations of one element in a dynamic environment to provide information on other elements, and consequently enables guided exploration.Interestingly, the sensors-control policy, directly derived from first principles, represents the intuitive trade-off between finding the most discriminative clues and maintaining overall awareness.Experiments on a simulated humanoid robot observing a human executing goal-oriented actions demonstrated improvement on recognition time and precision over baseline systems.", "title": "Towards Active Event Recognition"}, {"url": "https://www.ijcai.org/Abstract/13/368", "abstract": "In this paper, we propose a novel method for object discovery and dense modelling in RGB-D image sequences using motion cues. We develop our method as a building block for active object perception, such that robots can learn about the environment through perceiving the effects of actions. Our approach simultaneously segments rigid-body motion within key views, and discovers objects and hierarchical relations between object parts. The poses of the key views are optimized in a graph of spatial relations to recover the rigid-body motion trajectories of the camera with respect to the objects. In experiments, we demonstrate that our approach finds moving objects, aligns partial views on the objects, and retrieves hierarchical relations between the objects.", "title": "Hierarchical Object Discovery and Dense Modelling from Motion Cues in RGB-D Video"}, {"url": "https://www.ijcai.org/Abstract/13/369", "abstract": "Parsing human poses in images is fundamental in extracting critical visual information for artificial intelligent agents. Our goal is to learn self-contained body part representations from images, which we call visual symbols, and their symbol-wise geometric contexts in this parsing process. Each symbol is individually learned by categorizing visual features leveraged by geometric information. In the categorization, we use Latent Support Vector Machine followed by an efficient cross validation procedure. Then, these symbols naturally define geometric contexts of body parts in a fine granularity for effective inference. When the structure of the compositional parts is a tree, we derive an efficient approach to estimating human poses in images. Experiments on two large datasets suggest our approach outperforms state of the art methods.", "title": "Learning Visual Symbols for Parsing Human Poses in Images"}, {"url": "https://www.ijcai.org/Abstract/13/371", "abstract": "An important question when eliciting opinions from experts is how to aggregate the reported opinions. In this paper, we propose a pooling method to aggregate expert opinions. Intuitively, it works as if the experts were continuously updating their opinions in order to accommodate the expertise of others. Each updated opinion takes the form of a linear opinion pool, where the weight that an expert assigns to a peer's opinion is inversely related to the distance between their opinions. In other words, experts are assumed to prefer opinions that are close to their own opinions. We prove that such an updating process leads to consensus, i.e., the experts all converge towards the same opinion. Further, we show that if rational experts are rewarded using the quadratic scoring rule, then the assumption that they prefer opinions that are close to their own opinions follows naturally. We empirically demonstrate the efficacy of the proposed method using real-world data.", "title": "A Consensual Linear Opinion Pool"}, {"url": "https://www.ijcai.org/Abstract/13/372", "abstract": "When using graphical models for decision making, the presence of unobserved variables\u00a0 may hinder our ability to reach the correct decision.\u00a0 A fundamental question here is whether or not\u00a0 one is ready to make a decision (stopping criteria), and if not, what additional\u00a0 observations should be made in order to better prepare for a decision\u00a0 (selection criteria). \u00a0A recently introduced notion, the Same-Decision Probability (SDP), has been shown to be useful\u00a0 as both a stopping and a selection criteria.\u00a0 This query has been shown to be highly intractable,\u00a0 being PP^PP-complete, and is exemplary of a class of queries which correspond to the computation of certain expectations. \u00a0We propose the first exact algorithm for computing the SDP in this paper, and demonstrate its effectiveness on several real and synthetic networks.\u00a0 We also present a new complexity result for computing the SDP on models with a Naive Bayes structure.", "title": "An Exact Algorithm for Computing the Same-Decision Probability"}, {"url": "https://www.ijcai.org/Abstract/13/373", "abstract": "This paper concerns building probabilistic models with an underlying ontology that defines the classes and properties used in the model. In particular, it considers the problem of reasoning with properties that may not always be defined. Furthermore, we may even be uncertain about whether a property is defined for a given individual. One approach is to explicitly add a value \"undefined\" to the range of random variables, forming extended belief networks; however, adding an extra value to a random variable's range has a large computational overhead. In this paper, we propose an alternative, ontologically-based belief networks, where all properties are only used when they are defined, and we show how probabilistic reasoning can be carried out without explicitly using the value \"undefined\" during inference. We prove this is equivalent to reasoning with the corresponding extended belief network and empirically demonstrate that inference becomes more efficient.", "title": "Probabilistic Reasoning with Undefined Properties in Ontologically-Based Belief Networks"}, {"url": "https://www.ijcai.org/Abstract/13/374", "abstract": "Probabilistic logics combine the expressive power of logic with the ability to reason with uncertainty. Several probabilistic logic languages have been proposed in the past, each of them with their own features. In this paper, we propose a new probabilistic constraint logic programming language, which combines constraint logic programming with probabilistic reasoning. The language supports modeling of discrete as well as continuous probability distributions by expressing constraints on random variables. We introduce the declarative semantics of this language, present an exact inference algorithm to derive bounds on the joint probability distributions consistent with the specified constraints, and give experimental results. The results obtained are encouraging, indicating that inference in our language is feasible for solving challenging problems.", "title": "Inference for a New Probabilistic Constraint Logic"}, {"url": "https://www.ijcai.org/Abstract/13/375", "abstract": "We study map-matching, the problem of estimating the route that is traveled by a vehicle, where the points observed with the Global Positioning System are available. A state-of-the-art approach for this problem is a Hidden Markov Model (HMM). We propose a particular transition probability between latent road segments by the use of the number of turns in addition to the travel distance between the latent road segments. We use inverse reinforcement learning to estimate the importance of the number of turns relative to the travel distance. This estimated importance is incorporated in the transition probability of the HMM. We show, through numerical experiments, that the error of map-matching can be reduced substantially with the proposed transition probability.", "title": "Map Matching with Inverse Reinforcement Learning"}, {"url": "https://www.ijcai.org/Abstract/13/376", "abstract": "We have developed a method for using confidence scores to integrate labels provided by crowdsourcing workers. Although confidence scores can be useful information for estimating the quality of the provided labels, a way to effectively incorporate them into the integration process has not been established. Moreover, some workers are overconfident about the quality of their labels while others are underconfident, and some workers are quite accurate in judging the quality of their labels. This differing reliability of the confidence scores among workers means that the probability distributions for the reported confidence scores differ among workers. To address this problem, we extended the Dawid-Skene model and created two probabilistic models in which the values of unobserved true labels are inferred from the observed provided labels and reported confidence scores by using the expectation-maximization algorithm. Results of experiments using actual crowdsourced data for image labeling and binary question answering tasks showed that incorporating workers' confidence scores can improve the accuracy of integrated crowdsourced labels.", "title": "Accurate Integration of Crowdsourced Labels Using Workers\u2019 Self-Reported Confidence Scores"}, {"url": "https://www.ijcai.org/Abstract/13/377", "abstract": "A key decision facing autonomous systems with access to streams of sensory data is whether to act based on current evidence or to wait for additional information that might enhance the utility of taking an action. Computing the value of information is particularly difficult with streaming high-dimensional sensory evidence. We describe a belief projection approach to reasoning about information value in these settings, using models for inferring future beliefs over states given streaming evidence. These belief projection models can be learned from data or constructed via direct assessment of parameters and they fit naturally in modular, hierarchical state inference architectures. We describe principles of using belief projection and present results drawn from an implementation of the methodology within a conversational system.", "title": "Look versus Leap: Computing Value of Information with High-Dimensional Streaming Evidence"}, {"url": "https://www.ijcai.org/Abstract/13/378", "abstract": "In this paper, we consider the inclusion-exclusion rule \u2013 a known yet seldom used rule of probabilistic inference. Unlike the widely used sum rule which requires easy access to all joint probability values, the inclusion-exclusion rule requires easy access to several marginal probability values. We therefore develop a new representation of the joint distribution that is amenable to the inclusion-exclusion rule. We compare the relative strengths and weaknesses of the inclusion-exclusion rule with the sum rule and develop a hybrid rule called the inclusion- exclusion-sum (IES) rule, which combines their power. We apply the IES rule to junction trees, treating the latter as a target for knowledge compilation and show that in many cases it greatly reduces the time required to answer queries. Our experiments demonstrate the power of our approach. In particular, at query time, on several networks, our new scheme was an order of magnitude faster than the junction tree algorithm.", "title": "The Inclusion-Exclusion Rule and Its Application to the Junction Tree Algorithm"}, {"url": "https://www.ijcai.org/Abstract/13/379", "abstract": "We consider stochastic multiarmed bandit problems where each arm generates i.i.d. rewards according to an unknown distribution. Whereas classical bandit solutions only maximize the expected reward, we consider the problem of minimizing risk using notions such as the value-at-risk, the average value-at-risk, and the mean-variance risk. We present algorithms to minimize the risk over a single and multiple time periods, along with PAC accuracy guarantees given a finite number of reward samples. In the single-period case, we show that finding the arm with least risk requires not many more samples than the arm with highest expected reward. Although minimizing the multi-period value-at-risk is known to be hard, we present an algorithm with comparable sample complexity under additional assumptions.", "title": "Sample Complexity of Risk-Averse Bandit-Arm Selection"}, {"url": "https://www.ijcai.org/Abstract/13/380", "abstract": "Both SAT and #SAT can represent difficult problems in seemingly dissimilar areas such as planning, verification, and probabilistic inference. Here, we examine an expressive new language, #\u2203SAT, that generalizes both of these languages. #\u2203SAT problems require counting the number of satisfiable formulas in a concisely-describable set of existentially-quantified, propositional formulas. We characterize the expressiveness and worst-case difficulty of #\u2203SAT by proving that it is complete for the complexity class #P^{NP[1]} , and relating this class to more familiar complexity classes. We also experiment with three new general purpose #\u2203SAT solvers on a battery of problem distributions including a simple logistics domain. Our experiments show that, despite the formidable worst-case complexity of #P^{NP[1]} , many of the instances can be solved efficiently by noticing and exploiting a particular type of frequent structure.", "title": "A Generalization of SAT and #SAT for Robust Policy Evaluation"}, {"url": "https://www.ijcai.org/Abstract/13/382", "abstract": "Online social networks continue to witness a tremendous growth both in terms of the number of registered users and their mutual interactions. In this paper, we focus on online signed social networks where positive interactions among the users signify friendship or approval, whereas negative interactions indicate antagonism or disapproval. We introduce a novel problem which we call the link label prediction problem: Given the information about signs of certain links in a social network, we want to learn the nature of relationships that exist among the users by predicting the sign, positive or negative, of the remaining links. We propose a matrix factorization based technique MF-LiSP that exhibits strong generalization guarantees. We also investigate the applicability of logistic regression Leskove et al. in this setting. Our experiments on Wiki-Vote, Epinions and Slashdot data sets strongly corroborate the efficacy of these approaches.", "title": "Link Label Prediction in Signed Social Networks"}, {"url": "https://www.ijcai.org/Abstract/13/383", "abstract": "In past decade, more and more data are collected from multiple sources or represented by multiple views, where different views describe distinct perspectives of the data. Although each view could be individually used for finding patterns by clustering, the clustering performance could be more accurate by exploring the rich information among multiple views. Several multi-view clustering methods have been proposed to unsupervised integrate different views of data. However, they are graph based approaches, e.g. based on spectral clustering, such that they cannot handle the large-scale data. How to combine these heterogeneous features for unsupervised large-scale data clustering has become a challenging problem. In this paper, we propose a new robust large-scale multi-view clustering method to integrate heterogeneous representations of large-scale data. We evaluate the proposed new methods by six benchmark data sets and compared the performance with several commonly used clustering approaches as well as the baseline multi-view clustering methods. In all experimental results, our proposed methods consistently achieve superiors clustering performances.", "title": "Multi-View K-Means Clustering on Big Data"}, {"url": "https://www.ijcai.org/Abstract/13/384", "abstract": "Personalized point-of-interest (POI) recommendation is a significant task in location-based social networks (LBSNs) as it can help provide better user experience as well as enable third-party services, e.g., launching advertisements. To provide a good recommendation, various research has been conducted in the literature. However, pervious efforts mainly consider the \"check-ins\" in a whole and omit their temporal relation. They can only recommend POI globally and cannot know where a user would like to go tomorrow or in the next few days. In this paper, we consider the task of successive personalized POI recommendation in LBSNs, which is a much harder task than standard personalized POI recommendation or prediction. To solve this task, we observe two prominent properties in the check-in sequence: personalized Markov chain and region localization. Hence, we propose a novel matrix factorization method, namely FPMCLR, to embed the personalized Markov chains and the localized regions. Our proposed FPMC-LR not only exploits the personalized Markov chain in the check-in sequence, but also takes into account users' movement constraint, i.e., moving around a localized region. More importantly, utilizing the information of localized regions, we not only reduce the computation cost largely, but also discard the noisy information to boost recommendation. Results on two real-world LBSNs datasets demonstrate the merits of our proposed FPMC-LR.", "title": "Where You Like to Go Next: Successive Point-of-Interest Recommendation"}, {"url": "https://www.ijcai.org/Abstract/13/385", "abstract": "Recently how to recommend celebrities to the public becomes an interesting problem on the social network websites, such as Twitter and Tencent Weibo. In this paper, we proposed a unified hierarchical Bayesian model to recommend celebrities to the general users. Specifically, we proposed to leverage both social network and descriptions of celebrities to improve the prediction ability and recommendation interpretability. In our model, we combine topic model with matrix factorization for both social network of celebrities and user following action matrix. It works by regularizing celebrity factors through celebrity's social network and descriptive words associated with each celebrity. We also proposed to incorporate different confidences for different dyadic contexts to handle the situation that only positive observations exist. We conducted experiments on two real-world datasets from Twitter and Tencent Weibo, which are the largest and second largest microblog websites in USA and China, respectively. The experiment results show that our model achieves a higher performance and provide more effective results than the state-of-art methods especially when recommending new celebrities. We also show that our model captures user intertests more precisely and gives better recommendation interpretability.", "title": "Celebrity Recommendation with Collaborative Social Topic Regression"}, {"url": "https://www.ijcai.org/Abstract/13/386", "abstract": "Collaborative filtering, a widely-used user-centric recommendation technique, predicts an item's rating by aggregating its ratings from similar users. User similarity is usually calculated by cosine similarity or Pearson correlation coefficient. However, both of them consider only the direction of rating vectors, and suffer from a range of drawbacks. To solve these issues, we propose a novel Bayesian similarity measure based on the Dirichlet distribution, taking into consideration both the direction and length of rating vectors. Further, our principled method reduces correlation due to chance. Experimental results on six real-world data sets show that our method achieves superior accuracy.", "title": "A Novel Bayesian Similarity Measure for Recommender Systems"}, {"url": "https://www.ijcai.org/Abstract/13/387", "abstract": "Cross-domain collaborative filtering (CDCF), which aims to leverage data from multiple domains to relieve the data sparsity issue, is becoming an emerging research topic in recent years. However, current CDCF methods that mainly consider user and item factors but largely neglect the heterogeneity of domains may lead to improper knowledge transfer issues. To address this problem, we propose a novel CDCF model, the Bilinear Multilevel Analysis (BLMA), which seamlessly introduces multilevel analysis theory to the most successful collaborative filtering method, matrix factorization (MF). Specifically, we employ BLMA to more efficiently address the determinants of ratings from a hierarchical view by jointly considering domain, community, and user effects so as to overcome the issues caused by traditional MF approaches. Moreover, a parallel Gibbs sampler is provided to learn these effects. Finally, experiments conducted on a real-world dataset demonstrate the superiority of the BLMA over other state-of-the-art methods.", "title": "Cross-Domain Collaborative Filtering via Bilinear Multilevel Analysis"}, {"url": "https://www.ijcai.org/Abstract/13/388", "abstract": "The availability of microblogging, like Twitter and Sina Weibo, makes it a popular platform for spammers to unfairly overpower normal users with unwanted content via social networks, known as social spamming. The rise of social spamming can significantly hinder the use of microblogging systems for effective information dissemination and sharing. Distinct features of microblogging systems present new challenges for social spammer detection. First, unlike traditional social networks, microblogging allows to establish some connections between two parties without mutual consent, which makes it easier for spammers to imitate normal users by quickly accumulating a large number of \"human\" friends. Second, microblogging messages are short, noisy, and unstructured. Traditional social spammer detection methods are not directly applicable to microblogging. In this paper, we investigate how to collectively use network and content information to perform effective social spammer detection in microblogging. In particular, we present an optimization formulation that models the social network and content information in a unified framework. Experiments on a real-world Twitter dataset demonstrate that our proposed method can effectively utilize both kinds of information for social spammer detection.", "title": "Social Spammer Detection in Microblogging"}, {"url": "https://www.ijcai.org/Abstract/13/389", "abstract": "Individuals often express their opinions on social media platforms like Twitter and Facebook during public events such as the U.S. Presidential debate and the Oscar awards ceremony. Gleaning insights from these posts is of importance to analyzing the impact of the event. In this work, we consider the problem of identifying the segments and topics of an event that garnered praise or criticism, according to aggregated Twitter responses. We propose a flexible factorization framework, SocSent, to learn factors about segments, topics, and sentiments. To regulate the learning process, several constraints based on prior knowledge on sentiment lexicon, sentiment orientations (on a few tweets) as well as tweets alignments to the event are enforced. We implement our approach using simple update rules to get the optimal solution. We evaluate the proposed method both quantitatively and qualitatively on two large-scale tweet datasets associated with two events from different domains to show that it improves significantly over baseline models.", "title": "Listening to the Crowd: Automated Analysis of Events via Aggregated Twitter Sentiment"}, {"url": "https://www.ijcai.org/Abstract/13/390", "abstract": "Trust prediction, which explores the unobserved relationships between online community users, is an emerging and important research topic in social network analysis and many web applications. Similar to other social-based recommender systems, trust relationships between users can be also modeled in the form of matrices. Recent study shows users generally establish friendship due to a few latent factors, it is therefore reasonable to assume the trust matrices are of low-rank. As a result, many recommendation system strategies can be applied here. In particular, trace norm minimization, which uses matrix's trace norm to approximate its rank, is especially appealing. However, recent articles cast doubts on the validity of trace norm approximation. In this paper, instead of using trace norm minimization, we propose a new robust rank-k matrix completion method, which explicitly seeks a matrix with exact rank. Moreover, our method is robust to noise or corrupted observations. We optimize the new objective function in an alternative manner, based on a combination of ancillary variables and Augmented Lagrangian Multiplier (ALM) Method. We perform the experiments on three real-world data sets and all empirical results demonstrate the effectiveness of our method.", "title": "Social Trust Prediction Using Rank-k Matrix Recovery"}, {"url": "https://www.ijcai.org/Abstract/13/391", "abstract": "Conceptualization seeks to map a short text (i.e., a word or a phrase) to a set of concepts as a mechanism of understanding text. Most of prior research in conceptualization uses human-crafted knowledge bases that map instances to concepts. Such approaches to conceptualization have the limitation that the mappings are not context sensitive. To overcome this limitation, we propose a framework in which we harness the power of a probabilistic topic model which inherently captures the semantic relations between words. By combining latent Dirichlet allocation, a widely used topic model with Probase, a large-scale probabilistic knowledge base, we develop a corpus-based framework for context-dependent conceptualization. Through this simple but powerful framework, we improve conceptualization and enable a wide range of applications that rely on semantic understanding of short texts, including frame element prediction, word similarity in context, ad-query similarity, and query similarity.", "title": "Context-Dependent Conceptualization"}, {"url": "https://www.ijcai.org/Abstract/13/392", "abstract": "Recently, ontology stream reasoning has been introduced as a multidisciplinary approach, merging synergies from Artificial Intelligence, Database, World-Wide-Web to reason on semantic augmented data streams. Although knowledge evolution and real-time reasoning have been largely addressed in ontology streams, the challenge of predicting its future (or missing) knowledge remains open and yet unexplored. We tackle predictive reasoning as a correlation and interpretation of past semantics-augmented data over exogenous ontology streams. Consistent predictions are constructed as Description Logics entailments by selecting and applying relevant cross-streams association rules. The experiments have shown accurate prediction with real and live stream data from Dublin City in Ireland.", "title": "Predicting Knowledge in an Ontology Stream"}, {"url": "https://www.ijcai.org/Abstract/13/393", "abstract": "Online rating systems are now ubiquitous due to the success of recommender systems. In such systems, users are allowed to rate the items (movies, songs, commodities) in a predefined range of values. The ratings collected can be used to infer users' preferences as well as items' intrinsic features, which are then matched to perform personalized recommendation. Most previous work focuses on improving the prediction accuracy or ranking capability. Little attention has been paid to the problem of spammers or low-reputed users in such systems. Spammers contaminate the rating system by assigning unreasonable scores to items, which may affect the accuracy of a recommender system. There are evidences supporting the existence of spammers in online rating systems. Reputation estimation methods can be employed to keep track of users' reputation and detect spammers in such systems. In this paper, we propose a unified framework for computing the reputation score of a user, given only users' ratings on items. We show that previously proposed reputation estimation methods can be captured as special cases of our framework. We propose a new low-rank matrix factorization based reputation estimation method and demonstrate its superior discrimination ability.", "title": "A Unified Framework for Reputation Estimation in Online Rating Systems"}, {"url": "https://www.ijcai.org/Abstract/13/394", "abstract": "Several recent works have focused on harvesting HTML tables from the\u00a0Web and recovering their\u00a0semantics.\u00a0As a result, hundreds of millions of high quality structured data tables can now be explored by the users. In this paper, we argue that\u00a0those efforts only scratch the surface of the true value of structured data on the Web,\u00a0and study the challenging problem of synthesizing tables from the Web, i.e.,\u00a0producing never-before-seen tables from raw tables on the Web. Table synthesis\u00a0offers an important semantic advantage: when a set of related tables are combined into a\u00a0single union table, powerful mechanisms, such as temporal or geographical comparison and\u00a0visualization, can be employed to understand and mine the underlying data holistically. We focus on one fundamental task of table synthesis, namely, table stitching. \u00a0Within a given site,\u00a0many tables with identical schemas can be scattered across many pages. \u00a0The task of table stitching\u00a0involves combining such tables into a single meaningful union table and identifying extra attributes\u00a0and values for its rows so that rows from different original tables can be distinguished. Specifically, we\u00a0first define the notion of stitchable tables and identify collections of tables that can be stitched.\u00a0Second, we design an effective algorithm for extracting hidden attributes that are essential for the\u00a0stitching process and for aligning values of those attributes across tables to synthesize new columns. We\u00a0also assign meaningful names to these synthesized columns. Experiments on real world tables demonstrate the\u00a0effectiveness of our approach.", "title": "Synthesizing Union Tables from the Web"}, {"url": "https://www.ijcai.org/Abstract/13/395", "abstract": "Many web sites collect reviews of products and services and use them provide rankings of their quality. However, such rankings are not personalized. We investigate how the information in the reviews written by a particular user can be used to personalize the ranking she is shown. We propose a new technique, topic profile collaborative filtering, where we build user profiles from users' review texts and use these profiles to filter other review texts with the eyes of this user. We verify on data from an actual review site that review texts and topic profiles indeed correlate with ratings, and show that topic profile collaborative filtering provides both a better mean average error when predicting ratings and a better approximation of user preference orders.", "title": "Recommendation Using Textual Opinions"}, {"url": "https://www.ijcai.org/Abstract/13/396", "abstract": "One-class collaborative filtering or collaborative ranking with implicit feedback has been steadily receiving more attention, mostly due to the \"one-class\" characteristics of data in various services, e.g., \"like\" in Facebook and \"bought\" in Amazon. Previous works for solving this problem include pointwise regression methods based on absolute rating assumptions and pairwise ranking methods with relative score assumptions, where the latter was empirically found performing much better because it models users' ranking-related preferences more directly. However, the two fundamental assumptions made in the pairwise ranking methods, (1) individual pairwise preference over two items and (2) independence between two users, may not always hold. As a response, we propose a new and improved assumption, group Bayesian personalized ranking (GBPR), via introducing richer interactions among users. In particular, we introduce group preference, to relax the aforementioned individual and independence assumptions. We then design a novel algorithm correspondingly, which can recommend items more accurately as shown by various ranking-oriented evaluation metrics on four real-world datasets in our experiments.", "title": "GBPR: Group Preference Based Bayesian Personalized Ranking for One-Class Collaborative Filtering"}, {"url": "https://www.ijcai.org/Abstract/13/397", "abstract": "We study the problem of diverse promoting recommendation task: selecting a subset of diverse items that can better predict a given user's preference. Recommendation techniques primarily based on user or item similarity can suffer from the risk that users cannot get expected information from the over-specified recommendation lists. In this paper, we propose an entropy regularizer to capture the notion of diversity. The entropy regularizer has good properties in that it satisfies monotonicity and submodularity, such that when we combine it with a modular rating set function, we get submodular objective function, which can be maximized approximately by efficient greedy algorithm, with provable constant factor guarantee of optimality. We apply our approach on the top-$K$ prediction problem and evaluate its performance on MovieLens data set, which is a standard database containing movie rating data collected from a popular online movie recommender system. We compare our model with the state-of-the-art recommendation algorithms. Our experiments show that the entropy regularizer effectively captures diversity and hence improves the performance of recommendation task.", "title": "Promoting Diversity in Recommendation by Entropy Regularizer"}, {"url": "https://www.ijcai.org/Abstract/13/398", "abstract": "Matrix factorization (MF) is a popular collaborative filtering approach for recommender systems due to its simplicity and effectiveness. Existing MF methods either assume that all latent features are uncorrelated or assume that all are correlated. To address the important issue of what structure should be imposed on the features, we investigate the covariance matrix of the latent features learned from real data. Based on the findings, we propose an MF model with a sparse covariance prior which favors a sparse yet non-diagonal covariance matrix. Not only can this reflect the semantics more faithfully, but imposing sparsity can also have a side effect of preventing overfitting. Starting from a probabilistic generative model with a sparse covariance prior, we formulate the model inference problem as a maximum a posteriori (MAP) estimation problem. The optimization procedure makes use of stochastic gradient descent and majorization-minimization. For empirical validation, we conduct experiments using the MovieLens and Netflix datasets to compare the proposed method with two strong baselines which use different priors. Experimental results show that our sparse covariance prior can lead to performance improvement.", "title": "SCMF: Sparse Covariance Matrix Factorization for Collaborative Filtering"}, {"url": "https://www.ijcai.org/Abstract/13/399", "abstract": "With the fast development of social media, the information overload problem becomes increasingly severe and recommender systems play an important role in helping online users find relevant information by suggesting information of potential interests. Social activities for online users produce abundant social relations. Social relations provide an independent source for recommendation, presenting both opportunities and challenges for traditional recommender systems. Users are likely to seek suggestions from both their local friends and users with high global reputations, motivating us to exploit social relations from local and global perspectives for online recommender systems in this paper. We develop approaches to capture local and global social relations, and propose a novel framework LOCABAL taking advantage of both local and global social context for recommendation. Empirical results on real-world datasets demonstrate the effectiveness of our proposed framework and further experiments are conducted to understand how local and global social context work for the proposed framework.", "title": "Exploiting Local and Global Social Context for Recommendation"}, {"url": "https://www.ijcai.org/Abstract/13/400", "abstract": "Recently, tag recommendation (TR) has become a very hot research topic in data mining and related areas. However, neither co-occurrence based methods which only use the item-tag matrix nor content based methods which only use the item content information can achieve satisfactory performance in real TR applications. Hence, how to effectively combine the item-tag matrix, item content information, and other auxiliary information into the same recommendation framework is the key challenge for TR. In this paper, we first adapt the collaborative topic regression (CTR) model, which has been successfully applied for article recommendation, to combine both item-tag matrix and item content information for TR. Furthermore, by extending CTR we propose a novel hierarchical Bayesian model, called CTR with social regularization (CTR-SR), to seamlessly integrate the item-tag matrix, item content information, and social networks between items into the same principled model. Experiments on real data demonstrate the effectiveness of our proposed models.", "title": "Collaborative Topic Regression with Social Regularization for Tag Recommendation"}, {"url": "https://www.ijcai.org/Abstract/13/401", "abstract": "With the emergence of large-scale evolving (time-varying)networks, dynamic network analysis (DNA) has become a very hot research topic in recent years. Although a lot of DNA methods have been proposed by researchers from different communities, most of them can only model snapshot data recorded at a very rough temporal granularity. Recently, some models have been proposed for DNA which can be used to model large-scale citation networks at a fine temporal granularity. However, they suffer from a significant decrease of accuracy over time because the learned parameters or node features are static (fixed) during the prediction process for evolving citation networks. In this paper, we propose a novel model,called online egocentric model (OEM), to learn time-varying parameters and node features for evolving citation networks. Experimental results on real-world citation networks show that our OEM can not only prevent the prediction accuracy from decreasing over time but also uncover the evolution of topics in citation networks.", "title": "Online Egocentric Models for Citation Networks"}, {"url": "https://www.ijcai.org/Abstract/13/402", "abstract": "Automatically discovering cross-lingual links (CLs) between wikis can largely enrich the cross-lingual knowledge and facilitate knowledge sharing across different languages. In most existing approaches for cross-lingual knowledge linking, the seed CLs and the inner link structures are two important factors for finding new CLs. When there are insufficient seed CLs and inner links, discovering new CLs becomes a challenging problem. In this paper, we propose an approach that boosts cross-lingual knowledge linking by concept annotation. Given a small number of seed CLs and inner links, our approach first enriches the inner links in wikis by using concept annotation method, and then predicts new CLs with a regression-based learning model. These two steps mutually reinforce each other, and are executed iteratively to find as many CLs as possible. Experimental results on the English and Chinese Wikipedia data show that the concept annotation can effectively improve the quantity and quality of predicted CLs. With 50,000 seed CLs and 30% of the original inner links in Wikipedia, our approach discovered 171,393 more CLs in four runs when using concept annotation.", "title": "Boosting Cross-Lingual Knowledge Linking via Concept Annotation"}, {"url": "https://www.ijcai.org/Abstract/13/403", "abstract": "Recent years have witnessed increased interests in measuring authority and modelling influence in social networks. For a long time, PageRank has been widely used for authority computation and has also been adopted as a solid baseline for evaluating social influence related applications. However, the connection between authority measurement and influence modelling is not clearly established. To this end, in this paper, we provide a focused study on understanding of PageRank as well as the relationship between PageRank and social influence analysis. Along this line, we first propose a linear social influence model and reveal that this model is essentially PageRank with prior. Also, we show that the authority computation by PageRank can be enhanced with more generalized priors. Moreover, to deal with the computational challenge of PageRank with general priors, we provide an upper bound for top authoritative nodes identification. Finally, the experimental results on the scientific collaboration network validate the effectiveness of the proposed social influence model.", "title": "PageRank with Priors: An Influence Propagation Perspective"}, {"url": "https://www.ijcai.org/Abstract/13/404", "abstract": "To accurately and actively provide users with their potentially interested information or services is the main task of a recommender system. Collaborative filtering is one of the most widely adopted recommender algorithms, whereas it is suffering the issues of data sparsity and cold start that will severely degrade quality of recommendations. To address such issues, this article proposes a novel method, trying to improve the performance of collaborative filtering recommendation by means of elaborately integrating twofold sparse information, the conventional rating data given by users and the social trust network among the same users. It is a model-based method adopting matrix factorization technique to map users into low-dimensional latent feature spaces in terms of their trust relationship, aiming to reflect users' reciprocal influence on their own opinions more reasonably. The validations against a real-world dataset show that the proposed method performs much better than state-of-the-art recommendation algorithms for social collaborative filtering by trust.", "title": "Social Collaborative Filtering by Trust"}, {"url": "https://www.ijcai.org/Abstract/13/405", "abstract": "Recent years have witnessed the growing popularity of hashing for efficient large-scale similarity search. It has been shown that the hashing quality could be boosted by hash function learning(HFL). In this paper, we study HFL in the context of multimodal data for cross-view similarity search. We present a novel multimodal HFL method, called Parametric Local Multimodal Hashing(PLMH), which learns a set of hash functions to locally adapt to the data structure of each modality. To balance locality and computational efficiency, the hashing projection matrix of each instance is parameterized, with guaranteed approximation errorbound, as a linear combination of basis hashing projections of a small set of anchor points. A local optimal conjugate gradient algorithm is designed to learn the hash functions for each bit, and the overall hash codes are learned in a sequential manner to progressively minimize the bias. Experimental evaluations on cross-media retrieval tasks demonstrate that PLMH performs competitively against the state-of-the-art methods.", "title": "Parametric Local Multimodal Hashing for Cross-View Similarity Search"}, {"url": "https://www.ijcai.org/Abstract/13/406", "abstract": "We study an interesting phenomenon of social influence locality in a large microblogging network, which suggests that users' behaviors are mainly influenced by close friends in their ego networks. We provide a formal definition for the notion of social influence locality and develop two instantiation functions based on pairwise influence and structural diversity. The defined influence locality functions have strong predictive power. Without any additional features, we can obtain a F1-score of 71.65% for predicting users' retweet behaviors by training a logistic regression classifier based on the defined functions. Our analysis also reveals several intriguing discoveries. For example, though the probability of a user retweeting a microblog is positively correlated with the number of friends who have retweeted the microblog, it is surprisingly negatively correlated with the number of connected circles that are formed by those friends.", "title": "Social Influence Locality for Modeling Retweeting Behaviors"}, {"url": "https://www.ijcai.org/Abstract/13/407", "abstract": "A new algorithm is developed in this paper to support automatic name-face alignment for achieving more accurate cross-media news retrieval. We focus on extracting valuable information from large amounts of news images and their captions, where multi-level image-caption pairs are constructed for characterizing both significant names with higher salience and their cohesion with human faces extracted from news images. To remedy the issue of lacking enough related information for rare name, Web mining is introduced to acquire the extra multimodal information. We also emphasize on an optimization mechanism by our Improved Self-Adaptive Simulated Annealing Genetic Algorithm to verify the feasibility of alignment combinations. Our experiments have obtained very positive results.", "title": "Automatic Name-Face Alignment to Enable Cross-Media News Retrieval"}, {"url": "https://www.ijcai.org/Abstract/13/410", "abstract": "Monitoring and forecast of global spread of infectious diseases is difficult, mainly due to lack of fine-grained and timely data. Previous work in computational epidemiology has shown that mining data from the web can improve the predictability of high-level aggregate patterns of epidemics. By contrast, this paper explores how individuals contribute to the global spread of disease. We consider the important task of predicting the prevalence of flu-like illness in a given city based on interpersonal interactions of the city's residents with the outside world. We use the geo-tagged status updates of traveling Twitter users to infer properties of the flow of individuals between cities. While previous research considered only the raw volume of passengers, we estimate a number of latent variables, including the number of sick (symptomatic) travelers and the number of sick individuals to whom each traveler was exposed. We show that AI techniques provide insights into the mechanisms of disease spread and significantly improve predictability of future flu outbreaks. Our experiments involve over 51,000 individuals traveling between 75 cities prior and during a severe ongoing flu epidemic (October 2012 - January 2013). Our model leverages the text and interpersonal interactions recorded in over 6.5 million online status updates without any active user participation, enabling scalable public health applications.", "title": "Assessing the Resilience of Socio-Ecosystems: Coupling Viability Theory and Active Learning with kd-Trees \u2014 Application to Bilingual Societies"}, {"url": "https://www.ijcai.org/Abstract/13/410", "abstract": "Monitoring and forecast of global spread of infectious diseases is difficult, mainly due to lack of fine-grained and timely data. Previous work in computational epidemiology has shown that mining data from the web can improve the predictability of high-level aggregate patterns of epidemics. By contrast, this paper explores how individuals contribute to the global spread of disease. We consider the important task of predicting the prevalence of flu-like illness in a given city based on interpersonal interactions of the city's residents with the outside world. We use the geo-tagged status updates of traveling Twitter users to infer properties of the flow of individuals between cities. While previous research considered only the raw volume of passengers, we estimate a number of latent variables, including the number of sick (symptomatic) travelers and the number of sick individuals to whom each traveler was exposed. We show that AI techniques provide insights into the mechanisms of disease spread and significantly improve predictability of future flu outbreaks. Our experiments involve over 51,000 individuals traveling between 75 cities prior and during a severe ongoing flu epidemic (October 2012 - January 2013). Our model leverages the text and interpersonal interactions recorded in over 6.5 million online status updates without any active user participation, enabling scalable public health applications.", "title": "Towards Understanding Global Spread of Disease from Everyday Interpersonal Interactions"}, {"url": "https://www.ijcai.org/Abstract/13/411", "abstract": "Since wind has an intrinsically complex and stochastic nature, accurate wind power forecasts are necessary for the safety and economics of wind energy utilization. In this paper, we investigate a combination of numeric and probabilistic models: one-day-ahead wind power forecasts were made with Gaussian Processes (GPs) applied to the outputs of a Numerical Weather Prediction (NWP) model. Firstly the wind speed data from NWP was corrected by a GP. Then, as there is always a defined limit on power generated in a wind turbine due the turbine controlling strategy, a Censored GP was used to model the relationship between the corrected wind speed and power output. To validate the proposed approach, two real world datasets were used for model construction and testing. The simulation results were compared with the persistence method and Artificial Neural Networks (ANNs); the proposed model achieves about 11% improvement in forecasting accuracy (Mean Absolute Error) compared to the ANN model on one dataset, and nearly 5% improvement on another.", "title": "Short-Term Wind Power Forecasting Using Gaussian Processes"}, {"url": "https://www.ijcai.org/Abstract/13/412", "abstract": "Aerosol Optical Depth (AOD), recognized as one of the most important quantities in understanding and predicting the Earth's climate, is estimated daily on a global scale by several Earth-observing satellite instruments. Each instrument has different coverage and sensitivity to atmospheric and surface conditions, and, as a result, the quality of AOD estimated by different instruments varies across the globe. We present a method for learning how to aggregate AOD estimations from multiple satellite instruments into a more accurate estimation. The proposed method is semi-supervised, as it is able to learn from a small number of labeled data, where labels come from a few accurate and expensive ground-based instruments, and a large number of unlabeled data. The method uses a latent variable to partition the data, so that in each partition the expert AOD estimations are aggregated in a different, optimal way. We applied the method to combine AOD estimations from 5 instruments aboard 4 satellites, and the results indicate that it can successfully exploit labeled and unlabeled data to produce accurate aggregated AOD estimations.", "title": "Semi-Supervised Learning for Integration of Aerosol Predictions from Multiple Satellite Instruments"}, {"url": "https://www.ijcai.org/Abstract/13/413", "abstract": "The increasing demands on drinkable water, along with population growth, water-intensive agriculture and economic development, pose critical challenges to water sustainability. New techniques to long-term water conservation that incorporate principles of sustainability are expected. Recent studies have shown that providing customers with usage information of fixtures could help them save a considerable amount of water. Existing disaggregation techniques focus on learning consumption patterns for individual devices. Little attention has been given to the hierarchical decomposition structure of the aggregated consumption. In this paper, a Deep Sparse Coding based Recursive Disaggregation Model (DSCRDM) is proposed for water conservation. We design a recursive decomposition structure to perform the disaggregation task, and introduce sequential set to capture its characteristics. An efficient and effective algorithm deep sparse coding is developed to automatically learn the disaggregation architecture, along with discriminative and reconstruction dictionaries for each layer. We demonstrated that our proposed approach significantly improved the performance of the benchmark methods on a large scale disaggregation task and illustrated how our model could provide practical feedbacks to customers for water conservation.", "title": "Deep Sparse Coding Based Recursive Disaggregation Model for Water Conservation"}, {"url": "https://www.ijcai.org/Abstract/13/414", "abstract": "In Beijing, most taxi drivers intentionally avoid working during peak hours despite of the huge customer demand within these peak periods. This dilemma is mainly due to the fact that taxi drivers' congestion costs are not reflected in the current taxi fare structure. To resolve this problem, we propose a new pricing scheme to provide taxi drivers with extra incentives to work during peak hours. This differs from previous studies of taxi market by considering market variance over multiple periods, taxi drivers' profit-driven decisions, and their scheduling constraints regarding the interdependence among different periods. The major challenge of this research is the computational intensiveness to identify optimal strategy due to the exponentially large size of a taxi driver's strategy space and the scheduling constraints. We develop an atom schedule method to overcome these issues. It reduces the magnitude of the problem while satisfying the constraints to filter out infeasible pure strategies. Simulation results based on real data show the effectiveness of the proposed methods, which opens up a new door to improving the efficiency of taxi market in megacities (e.g., Beijing).", "title": "Optimal Pricing for Improving Efficiency of Taxi Systems"}, {"url": "https://www.ijcai.org/Abstract/13/415", "abstract": "Accurate estimates of daily crop evapotranspiration (ET) are needed for efficient irrigation management in regions where crop water demand exceeds rainfall. Daily grass or alfalfa reference ET values and crop coefficients are widely used to estimate crop water demand. Inaccurate reference ET estimates can hence have a tremendous impact on irrigation costs and the demands on freshwater resources. ET networks calculate reference ET using precise measurements of meteorological data. These networks are typically characterized by gaps in spatial coverage and lack of sufficient funding, creating an immediate need for alternative sources that can fill data gaps without high costs. Although non-agricultural weather stations provide publicly accessible meteorological data, there are concerns that the data may be unsuitable for estimating reference ET due to factors such as weather station siting, data formats and quality control issues. The objective of our research is to enable the use of alternative data sources, adapting sophisticated machine learning algorithms such as Gaussian process models and neural networks to discover and model the nonlinear relationships between non-ET weather station data and the reference ET computed by ET networks. Using data from the Texas High Plains region in the U.S., we demonstrate significant improvement in estimation accuracy in comparison with baseline regression models typically used for irrigation management applications.", "title": "Estimating Reference Evapotranspiration for Irrigation Management in the Texas High Plains"}, {"url": "https://www.ijcai.org/Abstract/13/416", "abstract": "Traffic sensing is a key baseline input for sustainable cities to plan and administer demand-supply management through better road networks, public transportation, urban policies etc., Humans sense the environment frugally using a combination of complementary information signals from different sensors. For example, by viewing and/or hearing traffic one could identify the state of traffic on the road. In this paper, we demonstrate a fusion based learning approach to classify the traffic states using low cost audio and image data analysis using real world dataset. Road side collected traffic acoustic signals and traffic image snapshots obtained from fixed camera are used to classify the traffic condition into three broad classes viz., Jam, Medium and Free. The classification is done on f10 sec audio,image snapshot in that 10 sec g data tuple. We extract traffic relevant features from audio and image data to form a composite feature vector. In particular, we extract the audio features comprising MFCC (Mel-Frequency Cepstral Coefficients) classifier based features, honk events and energy peaks. A simple heuristic based image classifier is used, where vehicular density and number of corner points within the road segment are estimated and are used as features for traffic sensing. Finally the composite vector is tested for its ability to discriminate the traffic classes using Decision tree classifier, SVM classifier, Discriminant classifier and Logistic regression based classifier. Information fusion at multiple levels (audio, image, overall) shows consistently better performance than individual level decision making. Low cost sensor fusion based on complementary weak classifiers and noisy features still generates high quality results with an overall accuracy of 93 - 96%.", "title": "Information Fusion Based Learning for Frugal Traffic State Sensing"}, {"url": "https://www.ijcai.org/Abstract/13/417", "abstract": "Sustainable supply chain management has been an increasingly important topic of research in recent years. At the strategic level, there are computational models which study supply and distribution networks with environmental considerations. At the operational level, there are, for example, routing and scheduling models which are constrained by carbon emissions. Our paper explores work in tactical planning with regards to vehicle resource allocation from distribution centers to customer locations in a multi-echelon logistics network. We formulate the bi-objective optimization problem exactly and design a memetic algorithm to efficiently derive an approximate Pareto front. We illustrate the applicability of our approach with a large real-world dataset.", "title": "A Multi-Objective Memetic Algorithm for Vehicle Resource Allocation in Sustainable Transportation Planning"}, {"url": "https://www.ijcai.org/Abstract/13/418", "abstract": "We will show how human computation insights can be key to identifying so-called backdoor variables in combinatorial optimization problems. Backdoor variables can be used to obtain dramatic speed- ups in combinatorial search. Our approach leverages the complementary strength of human input, based on a visual identification of problem structure, crowdsourcing, and the power of combinatorial solvers to exploit complex constraints. We describe our work in the context of the domain of materials discovery. The motivation for considering the materials discovery domain comes from the fact that new materials can provide solutions for key challenges in sustainability, e.g., in energy, new catalysts for more efficient fuel cell technology.", "title": "Crowdsourcing Backdoor Identification for Combinatorial Optimization"}, {"url": "https://www.ijcai.org/Abstract/13/419", "abstract": "The Common-pool resource (CPR) game is a social dilemma where agents have to decide how to consume a shared CPR. Either they each take their cut, completely destroying the CPR, or they restrain themselves, gaining less immediate profit but sustaining the resource and future profit. When no consumption takes place the CPR simply grows to its carrying capacity. As such, this dilemma provides a framework to study the evolution of social consumption strategies and the sustainability of resources, whose size adjusts dynamically through consumption and their own implicit population dynamics. The present study provides for the first time a detailed analysis of the evolutionary dynamics of consumption strategies in finite populations, focusing on the interplay between the resource levels and preferred consumption strategies. We show analytically which restrained consumers survive in relation to the growth rate of the resources and how this affects the resources' carrying capacity. Second, we show that population structures affect the sustainability of the resources and social welfare in the population. Current results provide an initial insight into the complexity of the CPR game, showing potential for a variety of different studies in the context of social welfare and resource sustainability.", "title": "Evolution of Common-Pool Resources and Social Welfare in Structured Populations"}, {"url": "https://www.ijcai.org/Abstract/13/420", "abstract": "In the last decade, latent Dirichlet allocation (LDA) successfully discovers the statistical distribution of the topics over a unstructured text corpus. Meanwhile, more and more document data come up with rich human-provided tag information during the evolution of the Internet, which called semi- structured data. The semi-structured data contain both unstructured data (e.g., plain text) and metadata, such as papers with authors and web pages with tags. In general, different tags in a document play different roles with their own weights. To model such semi-structured documents is non-trivial. In this paper, we propose a novel method to model tagged documents by a topic model, called Tag-Weighted Topic Model (TWTM). TWTM is a framework that leverages the tags in each document to infer the topic components for the documents. This allows not only to learn document-topic distributions, but also to infer the tag-topic distributions for text mining (e.g., classification, clustering, and recommendations). Moreover, TWTM automatically infers the probabilistic weights of tags for each document. We present an efficient variational inference method with an EM algorithm for estimating the model parameters. The experimental results show that our TWTM approach outperforms the baseline algorithms over three corpora in document modeling and text classification.", "title": "Tag-Weighted Topic Model for Mining Semi-Structured Documents"}, {"url": "https://www.ijcai.org/Abstract/13/421", "abstract": "Manifold alignment is to extract the shared latent semantic structure from multiple manifolds. The joint adjacency matrix plays a key role in manifold alignment. To construct the matrix, it is crucial to get more corresponding pairs. This paper proposes an approach to obtain more and reliable corresponding pairs in terms of local structure correspondence. The sparse reconstruction weight matrix of each manifold is established to preserve the local geometry of the original data set. The sparse correspondence matrices are constructed using the sparse local structures of corresponding pairs across manifolds. Further more, a new energy function for manifold alignment is proposed to simultaneously match the corresponding instances and preserve the local geometry of each manifold. The shared low dimensional embedding, which provides better descriptions for the intrinsic geometry and relations between different manifolds, can be obtained by solving the optimization problem with closed-form solution. Experiments demonstrate the effectiveness of the proposed algorithm.", "title": "Manifold Alignment Based on Sparse Local Structures of More Corresponding Pairs"}, {"url": "https://www.ijcai.org/Abstract/13/422", "abstract": "The road network design problem is to optimize the road network by selecting paths to improve or adding paths in the existing road network, under certain constraints, e.g., the weighted sum of modifying costs. Since its multi-objective nature, the road network design problem is often challenging for designers. Empirically, the smaller diameter a road network has, the more connected and efficient the road network is. Based on this observation, we propose a set of constrained convex models for designing road networks with small diameters. To be specific, we theoretically prove that the diameter of the road network, which is evaluated w.r.t the travel times in the network, can be bounded by the algebraic connectivity in spectral graph theory since that the upper and lower bounds of diameter are inversely proportional to algebraic connectivity. Then we can focus on increasing the algebraic connectivity instead of reducing the network diameter, under the budget constraints. The above formulation leads to a semi-definite program, in which we can get its global solution easily. Then, we present some simulation experiments to show the correctness of our method. At last, we compare our method with an existing method based on the genetic algorithm.", "title": "A Global Constrained Optimization Method for Designing Road Networks with Small Diameters"}, {"url": "https://www.ijcai.org/Abstract/13/423", "abstract": "We propose a machine learning approach to geophysical inversion problems for the exploration of earth resources. Our approach is based on nonparametric Bayesian methods, specifically, Gaussian processes, and provides a full distribution over the predicted geophysical properties whilst enabling the incorporation of data from different modalities. We assess our method both qualitatively and quantitatively using a real dataset from South Australia containing gravity and drill-hole data and through simulated experiments involving gravity, drill-holes and magnetics, with the goal of characterizing rock densities. The significance of our probabilistic inversion extends to general exploration problems with potential to dramatically benefit the industry.", "title": "Bayesian Joint Inversions for the Exploration of Earth Resources"}, {"url": "https://www.ijcai.org/Abstract/13/424", "abstract": "In this paper we study a dynamic problem of ridesharing and taxi sharing with time windows. We consider a scenario where people needing a taxi or interested in getting a ride use a phone app to designate their source and destination points in a city, as well others restrictions (such as maximum allowable time to be at the destination). On the other hand, we have taxis and people interested in giving a ride, with their current positions and also some constraints (vehicle capacity, destination, maximum time to destination). We want to maximize the number of shared trips: in the case of taxis, people going to close locations can share the costs of the trip, and in case of rides, the driver and passengers can share costs as well. This problem is dynamic since new calls for taxis or calls for rides arrive on demand. This give rise to an optimization problem which we prove to be NP-Hard. We then propose heuristics to deal with it. We focus on the taxi sharing problem, but we show that our model is easily extendable to model the ridesharing situation or even a situation where there are both taxis and car owners. In addition, we present a framework that consists basically of a client application and a server. The last one processes all incoming information in order to match vehicles to passengers requests. The entire system can be used by taxi companies and riders in a way to reduce traffic in the cities and to reduce the emission of greenhouse gases.", "title": "Dynamic Taxi and Ridesharing: A Framework and Heuristics for the Optimization Problem"}, {"url": "https://www.ijcai.org/Abstract/13/425", "abstract": "A key issue for the realization of the smart grid vision is the implementation of effective demand-side management. One possible approach involves exposing dynamic energy prices to end-users. In this paper, we consider a resulting problem on the user's side: how to adaptively heat a home given dynamic prices. The user faces the challenge of having to react to dynamic prices in real time, trading off his comfort with the costs of heating his home to a certain temperature. We propose an active learning approach to adjust the home temperature in a semi-automatic way. Our algorithm learns the user's preferences over time and automatically adjusts the temperature in real-time as prices change. In addition, the algorithm asks the user for feedback once a day. To find the best query time, the algorithm solves an optimal stopping problem. Via simulations, we show that our algorithm learns users' preferences quickly, and that using the expected utility loss as the query criterion outperforms standard approaches from the active learning literature.", "title": "An Active Learning Approach to Home Heating in the Smart Grid"}, {"url": "https://www.ijcai.org/Abstract/13/426", "abstract": "The next generation of power systems faces significant challenges, both in coping with increased loading of an aging infrastructure and incorporating renewable energy sources. Meeting these challenges requires a fundamental change in the operation of power systems by replacing human-in-the-loop operations with autonomous systems. This is especially acute in distribution systems, where renewable integration often occurs. This paper investigates the automation of power supply restoration (PSR), that is, the process of optimally reconfiguring a faulty distribution grid to resupply customers. The key contributions of the paper are (1) a flexible mixed-integer programming framework for solving PSR, (2) a model decomposition to obtain high-quality solutions within the required time constraints, and (3) an experimental validation of the potential benefits of the proposed PSR operations.", "title": "Planning with MIP for Supply Restoration in Power Distribution Systems"}, {"url": "https://www.ijcai.org/Abstract/13/427", "abstract": "We address the problem of forecasting the usage of multiple electrical appliances by domestic users, with the aim of providing suggestions about the best time to run appliances in order to reduce carbon emissions and save money (assuming time-of-use pricing), while minimising the impact on the users' daily habits.An important challenge related to this problem is the modelling the everyday routine of the consumers and of the inter-dependencies between the use of different appliances. Given this, we develop an important building block of future home energy management systems: a prediction algorithm, based on a graphical model, that captures the everyday habits and the inter-dependency between appliances by exploiting their periodic features.We demonstrate through extensive empirical evaluations on real-world data from a prominent database that our approach outperforms existing methods by up to 47%.", "title": "Forecasting Multi-Appliance Usage for Smart Home Energy Management"}, {"url": "https://www.ijcai.org/Abstract/13/428", "abstract": "A significant portion of the electricity network capacity is built to run only a few days a year when demand peaks. As a result, expensive power generation plants and equipment costing millions of dollars are sitting idle most of the time, which increases costs for everyone. We present randomized load control, a simple distributed approach for scheduling smart appliances. Randomized load control schedules the start time of appliances that are programmed to run within a specified time window, so that the aggregate load achieves a given ideal load. Our results show that we do achieve the given ideal load to a great extent. This is remarkable as the approach is completely distributed and preserves customer privacy as the scheduling happens within each house or building separately.", "title": "Randomized Load Control: A Simple Distributed Approach for Scheduling Smart Appliances"}, {"url": "https://www.ijcai.org/Abstract/13/429", "abstract": "Diffusion processes in networks are increasingly used to model dynamic phenomena such as the spread of information, wildlife, or social influence. Our work addresses the problem of learning the underlying parameters that govern such a diffusion process by observing the time at which nodes become active. A key advantage of our approach is that, unlike previous work, it can tolerate missing observations for some nodes in the diffusion process. Having incomplete observations is characteristic of offline networks used to model the spread of wildlife. We develop an EM algorithm to address parameter learning in such settings. Since both the E and M steps are computationally challenging, we employ a number of optimization methods such as nonlinear and difference-of-convex programming to address these challenges. Evaluation of the approach on the Red-cockaded Woodpecker conservation problem shows that it is highly robust and accurately learns parameters in various settings, even with more than 80% missing data.", "title": "Parameter Learning for Latent Network Diffusion"}, {"url": "https://www.ijcai.org/Abstract/13/430", "abstract": "Water pipe failures can not only have a great impact on people's daily life but also cause significant waste of water which is an essential and precious resource to human beings. As a result, preventative maintenance for water pipes, particularly in urban-scale networks, is of great importance for a sustainable society. To achieve effective replacement and rehabilitation, failure prediction aims to proactively find those 'most-likely-to-fail' pipes becomes vital and has been attracting more attention from both academia and industry, especially from the civil engineering field. This paper presents an already-deployed industrial computational system for pipe failure prediction. As an alternative to risk matrix methods often depending on ad-hoc domain heuristics, learning based methods are adopted using the attributes with respect to physical, environmental, operational conditions and etc. Further challenge arises in practice when lacking of profile attributes. A dive into the failure records shows that the failure event sequences typically exhibit temporal clustering patterns, which motivates us to use the stochastic process to tackle the failure prediction task. Specifically, the failure sequence is formulated as a self-exciting stochastic process which is, to our best knowledge, a novel formulation for pipe failure prediction. And we show that it outperforms a baseline assuming the failure risk grows linearly with aging. Broad new problems and research points for the machine learning community are also introduced for future work.", "title": "Towards Effective Prioritizing Water Pipe Replacement and Rehabilitation"}, {"url": "https://www.ijcai.org/Abstract/13/431", "abstract": "The Chance-Constrained Stochastic Programming (CCSP) is one of the models for decision making under uncertainty.\u00a0 In this paper, we consider the special case of the CCSP in which only the right-hand side vector is random with a discrete distribution having a finite support.\u00a0 The unit commitment problem is one of the applications of the special case of the CCSP.\u00a0 Existing methods for exactly solving the CCSP problems require an enumeration of scenarios when they model a CCSP problem using a Mixed Integer Programming (MIP).\u00a0 We show how to reduce the number of scenarios enumerated in the MIP model.\u00a0 In addition, we give another compact MIP formulation to approximately solve the CCSP problems.", "title": "Improved Integer Programming Approaches for Chance-Constrained Stochastic Programming"}, {"url": "https://www.ijcai.org/Abstract/13/432", "abstract": "Automated acoustic recognition of species aims to provide a cost-effective method for biodiversity monitoring. This is particularly appealing for detecting endangered animals with a distinctive call, such as the New Forest cicada. To this end, we pursue a crowdsourcing approach, whereby the millions of visitors to the New Forest will help to monitor the presence of this cicada by means of a smartphone app that can detect its mating call. However, current systems for acoustic insect classification are aimed at batch processing and not suited to a real-time approach as required by this system, because they are too computationally expensive and not robust to environmental noise. To address this shortcoming we propose a novel insect detection algorithm based on a hidden Markov model to which we feed as a single feature vector the ratio of two key frequencies extracted through the Goertzel algorithm. Our results show that this novel approach, compared to the state of the art for batch insect classification, is much more robust to noise while also reducing the computational cost.", "title": "A Hidden Markov Model-Based Acoustic Cicada Detector for Crowdsourced Smartphone Biodiversity Monitoring"}, {"url": "https://www.ijcai.org/Abstract/13/433", "abstract": "In complex dynamic systems, accurate forecasting of extreme events, such as hurricanes, is a highly underdetermined, yet very important sustainability problem. While physics-based models deserve their own merits, they often provide unreliable predictions for variables highly related to extreme events. In this paper, we propose a new supervised machine learning problem, which we call a forecast oriented classification of spatiotemporal extreme events. We formulate three important real-world extreme event classification tasks, including seasonal forecasting of (a) tropical cyclones in Northern Hemisphere, (b) hurricanes and landfalling hurricanes in North Atlantic, and (c) North African rainfall. Corresponding predictor and predict and data sets are constructed. These data present unique characteristics and challenges that could potentially motivate future Artificial Intelligent and Data Mining research.", "title": "Forecast Oriented Classification of Spatio-Temporal Extreme Events"}, {"url": "https://www.ijcai.org/Abstract/13/434", "abstract": "The best practice method for managing ecological systems under uncertainty is adaptive management (AM), an iterative process of reducing uncertainty while simultaneously optimizing a management objective. Existing solution methods used for AM problems assume that the system dynamics are stationary, i.e., described by one of a set of pre-defined models. In reality ecological systems are rarely stationary and evolve over time. Importantly, the effects of climate change on populations are unlikely to be captured by stationary models. Practitioners need efficient algorithms to implement AM on real-world problems. AM can be formulated as a hidden model Markov Decision Process (hmMDP), which allows the state space to be factored and shows promise for the rapid resolution of large problems. We provide an ecological dataset and performance metrics for the AM of a network of shorebird species utilizing the East Asian-Australasian flyway given uncertainty about the rate of sea level rise. The non-stationary system is modelled as a stationary POMDP containing hidden alternative models with known probabilities of transition between them. We challenge the POMDP community to exploit the simplifications allowed by structuring the AM problem as an hmMDP and improve our benchmark solutions.", "title": "Adaptive Management of Migratory Birds under Sea Level Rise"}, {"url": "https://www.ijcai.org/Abstract/13/435", "abstract": "The emergence and ubiquity of online social networks have enriched web data with evolving interactions and communities both at mega-scale and in real-time. This data offers an unprecedented opportunity for studying the interaction between society and disease outbreaks. The challenge we describe in this data paper is how to extract and leverage epidemic outbreak insights from massive amounts of social media data and how this exercise can benefit medical professionals, patients, and policymakers alike. We attempt to prepare the research community for this challenge with four datasets. Publishing the four datasets will commoditize the data infrastructure to allow a higher and more efficient focal point for the research community.", "title": "Detecting and Tracking Disease Outbreaks by Mining Social Media Data"}, {"url": "https://www.ijcai.org/Abstract/13/437", "abstract": "In this paper, we study user modeling on Twitter. We investigate different strategies for mining user interest profiles from microblogging activities ranging from strategies that analyze the semantic meaning of Twitter messages to strategies that adapt to temporal patterns that can be observed in the microblogging behavior. We evaluate the quality of the user modeling methods in the context of a personalized news recommendation system. Our results reveals that an understanding of the semantic meaning of microposts is key for generating high-quality user profiles.", "title": "Twitter-Based User Modeling for News Recommendations"}, {"url": "https://www.ijcai.org/Abstract/13/438", "abstract": "We introduce language-based games, a generalization of psychological games [Geanakoplos, Pearce, and Stacchetti, 1989] that can also capture reference-dependent preferences [Koszegi and Rabin, 2006], which extend the domain of the utility function to \"situations\", maximal consistent sets in some language. The role of the underlying language in this framework thus becomes particularly critical. Of particular interest are situations where the language can express only coarse beliefs [Mullainathan, 2002]. Despite the expressive power of the approach, we show that it it can describe games in a simple, natural way. Nash equilibrium and rationalizability are generalized to this setting; Nash equilibrium is shown not to exist in general, while the existence of rationalizable strategies is proved under mild conditions.", "title": "Language-Based Games (Extended Abstract)"}, {"url": "https://www.ijcai.org/Abstract/13/439", "abstract": "We establish the unexpected power of conflict driven clause learning (CDCL) proof search by proving that the sets of unsatisfiable clauses obtained from the guarded graph tautology principles of Alekhnovich, Johannsen, Pitassi and Urquhart have polynomial size pool resolution refutations that use only input lemmas as learned clauses. We further show that, under the correct heuristic choices, these refutations can be carried out in polynomial time by CDCL proof search without restarts, even when restricted to greedy, unit-propagating search. The guarded graph tautologies had been conjectured to separate CDCL without restarts from resolution; our results refute this conjecture.", "title": "An Improved Separation of Regular Resolution from Pool Resolution and Clause Learning (Extended Abstract)"}, {"url": "https://www.ijcai.org/Abstract/13/440", "abstract": "We investigate the\u00a0 complexity of\u00a0 satisfiability for one-agent refinement modal logic (RML), an extension of basic modal logic (ML) obtained by adding refinement quantifiers on structures.\u00a0 RML is known to have the same expressiveness as ML, but the translation of RML into ML is of non-elementary complexity, and RML is at least doubly exponentially more succinct than ML. In this paper we show that RML-satisfiability is 'only' singly exponentially harder than ML-satisfiability, the latter being a well-known PSPACE-complete problem.", "title": "The Complexity of One-Agent Refinement Modal Logic"}, {"url": "https://www.ijcai.org/Abstract/13/441", "abstract": "Learning for sentence rewriting is a fundamental task in natural language processing and information retrieval. In this paper, we propose a new class of kernel functions, referred to as string rewriting kernel, to address the problem. A string re-writing kernel measures the similarity between two pairs of strings. It can capture the lexical and structural similarity between sentence pairs without the need of constructing syntactic trees. We further propose an instance of string re-writing kernel which can be computed efficiently. Experimental results on benchmark datasets show that our method can achieve comparable results with state-of-the-art methods on two sentence re-writing learning tasks: paraphrase identification and recognizing textual entailment.", "title": "An Introduction to String Re-Writing Kernel"}, {"url": "https://www.ijcai.org/Abstract/13/442", "abstract": "This paper presents a new application of logic programming to a real-life problem in hydraulic engineering. The work is developed as a collaboration of computer scientists and hydraulic engineers, and applies Constraint Logic Programming to solve a hard combinatorial problem. This application deals with one aspect of the design of a water distribution network, i.e., the valve isolation system design. We take the formulation of the problem by [Giustolisi and Savic, 2008] and show how, thanks to constraint propagation, we can get better solutions than the best solution known in the literature for the Apulian distribution network.", "title": "Optimal Valve Placement in Water Distribution Networks with CLP(FD)"}, {"url": "https://www.ijcai.org/Abstract/13/443", "abstract": "We target the problem of accuracy and robustness in causal inference from finite data sets. Our aim is to combine the inherent robustness of the Bayesian approach with the theoretical strength and clarity of constraint-based methods. We use a Bayesian score to obtain probability estimates on the input statements used in a constraint-based procedure. These are subsequently processed in decreasing order of reliability, letting more reliable decisions take precedence in case of conflicts, until a single output model is obtained. Tests show that a basic implementation of the resulting Bayesian Constraint-based Causal Discovery (BCCD) algorithm already outperforms established procedures such as FCI and Conservative PC. It indicates which causal decisions in the output have high reliability and which do not. The approach is easily adapted to other application areas such as complex independence tests.", "title": "Bayesian Probabilities for Constraint-Based Causal Discovery"}, {"url": "https://www.ijcai.org/Abstract/13/444", "abstract": "Satisfiability Modulo Constraint Handling Rules (SMCHR) is the integration of the Constraint Handling Rules (CHRs) solver programming language into a Satisfiability Modulo Theories (SMT) solver framework. \u00a0Constraint solvers are implemented in CHR as a set of high-level rules that specify the simplification (rewriting) and constraint propagation behavior. \u00a0The traditional CHR execution algorithm manipulates a global store representing a flat conjunction of constraints. \u00a0This paper introduces SMCHR: a tight integration of CHR with a modern Boolean Satisfiability (SAT) solver. \u00a0Unlike CHR, SMCHR can handle (quantifier-free) formulae with an arbitrary propositional structure. \u00a0SMCHR is essentially a Satisfiability Modulo Theories (SMT) solver where the theory T is implemented in CHR.", "title": "Satisfiability Modulo Constraint Handling Rules (Extended Abstract)"}, {"url": "https://www.ijcai.org/Abstract/13/445", "abstract": "This paper proposes an approach for the adaptation of spatial or temporal cases in a case-based reasoning system. Qualitative algebras are used as spatial and temporal knowledge representation languages. The intuition behind this adaptation approach is to apply a substitution and then repair potential inconsistencies, thanks to belief revision on qualitative algebras. A temporal example from the cooking domain is given.", "title": "Case Adaptation with Qualitative Algebras"}, {"url": "https://www.ijcai.org/Abstract/13/446", "abstract": "CPM or cost per thousand impressions is the prevalent metric used for selling online display ads. In previous work, we have shown that the exposure duration of an ad has strong effects on the likelihood of an ad being remembered, with the first seconds of exposure having the greatest impact on memory. Because an ad pricing metric that is based on both time and impressions should be more exact than one based on impressions alone, the industry has good reasons to move towards time-based advertising. We address the following unanswered question: how should time-based ads be scheduled? We test and present one schedule that leads to greater total recollection, which advertisers want, and increased revenue, which publishers want. First, we find that presenting two short, successive ads results in more total recollection than presenting one longer ad of twice the duration. Second, we show that this effect disappears as the duration of these ads increases. Together, these findings suggest a form of time-based ad pricing that should appeal to advertisers and publishers alike.", "title": "Improving the Effectiveness of Time-Based Display Advertising (Extended Abstract)"}, {"url": "https://www.ijcai.org/Abstract/13/447", "abstract": "Building on recent research on preference handling in artificial intelligence and related fields, our goal is to develop a coherent and generic methodological framework for case-based reasoning (CBR) on the basis of formal concepts and methods for knowledge representation and reasoning with preferences. A preference-based approach to CBR appears to be appealing for several reasons, notably because case-based experiences naturally lend themselves to representations in terms of preference or order relations. Moreover, the flexibility and expressiveness of a preference-based formalism well accommodate the uncertain and approximate nature of case-based problem solving. In this paper, we outline the basic ideas of preference-based CBR and sketch a formal framework for realizing these ideas.", "title": "Preference-Based CBR: General Ideas and Basic Principles"}, {"url": "https://www.ijcai.org/Abstract/13/448", "abstract": "We address the issue of Ontology-Based Data Access which consists of exploiting the semantics expressed in ontologies while querying data. Ontologies are represented in the framework of existential rules, also known as Datalog+/-. We focus on the backward chaining paradigm, which involves rewriting the query (assumed to be a conjunctive query, CQ) into a set of CQs (seen as a union of CQs). The proposed algorithm accepts any set of existential rules as input and stops for so-called finite unification sets of rules (fus). The rewriting step relies on a graph notion, called a piece, which allows to identify subsets of atoms from the query that must be processed together. We first show that our rewriting method computes a minimal set of CQs when this set is finite, i.e., the set of rules is a fus. We then focus on optimizing the rewriting step. First experiments are reported in the associated technical report.", "title": "Sound, Complete, and Minimal Query Rewriting for Existential Rules"}, {"url": "https://www.ijcai.org/Abstract/13/449", "abstract": "We propose a collaborative filtering (CF) recommendation framework which is based on viewing user feedback on products as ordinal, rather than the more common numerical view. Such an ordinal view frequently provides a more natural reflection of the user intention when providing qualitative ratings, allowing users to have different internal scoring scales. Moreover, we can address scenarios where assigning numerical scores to different types of user feedback would not be easy. The framework can wrap most collaborative filtering algorithms, enabling algorithms previously designed for numerical values to handle ordinal values. We demonstrate our framework by wrapping a leading matrix factorization CF method. A cornerstone of our method is its ability to predict a full probability distribution of the expected item ratings, rather than only a single score for an item. One of the advantages this brings is a novel approach to estimating the confidence level in each individual prediction. Compared to previous approaches to confidence estimation, ours is more principled and empirically superior in its accuracy. We demonstrate the efficacy of the approach on two of the largest publicly available datasets: the Netflix data and the Yahoo! Music data.", "title": "Collaborative Filtering on Ordinal User Feedback"}, {"url": "https://www.ijcai.org/Abstract/13/450", "abstract": "The Distributed Ontology Language DOL, currently being standardized as ISO WD 17347 within the OntoIOp (Ontology Integration and Interoperability) activity of ISO/TC 37, provides a unified framework for (1) ontologies formalized in heterogeneous logics, (2) modular ontologies, (3) links between ontologies, and (4) ontology annotation. A DOL ontology consists of modules formalized in languages such as OWL or Common Logic, serialized in the existing syntaxes of these languages. On top, DOL's meta level allows for expressing heterogeneous ontologies and links between ontologies, including (heterogeneous) imports and alignments, conservative extensions, and theory interpretations. We present the abstract syntax of these meta-level constructs, with three alternative semantics: direct, translational, and collapsed semantics.", "title": "Three Semantics for the Core of the Distributed Ontology Language (Extended Abstract)"}, {"url": "https://www.ijcai.org/Abstract/13/451", "abstract": "Recently, large amounts of data are being published using Semantic Web standards. Simultaneously, there has been a steady rise in links between objects from multiple sources. However, the ontologies behind these sources have remained largely disconnected, thereby challenging the interoperability goal of the Semantic Web. We address this problem by automatically finding alignments between concepts from multiple linked data sources. Instead of only considering the existing concepts in each ontology, we hypothesize new composite concepts, defined using conjunctions and disjunctions of (RDF) types and value restrictions, and generate alignments between them. In addition, our techniques provide a novel method for curating the linked data web by pointing to likely incorrect or missing assertions. Our approach provides a deeper understanding of the relationships between linked data sources and increases the interoperability among previously disconnected ontologies.", "title": "Discovering Alignments in Ontologies of Linked Data"}, {"url": "https://www.ijcai.org/Abstract/13/452", "abstract": "We propose a method to deform robot trajectories based on affine transformations. At the heart of our approach is the concept of affine invariance: trajectories are deformed in order to avoid unexpected obstacles or to attain new goals but, at the same time, certain precise features of the original motions are preserved. Such features include for instance trajectory smoothness, periodicity, affine velocity, or more generally, all affine-invariant features, which are of particular importance in human-centered applications. Furthermore, the proposed method is very efficient and easy to implement: there is no need to re-integrate even a part of the trajectory and, in most cases, closed-form solutions can be worked out. The method is also versatile: optimization of geometric and dynamics parameters or satisfaction of inequality constraints can be taken into account in a very natural way. As illustration, we present a method for transferring human motions to humanoid robots while preserving equi-affine velocity. Building on the presented affine deformation framework, we finally revisit the concept of trajectory redundancy from the viewpoint of group theory.", "title": "A New Trajectory Deformation Algorithm Based on Affine Transformations"}, {"url": "https://www.ijcai.org/Abstract/13/453", "abstract": "In this paper we offer a potential solution to the cold-start problem in group recommender systems. To do so, we use information about previous group recommendation events and copy ratings from a user who played a similar role in some previous group event. We show that copying in this way, i.e. conditioned on groups, is superior to copying nothing and also superior to copying ratings from the most similar user known to the system.", "title": "A Case-Based Solution to the Cold-Start Problem in Group Recommenders"}, {"url": "https://www.ijcai.org/Abstract/13/454", "abstract": "Most time series data mining algorithms use similarity search as a core subroutine, and thus the time taken for similarity search is the bottleneck for virtually all time series data mining algorithms. The difficulty of scaling search to large datasets largely explains why most academic work on time series data mining has plateaued at considering a few millions of time series objects, while much of industry and science sits on billions of time series objects waiting to be explored. In this work we show that by using a combination of four novel ideas we can search and mine truly massive time series for the first time. We demonstrate the following extremely unintuitive fact; in large datasets we can exactly search under DTW much more quickly than the current state-of-the-art Euclidean distance search algorithms. We demonstrate our work on the largest set of time series experiments ever attempted. We show that our ideas allow us to solve higher-level time series data mining problems at scales that would otherwise be untenable.", "title": "Data Mining a Trillion Time Series Subsequences under Dynamic Time Warping"}, {"url": "https://www.ijcai.org/Abstract/13/455", "abstract": "We present a reformulation of the stochastic optimal control problem in terms of KL divergence minimisation, not only providing a unifying perspective of previous approaches in this area, but also demonstrating that the formalism leads to novel practical approaches to the control problem. Specifically, a natural relaxation of the dual formulation gives rise to exact iterative solutions to the finite and infinite horizon stochastic optimal control problem, while direct application of Bayesian inference methods yields instances of risk sensitive control.", "title": "On Stochastic Optimal Control and Reinforcement Learning by Approximate Inference (Extended Abstract)"}, {"url": "https://www.ijcai.org/Abstract/13/456", "abstract": "This paper considers how consumers might relate to future smart energy grids. We used animated sketches to convey the nature of a future energy infrastructure based on software agents. Users showed a considerable lack of trust in energy companies raising a dilemma of design. While users might welcome software agents to help in engaging with complex energy infrastructures, they had little faith in those that might provide them. This suggests the need to consider how to design software agents to enhance trust in these socio-economic settings.", "title": "At Home with Agents: Exploring Attitudes Towards Future Smart Energy Infrastructures"}, {"url": "https://www.ijcai.org/Abstract/13/457", "abstract": "Given a set of data, recorded by observing the decisions of an expert player, we present a case-based framework that allows the successful generalisation of those decisions in the game of no limit Texas Hold'em. We address the problems of determining a suitable action abstraction and the resulting state translation that is required to map real-value bet amounts into a discrete set of abstract actions. We also detail the similarity metrics used in order to identify similar scenarios, without which no generalisation of playing decisions would be possible. We show that we were able to successfully generalise no limit betting decisions from recorded data via our agent, SartreNL, which achieved a 5th place finish out of 11 opponents at the 2012 Annual Computer Poker Competition.", "title": "Decision Generalisation from Game Logs in No Limit Texas Hold\u2019em"}, {"url": "https://www.ijcai.org/Abstract/13/458", "abstract": "Location plays an essential role in our lives, bridging our online and offline worlds. This paper explores the interplay of people's location, interactions, and social ties within a large real-world dataset. We present and evaluate Flap, a system that solves two intimately related tasks: link and location prediction in online social networks. For link prediction, Flap infers social ties by considering patterns in friendship formation, the content of people's messages, and user location. We show that while each component is a weak predictor of friendship alone, combining them results in a strong model---accurately identifying the majority of friendships. For location prediction, Flap implements a scalable probabilistic model of human mobility, where we treat users with known GPS positions as noisy sensors of the location of their friends. We explore supervised and unsupervised learning scenarios, and focus on the efficiency of both learning and inference. We evaluate Flap on a large sample of highly active users from two distinct geographical areas and show that it (1) reconstructs the entire friendship graph with high accuracy even when no edges are given; and (2) infers people's fine-grained location, even when they keep their data private and we can only access the location of their friends. Our models significantly outperform current approaches to either task.", "title": "Modeling the Interplay of People\u2019s Location, Interactions, and Social Ties"}, {"url": "https://www.ijcai.org/Abstract/13/459", "abstract": "Evaluating the quality of ranking functions is a core task in web search and other information retrieval domains. Because query distributions and item relevance change over time, ranking models often cannot be evaluated accurately on held-out training data. Instead, considerable effort is spent on manually labeling the relevance of query results for test queries in order to track ranking performance. We address the problem of estimating ranking performance as accurately as possible on a fixed labeling budget. Estimates are based on a set of most informative test queries selected by an active sampling distribution. Query labeling costs depend on the number of result items and item-specific attributes such as document length. We derive cost-optimal sampling distributions for commonly used ranking performance measures. Experiments on web search engine data illustrate significant reductions in labeling costs.", "title": "Active Evaluation of Ranking Functions Based on Graded Relevance (Extended Abstract)"}, {"url": "https://www.ijcai.org/Abstract/13/460", "abstract": "In this paper we tackle the problem of recommendation in the scenarios with binary relevance data, when only a few (k) items are recommended to individual users. Past work on Collaborative Filtering (CF) has either not addressed the ranking problem for binary relevance datasets, or not specifically focused on improving top-k recommendations. To solve the problem we propose a new CF approach, Collaborative Less-is-More Filtering (CLiMF). In CLiMF the model parameters are learned by directly maximizing the Mean Reciprocal Rank (MRR), which is a well-known information retrieval metric for capturing the performance of top-k recommendations. We achieve linear computational complexity by introducing a lower bound of the smoothed reciprocal rank metric. Experiments on two social network datasets show that CLiMF significantly outperforms a naive baseline and two state-of-the-art CF methods.", "title": "CLiMF: Collaborative Less-Is-More Filtering"}, {"url": "https://www.ijcai.org/Abstract/13/461", "abstract": "We present probabilistic Symbol-Refined Tree Substitution Grammars (SR-TSG) for statistical parsing of natural language sentences. An SR-TSG is an extension of the conventional TSG model where each nonterminal symbol can be refined (subcategorized) to fit the training data. Our probabilistic model is consistent based on the hierarchical Pitman-Yor Process to encode backoff smoothing from a fine-grained SR-TSG to simpler CFG rules, thus all grammar rules can be learned from training data in a fully automatic fashion. Our SR-TSG parser achieves the state-of-the-art performance on the Wall Street Journal (WSJ) English Penn Treebank data.", "title": "Statistical Parsing with Probabilistic Symbol-Refined Tree Substitution Grammars"}, {"url": "https://www.ijcai.org/Abstract/13/462", "abstract": "We consider the problem of learning sparsely used dictionaries with an arbitrary square dictionary and a random, sparse coefficient matrix. We prove that O (n log n) samples are sufficient to uniquely determine the coefficient matrix. Based on this proof, we design a polynomial-time algorithm, called Exact Recovery of Sparsely-Used Dictionaries (ER-SpUD), and prove that it probably recovers the dictionary and coefficient matrix when the coefficient matrix is sufficiently sparse. Simulation results show that ER-SpUD reveals the true dictionary as well as the coefficients with probability higher than many state-of-the-art algorithms.", "title": "Exact Recovery of Sparsely-Used Dictionaries"}, {"url": "https://www.ijcai.org/Abstract/13/463", "abstract": "The community-based generation of content has been tremendously successful in the World Wide Web \u2014 people help each other by providing information that could be useful to others. We are trying to transfer this approach to robotics in order to help robots acquire the vast amounts of knowledge needed to competently perform everyday tasks. RoboEarth is intended to be a web community by robots for robots to autonomously share descriptions of tasks they have learned, object models they have created, and environments they have explored. In this paper, we report on the formal language we developed for encoding this information and present our approaches to solve the inference problems related to finding information, to determining if information is usable by a robot, and to grounding it on the robot platform.", "title": "The RoboEarth Language: Representing and Exchanging Knowledge about Actions, Objects, and Environments (Extended Abstract)"}, {"url": "https://www.ijcai.org/Abstract/13/464", "abstract": "Counting the number of social media posts on a target phenomenon has become a popular method to monitor a spatiotemporal signal. However, such counting is plagued by biased, missing, or scarce data. We address these issues by formulating signal recovery as a Poisson point process estimation problem. We explicitly incorporate human population bias, time delays and spatial distortions, and spatiotemporal regularization into the model to address the data quality issues. Our model produces qualitatively convincing results in a case study on wildlife roadkill monitoring.", "title": "Socioscope: Spatio-Temporal Signal Recovery from Social Media (Extended Abstract)"}, {"url": "https://www.ijcai.org/Abstract/13/465", "abstract": "We propose a variant of Alternating-time Temporal Logic (ATL) grounded in the agents' operational know-how, as defined by their libraries of abstract plans. In our logic, it is possible to refer to \"rational\" strategies for agents developed under the Belief-Desire-Intention agent paradigm. This allows us to express and verify properties of BDI systems using ATL-type logical frameworks.", "title": "Using Strategic Logics to Reason about Agent Programs"}, {"url": "https://www.ijcai.org/Abstract/13/466", "abstract": "User-Centered Programming by Demonstration is an approach that places the needs of people above algorithmic constraints and requirements. In this paper we present a user-centered programming by demonstration project for authoring interactive robotic locomotion style. The style in which a robot moves about a space, expressed through its motions, can be used for communication. For example, a robot could move aggressively in reaction to a person's actions, or alternatively react using careful, submissive movements. We present a new demonstration interface, algorithm, and evaluation results.", "title": "User-Centered Programming by Demonstration: Stylistic Elements of Behavior"}, {"url": "https://www.ijcai.org/Abstract/13/467", "abstract": "Online content have become an important medium to disseminate information and express opinions. With their proliferation, users are faced with the problem of missing the big picture in a sea of irrelevant and/or diverse content. In this paper, we addresses the problem of information organization of online document collections, and provide algorithms that create a structured representation of the otherwise unstructured content. We leverage the expressiveness of latent probabilistic models (e.g., topic models) and non-parametric Bayes techniques (e.g., Dirichlet processes), and give online and distributed inference algorithms that scale to terabyte datasets and adapt the inferred representation with the arrival of new documents. This paper is an extended abstract of the 2012 ACM SIGKDD best doctoral dissertation award of Ahmed [2011].", "title": "Scalable Dynamic Nonparametric Bayesian Models of Content and Users"}, {"url": "https://www.ijcai.org/Abstract/13/468", "abstract": "Combinatorial Optimization is an important area of computer science that has many theoretical and practical applications. In the thesis, we present important contributions to several different areas of combinatorial optimization, including nogood learning, symmetry breaking, dominance, relaxations and parallelization. We develop a new nogood learning technique based on constraint projection that allows us to exploit subproblem dominances that arise when two different search paths lead to subproblems which are identical on the remaining unfixed variables. We present a new symmetry breaking technique called SBDS-1UIP, which extends Symmetry Breaking During Search (SBDS) by using the more powerful 1UIP nogoods generated by Lazy Clause Generation (LCG) solvers. We present two new general methods for exploiting almost symmetries by modifying SBDS-1UIP and by using conditional symmetry breaking constraints. We solve the Minimization of Open Stacks Problem, the Talent Scheduling Problem (CSPLib prob039), and the Maximum Density Still Life Problem (CSPLib prob032) many orders of magnitude faster than the previous state of the art by applying various powerful techniques such as nogood learning, dynamic programming, dominance and relaxations. We present cache aware data structures for SAT solvers which allows sequential and parallel versions of SAT solvers to run more quickly. And we present a new load balancing scheme for parallel search called confidence based work stealing, which allows the parallel search to make use of the information contained in the branching heuristic.", "title": "Improving Combinatorial Optimization: Extended Abstract"}, {"url": "https://www.ijcai.org/Abstract/13/469", "abstract": "In human conversation, meaning is transported through several channels such as verbal and nonverbal behavior. Certain of these behavioral aspects are culturally dependent. Mutual understanding or acceptance is thus, amongst others, depended on the cultural background of the interlocutors. When designing virtual character behavior, culture should be considered as it may improve the character's acceptance by users of certain cultural backgrounds. This paper proposes a hybrid approach for the generation of culture-specific behaviors in a multiagent system. A computational model has been established by refining theoretical knowledge of culture-specific behavior with statistical data extracted from a video corpus of German and Japanese first-time meetings. Evaluation studies of such culturally enhanced virtual characters were conducted in both targeted cultures. Results indicate that human observers tend to prefer character behavior that was designed to resemble their own cultural background.", "title": "Cultural Diversity for Virtual Characters (Extended Abstract)"}, {"url": "https://www.ijcai.org/Abstract/13/470", "abstract": "Automated planning is the process of automatically selecting actions that achieve a desired outcome. This paper summarises several contributions that improve the efficiency of automated planning via heuristic search. We discuss novel heuristics based on landmarks and a search algorithm for anytime planning. Furthermore, we analyse various search-enhancement techniques and show how the combination of these techniques lead to a planning system that proved highly successful in the 2008 and 2011 International Planning Competitions.", "title": "Landmark-Based Heuristics and Search Control for Automated Planning (Extended Abstract)"}, {"url": "https://www.ijcai.org/Abstract/13/471", "abstract": "Mobile manipulation robots are envisioned to provide many useful services both in domestic environments as well as in the industrial context. \u00a0In this paper, we present novel approaches to allow mobile maniplation systems to autonomously adapt to new or changing situations. The approaches developed in this paper cover the following four topics: (1) learning the robot's kinematic structure and properties using actuation and visual feedback, (2) learning about articulated objects in the environment in which the robot is operating, (3) using tactile feedback to augment visual perception, and (4) learning novel manipulation tasks from human demonstrations.", "title": "Learning Probabilistic Models for Mobile Manipulation Robots"}, {"url": "https://www.ijcai.org/Abstract/13/472", "abstract": "Social norms are one of the mechanisms for decentralized societies to achieve coordination amongst individuals. Such norms are conflict resolution strategies that develop from the population interactions instead of a centralized entity dictating agent protocol.One of the most important characteristics of social norms is that they are imposed by the members of the society, and they are responsible for the fulfillment and defense of these norms. By allowing agents to manage (impose, abide by and defend) social norms, societies achieve a higher degree of freedom by lacking the necessity of authorities supervising all the interactions amongst agents. In this article we summarize the contributions of my dissertation, where we provide an unifying framework for the analysis of social norms in virtual societies, providing an strong emphasis on virtual agents and humans.", "title": "Social Norms for Self-Policing Multi-Agent Systems and Virtual Societies (Extended Abstract)"}, {"url": "https://www.ijcai.org/Abstract/13/474", "abstract": "Although, Chinese and Spanish are two of the most spoken languages in the world, not much research has been done in machine translation for this language pair. This paper focuses on investigating the state-of-the-art of Chinese-to-Spanish statistical machine translation (SMT), which nowadays is one of the most popular approaches to machine translation. We conduct experimental work with the largest of these three corpora to explore alternative SMT strategies by means of using a pivot language. Three alternatives are considered for pivoting: cascading, pseudo-corpus and triangulation. As pivot language, we use either English, Arabic or French. Results show that, for a phrase-based SMT system, English is the best pivot language between Chinese and Spanish. We propose a system output combination using the pivot strategies which is capable of outperforming the direct translation strategy. The main objective of this work is motivating and involving the research community to work in this important pair of languages given their demographic impact.", "title": "Evaluating Indirect Strategies for Chinese\u2013Spanish Statistical Machine Translation: Extended Abstract"}, {"url": "https://www.ijcai.org/Abstract/13/475", "abstract": "Just as conventional institutions are organisational structures for coordinating the activities of multiple interacting individuals, electronic institutions providea computational analogue for coordinating theactivities of multiple interacting software agents.In this paper, we argue that open multi-agent systemscan be effectively designed and implementedas electronic institutions, for which we provide acomprehensive computational model. More specifically, the paper provides an operational semanticsfor electronic institutions, specifying the essential data structures, the state representation and the key operations necessary to implement them.", "title": "Communicating Open Systems (Extended Abstract)"}, {"url": "https://www.ijcai.org/Abstract/13/476", "abstract": "Bilingual machine-readable dictionaries are knowledge resources useful in many automatic tasks.However, compared to monolingual computational lexicons like WordNet, bilingual dictionaries typically provide a lower amount of structured information such as lexical and semantic relations, and often do not cover the entire range of possible translations for a word of interest. In this paper we present Cycles and Quasi-Cycles (CQC), a novel algorithm for the automated disambiguation of ambiguous translations in the lexical entries of a bilingual machine-readable dictionary.", "title": "The CQC Algorithm: Cycling in Graphs to Semantically Enrich and Enhance a Bilingual Dictionary (Extended Abstract)"}, {"url": "https://www.ijcai.org/Abstract/13/477", "abstract": "We present algorithms for generating alternative solutions for explicit acyclic AND/OR structures in non-decreasing order of cost. Our algorithms use a best first search technique and report the solutions using an implicit representation ordered by cost. Experiments on randomly constructed AND/OR DAGs and problem domains including matrix chain multiplication, finding the secondary structure of RNA, etc, show that the proposed algorithms perform favorably to the existing approach in terms of time and space.", "title": "Algorithms for Generating Ordered Solutions for Explicit AND/OR Structures: Extended Abstract"}, {"url": "https://www.ijcai.org/Abstract/13/478", "abstract": "We present YAGO2, an extension of the YAGO knowledge base, in which entities, facts, and events\u00a0 are anchored in both time and space. YAGO2 is built automatically from Wikipedia, GeoNames, and\u00a0 WordNet. It contains 447 million facts about 9.8 million entities. Human evaluation confirmed an\u00a0 accuracy of 95% of the facts in YAGO2. In this paper, we present the extraction methodology and\u00a0 the integration of the spatio-temporal dimension.", "title": "YAGO2: A Spatially and Temporally Enhanced Knowledge Base from Wikipedia (Extended Abstract)"}, {"url": "https://www.ijcai.org/Abstract/13/479", "abstract": "Based on psychological attribution theory, this paper presents a domain-independent computational model to automate social causality and responsibility judgment according to an agent's causal knowledge and observations of interaction. The proposed model is also empirically validated via experimental study.", "title": "Modeling Social Causality and Responsibility Judgment in Multi-Agent Interactions: Extended Abstract"}, {"url": "https://www.ijcai.org/Abstract/13/480", "abstract": null, "title": "The Extended Global Cardinality Constraint: An Empirical Survey (Extended Abstract)"}, {"url": "https://www.ijcai.org/Abstract/13/481", "abstract": "In automatic summarization, centrality-as- relevance means that the most important content of an information source, or of a collection of information sources, corresponds to the most central passages, considering a representation where such notion makes sense (graph, spatial, etc.). We assess the main paradigms and intro- duce a new centrality-based relevance model for automatic summarization that relies on the use of support sets to better estimate the relevant content. Geometric proximity is used to compute semantic relatedness. Centrality (relevance) is determined by considering the whole input source (and not only local information), and by taking into account the existence of minor topics or lateral subjects in the information sources to be summarized. The method consists in creating, for each passage of the input source, a support set consisting only of the most semantically related passages. Then, the determination of the most relevant content is achieved by selecting the passages that oc- cur in the largest number of support sets. This model produces extractive summaries that are generic, and language- and domain-independent. Thorough automatic evaluation shows that the method achieves state-of-the-art performance, both in written text, and automatically transcribed speech summarization, even when compared to considerably more complex approaches.", "title": "Revisiting Centrality-as-Relevance: Support Sets and Similarity as Geometric Proximity: Extended Abstract"}, {"url": "https://www.ijcai.org/Abstract/13/482", "abstract": "Large bilingual parallel texts (also known as bitexts) are usually stored in a compressed form, and previous work has shown that they can be more efficiently compressed if the fact that the two texts are mutual translations is exploited. For example, a bitext can be seen as a sequence of biwords \u2014 pairs of parallel words with a high probability of co-occurrence \u2014 that can be used as an intermediate representation in the compression process. However, the simple biword approach described in the literature can only exploit one-to-one word alignments and cannot tackle the reordering of words. We therefore introduce a generalization of biwords which can describe multi-word expressions and reorderings. We also describe some methods for the binary compression of generalized biword sequences, and compare their performance when different schemes are applied to the extraction of the biword sequence. In addition, we show that this generalization of biwords allows for the implementation of an efficient algorithm to look on the compressed bitext for words or text segments in one of the texts and retrieve their counterpart translations in the other text \u2014 an application usually referred to as translation spotting \u2014 with only some minor modifications in the compression algorithm.", "title": "Generalized Biwords for Bitext Compression and Translation Spotting: Extended Abstract"}, {"url": "https://www.ijcai.org/Abstract/13/483", "abstract": "We propose methods for computing semantic relatedness between words or texts by using knowledge from hypertext encyclopedias such as Wikipedia. A network of concepts is built by filtering the encyclopedia's articles, each concept corresponding to an article. A random walk model based on the notion of Visiting Probability (VP) is employed to compute the distance between nodes, and then between sets of nodes.To transfer learning from the network of concepts to text analysis tasks, we develop two common representation approaches. In the first approach, the shared representation space is the set of concepts in the network and every text is represented in this space. In the second approach, a latent space is used as the shared representation, and a transformation from words to the latent space is trained over VP scores.We applied our methods to four important tasks in natural language processing: word similarity, document similarity, document clustering and classification, and ranking in information retrieval. The performance is state-of-the-art or close to it for each task, thus demonstrating the generality of the proposed knowledge resource and the associated methods.", "title": "Computing Text Semantic Relatedness Using the Contents and Links of a Hypertext Encyclopedia: Extended Abstract"}, {"url": "https://www.ijcai.org/Abstract/13/484", "abstract": "Evolutionary algorithms (EAs) are a large family of heuristic optimization algorithms inspired by natural phenomena, and are often used in practice to obtain satisficing instead of optimal solutions. In this work, we investigate a largely underexplored issue: the approximation performance of EAs in terms of how close the obtained solution is to an optimal solution. We study an EA framework named simple EA with isolated population (SEIP) that can be implemented as a single- or multi-objective EA. We present general approximation results of SEIP, and specifically on the minimum set cover problem, we find that SEIP achieves the currently best-achievable approximation ratio. Moreover, on an instance class of the k-set cover problem, we disclose how SEIP can overcome the difficulty that limits the greedy algorithm.", "title": "On the Approximation Ability of Evolutionary Optimization with Application to Minimum Set Cover: Extended Abstract"}, {"url": "https://www.ijcai.org/Abstract/13/485", "abstract": "Qualitative models are predictive models that describe how changes in values of input variables affect the output variable in qualitative terms, e.g. increasing or decreasing. We describe Pad\u00e9, a new method for qualitative learning which estimates partial derivatives of the target function from training data and uses them to induce qualitative models of the target function. We formulated three methods for computation of derivatives, all based on using linear regression on local neighbourhoods. The methods were empirically tested on artificial and real-world data. We also provide a case study which shows how the developed methods can be used in practice.", "title": "Learning Qualitative Models from Numerical Data: Extended Abstract"}, {"url": "https://www.ijcai.org/Abstract/13/487", "abstract": "Decision theory focuses on the problem of making decisions under uncertainty. This uncertainty arises from the unknown aspects of the state of the world the decision maker is in or the unknown utility function of performing actions. The uncertainty can be modeled as a probability distribution capturing our belief about the world the decision maker is in. Upon making new observations, the decision maker becomes more confident about this model. In addition, if there is a prior belief on this uncertainty that may have obtained from similar experiments, the Bayesian methods may be employed. The loss incurred by the decision maker can also be utilized for the optimal action selection. Most machine learning algorithms developed though focus on one of these aspects for learning and prediction; either learning the probabilistic model or minimizing the loss. In probabilistic models, approximate inference, the process of obtaining the desired model from the observations when its is not tractable, does not consider the task loss. On the other end of the spectrum, the common practice in learning is to minimize the task loss without considering the uncertainty of prediction model. Therefore, we investigate the intersection of decision theory and machine learning considering both uncertainty in prediction model and the task loss.", "title": "Decision-Theoretic Approximations for Machine Learning"}, {"url": "https://www.ijcai.org/Abstract/13/488", "abstract": "The problem of finding the set of pareto optimals for constraints and qualitative preferences together is of great interest to many application areas. It can be viewed as a preference constrained optimization problem where the goal is to find one or more feasible solutions that are not dominated by other feasible outcomes. Our work aims to enhance the current literature of the problem by providing solving methods targeting the problem in a dynamic environments. We target the problem with an eye on adopting and benefiting from the current constraint satisfaction techniques.", "title": "Managing Qualitative Preferences and Constraints in a Dynamic Environment"}, {"url": "https://www.ijcai.org/Abstract/13/489", "abstract": "Since it is a relatively new concept, little is known about the computational complexity of fuzzy answer set programming (FASP) and almost no techniques are available to compute answer sets of FASP programs. Furthermore, the connections of FASP to other paradigms of nonmonotonic reasoning with continuous values are largely unexplored. In our dissertation, we contribute to the ongoing research on FASP on two different levels: complexity and connections to fuzzy modal logics.", "title": "Towards a Deeper Understanding of Nonmonotonic Reasoning with Degrees"}, {"url": "https://www.ijcai.org/Abstract/13/490", "abstract": "The increasing variety of robotic systems create the need for flexible architectures enabling easy integration of new robot configurations into existing multi-robot systems. This requires methods for general reasoning about what different robots are capable of doing. Teamwork is a very important factor in complex, dynamic domains. In heterogeneous teams, robustness and flexibility are increased by the diversity of the robots, each contributing different capabilities. Consequently it is reasonable to explicitly take the robots' capabilities into account when determining which robot is best suited for a task. This work develops a framework that formalizes robots' capabilities, relating to hard- and software configurations and providing a means to estimate a robot's suitability for a task. A learning algorithm for robot capabilities is included.", "title": "Capabilities in Heterogeneous Multi Robot Systems"}, {"url": "https://www.ijcai.org/Abstract/13/491", "abstract": "We introduce a new family negotiation algorithms for complex domains with a large space of possible solutions, non-linear utility functions, limited time and many agents. This family of algorithms applies a Branch & Bound search tree to search for deals that can be proposed to other agents. We test it on two test cases: the Negotiating Salesmen Problem and the strategic board game Diplomacy.", "title": "Negotiation Algorithms for Large Agreement Spaces"}, {"url": "https://www.ijcai.org/Abstract/13/492", "abstract": "Our research is within the subfield of modeling trust and reputation in multi-agent systems for online communities. Specifically, in an online community involving users and entities, users provide opinions (ratings) to entities. For each user, we are interested in addressing two problems: (1) how to accurately model the reputation of entities by aggregating opinions from all the users (advisors); and (2) how to cope with the dishonesty of an advisor in providing opinions as well as her subjectivity difference with the user.", "title": "Trust Modeling for Opinion Evaluation by Coping with Subjectivity and Dishonesty"}, {"url": "https://www.ijcai.org/Abstract/13/493", "abstract": "In this paper, we state the challenges of high-level program execution in multi-agent settings. We first introduce high-level program execution and the related work. Then we describe the completed work, the future work and its approaches. We conclude with the expected contributions of our research.", "title": "High-Level Program Execution in Multi-Agent Settings"}, {"url": "https://www.ijcai.org/Abstract/13/494", "abstract": "The effectiveness of machine learning models can often be improved by feature selection as a pre-processing step. Often this is a data driven process only and can result in models that may not correspond to true relationships present in the data set due to overfitting. In this work, we propose leveraging known relationships between variables to constrain and guide feature selection. Using commonalities across domains, we provide a framework for the user to express model constraints while still making the feature selection process data driven and sensitive to actual relationships in the data.", "title": "Using Domain Knowledge to Systematically Guide Feature Selection"}, {"url": "https://www.ijcai.org/Abstract/13/495", "abstract": "Collaborative filtering is a general technique for recommender systems, aiming to provide users with personalized recommendations. However, it suffers from two severe issues known as data sparsity and cold start. In this research, we present two different solutions to ameliorate these issues. First, we propose a trust-aware recommender system to incorporate the ratings of trusted neighbors and to form a more complete rating profile for the active users. Second, we also propose a novel Bayesian similarity measure by taking into account both the direction and length of rating profiles. Both research work shows promising empirical results based on real-world data sets. Finally, we outline the future research on trust-based clustering methods to further alleviate the concerned problems.", "title": "Improving the Performance of Recommender Systems by Alleviating the Data Sparsity and Cold Start Problems"}, {"url": "https://www.ijcai.org/Abstract/13/496", "abstract": "Interactions among agents are complicated since in order to make the best decisions, each agent has to take into account not only the strategy used by other agents but also how those strategies might change in the future (and what causes these changes). The objective of my work will be to develop a framework for learning agent models (opponent or teammate) more accurately and with less interactions, with a special focus on fast learning non-stationary strategies. As preliminary work we have proposed an initial approach for learning non-stationary strategies in repeated games. We use decision trees to learn a model of the agent, and we transform the learned trees into a MDP and solve it to obtain the optimal policy.", "title": "Strategic Interactions among Agents with Bounded Rationality"}, {"url": "https://www.ijcai.org/Abstract/13/497", "abstract": "Our initial line of research has shown that, to achieve the best performance on a constraint satisfaction problem, it may be beneficial to translate it to a satisfiability problem. For this translation, it is important to choose both the encoding and satisfiability solver in combination. By doing so, the contrasting performance among solvers on different representations of the same problem can be exploited. In taking these considerations into account, the performance of a solver portfolio augmented with multiple problem transformations can be improved significantly compared to restricting the portfolio to a single problem representation.", "title": "Problem Transformations and Algorithm Selection for CSPs"}, {"url": "https://www.ijcai.org/Abstract/13/498", "abstract": "After a disaster, human rescuers may have to wait for better conditions before beginning to search for survivors. A team of robots could enter long before the humans and scope out the environment to gather information that could help to prioritize tasks for the rescue operation. We have developed an algorithm to allow a small group of robots to progressively explore an unknown environment, moving as a group until full exploration is achieved. The novel concept behind this algorithm comes from the way in which the team stays together as a group, maintaining communication, in order to ensure full exploration as well as a path to the exit. We demonstrate in simulation that the algorithm works in multiple environments under varying conditions.", "title": "Rolling Dispersion and Exploration for Robot Teams"}, {"url": "https://www.ijcai.org/Abstract/13/499", "abstract": "In reputation systems for multiagent-based e-marketplaces, buying agents model the reputation of selling agents based on ratings shared by other buyers (called advisors). With the existence of unfair rating attacks from dishonest advisors, the effectiveness of reputation systems thus heavily relies on whether buyers can accurately determine which advisors to include in trust networks and their trustworthiness. In this paper, we propose two approaches to deal with unfair rating attacks. The first method is to combine the advantages of different categorical trust models. Secondly, we propose a novel multiagent evolutionary trust model (MET) where each buyer constructs its trust network (information about which advisors should be include in the network and their trustworthiness) by the evolutionary model. Experimental results demonstrate the proposed algorithms are more robust than the state-of-the-art trust models against various unfair rating attacks.", "title": "Towards the Design of Robust Trust and Reputation Systems"}, {"url": "https://www.ijcai.org/Abstract/13/500", "abstract": "Gutierrez and Meseguer show how to enforce consistency during distributed search in the BnB-ADOPT+ algorithm for distributed constraint optimization, but they consider only unconditional deletions. However, during search, more values can be pruned conditionally according to variable instantiations that define subproblems. Enforcing consistency in these subproblems can cause further search space reduction. Here we introduce methods to maintain soft arc consistencies in every subproblem during search. Difficulties lie in the asynchronicity of the algorithm and on the overheads induced by backtracking and undoing. After a careful implementation, experimental results show substantial benefits on several benchmarks.", "title": "Maintaining Soft Arc Consistencies in BnB-ADOPT+ during Search"}, {"url": "https://www.ijcai.org/Abstract/13/501", "abstract": "This thesis investigates the generation of new concepts from combinations of existing concepts as a language evolves. We give a method for combining concepts, and will be investigating the utility of composite concepts in language evolution and thence the utility of concept generation.", "title": "Concept Generation in Language Evolution"}, {"url": "https://www.ijcai.org/Abstract/13/502", "abstract": "Institutions (also called normative frameworks) provide an effective mechanism to govern agents in open distributed systems. An institution specifies a set of norms, with respect to the achievement of a goal or goals, that regulate agents' behaviours in terms of permissions, empowerments and obligations. However, in most real circumstances, several institutions probably have to cooperate to govern the same entities simultaneously, which is very likely to give rise to norm conflicts simply if institutions will be designed independently and typically with different goals. In this thesis, we aim: (i) to identify the different ways to combine institutions, (ii) to model those ways formally and computationally by extending an existing model for single institutions, (iii) to detect conflicts in different types of combined institutions automatically, and (iv) to resolve those conflicts via automatic norm revision using an approach based on inductive learning.", "title": "Normative Conflict Detection and Resolution in Cooperating Institutions"}, {"url": "https://www.ijcai.org/Abstract/13/503", "abstract": "My thesis work aims to study change operations for argumentation systems, especially for abstract argumentation systems \u00e0 la Dung. This paper presents a first study of the AGM revision adapted to the case of argumentation. We also sketch future research works planned to complete the one already achieved.", "title": "Dynamic of Argumentation Frameworks"}, {"url": "https://www.ijcai.org/Abstract/13/504", "abstract": "Many tasks in probabilistic reasoning can be cast as max-sum-product problems, a hard class of combinatorial problems. We describe our results in obtaining a new approximation scheme for the problem, that can be turned into an anytime procedure. For many tasks, this scheme can be shown to be asymptotically the best possible heuristic.", "title": "Approximation Algorithms for Max-Sum-Product Problems"}, {"url": "https://www.ijcai.org/Abstract/13/505", "abstract": "This research proposes the use of imitation based learning to build collaborative strategies for a team of agents. Imitation based learning involves learning from an expert by observing her demonstrating a task and then replicating it. This mechanism makes it extremely easy for a knowledge engineer to transfer knowledge to a software agent via human demonstrations. This research aims to apply imitation to learn not only the strategy of an individual agent but also the collaborative strategy of a team of agents to achieve a common goal. The effectiveness of the proposed methodology is being assessed in the domain of RoboCup Soccer Simulation 3D which is a promising platform to address many of the complex real-world problems and offers a truly dynamic, stochastic, and partially-observable environment.", "title": "On Teaching Collaboration to a Team of Autonomous Agents via Imitation"}, {"url": "https://www.ijcai.org/Abstract/13/506", "abstract": "The objective of the thesis is to explore how complex data can be treated using unsupervised machine learning techniques, in which additional information is injected to guide the exploratory process. Starting from specific problems, our contributions take into account the different dimensions of the complex data: their nature (image, text), the additional information attached to the data (labels, structure, concept ontologies) and the temporal dimension. A special attention is given to data representation and how additional information can be leveraged to improve this representation.", "title": "Semi-Supervised Structuring of Complex Data"}, {"url": "https://www.ijcai.org/Abstract/13/507", "abstract": "A novel proposal for object recognition based on relational grammars and Bayesian Networks is presented. Based on this grammar an object is represented as a hierarchy of features and spatial relations. This representation is transformed to a Bayesian network structure which parameters are learned from examples. Thus, recognition is based on probabilistic inference in the Bayesian network representation. Preliminary results in modeling natural objects are presented.", "title": "Object Recognition Based on Visual Grammars and Bayesian Networks"}, {"url": "https://www.ijcai.org/Abstract/13/508", "abstract": "Researchers have made significant strides in developing recognition techniques for surface sketches, with realized and potential applications to motivate extending these techniques towards analogous surfaceless sketches. Yet surface sketch recognition techniques remain largely untested in surfaceless environments and are still highly constrained for related surfaceless gesture recognition techniques. The focus of the research is to investigate the performance of surface sketch recognition techniques in more challenging surfaceless environments, with the aim of addressing existing limitations through improved surfaceless sketch recognition techniques.", "title": "Adapting Surface Sketch Recognition Techniques for Surfaceless Sketches"}, {"url": "https://www.ijcai.org/Abstract/13/509", "abstract": "The aim of my Ph.D thesis is to identify expressive decidable classes, study the complexity of reasoning for these classes, and design efficient algorithms in the sense that they improve state of the art algorithms.", "title": "Ontology Based Query Answering with Existential Rules"}, {"url": "https://www.ijcai.org/Abstract/13/510", "abstract": "The behavior composition problem involves the automatic synthesis of a controller that is able to \"realize\" (i.e., implement) a desired target behavior specification by suitably coordinating a set of already available behaviors. While the problem has been thoroughly studied, one open issue has resisted a principled solution: if the target specification is not fully realizable, is there a way to realize it \"at best\"? In this doctoral work, we look at quantitative and qualitative ways to address this question.", "title": "Behavior Composition Optimization"}, {"url": "https://www.ijcai.org/Abstract/13/511", "abstract": "We review the challenges of Bayesian network learning, especially parameter learning, and specify the problem of learning with sparse data. We explain how it is possible to incorporate both qualitative knowledge and data with a multinomial parameter learning method to achieve more accurate predictions with sparse data.", "title": "Incorporating Expert Judgement into Bayesian Network Machine Learning"}, {"url": "https://www.ijcai.org/Abstract/13/512", "abstract": "In cooperative games with overlapping coalitions (overlapping coalition formation games, or OCF games), agents may devote only some of their resources to a coalition, allowing the formation of overlapping coalition structures. Having formed coalitions and generated profits, agents must agree on some reasonable manner in which to divide the payoffs from the coalitions they are members of. In this thesis, we study stability in OCF games. As shown in Chalkiadakis et al. (2010), stability in OCF games strongly depends on the way non-deviators react to deviation; this is because when a set deviates, it may still interact with non-deviators post deviation. We begin by proposing a formal framework for handling deviation in OCF games, which we term arbitration functions. Given a set deviation, the arbitration function specifies the payoff each coalition involving non-deviators is willing to give the deviating set. Using our framework, we define and analyze several OCF solution concepts. We show that under some assumptions on the underlying structure of the OCF game, one can find core outcomes in OCF games in polynomial time. Finally, we provide sufficient conditions for core stability for the conservative, refined and optimistic arbitration functions,", "title": "Arbitration and Stability in Cooperative Games with Overlapping Coalitions"}]