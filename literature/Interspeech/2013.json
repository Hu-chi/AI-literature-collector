[{"url": "https://www.isca-speech.org/archive/interspeech_2013/hermansky13_interspeech.html", "abstract": "I intend to mention some techniques I got involved in during the past 40 years. I will not dwell too much on details of the techniques. These are documented in various publications. Rather, I will try to talk about things which we, researchers, may say in private but seldom write about: about personal intuitions and beliefs, about excitements, frustrations, surprises, and interesting encounters on the road, while struggling to understand and emulate one of the most significant achievements of human race, the ability to communicate by speech.", "title": "My adventures with speech"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/munson13_interspeech.html", "abstract": "The speech signal is remarkably rich. As discussed by Munson, Edwards, and Beckman (2012), a single production of the word cat can index not only the regular semantic features of felis catus, but also the word\u2019s position in utterance\u2019s larger prosodic structure, the speaker\u2019s stance toward the topic being discussed, the speaker\u2019s intentions for how the word should be interpreted relative to the ongoing discourse, and aspects of the speaker\u2019s social identity (such as their gender and sexuality) and emotional state. Humans and automatic speech processing systems must be able to unpack these different messages from this complex signal. In this talk, I discuss how different types of information interact in speech production and perception. I give special attention to contrasting typical speakers and listeners with atypical populations, i.e., populations other than native language speaking adults with no history of speech, language, or hearing impairments. Together, the results I present are a \u2019call to action\u2019 for the INTERSPEECH community to consider a broader set of sources of variability when modeling spoken language production and comprehension.", "title": "On the interaction of social and linguistic factors in phonetic variation in typical and atypical speakers"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/giraud13_interspeech.html", "abstract": "Neuronal oscillations are ubiquitous in the brain and may contribute to cognition in a number of ways, for example by segregating information and organizing spike timing. Recent data show that delta, theta, and gamma oscillations are specifically engaged by the multi-timescale, quasi-rhythmic properties of speech and can track its dynamics. I will present theoretical and experimental data suggesting that auditory cortical oscillatory neural behaviour play a foundational role in speech and language processing by \u2018packaging\u2019 incoming information into units of the appropriate temporal granularity, and enabling their readout by higher order brain areas.", "title": "Are cortical oscillations a useful ingredient of speech perception?"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/clerc13_interspeech.html", "abstract": "Brain Computer Interfaces (BCI) provide a way of communicating directly from brain activity, bypassing muscular control. Research on BCI is concerned with designing reliable interaction protocols, and embedding them in systems that are both automatic and adaptive. Many types of brain activity are considered for BCI: some that are related to actual activity, such as imaginary motion or speech, and others that are not, such as evoked potentials or slow cortical potentials. This brain activity is measured through a diversity of modalities, either invasive or non-invasive. In this talk I report some recent advances in a BCI communication system called the P300 speller, which is a virtual brain-operated keyboard. This system relies on electroencephalographic activity time-locked to the flashing of the desired letters. It requires calibration of the system, but very little training from the user. Clinical tests are being conducted on a target population of patients suffering from Amyotrophic Lateral Sclerosis, in order to confirm the usability of the P300 speller for reliable communication.", "title": "Verbal communication through brain computer interfaces"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/anguera13_interspeech.html", "abstract": "In this paper we introduce a novel dynamic programming algorithm called Information Retrieval-based Dynamic Time Warping (IR-DTW) used to find non-linearly matching subsequences between two time series where matching start and end points are not known a priori. In this paper our algorithm is applied for audio matching within the query by example (QbE) spoken term detection (STD) task, although it is applicable to many other problems. The main advantages of the proposed algorithm in comparison to similar approaches are twofold. On the one hand, IR-DTW requires a much smaller memory footprint than standard Dynamic Time Warping (DTW) approaches. On the other hand, it allows for the application of indexing techniques to the search collection for increased matching speed, which makes IR-DTW suitable for application in large scale implementations. We show through preliminary experimentation with a QbE-STD task that the memory footprint is greatly reduced in comparison to a baseline subsequence-DTW (S-DTW) implementation and that its matching accuracy is much better than that of pure diagonal matching and just slightly worse than that of S-DTW.", "title": "Information retrieval-based dynamic time warping"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/can13_interspeech.html", "abstract": "Factor automaton is an efficient data structure for representing all factors (substrings) of a set of strings (e.g. a finite-state automaton). This notion can be generalized to weighted automata by associating a weight to each factor. In this paper, we consider the problem of computing expected document frequency (DF), and TF-IDF statistics for all substrings seen in a collection of word lattices by means of factor automata. We present an algorithm which transforms an acyclic weighted automaton, e.g. an ASR lattice, to a weighted factor automaton where the path weight of each factor represents the total weight associated by the input automaton to the set of strings including that factor at least once. We show how this automaton can be used to efficiently construct other types of weighted factor automata representing DF and TF-IDF statistics for all factors seen in a large speech corpus. Compared to the state-of-the-art in computing these statistics from spoken documents, our approach i) generalizes the statistics from single words to contiguous substrings, ii) provides significant gains in terms of average run-time and storage requirements and iii) constructs efficient inverted index structures for retrieval of such statistics. Experiments on a Turkish data set corroborate our claims. Acceleration of Spoken Term Detection Using a", "title": "On the computation of document frequency statistics from spoken corpora using factor automata"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/katsurada13_interspeech.html", "abstract": "We previously proposed a fast spoken term detection method that uses a suffix array data structure for searching large-scale speech documents. The method reduces search time via techniques such as keyword division and iterative lengthening search. In this paper, we propose a statistical method of assigning different threshold values to sub-keywords to further accelerate search. Specifically, the method estimates the numbers of results for keyword searches and then reduces them by adjusting the threshold values assigned to sub-keywords. We also investigate the theoretical condition that must be satisfied by these threshold values. Experiments show that the proposed search method is 10% to 30% faster than previous methods.", "title": "Acceleration of spoken term detection using a suffix array by assigning optimal threshold values to sub-keywords"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/mandal13_interspeech.html", "abstract": "We present design strategies for a keyword spotting (KWS) system that operates in highly degraded channel conditions with very low signal-to-noise ratio levels. We employ a system combination approach by combining the outputs of multiple large vocabulary automatic speech recognition (LVCSR) systems, each of which employs a different system design approach targeting three different levels of information: front-end signal processing features (standard cepstra-based, noise-robust modulation and multi layer perceptron features), statistical acoustic models (gaussian mixtures models (GMM) and subspace GMMs) and keyword search strategies (word-based and phone-based). We also use keyword-aware capabilities in the system at two levels: in the LVCSR language models by assigning higher weights to n-grams with keywords in them and in LVCSR search by using a relaxed pruning threshold for keywords. The LVCSR system outputs are represented as lattice-based unigram indices whose scores are fused by a logistic-regression based classifier to produce the final system combination output. We present the performance of our system in the phase II evaluations of DARPA's Robust Automatic Transcription of Speech (RATS) program for both Levantine Arabic and Farsi conversational speech corpora.", "title": "Strategies for high accuracy keyword detection in noisy channels"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/abad13_interspeech.html", "abstract": "The combination of several heterogeneous systems is known to provide remarkable performance improvements in verification and detection tasks. In Spoken Term Detection (STD), two important issues arise: (1) how to define a common set of detected candidates, and (2) how to combine system scores to produce a single score per candidate. In this paper, a discriminative calibration/fusion approach commonly applied in speaker and language recognition is adopted for STD. Under this approach, we first propose several heuristics to hypothesize scores for systems that do not detect a given candidate. In this way, the original problem of several unaligned detection candidates is converted into a verification task. As for other verification tasks, system weights and offsets are then estimated through linear logistic regression. As a result, the combined scores are well calibrated, and the detection threshold is automatically given by application parameters (priors and costs). The proposed method not only offers an elegant solution for the problem of fusion and calibration of multiple detectors, but also provides consistent improvements over a baseline approach based on majority voting, according to experiments on the MediaEval 2012 Spoken Web Search (SWS) task involving 8 heterogeneous systems developed at two different laboratories.", "title": "On the calibration and fusion of heterogeneous spoken term detection systems"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/narumi13_interspeech.html", "abstract": "Triphone acoustic models are often used as subword models for detecting out-of-vocabulary query terms in Spoken Term Detection (STD) systems. Our preliminary experiments revealed that the training data for a large portion of the approximately 8,000 triphone models are insufficient. Assuming that such insufficient models deteriorate the performance of STD, this paper proposes intensive triphone models constructed by integrating low-occurrence triphone models into high-occurrence ones. Experiments conducted using an actual lecture speech corpus showed that the proposed method improves the STD performance with regard to both triphones and demiphones, demonstrating its effectiveness.", "title": "Intensive acoustic models constructed by integrating low-occurrence models for spoken term detection"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/kane13_interspeech.html", "abstract": "Parameterisation of the glottal source has become increasingly useful for speech technology. For many applications it may be desirable to restrict the glottal source feature data to only speech regions where it can be reliably extracted. In this paper we exploit the previously proposed set of binary phonetic feature extractors to help determine optimal regions for glottal source analysis. Besides validation of the phonetic feature extractors, we also quantitatively assess their usefulness for improving voice quality classification and find highly significant reductions in error rates in particular when nasals and fricative regions are excluded.", "title": "Using phonetic feature extraction to determine optimal speech regions for maximising the effectiveness of glottal source analysis"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/kawahara13_interspeech.html", "abstract": "A new spectral envelope estimation procedure is proposed to recover details beyond band limitation imposed by the Shannon\u0081fs sampling theory when interpreting periodic excitation of voiced sounds as the sampling operation in the frequency domain. The proposed procedure is a hybrid of STRAIGHT, a F0-adaptive spectral envelope estimation and the auto regressive model parameter estimation. Wavelet analyses of these spectral models on the frequency domain enabled objective evaluation of this recovery procedure. The proposed procedure provides better speech quality especially when parameter manipulation is introduced.", "title": "Beyond bandlimited sampling of speech spectral envelope imposed by the harmonic structure of voiced sounds"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/lee13_interspeech.html", "abstract": "This paper presents a source-filter based adaptive harmonic model (aHM) that can modify prosody of given speech signals. Although the conventional aHM generates a homogeneous replication of the input speech, it is not suitable for prosody modification since temporal and spectral information are interweaved. The proposed method overcomes such limitation by further decomposing the harmonic parameter extracted from aHM into source and filter related components. By applying source-filter structure to aHM, the proposed algorithm can modify pitch of the synthesized speech with introducing only minor degradation. Both objective and subjective test results show that the proposed algorithm can naturally manipulate pitch contour, of which performance is much better than conventional algorithms such as pitch synchronous overlap add (PSOLA) and speech transformation and representation using adaptive interpolation of weighted spectrum (STRAIGHT).", "title": "A source-filter based adaptive harmonic model and its application to speech prosody modification"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/ramesh13_interspeech.html", "abstract": "The objective of this work is to develop an automatic method for estimating glottal opening instants (GOIs) using Hilbert envelope (HE). The GOIs are secondary major excitations after glottal closure instants (GCIs) during the production of voiced speech. The HE is defined as the magnitude of complex time function (CTF) of a given signal. The unipolar property of HE is exploited for picking the second largest peak present in a given glottal cycle and hypothesize as glottal opening instant (GOI). The electroglottogram (EGG) / speech signal is first passed through the zero frequency filtering (ZFF) method to extract GCIs. With the help of detected GCIs, the secondary peaks present in the HE of dEGG / residual are hypothesized as GOIs. The hypothesized GOIs are compared with secondary peaks estimated from the dEGG / residual. The GOIs hypothesized by the proposed method show less variance compared to peak picking from dEGG / residual.", "title": "Detection of glottal opening instants using Hilbert envelope"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/gowda13_interspeech.html", "abstract": "In this paper, we propose a robust spectral representation for detecting formants in heavily degraded conditions. The method combines the temporal robustness of the stabilized weighted linear prediction (SWLP) with the robustness of group delay (GD) function in the frequency domain. Weighting of the cost function in linear prediction analysis with the short-time energy of the speech signal improves the robustness of the resultant spectrum. It also improves the accuracy of the estimated resonances as the weighting function gives more weightage to the closed phase of the glottal cycle, which is also the high SNR region of the signal. The group delay spectrum computed as the sum of individual resonances denoted by the roots of the SWLP coefficients, improves the robustness of weaker higher order resonances. The proposed SWLP-GD spectrum performs better than the conventional LP spectrum and the STRAIGHT spectrum in terms of spectral distortion measure and formant detection accuracies.", "title": "Robust formant detection using group delay function and stabilized weighted linear prediction"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/hezard13_interspeech.html", "abstract": "This paper addresses the source-filter separation problem in the context of causal/anticausal linear filter model of voice production. An algorithm based on standard signal processing tools is proposed for the class of quasi-periodic signals (voiced sounds with quasi-stationary pitch). At first, a one-period frame of an equivalent stationary infinitely periodic signal is built. A particular attention is given to the problems of windowing and temporal aliasing. Secondly, an exact pole decomposition of this signal is computed within the class of T0-periodic signals. Finally, the glottal closure instant (GCI) and the causal-anticausal factorization of the initial frame are jointly estimated from the latter decomposition. The performance of this algorithm on synthetic signals is demonstrated and the performance on real speech is discussed. In conclusion, application of this new algorithm in a complete voice analysis-synthesis system is discussed.", "title": "A source-filter separation algorithm for voiced sounds based on an exact anticausal/causal pole decomposition for the class of periodic signals"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/godoy13_interspeech.html", "abstract": "Among the key acoustic features attributed with the intelligibility gain of Clear speech are the observed reduction in speaking rate and expansion of vowel space, representing greater articulation and vowel discrimination. Considering the slower speaking rate, previous works have attempted to assess the intelligibility impact of time-scaling casual speech to mimic Clear speech. In a complementary fashion, this work addresses the latter of the key traits observed in Clear speech, notably vowel space expansion. Specifically, a novel Clear speech-inspired frequency warping method is described and shown to successfully achieve vowel space expansion when applied to casual speech. The intelligibility impact resulting from this expansion is then evaluated objectively and subjectively through formal listening tests. Much like the relevant time-scaling works, the frequency warping that expands vowel space is not shown to yield intelligibility gains. The implications are thus that further analyses and studies are merited in order to isolate the pertinent acoustic-phonetic cues that lead to the improved intelligibility of Clear speech.", "title": "Assessing the intelligibility impact of vowel space expansion via clear speech-inspired frequency warping"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/jensen13_interspeech.html", "abstract": "This paper deals with the problem of predicting the average intelligibility of noisy and potentially processed speech signals, as observed by a group of normal hearing listeners. We propose a prediction model based on the hypothesis that intelligibility is monotonically related to the amount of Shannon information the critical-band amplitude envelopes of the noisy/processed signal convey about the corresponding clean signal envelopes. The resulting intelligibility predictor turns out to be a simple function of the correlation between noisy/processed and clean amplitude envelopes. The proposed predictor performs well (\u0192\u00cf > 0.95) in predicting the intelligibility of speech signals contaminated by additive noise and potentially non-linearly processed using time-frequency weighting.", "title": "Prediction of intelligibility of noisy and time-frequency weighted speech based on mutual information between amplitude envelopes"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/jokinen13_interspeech.html", "abstract": "Post-filtering methods are used in mobile communications to improve the quality and intelligibility of speech. This paper introduces a frequency-adaptive post-filtering algorithm that selects from a predefined set of filters the one that reallocates the largest amount of speech energy from low to high frequencies. The proposed method and another post-filtering technique were compared to unprocessed speech in subjective listening tests in terms of intelligibility. The results indicate that the proposed method outperforms the reference method in difficult noise conditions.", "title": "Frequency-adaptive post-filtering for intelligibility enhancement of narrowband telephone speech"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/li13_interspeech.html", "abstract": "In this paper, eight state-of-the-art objective speech intelligibility prediction measures are comparatively investigated for noisy signals before and after noise-reduction processing between Mandarin and Japanese. Clean speech signals (Chinese words and Japanese words) were first corrupted by three types of noise at two signal-to-noise ratios and then processed by normal-hearing listeners for recognition, whose intelligibility was subsequently predicted by objective measures. Further investigations were conducted for objective measures in predicting speech intelligibility of noise-reduced signals between subjective evaluation scores and objective prediction results, and of noisy signals before and after noise-reduction processing, in terms of correlation analysis and prediction errors. Results showed that the majority of objective measures behave differently for Mandarin and Japanese in predicting the subjective ratings, and the STOI measure consistently provided the best ability in predicting the effect on speech intelligibility of the noise-reduction processing for both Mandarin and Japanese.", "title": "Comparative investigation of objective speech intelligibility prediction measures for noise-reduced signals in Mandarin and Japanese"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/hines13_interspeech.html", "abstract": "This paper presents work on a real-time temporal clipping monitoring tool for VoIP. Temporal clipping can occur as a result of voice activity detection (VAD) or echo cancellation where comfort noise in used in place of clipped speech segments. The algorithm presented will form part of a no-reference objective model for quantifying perceived speech quality in VoIP. The overall approach uses a modular design that will help pinpoint the reason for degradations in addition to quantifying their impact on speech quality. The new algorithm was tested for VAD compared over a range of thresholds and varied speech frame sizes. The results are compared to objective Mean Opinion Scores (MOS-LQO) from POLQA. The results show that the proposed algorithm can efficiently predict temporal clipping in speech and correlates well with the full reference quality predictions from POLQA. The model shows good potential for use in a real-time monitoring tool.", "title": "Monitoring the effects of temporal clipping on voIP speech quality"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/yuan13_interspeech.html", "abstract": "This study investigated the dynamic spectral patterns of vowels in Mandarin Chinese using a corpus of monosyllabic words spoken in isolation. Mel-frequency cepstral coefficients (MFCCs) were parameterized in different ways to test the nature of the dynamic information in vowels through automatic vowel classification. Compared to the MFCCs extracted at the vowel midpoint, using the MFCCs extracted at two or three points (vowel onset, offset, and midpoint) greatly improved classification accuracies. Legendre polynomials fitted to the MFCCs over the entire vowel duration achieved approximately 30% relative error reductions over the three-point model. Euclidean cepstral distance was employed to measure the magnitude of spectral change. A negative correlation was found between the rate of spectral change and vowel duration. Vowel-dependent spectral changes appear primarily in the first half of a vowel. There is great diversity among the diphthongs and a considerable overlap between the diphthongs and the monophthongs in terms of the spectral dynamics.", "title": "The spectral dynamics of vowels in Mandarin Chinese"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/slaney13_interspeech.html", "abstract": "Calculating speaker pitch (or F0) is typically the first computational step in modeling tone and intonation for spoken language understanding. Usually pitch is treated as a fixed, single-valued quantity. The inherent ambiguity judging the octave of pitch, as well as spurious values, leads to errors in modeling pitch gestures that propagate in a computational pipeline. We present an alternative that instead measures changes in the harmonic structure using a subband autocorrelation change detector (SACD). This approach builds upon new machine-learning ideas for how to integrate autocorrelation information across subbands. Importantly however, for modeling gestures, we preserve multiple hypotheses and integrate information from all harmonics over time. The benefits of SACD over standard pitch approaches include robustness to noise and amount of voicing. This is important for real-world data in terms of both acoustic conditions and speaking style. We discuss applications in tone and intonation modeling, and demonstrate the efficacy of the approach in a Mandarin Chinese tone-classification experiment. Results suggest that SACD could replace conventional pitch-based methods for modeling gestures in selected spoken-language processing tasks.", "title": "Pitch-gesture modeling using subband autocorrelation change detection"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/gangamohan13_interspeech.html", "abstract": "Emotional speech is produced when a speaker is in a state different from normal state. The objective of this study is to explore the deviations in the excitation source features of an emotional speech compared to normal speech. The features used for analysis are extracted at subsegmental level (1.3 ms) of speech. A comparative study of these features across different emotions indicates that there are significant deviations in the subsegmental level features of speech in emotional state when compared to normal state.", "title": "Analysis of emotional speech at subsegmental level"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/morise13_interspeech.html", "abstract": "A periodicity extraction method is introduced to analyze voiced sounds with a complex excitation behavior. Although general voiced sound has only one periodicity, some voiced sounds such as the pathological voice and the singing voice often have multiple periodicities. A method for estimating multiple periodicities from voiced sounds to deal with these kinds of voices is proposed in this article. At first, a definition of the multiple periodicity and its causes are explained, and then the principle of the proposed method is introduced. The proposed method was evaluated by using several artificial signals and pathological voices recorded in a real environment. The analysis results from the artificial signals indicated that the proposed method can extract multiple periodicities, and that of the pathological voices shows a similar tendency. These results suggest that the proposed method is effective at extracting the multiple periodicities.", "title": "Periodicity extraction for voiced sounds with multiple periodicity"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/taylor13_interspeech.html", "abstract": "This paper proposes using a hidden Markov model (HMM) to model a speech signal in terms of its speech class (voiced, unvoiced and nonspeech) and for voiced speech its fundamental frequency. States of the HMM represent unvoiced speech and nonspeech with multiple voiced states that model different fundamental frequencies. The transition matrix of the HMM models temporal changes in speech class and the time-varying fundamental frequency contour. The model is then applied to voicing and fundamental frequency estimation by extracting acoustic features from a speech signal and then applying Viterbi decoding. Experimental results are presented that investigate the estimation accuracy of the proposed system and a comparison is made against conventional methods.", "title": "Modelling and estimation of the fundamental frequency of speech using a hidden Markov model"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/pohjalainen13_interspeech.html", "abstract": "Temporally weighted linear predictive methods have recently been successfully used for robust feature extraction in speech and speaker recognition. This paper introduces their general formulation, where various efficient temporal weighting functions can be included in the optimization of the all-pole coefficients of a linear predictive model. Temporal weighting is imposed by multiplying elements of instantaneous autocorrelation \"snapshot\" matrices computed from speech data. With this novel autocorrelation-snapshot formulation of weighted linear prediction, it is demonstrated that different temporal aspects of speech can be emphasized in order to enhance robustness of feature extraction in speech emotion recognition.", "title": "Extended weighted linear prediction using the autocorrelation snapshot \u2014 a robust speech analysis method and its application to recognition of vocal emotions"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/asgari13_interspeech.html", "abstract": "Accurate and robust estimation of pitch plays a central role in speech processing. Various methods in time, frequency and cepstral domain have been proposed for generating pitch candidates. Most algorithms excel when the background noise is minimal or for specific types of background noise. In this work, our aim is to improve the robustness and accuracy of pitch estimation across a wide variety of background noise conditions. For this we have chosen to adopt, the harmonic model of speech, a model that has gained considerable attention recently. We address two major weakness of this model. The problem of pitch halving and doubling, and the need to specify the number of harmonics. We exploit the energy of frequency in the neighborhood to alleviate halving and doubling. Using a model complexity term with a BIC criterion, we chose the optimal number of harmonics. We evaluated our proposed pitch estimation method with other state of the art techniques on Keele data set in terms of gross pitch error and fine pitch error. Through extensive experiments on several noisy conditions, we demonstrate that the proposed improvements provide substantial gains over other popular methods under different noise levels and environments.", "title": "Improving the accuracy and the robustness of harmonic model for pitch estimation"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/kane13b_interspeech.html", "abstract": "The robust and efficient extraction of features related to the glottal excitation source has become increasingly important for speech technology. The glottal open quotient (OQ) is one relevant measurement which is known to significantly vary with changes in voice quality on a breathy to tense continuum. The extraction of OQ, however, is hampered in the time-domain by the difficulty in consistently locating the point of glottal opening as well the computational load of its measurement. Determining OQ correlates in the frequency domain is an attractive alternative, however the lower frequencies of glottal source spectrum are also affected by other aspects of the glottal pulse shape thereby precluding closed-form solutions and straightforward mappings. The present study provides a comparison of three OQ estimation methods and shows a new method based on spectral features and artificial neural networks to outperform existing methods in terms of discrimination of voice quality, lower error values on a large volume of speech data and dramatically reduced computation time.", "title": "A comparative study of glottal open quotient estimation techniques"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/kasess13_interspeech.html", "abstract": "Branched-tube models can be used for modeling nasal speech such as nasal stops and nasalized vowels. Previously, it has been shown that the use of probabilistic prior information such as smoothness priors can reduce the within-speaker variability of the vocal tract estimates. This model, however, lacked a representation of paranasal cavities and thus a model with a more complex branching structure is desirable. This raises the question of what prior information is necessary for physically plausible parameter estimates. Here, a model with one maxillary sinus is estimated. The sinus is parameterized in terms of its resonance using radius and angle in the z-plane, and the coupling area ratio. The probabilistic scheme mentioned above is used to estimate nasal stops /m/ and /n/ extracted from the TIMIT database. Different prior assumptions concerning resonance frequency, bandwidth, and coupling of the sinus to the nasal cavity are tested. Results show, on average, a better model fit for the model including the sinus. Further, prior assumptions are shown to have a large influence on the estimated resonance of the sinus. In particular, the lack of anatomically motivated assumptions about the bandwidth and/or the resonance frequency yields unrealistic estimates of these values.", "title": "Estimation of multiple-branch vocal tract models: the influence of prior assumptions"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/geiger13_interspeech.html", "abstract": "Detecting segments of overlapping speech (when two or more speakers are active at the same time) is a challenging problem. Previously, mostly HMM-based systems have been used for overlap detection, employing various different audio features. In this work, we propose a novel overlap detection system using Long Short-Term Memory (LSTM) recurrent neural networks. LSTMs are used to generate framewise overlap predictions which are applied for overlap detection. Furthermore, a tandem HMM-LSTM system is obtained by adding LSTM predictions to the HMM feature set. Experiments with the AMI corpus show that overlap detection performance of LSTMs is comparable to HMMs. The combination of HMMs and LSTMs improves overlap detection by achieving higher recall.", "title": "Detecting overlapping speech with long short-term memory recurrent neural networks"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/sasou13_interspeech.html", "abstract": "Voice-pathology detection from a subject's voice is a promising technology for the pre-diagnosis of larynx diseases. Glottal source estimation in particular plays a very important role in voice-pathology analysis. To more accurately estimate the spectral envelope and glottal source of the pathology voice, we propose a method that can automatically generate the topology of the Glottal Source Hidden Markov Model (GS-HMM), as well as estimate the Auto-Regressive (AR)-HMM parameter by combining the AR-HMM parameter estimation method and the Minimum Description Length-based Successive State Splitting (MDL-SSS) algorithm. This paper evaluates the fundamental validity of pathology-voice analysis based on the proposed method. The experiment results confirmed the feasibility and fundamental validity of the proposed method.", "title": "Evaluation of fundamental validity in applying AR-HMM with automatic topology generation to pathology voice analysis"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/adiga13_interspeech.html", "abstract": "The objective of this work is to demonstrate the significance of instants of significant excitation for source modeling. Instants of significant excitation correspond to the glottal closure, glottal opening, onset of burst, frication and a small number of excitation instants around them. The speech signal is processed independently by zero frequency filtering (ZFF) to obtain epochs. The epochs are used as anchor points for extracting the instants of significant excitation from different representations of speech. The different representations include sequence of strength weighted epochs, small range of samples around epochs from the linear prediction (LP) residual, Hilbert envelope (HE) of LP residual and the cosine of phase sequence. The strength weighted epoch sequence generates speech which is intelligible, but synthetic in nature. By considering a small region of instants of significant excitation around the epochs, the naturalness of synthesized speech increases significantly.", "title": "Significance of instants of significant excitation for source modeling"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/arya13_interspeech.html", "abstract": "The significance of varying the height and bandwidth of the group delay spectrum is hitherto unexplored in the spectral reconstruction of speech signals. In this paper, a family of variable height-bandwidth filters are designed to evaluate their performance in the reconstruction of speech. The design procedure for higher order group delay filters as a cascade of second order filters is first described. These higher order filters enable the synthesis of speech sounds by simultaneously varying the height and bandwidth of the group delay spectrum. The group delay filter response is corrected by removing zeros in close proximity to the unit circle which give rise to abrupt phase transitions at points of significant excitation. Experiments on spectral reconstruction and perception of speech using variable height bandwidth group delay filters are conducted to compute the optimal height and bandwidth of the group delay filter. The experimental results indicate that the optimal height bandwidth obtained from a family of variable height bandwidth group delay filters does indeed improve the spectral reconstruction and perception of speech sounds when compared to fixed height bandwidth group delay filters.", "title": "Significance of variable height-bandwidth group delay filters in the spectral reconstruction of speech"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/patil13_interspeech.html", "abstract": "Linear Prediction (LP) analysis has proven to be very effective and successful in speech analysis and speech synthesis applications. This may be due to the fact that LP analysis captures implicitly the time-varying vocal tract area function. However, it captures only the second-order statistical relationships and only the linear dependencies in the sequence of samples of speech signals (and not the higher-order relations), as a result of which the LP residual is also intelligible. This paper studies the effectiveness of nonlinear prediction (NLP) of the speech signal by using the state-of-the-art Volterra-Wiener series and uses a novel chaotic titration method to analyze the chaotic characteristics of the residual obtained by both the LP and NLP methods. The experimental results demonstrate that the proposed NLP approach gives less prediction error, relatively flat residual spectrum, less PESQ score (i.e., objective evaluation of MOS to a certain extent) and less chaoticity than its LP counterpart. Finally, the L1 norm and L2 norm of NLP residual was found be relatively less than LP residual for five instances of voiced and unvoiced regions extracted from speakers of TIMIT database.", "title": "Nonlinear prediction of speech signal using volterra-wiener series"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/satt13_interspeech.html", "abstract": "This paper describes a study of a protocol and a system for automatic detection and status tracking of early-stage dementia and Mild Cognitive Impairment (MCI), from speech and voice recordings. The research has been performed in the scope of the EU FP7 Dem@Care project. We describe the speech and voice recording protocol, different families of vocal features as derived from the recorded data, the statistical properties of the vocal features, a classifier based on support vector machine, and the classification results. The vocal features we used detect the manifestation of dementia in the human voice and speech, in three axes: the impact of cognitive deficit and slower brain processing, the impact of certain mood states often observed in dementia, and the impact of impairments of the neuromuscular mechanism of the speech production. Our analysis is based on recordings of over 80 diagnosed subjects; it yields dementia and MCI detection equal-error-rate below 20%, and demonstrates the high value of using speech and voice analysis for automatic screening and status tracking of dementia from the very early stage of MCI.", "title": "Evaluation of speech-based protocol for detection of early-stage dementia"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/azarov13_interspeech.html", "abstract": "This paper introduces a framework for parametric speech modeling that can be used in various speech applications such as text-to-speech synthesis, voice conversion etc. In order to reduce impact of pitch variations the harmonic analysis is done in the warped time scale that is aligned with instantaneous pitch values. It is assumed that each harmonic has its own periodic excitation source that evolves in time and can be modeled as a sum of several sinusoidal components with close frequencies. The parameters of the excitation components are estimated using a modified instantaneous Prony's method. The proposed analysis/synthesis technique is compared with TANDEM-STRAIGHT.", "title": "Instantaneous harmonic representation of speech using multicomponent sinusoidal excitation"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/babacan13_interspeech.html", "abstract": "Glottal closure instant (GCI) estimation is a well-studied topic that plays a critical role in several speech processing applications. Many GCI estimation algorithms have been proposed in the literature and shown to provide excellent results on the speech signal. Nonetheless the efficiency of these algorithms for the analysis of the singing voice is still unknown. The goal of this paper is to assess the performance of existing GCI estimation methods on the singing voice with a quantitative comparison. A second goal is to provide a starting point for the adaptation of these algorithms to the singing voice by identifying weaknesses and strengths under different conditions. This study is carried out on a large database of singing sounds with synchronous electroglottography (EGG) recordings, containing a variety of singer categories and singing techniques. The evaluated algorithms are Dynamic Programming Phase Slope Algorithm (DYPSA), Hilbert Envelope-based detection (HE), Speech Event Detection using the Residual Excitation And a Mean-based Signal (SEDREAMS), Yet Another GCI Algorithm (YAGA) and Zero Frequency Resonator-based method (ZFR). The algorithms are evaluated in terms of both reliability and accuracy, over different singing categories, laryngeal mechanisms, and voice qualities. Additionally, the robustness of the algorithms to reverberation is analyzed.", "title": "A quantitative comparison of glottal closure instant estimation algorithms on a large variety of singing sounds"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/gomezgarcia13_interspeech.html", "abstract": "The design of voice pathology automatic detection systems is gaining attention in the last few years for presenting advantages compared to traditional diagnosis methods. However, the performance of these systems is influenced by aspects related to the inter-speaker variability, and specially to the heterogeneity introduce by gender differences. To overcome that, a gender recognizer may be employed as a preprocessing stage in order to stratify the speakers and further adjust the detectors to the specific characteristics of each target group. Nevertheless, the reliability of gender recognizers on pathological speech has not been investigated. Having this in mind, the present paper studies the effectiveness of an automatic gender recognizer, based on mel frequency cepstral coefficients and gaussian mixture models, on normal and pathological speech. The analysis is carried out parameterizing the speech, the glottal waveform extracted from speech via inverse filtering, and a vocal tract model. The experiments were carried out using sustained vowels taken from the Saarbrucken and UPM voice disorders databases, and suggest that the gender might be effectively classified when using the proposed methodology. They also suggest that gender recognizers can be successfully employed as a preprocessing stage for a more accurate design of gender-dependent pathology detection systems.", "title": "Automatic gender recognition in normal and pathological speech"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/cai13_interspeech.html", "abstract": "Knowledge of vocal-tract (VT) length is a logical prerequisite for acoustic-to-articulatory inversion. Prior work has treated VT length estimation (VTLE) and inversion largely as separate problems. We describe a new algorithm for VTLE based on acoustic-toarticulatory inversion. Our inversion process uses the Maeda model (MM) and combines global search and dynamic programming for transforming speech waveforms into articulatory trajectories. The VTLE algorithm searches for the VT length of MM that generates the most accurate and smooth inversion result. This new algorithm was tested on samples of non-nasalized diphthongs (e.g., [ai]) synthesized with MM itself, with TubeTalker (a different VT model) and collected from children and adult speakers; its performance was compared with that from a conventional formant frequency-based method. Results of VTLE on synthesized speech indicate that the inversion-based algorithm led to greater VTLE accuracy and robustness against phonetic variation than the formant-based method. Furthermore, compared to the formantbased method, results from the inversion-based algorithm showed stronger correlation with a MRI-derived VTL measure in adults and greater consistency with formerly reported age-VTL relations in children.", "title": "Unsupervised vocal-tract length estimation through model-based acoustic-to-articulatory inversion"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/mirzaei13_interspeech.html", "abstract": "In earlier work, we have shown that vocabulary discovery from spoken utterances and subsequent recognition of the acquired vocabulary can be achieved through Non-negative Matrix Factorization (NMF). An open issue for this task is to determine automatically how many different word representations should be included in the model. In this paper, Bayesian NMF is applied to estimate the model order. The per-utterance word activations are given a gamma prior while the word models are assumed deterministic. Two Bayesian approaches are applied for obtaining optimal parameter values. First, the penalized joint log-likelihood of the parameters is considered as the objective function. Then, maximal marginal likelihood estimator (MMLE) is implemented which obtains the word models maximizing the likelihood after integration over the activations. The variational Bayesian algorithm, which maximizes a lower bound of the marginal log-likelihood, is applied to this optimization problem. The number of required latent components or basis vectors (model order) is estimated by evaluating likelihood metrics. The inferred model order is validated by observing error criteria on a test set. Experiments on synthetic data as well as real speech show that MMLE is more effective for the purpose of model order selection.", "title": "Model order estimation using Bayesian NMF for discovering phone patterns in spoken utterances"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/liu13_interspeech.html", "abstract": "In current language recognition systems, the process of feature extraction from an utterance is usually independent of other utterances. In this paper, we present an approach that build an parallel \"relative feature\" using the features that have been produced, which is the measurement of the relationship of one utterance with others. The relative feature focuses on \"where it is\" instead of \"what it is\", and is more related to the classification than the traditional features. In this work, the method to build and properly use the parallel absolute-relative feature (PARF) language recognition system is also fully explained and developed. To evaluate the system, experiments were carried out on the 2009 National Institute of Standards and Technology language recognition evaluation (NIST LRE) database. The experimental results showed that the relative feature performs better than the absolute feature using a low dimension feature, especially for short test segment. The PARF system yielded 1.84%, 6.04%, 19.89% equal error rate (EER), which achieved a 15.20%, 20.63%, 16.77% relative improvements respectively for 30s, 10s, 3s compared to the baseline system.", "title": "Parallel absolute-relative feature based phonotactic language recognition"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/diez13_interspeech.html", "abstract": "In a previous work, we introduced the use of log-likelihood ratios of phone posterior probabilities, called Phone Log-Likelihood Ratios (PLLR) as features for language recognition under an iVector-based approach, yielding high performance and promising results. However, the high dimensionality of the PLLR feature vectors (with regard to MFCC/SDC features) results in comparatively higher computational costs. In this work, several supervised and unsupervised dimensionality reduction techniques are studied, based on either fusions or selection of phone posteriors, finding that PLLR feature vectors can be reduced to almost a third of their original size attaining similar performance. Finally, Principal Component Analysis (PCA) is also applied to the original PLLR vector as a feature projection method for comparison purposes. Results show that PCA stands out among all the techniques studied, revealing that it does not only reduce computational costs, but also improves system performance significantly.", "title": "Dimensionality reduction of phone log-likelihood ratio features for spoken language recognition"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/ma13_interspeech.html", "abstract": "This paper presents a set of techniques that we used to develop the language identification (LID) system for the second phase of the DARPA RATS (Robust Automatic Transcription of Speech) program, which seeks to advance state-of-the-art detection capabilities on audio from highly degraded radio communication channels. We report significant gains due to (a) improved speech activity detection, (b) special handling of training data so as to enhance performance on short duration audio samples, and (c) noise robust feature extraction and normalization methods, including the use of multi-layer perceptron (MLP) based phoneme posteriors. We show that on this type of noisy data, the above techniques provide on average a 27% relative improvement in equal error rate (EER) across several test duration conditions.", "title": "Improvements in language identification on the RATS noisy speech corpus"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/soufifar13_interspeech.html", "abstract": "Phonotactic language identification (LID) by means of n-gram statistics and discriminative classifiers is a popular approach for the LID problem. Low-dimensional representation of the n-gram statistics leads to the use of more diverse and efficient machine learning techniques in the LID. Recently, we proposed phototactic iVector as a low-dimensional representation of the n-gram statistics. In this work, an enhanced modeling of the n-gram probabilities along with regularized parameter estimation is proposed. The proposed model consistently improves the LID system performance over all conditions up to 15% relative to the previous state of the art system. The new model also alleviates memory requirement of the iVector extraction and helps to speed up subspace training. Results are presented in terms of Cavg over NIST LRE2009 evaluation set.", "title": "Regularized subspace n-gram model for phonotactic ivector extraction"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/behravan13_interspeech.html", "abstract": "I-vector based recognition is a well-established technique in stateof- the-art speaker and language recognition but its use in dialect and accent classification has received less attention. We represent an experimental study of i-vector based dialect classification, with a special focus on foreign accent detection from spoken Finnish. Using the CallFriend corpus, we first study how recognition accuracy is affected by the choices of various i-vector system parameters, such as the number of Gaussians, i-vector dimensionality and reduction method. We then apply the same methods on the Finnish national foreign language certificate (FSD) corpus and compare the results to traditional Gaussian mixture model - universal background model (GMM-UBM) recognizer. The results, in terms of equal error rate, indicate that i-vectors outperform GMM-UBM as one expects. We also notice that in foreign accent detection, 7 out of 9 accents were more accurately detected by Gaussian scoring than by cosine scoring.", "title": "Foreign accent detection from spoken Finnish using i-vectors"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/mclaren13_interspeech.html", "abstract": "This paper proposes adaptive Gaussian backend (AGB), a novel approach to robust language identification (LID). In this approach, a given test sample is compared to language-specific training data in order to dynamically select data for a trial-specific language model. Discriminative AGB additionally weights the training data to maximize discrimination against the test segment. Evaluated on heavily degraded speech data, discriminative AGB provides relative improvements of up to 45% and 38% in equal error rates (EER) over the widely adopted Gaussian backend (GB) and neural network (NN) approaches to LID, respectively. Discriminative AGB also significantly outperforms those techniques at shorter test durations, while demonstrating robustness to limited training resources and to mismatch between training and testing speech duration. The efficacy of AGB is validated on clean speech data from National Institute of Standards and Technology (NIST) language recognition evaluation (LRE) 2009, on which it was found to provide improvements over the GB and NN approaches.", "title": "Adaptive Gaussian backend for robust language identification"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/paulik13_interspeech.html", "abstract": "This paper investigates a method for training bottleneck (BN) features in a more targeted manner for their intended use in GMM-HMM based ASR. Our approach adds a GMM acoustic model activation layer to a standard BN feature extraction (FE) neural network and performs lattice-based MMI training on the resulting network. After training, the network is reverted back into a working BN FE network by removing the GMM activation layer, and we then train a GMM system on top of the bottleneck features in the normal way. Our results show that this approach can significantly improve recognition accuracy when compared to a baseline system that uses standard BN features. Further, we show that our approach can be used to perform unsupervised speaker adaptation, yielding significantly improved results compared to global cMLLR adaptation.", "title": "Lattice-based training of bottleneck feature extraction neural networks"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/gehring13_interspeech.html", "abstract": "In this work, we propose a modular combination of two popular applications of neural networks to large-vocabulary continuous speech recognition. First, a deep neural network is trained to extract bottleneck features from frames of mel scale filterbank coefficients. In a similar way as is usually done for GMM/HMM systems, this network is then applied as a non-linear discriminative feature-space transformation for a hybrid setup where acoustic modeling is performed by a deep belief network. This effectively results in a very large network, where the layers of the bottleneck network are fixed and applied to successive windows of feature frames in a time-delay fashion. We show that bottleneck features improve the recognition performance of DBN/HMM hybrids, and that the modular combination enables the acoustic model to benefit from a larger temporal context. Our architecture is evaluated on a recently released and challenging Tagalog corpus containing conversational telephone speech.", "title": "Modular combination of deep neural networks for acoustic modeling"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/chang13_interspeech.html", "abstract": "Spectro-temporal Gabor features based on auditory knowledge have improved word accuracy for automatic speech recognition in the presence of noise. In previous work, we generated robust spectro-temporal features that incorporated the power normalized cepstral coefficient (PNCC) algorithm. The corresponding power normalized spectrum (PNS) is then processed by many Gabor filters, yielding a high dimensional feature vector. In tandem processing, an MLP with one hidden layer is often employed to learn discriminative transformations from front end features, in this case Gabor filtered power spectra, to probabilistic features, which are referred as PNS-Gabor MLP. Here we improve PNS-Gabor MLP in two ways. First, we select informative Gabor features using sparse principle component analysis (sparse PCA) before tandem processing. Second, we use a deep neural network (DNN) with bottleneck structure. Experiments show that the high-dimensional Gabor features are redundant. In our experiment, sparse principal component analysis suggests Gabor filters with longer time scales are particularly informative. The best of our experimental modifications gave an error rate reduction of 15.5% relative to PNS-Gabor MLP plus MFCC, and 41.4% better than an MFCC baseline on a large vocabulary continuous speech recognition task using noisy data.", "title": "Informative spectro-temporal bottleneck features for noise-robust speech recognition"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/yan13_interspeech.html", "abstract": "We present a new scalable approach to using deep neural network (DNN) derived features in Gaussian mixture density hidden Markov model (GMM-HMM) based acoustic modeling for large vocabulary continuous speech recognition (LVCSR). The DNN-based feature extractor is trained from a subset of training data to mitigate the scalability issue of DNN training, while GMM-HMMs are trained by using state-of-the-art scalable training methods and tools to leverage the whole training set. In a benchmark evaluation, we used 309-hour Switchboard-I (SWB) training data to train a DNN first, which achieves a word error rate (WER) of 15.4% on NIST-2000 Hub5 evaluation set by a traditional DNN-HMM based approach. When the same DNN is used as a feature extractor and 2,000-hour \"SWB+Fisher\" training data is used to train the GMM-HMMs, our DNN-GMM-HMM approach achieves a WER of 13.8%. If per-conversation-side based unsupervised adaptation is performed, a WER of 13.1% can be achieved.", "title": "A scalable approach to using DNN-derived features in GMM-HMM based acoustic modeling for LVCSR"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/rath13_interspeech.html", "abstract": "In this paper, we investigate alternative ways of processing MFCCbased features to use as the input to Deep Neural Networks (DNNs). Our baseline is a conventional feature pipeline that involves splicing the 13-dimensional front-end MFCCs across 9 frames, followed by applying LDA to reduce the dimension to 40 and then further decorrelation using MLLT. Confirming the results of other groups, we show that speaker adaptation applied on the top of these features using feature-space MLLR is helpful. The fact that the number of parameters of a DNN is not strongly sensitive to the input feature dimension (unlike GMM-based systems) motivated us to investigate ways to increase the dimension of the features. In this paper, we investigate several approaches to derive higher-dimensional features and verify their performance with DNN. Our best result is obtained from splicing our baseline 40-dimensional speaker adapted features again across 9 frames, followed by reducing the dimension to 200 or 300 using another LDA. Our final result is about 3% absolute better than our best GMM system, which is a discriminatively trained model.", "title": "Improved feature processing for deep neural networks"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/vinyals13_interspeech.html", "abstract": "It has now been established that incorporating neural networks can be useful for speech recognition, and that machine learning methods can make it practical to incorporate a larger number of hidden layers in a \"deep\" structure. Here we incorporate the constraint of freezing the number of parameters for a given task, which in many applications corresponds to practical limitations on storage or computation. Given this constraint, we vary the size of each hidden layer as we change the number of layers so as to keep the total number of parameters constant. In this way we have determined, for a common task of noisy speech recognition (Aurora2), that a large number of layers is not always optimum; for each noise level there is an optimum number of layers. We also use state-of-the-art optimization algorithms to further understand the effect of initialization and convergence properties of such networks, and to have an efficient implementation that allows us to run more experiments with a standard desktop machine with a single GPU.", "title": "Deep vs. wide: depth on a budget for robust speech recognition"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/braun13_interspeech.html", "abstract": "This paper focuses on an early instance of the concept of voice onset time (VOT) which is traditionally associated with the work of Leigh Lisker and Arthur Abramson in the 1960s. Evidence is presented here that the idea behind VOT . if not the name . is much older than commonly thought. A publication of an Armenian scientist who worked at the Abbe Rousselot's experimental phonetics laboratory at the Sorbonne in the late 19th century is discussed. That researcher studied several varieties of Armenian and categorized them in terms of what is nowadays called VOT. His paper can be regarded as an exemplar of \"modern\" phonetic thinking long before the evolution of digital sound processing.", "title": "An early case of \u201cVOT\u201d"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/fox13_interspeech.html", "abstract": "This acoustic study explored dialect effects on realization of nuclear pitch accents in three regional varieties of American English spoken in central Ohio, southeastern Wisconsin and western North Carolina. Fundamental frequency (F0) change from vowel onset to offset in the most prominent syllable in a sentence was examined along four parameters: maximum F0 change, relative location of F0 maximum, F0 offset and F0 fall from maximum to offset. A robust finding was that the F0 contours in the Southern (North Carolina) variants were significantly distinct from the two Midwestern varieties whose contours did not differ significantly from one another. The Southern vowels had an earlier F0 rise, a greater F0 fall and a lower F0 offset than either Ohio or Wisconsin vowels. There was a sharper F0 drop preceding a voiceless than a voiced syllable coda. No significant dialect-related differences were found for flat F0 contours in unstressed vowels, which were also examined in the study. This study contributes the finding that dynamic variations in pitch are greater for vowels which also exhibit a greater amount of spectral dynamics. The interaction of these two sets of cues contributes to the melodic component associated with a specific regional accent.", "title": "Pitch pattern variations in three regional varieties of American English"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/lienard13_interspeech.html", "abstract": "This study investigates the possibility to recover the voice strength, i.e. the sound level produced by the speaker, from the signal recorded. The dataset consists of a set of isolated vowels (720 tokens) recorded in a situation where two interlocutors interacted orally at a distance comprised between 0.40 and 6 meters, in a furnished room. For each token, voice strength is measured at the intensity peak, and several sets of acoustic cues are extracted from the signal spectrum, after frequency weighting and intensity normalization. In the first phase, the tokens are grouped into increasing voice strength categories. Discriminant Analysis produces a classifier which takes into account all the signal dimensions implicitly coded in the set of cues. In the second phase, the cues of a new token are given to the classifier, which in turn produces its distances to the groups, providing the basis for estimating the unknown voice strength. The quality of the process is evaluated either in self-consistency mode or by cross-validation, i.e. by comparing the estimate with the value initially measured on the same token. The statistical margin of error is quite low, of the order of 3 dB, depending on the sets of cues used.", "title": "Fine-grain voice strength estimation from vowel spectral cues"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/godoy13b_interspeech.html", "abstract": "The increased vocal effort associated with the Lombard reflex produces speech that is perceived as louder and judged to be more intelligible in noise than normal speech. Previous work illustrates that, on average, Lombard increases in loudness result from boosting spectral energy in a frequency band spanning the range of formants F1-F3, particularly for voiced speech. Observing additionally that increases in loudness across spoken sentences are spectro-temporally localized, the goal of this work is to further isolate these regions of maximal loudness by linking them to specific formant trends, explicitly considering here the vowel formant separation. For both normal and Lombard speech, this work illustrates that, as loudness increases in frequency bands containing formants (e.g. F1-F2 or F2-F3), the observed separation between formant frequencies decreases. From a production standpoint, these results seem to highlight a physiological trait associated with how humans increase the loudness of their speech, namely moving vocal tract resonances closer together. Particularly, for Lombard speech, this phenomena is exaggerated: that is, the Lombard speech is louder and formants in corresponding spectro-temporal regions are even closer together.", "title": "Linking loudness increases in normal and lombard speech to decreasing vowel formant separation"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/motoki13_interspeech.html", "abstract": "A method to compute the acoustic characteristics of a simplified three-dimensional vocal-tract model with asymmetric wall impedances is presented. The acoustic field is represented in terms of both plane waves and higher-order modes in tubes. This model is constructed using a connected structure of rectangular acoustic tubes, and can parametrically represent acoustic characteristics in higher frequencies where the assumption of plane wave propagation does not hold. The propagation constants of the plane waves and the higher-order modes are calculated taking account of the asymmetric distribution of the wall impedances which can be specified as different values on four sides of each rectangular tube. The frequency characteristics of the propagation constants and the transfer characteristics of multiple section models are presented.", "title": "Three-dimensional rectangular vocal-tract model with asymmetric wall impedances"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/airaksinen13_interspeech.html", "abstract": "This study presents a new glottal inverse filtering (GIF) technique based on the closed phase analysis over multiple fundamental periods. The proposed Quasi Closed Phase Analysis (QCP) method utilizes Weighted Linear Prediction (WLP) with a specific Attenuated Main Excitation (AME) weighting function that attenuates the contribution of the glottal source in the linear prediction model optimization. This enables the use of the autocorrelation criterion in linear prediction in comparison to the conventional covariance criterion used in the closed phase analysis. The proposed method was compared to previously developed methods by using a synthetic vowel database created with a physical modeling approach. The obtained objective measures show that the proposed method improves the GIF performance for both low- and high-pitched vowels.", "title": "Quasi closed phase analysis for glottal inverse filtering"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/schuller13_interspeech.html", "abstract": "The INTERSPEECH 2013 Computational Paralinguistics Challenge provides for the first time a unified test-bed for Social Signals such as laughter in speech. It further introduces conflict in group discussions as a new task and deals with autism and its manifestations in speech. Finally, emotion is revisited as task, albeit with a broader range of overall twelve enacted emotional states. In this paper, we describe these four Sub-Challenges, their conditions, baselines, and a new feature set by the openSMILE toolkit, provided to the participants.", "title": "The INTERSPEECH 2013 computational paralinguistics challenge: social signals, conflict, emotion, autism"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/janicki13_interspeech.html", "abstract": "This paper describes an algorithm for detection of non-linguistic vocalisations, such as laughter or fillers, based on acoustic features. The algorithm proposed combines the benefits of Gaussian mixture models (GMM) and the advantages of support vector machines (SVMs). Three GMMs were trained for garbage, laughter, and fillers, and then an SVM model was trained in the GMM score space. Various experiments were run to tune the parameters of the proposed algorithm, using the data sets originating from the SSPNet Vocalisation Corpus (SVC) provided for the Social Signals Sub-Challenge of the INTERSPEECH 2013 Computational Paralinguistics Challenge. The results showed a remarkable growth of the unweighted average of the area under the receiver operating curve (UAAUC) compared to the baseline results (from 87.6% to over 94% for the development set), which confirmed the efficiency of the proposed method.", "title": "Non-linguistic vocalisation recognition based on hybrid GMM-SVM approach"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/oh13_interspeech.html", "abstract": "Trying to automatically detect laughter and other nonlinguistic events in speech raises a fundamental question: Is it appropriate to simply adopt acoustic features that have traditionally been used for analyzing linguistic events? Thus we take a step back and propose syllabic-level features that may show a contrast between laughter and speech in their intensity-, pitch-, and timbral-contours and rhythmic patterns. We motivate and define our features and evaluate their effectiveness in correctly classifying laughter from speech. Inclusion of our features in the baseline feature set for the Social Signals Sub-Challenge of the Computational Paralinguistics Challenge yielded an improvement of 2.4% in Unweighted Average Area Under the Curve (UAAUC). But beyond objective metrics, analyzing laughter at a phonetically meaningful level has allowed us to examine the characteristic contours of laughter and to recognize the importance of the shape of its intensity envelope.", "title": "Characteristic contours of syllabic-level units in laughter"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/krikke13_interspeech.html", "abstract": "In this paper, we analyze acoustic profiles of fillers (i.e. filled pauses, FPs) and laughter with the aim to automatically localize these nonverbal vocalizations in a stream of audio. Among other features, we use voice quality features to capture the distinctive production modes of laughter and spectral similarity measures to capture the stability of the oral tract that is characteristic for FPs. Classification experiments with Gaussian Mixture Models and various sets of features are performed. We find that Mel-Frequency Cepstrum Coefficients are performing relatively well in comparison to other features for both FPs and laughter. In order to address the large variation in the frame-wise decision scores (e.g., log-likelihood ratios) observed in sequences of frames we apply a median filter to these scores, which yields large performance improvements. Our analyses and results are presented within the framework of this year's Interspeech Computational Paralinguistics sub-Challenge on Social Signals.", "title": "Detection of nonverbal vocalizations using Gaussian mixture models: looking for fillers and laughter in conversational speech"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/wagner13_interspeech.html", "abstract": "Laughter and fillers like \"uhm\" and \"ah\" are social cues expressed in human speech. Detection and interpretation of such non-linguistic events can reveal important information about the speakers\u0081f intensions and emotional state. The INTERSPEECH 2013 Social Signals Sub-Challenge sets the task to localize and classify laughter and fillers in the \"SSPNet Vocalization Corpus\" (SVC) based on acoustics. In the paper at hand we investigate phonetic patterns extracted from raw speech transcriptions obtained with the CMU Sphinx toolkit for speech recognition. Even though Sphinx was used out of the box and no dedicated training on the target classes was applied, we were able to successfully predict laughter and filler frames in the development set with .87% accuracy (unweighted average Area Under the Curve (AUC)). By accumulating our features with a set of standard features provided by the challenge organizers results increased above 92%. When applying the combined set to the test corpus we achieved 87.7% as highest score, which is 4.4% above the challenge baseline.", "title": "Using phonetic patterns for detecting social cues in natural conversations"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/gupta13_interspeech.html", "abstract": "Non-verbal speech cues serve multiple functions in human interaction such as maintaining the conversational flow as well as expressing emotions, personality, and interpersonal attitude. In particular, non-verbal vocalizations such as laughters are associated with affective expressions while vocal fillers are used to hold the floor during a conversation. The Interspeech 2013 Social Signals Sub-Challenge involves detection of these two types of non-verbal signals in telephonic speech dialogs. We extend the challenge baseline system by using filtering and masking techniques on probabilistic time series representing the occurrence of a vocal event. We obtain improved area under receiver operating characteristic (ROC) curve of 93.3% (10.4% absolute improvement) for laughters and 89.7% (6.1% absolute improvement) for fillers on the test set. This improvement suggests the importance of using temporal context for detecting these paralinguistic events.", "title": "Paralinguistic event detection from speech using probabilistic time-series smoothing and masking"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/an13_interspeech.html", "abstract": "Identifying laughter and filled pauses is important to understanding spontaneous human speech. These are two common vocal expressions that are non-lexical and incredibly communicative. In this paper, we use a two-tiered system for identifying laughter and filled pauses. We first generate frame level hypotheses and subsequently rescore these based on features derived from acoustic syllable segmentation. Using Interspeech 2013 ComParE challenge corpus, SVC, we find that these rescoring experiments and inclusion of syllable based acoustic/prosodic features allow for the detection of laughter and filled pauses by at 89.3% UAAUC on the development set, an improvement of 1.7% over the challenge baseline.", "title": "Detecting laughter and filled pauses using syllable-based features"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/bone13_interspeech.html", "abstract": "Speech and spoken language cues offer a valuable means to measure and model human behavior. Computational models of speech behavior have the potential to support health care through assistive technologies, informed intervention, and efficient longterm monitoring. The Interspeech 2013 Autism Sub-Challenge addresses two developmental disorders that manifest in speech: autism spectrum disorders and specific language impairment. We present classification results with an analysis on the development set including a discussion of potential confounds in the data such as recording condition differences. We hence propose study of features within these domains that may inform realistic separability between groups as well as have the potential to be used for behavioral intervention and monitoring. We investigate templatebased prosodic and formant modeling as well as goodness of pronunciation modeling, reporting above chance classification accuracies.", "title": "Classifying language-related developmental disorders from speech cues: the promise and the potential confounds"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/kirchhoff13_interspeech.html", "abstract": "We present our system for the Interspeech 2013 Computational Paralinguistics Autism Sub-challenge. Our contribution focuses on improving classification accuracy of developmental disorders by applying a novel feature selection technique to the rich set of acoustic-prosodic features provided for this purpose. Our feature selection approach is based on submodular function optimization. We demonstrate significant improvements over systems using the full feature set and over a standard feature selection approach. Our final system outperforms the official Challenge baseline system significantly on the development set for both classification tasks, and on the test set for the Typicality task. Finally, we analyze the subselected features and identify the most important ones.", "title": "Classification of developmental disorders from speech signals using submodular feature selection"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/asgari13b_interspeech.html", "abstract": "In this paper, we report experiments on the Interspeech 2013 Autism Challenge, which comprises of two subtasks . detecting children with ASD and classifying them into four subtypes. We apply our recently developed algorithm to extract speech features that overcomes certain weaknesses of other currently available algorithms. From the input speech signal, we estimate the parameters of a harmonic model of the voiced speech for each frame including the fundamental frequency (F0). From the fundamental frequencies and the reconstructed noise-free signal, we compute other derived features such as Harmonic-to-Noise Ratio (HNR), shimmer, and jitter. In previous work, we found that these features detect voiced segments and speech more accurately than other algorithms and that they are useful in rating the severity of a subject's Parkinson's disease. Here, we employ these features, along with standard features such as energy, cepstral, and spectral features. With these features, we detect ASD using a regression and identify the sub-type using a classifier. We find that our features improve the performance, measured in terms of unweighted average recall (UAR), of detecting autism spectrum disorder by 2.3% and classifying the disorder into four categories by 2.8% over the baseline results.", "title": "Robust and accurate features for detecting and diagnosing autism spectrum disorders"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/martinez13_interspeech.html", "abstract": "This paper investigates the efficiency of several acoustic features in classifying pervasive developmental disorders, pervasive developmental disorders not-otherwise specified, dysphasia, and a group of control patients. One of the main characteristics of these disorders is the misuse and misrecognition of prosody in daily conversations. To capture this behaviour pitch, energy, and formants are modelled in long-term intervals, and the interval duration, shifted-delta cepstral coefficients, AM modulation index, and speaking rate complete our acoustic information. The concept of total variability space, or iVector space, is introduced as feature extractor for autism classification. This work is framed in the Interspeech 2013 Computational Paralinguistics Challenge as part of the Autism Subchallenge. Results are given on the Child Pathological Speech Database (CPSD), and an 87.6% and 45.1% unweighted average recall are obtained for the typicality (typical vs. atypical developing children) and diagnosis (classification into the 4 groups) tasks, respectively, on the development dataset. In addition, the combination of the new and the baseline features offers promising improvements.", "title": "Suprasegmental information modelling for autism disorder spectrum and specific language impairment classification"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/grezes13_interspeech.html", "abstract": "The automated detection of conflict will be a crucial feature of emerging speech-analysis technologies, whether the purpose is to assuage conflict in online applications or simply to mark its location for corpus analysis. In this study, we examine the predictive potential of overlapping speech in determining conflict, and we find that this feature alone is strongly correlated with high conflict levels as rated by human judges. In analyzing the SSPNET debate corpus, we effect a 2.3% improvement over baseline accuracy using speaker overlap ratio as a predicted value, suggesting that this feature is a reliable proxy for conflict level. In a follow-up experiment, we analyze the patterns of predicted conflict in the beginning, middle and end of an audio clip. Our findings show that the beginning and final segments are more predictive than the middle, which indicates that a primacy-recency effect is bearing on the perception of conflict. Since the beginning segment itself can be quite predictive, we also show that accurate predictions can be made dynamically, allowing for real-time classification during live debates.", "title": "Let me finish: automatic conflict detection using speaker overlap"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/sethu13_interspeech.html", "abstract": "This paper describes the University of New South Wales system for the Interspeech 2013 ComParE emotion sub-challenge. The primary aim of the submission is to explore the performance of model based variability compensation techniques applied to emotion classification and as a consequence of being a part of a challenge, to enable a comparison of these methods to alternative approaches. In keeping with this focused aim, a simple frame based front-end of MFCC and \u0192\u00a2MFCC is utilised. The systems outlined in this paper consists of a joint factor analysis based system and one based on a library of speaker-specific emotion models along with a basic GMM based system. The best combined system has an accuracy (UAR) of 47.8% as evaluated on the challenge development set and 35.7% as evaluated on the test set.", "title": "GMM based speaker variability compensated system for interspeech 2013 compare emotion challenge"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/rasanen13_interspeech.html", "abstract": "This work studies automatic recognition of paralinguistic properties of speech. The focus is on selection of the most useful acoustic features for three classification tasks: 1) recognition of autism spectrum developmental disorders from child speech, 2) classification of speech into different affective categories, and 3) recognizing the level of social conflict from speech. The feature selection is performed using a new variant of random subset sampling methods with k-nearest neighbors (kNN) as a classifier. The experiments show that the proposed system is able to learn a set of important features for each recognition task, clearly exceeding the performance of the same classifier using the original full feature set. However, some effects of overfitting the feature sets to finite data are also observed and discussed.", "title": "Random subset feature selection in automatic recognition of developmental disorders, affective states, and level of conflict from speech"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/lee13b_interspeech.html", "abstract": "This study investigates the classification performances of emotion and autism spectrum disorders from speech utterances using ensemble classification techniques. We first explore the performances of three well-known machine learning techniques, namely, support vector machines (SVM), deep neural networks (DNN) and k-nearest neighbours (KNN), with acoustic features extracted by the openSMILE feature extractor. In addition, we propose an acoustic segment model (ASM) technique, which incorporates the temporal information of speech signals to perform classification. A set of ASMs is automatically learned for each category of emotion and autism spectrum disorders, and then the ASM sets decode an input utterance into series of acoustic patterns, with which the system determines the category for that utterance. Our ensemble system is a combination of the machine learning and ASM techniques. The evaluations are conducted using the data sets provided by the organizer of the INTERSPEECH 2013 Computational Paralinguistics Challenge.", "title": "Ensemble of machine learning and acoustic segment model techniques for speech emotion and autism spectrum disorders recognition"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/gosztolya13_interspeech.html", "abstract": "In the area of speech technology, tasks that involve the extraction of non-lingustic information have been receiving more attention recently. The Computational Paralinguistics Challenge (ComParE 2013) sought to develop techniques to efficiently detect a number of paralinguistic events, including the detection of non-linguistic events (laughter and fillers) in speech recordings as well as categorizing whole (albeit short) recordings by speaker emotion, conflict or the presence of development disorders (autism). We treated these sub-challenges as general classification tasks and applied the general-purpose machine learning meta-algorithm, AdaBoost.MH, and its recently proposed variant, AdaBoost.MH.BA, to them. The results show that these new algorithms convincingly outperform baseline SVM scores.", "title": "Detecting autism, emotions and social signals using adaboost"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/niebuhr13_interspeech.html", "abstract": "German knows two plateau-based phrase-final intonation contours: the high level plateau of the continuation rise and the descending plateau sequence of the calling contour. They occur within a narrow scaling range of only a few semitones. The paper presents production and perception evidence for a third plateau-based phrase-final intonation contour inside this narrow scaling range. The new plateau contour shows a F0 decrease of between 1.3 st (in the form of a slightly declining plateau or a descending plateau sequence), involves additional lengthening of the vowels underneath the plateau, and occurs when resistance is futile, i.e. when speakers signal that they finally, but reluctantly, give in to a demand of the dialogue partner. Phonological implications are briefly outlined.", "title": "Resistance is futile \u2014 the intonation between continuation rise and calling contour in German"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/mixdorff13_interspeech.html", "abstract": "The presented study concerns the influence of the syllabic structure on perceived prominence. We examined how gaps in the F0 contour due to unvoiced consonants affect prominence perception, given that such gaps can either be filled or blinded out by listeners. For this purpose we created a stimulus set of real disyllabic words which differed in the quantity of the vowel of the accented syllable nucleus and the types of subsequent intervocalic consonant(s). Results include, inter alia, that stimuli with unvoiced gaps in the F0 contour are indeed perceived as less prominent. The prominence reduction is smaller for monotonous stimuli than for stimuli with F0 excursions across the accented syllable. Moreover, in combination with F0 excursions, it also mattered whether F0 had to be interpolated or extrapolated, and whether or not the gap included a fricative sound. The results support both the filling-in and blinding-out of F0 gaps, which fits in well with earlier experiments on the production and perception of pitch.", "title": "The influence of F0 contour continuity on prominence perception"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/smith13_interspeech.html", "abstract": "Rapid Prosody Transcription (RPT) was used to investigate listeners\u0081f perceptions of prosody in reading by native and non-native English speakers. RPT offers a language-independent tool to access listeners' holistic understanding of prosody. Listeners hear an audio recording of speech while following along on an orthographic, unpunctuated transcript of the recording. They indicate their perception of phrasal boundaries or prominent words by marking them on the transcript in real time. Our listeners showed higher agreement for boundary-marking in the native speakers' reading than the non-natives\u0081f. Listeners marked more boundaries in the non-natives' reading, likely because the non-natives paused more often, although listeners partially compensated by not marking boundaries as often when non-natives made short pauses. For prominence, rates of agreement were higher for the non-natives. This may be due to listeners' marking fewer prominences in the non-natives' reading, meaning that they agreed on the absence of prominent words. Compared to acoustic analysis, studying listener reactions provides more insight into what aspects of non-native prosody are most salient. This may be useful in guiding learners to the most effective ways to improve their prosody.", "title": "Native English listeners' perceptions of prosody in L1 and L2 reading"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/tsurutani13_interspeech.html", "abstract": "This study aims to investigate native speakers' perception of prosodic variation of Chinese utterances. It is known that timing is crucial for intelligibility in English, Japanese and other accent languages [9, 13 ,14 ,16]. A tone language, Chinese relies heavily on the use of pitch more than do other languages for the purpose of distinguishing the meaning of the segmentally same words as well as expressing intonation. It is expected that pitch is the most important prosodic factor for the naturalness judgment of Chinese. However, no empirical data have been presented to support this view to date. In general, listeners are more sensitive to the deviation of timing than pitch. Pitch change also triggers slight change in duration. Whether the significance of timing for naturalness is universal across languages and applies to tone languages as well, relative importance of timing and pitch in Chinese was investigated using L2 Chinese speech. The results indicated that Chinese native listeners do notice the deviation of timing and regard it as accented speech.", "title": "Naturalness judgement of L2 Mandarin Chinese \u2014 does timing matter?"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/aalto13_interspeech.html", "abstract": "The fundamental frequency of a complex sound modulates the perceived duration of a sound. Higher pitch sounds are perceived longer compared to lower pitch sounds as shown by several independent studies since 1973. In this paper, the effect of language background is studied: native speakers of Finnish and German participated in a two alternative forced choice duration discrimination experiment where the duration and frequency of two sounds are randomly varied. The overall duration discrimination sensitivity was similar to both groups but the speakers of Finnish were influenced more by the pitch in their judgements. In addition, the difference in the two sounds' pitch period explained the response data better than the difference in pitch frequencies or the pitch interval. As the Finnish quantity system is known to employ both duration and pitch cues, the present results suggest that the speakers are shaped by the language environment even when the task is purely non-linguistic.", "title": "Language background affects the strength of the pitch bias in a duration discrimination task"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/zellers13_interspeech.html", "abstract": "In many cases of turn transition in conversation, a new speaker may respond to phonetic cues from the end of the prior turn, including variation in prosodic features such as pitch and final lengthening. Although consistent pitch and lengthening features are well-established for some languages at potential points of turn transition, this is not necessarily the case for Swedish. The current study uses a two-alternative forced choice task to investigate how variation in pitch contour and lengthening at the ends of syntactically complete turns can influence listeners' expectations of turn hold or turn transition. Both lengthening and pitch contour features were found to influence listeners' judgments about whether turn transition would occur, with shorter length and higher final pitch peaks associated with turn hold. Furthermore, listeners were more certain about their judgments when asked about turn-hold rather than turn-change, suggesting an imbalance in the strength of turn-hold versus turn-transition cues.", "title": "Pitch and lengthening as cues to turn transition in Swedish"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/bissiri13_interspeech.html", "abstract": "Glottalization is often associated with low pitch in intonation languages, but evidence from many languages indicates that this is not an obligatory association. We asked speakers of German, English and Swedish to compare glottalized stimuli with several pitch contour alternatives in an AXB listening test. Although the low F0 in the glottalized stimuli tended to be perceived as most similar to falling pitch contours, this was not always the case, indicating that pitch perception in glottalization cannot be predicted by F0 alone. We also found evidence for cross-linguistic differences in the degree of flexibility of pitch judgments in glottalized stretches of speech.", "title": "Perception of glottalization in varying pitch contexts across languages"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/walsh13_interspeech.html", "abstract": "This paper presents the results of a pitch accent categorisation simulation which attempts to classify L*H and H*L accents using a psychologically motivated exemplar-theoretic model of categorisation. Pitch accents are represented in terms of six linguistically meaningful parameters describing their shape. No additional information is employed in the categorisation process. The results indicate that these accents can be successfully categorised, via exemplar-based comparison, using a limited number of purely tonal features.", "title": "Exemplar-based pitch accent categorisation using the generalized context model"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/braun13b_interspeech.html", "abstract": "In two speeded acceptability experiments we tested which combination of prenuclear accent, nuclear accent and F0-interpolation between them is best suited to signal a double contrast in German (i.e., a contrastive topic followed by a contrastive focus). The experimental utterances differed in the prenuclear accent (medialvs. late-peak, i.e., L+H* vs. L*+H), the nuclear accent (early- vs. medial-peak, i.e., H+L* vs. H*) and the F0-interpolation between them (high or dipping). All utterances were judged for their acceptability in a contrastive (Experiment 1) and a non-contrastive context (control Experiment 2). Our results showed that the combination of a late-peak prenuclear accent (L*+H) and an early-peak nuclear accent (H+L*) is best suited to signal a double contrast, independent of the F0-interpolation. The reaction time data also support the view that the F0-interpolation is not necessary for the interpretation of a double contrast.", "title": "Double contrast is signalled by prenuclear and nuclear accent types alone, not by f0-plateaux"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/correia13_interspeech.html", "abstract": "Previous research has reported stress \"deafness\" for languages with predictable stress, like French, contrary to languages with nonpredictable stress, like Spanish. The contrastive nature of stress appears to inhibit stress \"deafness\", but segmental and/or suprasegmental cues may also enhance stress discrimination. In this study we carried out two experiments aiming to investigate stress perception in European Portuguese (EP), a language with non-predictable stress that utilizes duration and vowel reduction as main cues to stress. We used nonsense words that differed only in stress location, thus removing vowel reduction as a cue to stress. Experiment 1 was an ABX discrimination task. Experiment 2 was a sequence recall task. In both experiments, the stress contrast condition was compared with a phoneme control condition, in nuclear and post-nuclear position. Results of both experiments strongly suggest a stress \"deafness\" effect in EP. Despite its variable nature, word stress is hardly perceived by EP native-speakers in the absence of vowel reduction. These findings have implications for claims on prosodic-based cross-linguistic perception of word stress in the absence of vowel quality, and for stress \"deafness\" as a consequence of a predictable stress grammar.", "title": "Word stress perception in European Portuguese"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/arnold13_interspeech.html", "abstract": "The perception of prosodic prominence is influenced by different sources like different acoustic cues, linguistic expectations and context. We use a generalized additive model and a random forest to model the perceived prominence on a corpus of spoken German. Both models are able to explain over 80% of the variance. While the random forests give us some insights on the relative importance of the cues, the general additive model gives us insights on the interaction between different cues to prominence.", "title": "Using generalized additive models and random forests to model prosodic prominence in German"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/pfitzinger13_interspeech.html", "abstract": "The effect of time-compression and -expansion on the perception of speech rate differences is investigated. Natural utterances were compared with modified versions time-scaled to the same duration. A set of ten German sentences was produced by one native speaker at slow and fast speed. In a forced choice discrimination task 15 participants were asked to select the faster one of two versions of the same sentence. In the case of low speech rate, versions that had been slowed down were perceived as slower than the corresponding natural utterances, whereas at high speech rates, stimuli with increased speed were judged as relatively faster. The effect turned out to be stronger for the slow stimuli. These findings suggest that the underlying articulatory effort plays an important role in the perception of speech rate.", "title": "Perceiving speech rate differences between natural and time-scale modified utterances"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/barbosa13_interspeech.html", "abstract": "This work aims at examining three classes of acoustic correlates of lexical stress in Brazilian Portuguese (BP) in three speaking styles: informal interview, phrase reading and word list reading. In the framework of an international collaboration, a parallel corpus was recorded in the three speaking styles with 10 subjects so far in each one of the following languages: Swedish, English, French, Italian, Estonian and BP. In BP, duration, F0 standard-deviation and spectral emphasis values for stressed vowels tend to be higher in comparison with vowel acoustic parameters in unstressed position. These three parameters are robust across styles, especially vowel duration, for which circa 50% of the variance is explained by stress and speaking style factors. The parameters pattern according to stress level is very similar between interview and phrase reading styles, which points to a similar effectiveness of reading and spontaneous styles in uncovering the word stress acoustic correlates in BP.", "title": "On the robustness of some acoustic parameters for signalling word stress across styles in Brazilian Portuguese"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/lyu13_interspeech.html", "abstract": "This paper tried to probe the merging tones and the application of sandhi rule for unchecked tones (/24/[33] and /33/[33]) in Hailu variety of Hakka language. Generally speaking, the sandhi rule was applied in both the younger (under 30) and the older (above 50) groups. Tone merging phenomenon (/24/[24] merging toward /33/[33]) was not found in the older group. Yet, the merging phenomenon was found in the younger group. The sandhi rule for /24/ and /33/ was not observed in most of the young speakers. It is proposed that the /24/ and /33/ tones were undergoing sound change by the younger generation.", "title": "Reexamine the sandhi rules and the merging tones in hakka language"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/tabain13_interspeech.html", "abstract": "This study examines the contrast between (alveo-)palatal stop bursts and velar stop bursts (/c/ vs. /k/), with particular focus on how the contrast between the two is enhanced in Word-initial vs. Word-medial position. Data are presented from nine speakers of Pitjantjatjara, a language of Central Australia. Analyses show that although there are formant differences between palatal and velar stop bursts, the formant contrast is not enhanced in Word-initial position, with the exception of a lower F3 for /k/ preceding the vowel /a/. By contrast, spectral tilt and the 3rd spectral moment (skewness) are particularly effective at enhancing the contrast between /c/ and /k/ preceding the vowels /a/ and /i/ (with /c/ having less steep tilt values and lower skewness values than /k/); and the 4th spectral moment (kurtosis) is particularly effective at enhancing the same contrast preceding the vowel /u/ (with /c/ having higher kurtosis values than /k/). These results suggest that Word-initial position in this language is marked not only by pitch movement and extra duration, but also by spectral properties of the stop burst.", "title": "A preliminary spectral analysis of palatal and velar stop bursts in pitjantjatjara"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/mahanta13_interspeech.html", "abstract": "Intonation and prosodic structures, apart from other functions, play a significant role in conveying the focus in an utterance. The term \u0081efocus' is applied to a constituent which is informationally more important or salient than other backgrounded parts of the same sentence, and in a sentence the part that receives presentational focus which answers a wh-question. In this paper, we will try to establish that Narbaria Variety of Assamese (NVA), which is a variety of Standard Colloquial Assamese (SCA), an Indo-Aryan language, employs phrasing as a marker of presentational focus. In marking the focus of the focused constituent, duration plays a significant role; it is not the durational increase of the focused P-phrase only which is significant but the decrease of duration in the given constituent is equally decisive. The present work considers sentences with presentational focus on Subject and on Object in two different word orders i.e. SOV (Subject-Object-Verb) and OSV (Object-Subject-Verb). Different phonetic cues of focus or prominence like F0-max, F0-min, F0-range and duration values are measured and compared against respective wide-focus baseline.", "title": "Presentational focus realisation in nalbaria variety of assamese"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/cruz13_interspeech.html", "abstract": "Intonational phrasing and pitch accent distribution (PAD) have been proposed to be interdependent (Portuguese) and independent properties of prosodic systems (Egyptian). This paper examines the relation between intonational phrasing and pitch accent distribution in two center-southern varieties of European Portuguese . Alentejo (Ale) and Algarve (Alg). Sentences obtained in a reading task systematically varied syntactic complexity (presence/absence of branching in subjects and objects) and length (in number of syllables). The results showed that (S)(VO) prevails in Ale, whereas in Alg (SVO) is preferred as in Standard European Portuguese (SEP). Unlike in SEP, both in Ale and Alg nearly every prosodic word is pitch accented, yielding a dense PAD. If in Ale intonational phrasing and PAD seem to be correlated (more phrases, dense PAD) the same does not apply in Alg (fewer phrases, but dense PAD). Our findings favor the view of phrasing and PAD as orthogonal dimensions to take into account in a description of prosodic variation within and across languages.", "title": "On the relation between intonational phrasing and pitch accent distribution. evidence from European Portuguese varieties"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/nemoto13_interspeech.html", "abstract": "The aim of this paper is twofold: (i) give a large-scale description in realized word-final schwas of French lexical words for different regions (North vs. South) and different speaking styles (read vs. spontaneous speech); (ii) highlight differences in prosodic features and test these differences via automatic classification techniques. The proposed study relies on a subset of 12.5 hours of the French PFC corpus. Manually transcribed speech was segmented and labeled using automatic speech alignment and a pronunciation dictionary including optional word-final schwas for all words ending in a consonant. F0 and intensity values were extracted and averaged over segments. Our study revealed that, for both speaking styles, word-final schwas of southern French tended to keep relatively high F0 values and longer durations in comparison with northern French where F0 tends to drop on a word-final schwa. On average, spontaneous speech featured smaller F0 drops between final full vowel and subsequent word-final schwa vowel as well as longer durations. The automatic North/South classification of word-final schwas achieved better results for spontaneous speech. As for distinguishing between speaking styles, southern French obtained slightly better scores than the northern varieties.", "title": "How are word-final schwas different in the north and south of france?"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/ashby13_interspeech.html", "abstract": "While a growing number of studies deal with the modeling of accents by non-native speakers - with a strong bias towards English - relatively little is understood about postcolonial language varieties and the effects of substratal transfer (i.e. interference from local tongues) on the resulting sound systems. As the next information technology frontier, Africa presents unique challenges for language technologists tasked with adapting existing speech and language technologies (SLTs) to accommodate for this often systematic form of interference. As part of an effort to model and generate rule-based pronunciation lexicons for resource-scarce varieties, we present our work on the variety of Portuguese spoken in the Mozambican capital of Maputo. We highlight some of the segmental processes that result from contact with local Bantu languages, i.e. denasalization, aspiration, and rhotic variation. We then discuss these in the context of adapting Mozambican Portuguese (MP) to existing SLTs. Problems of interand intra-speaker variability and the lack of a well defined standard are also considered, along with implications for adopting a suitable model of MP for speech synthesis.", "title": "Modeling postcolonial language varieties: challenges and lessons learned from mozambican Portuguese"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/sahkai13_interspeech.html", "abstract": "The study contributes to the cross-linguistic discussion on the prosodic correlates and categorial status of contrastive focus. It brings in data from Estonian, a flexible word order language where the prosodic expression of Information Structure has been very little studied. The study compares narrow contrastive focus with narrow and broad information focus in semi-spontaneous production data, examining word order, pitch accent types and distribution, word duration changes, and F0 values. Interestingly, neither contrastive nor information focus induces the expected word order changes. The examined phonological features, i.e. accent types and distribution, show some correlation with contrastive focus, but the strongest determinant of accent type is the position of focus (final vs. non-final). An interesting finding is that in parallel constructions with symmetric foci, the two foci tend to bear different accents. Duration changes distinguish between all three focus categories; interestingly, a three-way distinction between categories is obtained by way of different distributions of binary durational distinctions. F0 differences between conditions are significant only within certain informants and distinguish between contrastive and information focus only in non-final position; clause-finally, F0 distinguishes between broad and narrow information focus.", "title": "Prosody of contrastive focus in estonian"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/kisler13_interspeech.html", "abstract": "This study is a contribution to link the abstract phonological level to the acoustic signal level by identifying the main acoustic correlates for the distinctive feature set developed by Chomsky and Halle (1968). The acoustic features were extracted by the openSMILE toolkit from spontaneous speech data. For each distinctive feature a set of closely related acoustic features was derived by means of correlation-based feature selection. Based on the respective acoustic feature pools C4.5 trees and support vector machines for binary feature classification were trained. The classification performance ranged from 76 to 89% for vocalic features and from 78 to 93% for consonantal features. The methods proposed in this study can be of use to identify systematic speech signal correspondencies for phonological models and as a starting point for distinctive feature detection in speech recognition.", "title": "Exploring the connection of acoustic and distinctive features"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/cunha13_interspeech.html", "abstract": "The purpose of this study was to make use of physiological data to test whether there is a greater overlap between tense and lax vowels in Standard Austrian German (SAG) compared with those of Standard German (SG). In order to do so, movement data was analyzed of various tense/lax vowels pairs produced in symmetrical consonantal contexts using electromagnetic articulometry from seven SG and eight SAG speakers. Parafac and principal components analysis were used to compress the multi-channel articulatory data at the temporal midpoint of the vowel to a two-dimensional space whose axes were related to phonetic backness and height. The results of these analyses showed a closer approximation between tense/lax vowel pairs vertically but much less so horizontally for SAG compared with SG vowels.", "title": "A physiological analysis of the tense/lax vowel contrast in two varieties of German"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/meister13_interspeech.html", "abstract": "Estonian and Finnish are closely related languages both exploiting duration cues contrastively; however, the quantity systems in the two languages are different. The Estonian quantity system involves three contrastive patterns referred to as short (Q1), long (Q2) and overlong (Q3) quantity degrees. These contrasts are manifested by a complex interaction of durational and tonal cues in a disyllabic foot. In Finnish, the quantity contrast is binary and phonologically clearly segmental. Research has shown that L2 subjects with Finnish language background failed to produce the Estonian Q2 vs. Q3 contrast in vowel-peaked structures (CVVCV vs. CVV:CV) not distinguished in the orthography. In order to test the effect of L2 orthographic input on L2 pronunciation target words involving consonantpeaked quantity contrast (CVCCV vs. CVC:CV) have been used in the current study; in the case of plosives between the first and second syllable vowels, the quantity contrast is manifested orthographically. The results confirm the role of L2 orthographic input in L2 pronunciation . in the case of target words with plosives L2 subjects produced different patterns for Q2 and Q3 structures, but not in the case of target words with non-plosives.", "title": "Production of estonian quantity contrasts by native speakers of Finnish"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/meynadier13_interspeech.html", "abstract": "This paper presents analyses on the phonological voicing contrast in whispered speech, which is characterized by the absence of vocal fold vibrations. In modal speech, besides glottal vibration, the contrast between voiced and unvoiced consonants is realized by other phonetic correlates: e.g. consonant and pre-consonantal vowel durations, intraoral pressure differences. The analysis of these voicing cues shows they are well preserved in whispered speech. The results are consistent with previous studies supporting the perception of the voicing contrast even without physical voicing. Additionally, the patterns observed for Po during obstruents could reflect constraints connected to competing needs of intelligibility and of discretion in whisper.", "title": "Aerodynamic and durational cues of phonological voicing in whisper"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/reichel13_interspeech.html", "abstract": "Established phonological theories postulate uniform syllable constituent structures. From a traditional hierarchical point of view, syllables are right branching implying a close connection between the nucleus and the coda. Articulatory Phonology in contrast suggests a stronger cohesion between onsets and nuclei than between nuclei and codas. This claim is empirically supported by the c-center effect which initially has been observed for onsets only. Nevertheless, recent studies revealed that this effect does not occur in all complex onsets and can also be observed in codas. To account for this structure non-uniformity, we propose an information theoretic approach to measure connection strengths between syllable constituents in terms of their pointwise mutual information. It turned out that the derived constituent structures correspond well to the empirical c-center findings on American English and German data. The results are discussed from a Usage-based Phonology perspective considering c-centers to be a frequency effect.", "title": "Information theoretic syllable structure and its relation to the c-center effect"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/andreeva13_interspeech.html", "abstract": "The reduction of the six Bulgarian vowels (i, E, a, 3, O, u) to a four- or (in some dialects) three-vowel subsystem (i, (3), 3, u) in unstressed syllables is generally accepted. But a number of studies disagree on the exact nature of the reduction process. Claims differ as to whether or not /a/ merges phonetically with /3/, and /O/ with /u/, or whether the assumed neutralized oppositions take a phonetically intermediate quality ([5] and [o]). Previous acoustic analyses have been based on very few, and almost exclusively male speakers. The present study uses all the men's and women\u0081fs vowels from the Babel contemporary standard Bulgarian (CSB) read speech corpus. Mid-vowel F1 and F2 values were normalized to remove inter-speaker differences and statistical comparisons of stressed and unstressed vowel productions performed. Results confirm the raising of unstressed /a/ and /O/ reported previously, but raising is found to affect all vowels when unstressed except /i/. Unstressed /a/ and /O/ are raised to the quality of stressed /3/ and /u/ respectively (effectively neutralizing the opposition), but remain distinct from unstressed /3/ and /u/, which are raised from their stressed vowel positions. The mechanism underlying the reduction process is discussed.", "title": "The bulgarian stressed and unstressed vowel system. a corpus study"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/promon13_interspeech.html", "abstract": "This paper reports preliminary results of our effort to address the acoustic-to-articulatory inversion problem. We tested an approach that simulates speech production acquisition as a distal learning task, with acoustic signals of natural utterances in the form of MFCC as input, VocalTractLab . a 3D articulatory synthesizer controlled by target approximation models as the learner, and stochastic gradient descent as the training method. The approach was tested on a number of natural utterances, and the results were highly encouraging.", "title": "Training an articulatory synthesizer with continuous acoustic data"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/kiss13_interspeech.html", "abstract": "Modeling speaker-specific intonation is important in several areas, including speaker identification, verification, and imitation using text-to-speech synthesis. However the choice of the intonation model and the estimation of its parameters from spontaneous speech remains a challenge. We propose a way to estimate speakerspecific intonation parameters for a particular superpositional model, the Simplified Linear Alignment Model, using robust per-utterance and overall statistics of spontaneous speech. We used this method to compare the intonation of children with autism or language impairment, who often have atypical speech prosody, with that of typically developing children. We found significant differences between the groups, which demonstrates the effectiveness of the proposed method.", "title": "Estimating speaker-specific intonation patterns using the linear alignment model"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/sung13_interspeech.html", "abstract": "In our previous work, we proposed factored maximum likelihood linear regression (FMLLR) adaptation where each MLLR parameter is defined as a function of a control vector. In this paper, we introduce a novel technique called factored maximum likelihood kernelized regression (FMLKR) for HMM-based style adaptive speech synthesis. In FMLKR, nonlinear regression between the mean vector of the base model and the corresponding mean vectors of the adaptation data is performed with the use of kernel method based on the FMLLR framework. In a series of experiments on artificial generation of singing voice, the proposed technique shows better performance than the other conventional methods.", "title": "Factored maximum likelihood kernelized regression for HMM-based singing voice synthesis"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/takamichi13_interspeech.html", "abstract": "In this paper, we improve parameter generation with rich context models by modifying an initialization method and further apply it to both spectral and F0 components in HMM-based speech synthesis. To alleviate over-smoothing effects caused by the traditional parameter generation methods, we have previously proposed an iterative parameter generation method with rich context models. It has been reported that this method yields quality improvements in synthetic speech but there are still limitations. This is because 1) this generation method still suffers from the over-smoothing effect, as it uses the parameters generated by the traditional method as an initial parameters, which strongly affect on the finally generated parameters and 2) it is applied to only the spectral component. To address these issues, we propose 1) an initialization method to generate less smoothed but more discontinuous initial parameters that tend to yield better generated parameters, and 2) a parameter generation method with rich context models for the F0 component. Experimental results show that the proposed methods yield significant improvements in quality of synthetic speech.", "title": "Improvements to HMM-based speech synthesis based on parameter generation with rich context models"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/nakashika13_interspeech.html", "abstract": "This paper presents a voice conversion technique using Deep Belief Nets (DBNs) to build high-order eigen spaces of the source/target speakers, where it is easier to convert the source speech to the target speech than in the traditional cepstrum space. DBNs have a deep architecture that automatically discovers abstractions to maximally express the original input features. If we train the DBNs using only the speech of an individual speaker, it can be considered that there is less phonological information and relatively more speaker individuality in the output features at the highest layer. Training the DBNs for a source speaker and a target speaker, we can then connect and convert the speaker individuality abstractions using Neural Networks (NNs). The converted abstraction of the source speaker is then brought back to the cepstrum space using an inverse process of the DBNs of the target speaker. We conducted speaker-voice conversion experiments and confirmed the efficacy of our method with respect to subjective and objective criteria, comparing it with the conventional Gaussian Mixture Model-based method.", "title": "Voice conversion in high-order eigen space using deep belief nets"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/silen13_interspeech.html", "abstract": "Voice conversion aims at converting speech from one speaker to sound as if it was spoken by another specific speaker. The most popular voice conversion approach based on Gaussian mixture modeling tends to suffer either from model overfitting or oversmoothing. To overcome the shortcomings of the traditional approach, we recently proposed to use dynamic kernel partial least squares (DKPLS) regression in the framework of parallel-data voice conversion. However, the availability of parallel training data from both the source and target speaker is not always guaranteed. In this paper, we extend the DKPLS-based conversion approach for non-parallel data by combining it with a well-known INCA alignment algorithm. The listening test results indicate that high-quality conversion can be achieved with the proposed combination. Furthermore, the performance of two variations of INCA are evaluated with both intra-lingual and cross-lingual data.", "title": "Voice conversion for non-parallel datasets using dynamic kernel partial least squares regression"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/nose13_interspeech.html", "abstract": "This paper proposes a technique for controlling singing style in the HMM-based singing voice synthesis. A style control technique based on multiple regression HSMM (MRHSMM), which was originally proposed for the HMM-based expressive speech synthesis, is applied to the conventional technique. The idea of pitch adaptive training is introduced into the MRHSMM to improve the modeling accuracy of fundamental frequency (F0) associated with notes. A robust vibrato modeling technique based on a moving average filter is also proposed to reproduce a natural-sounding vibrato expression even when the vibrato expression of the original singing voice is unclear. Subjective evaluation results show that users can intuitively control a singing style while keeping naturalness of the synthetic voice.", "title": "A style control technique for singing voice synthesis based on multiple-regression HSMM"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/hinterleitner13_interspeech.html", "abstract": "We extract 1495 speech features from 2 subjectively evaluated text- to-speech (TTS) databases. These features are extracted from pitch, loudness, MFCCs, spectrals, formants, and intensity. The speech material is synthesized using up to 15 different TTS systems, some of them with up to 8 different voices. We develop quality predictors for TTS signals following two different approaches to handle the huge set of speech features: a three-step feature selection followed by a stepwise multiple linear regression and an approach based on support vector machines. The predictors are cross-validated via 3-fold cross validation (CV) and leave-one-test-out (LOTO) CV. Due to the high number of features we apply a strict CV method where the partitioning is realized prior to the feature scaling and feature selection steps. In comparison we also follow a semi-strict approach where the partitioning effectively takes place after these steps. In the 3-fold CV case we achieve correlations as high as .75 for strict CV and .89 for semi-strict CV. The more ambitious LOTO CV yields correlations around .80 for the male speakers whereas the results for the female voices show the need for improvement.", "title": "Predicting the quality of text-to-speech systems from a large-scale feature set"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/nurminen13_interspeech.html", "abstract": "Unit selection based text-to-speech systems can generally obtain high speech quality provided that the database is large enough. In embedded applications, the related memory requirements may be excessive and often the database needs to be both pruned and compressed to fit it into the available memory space. In this paper, we study the topic of database compression. In particular, the focus is on speaker-specific optimization of the quantizers used in the database compression. First, we introduce the simple concept of dynamic quantizer structures, facilitating the use of speaker-specific optimizations by enabling convenient run-time updates. Second, we show that significant memory savings can be obtained through speaker-specific retraining while perfectly maintaining the quantization accuracy, even when the memory required for the additional codebook data is taken into account. Thus, the proposed approach can be considered effective in reducing the conventionally large footprint of unit selection based text-to-speech systems.", "title": "Speaker-specific retraining for enhanced compression of unit selection text-to-speech databases"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/huckvale13_interspeech.html", "abstract": "This paper presents a radical new therapy for persecutory auditory hallucinations (\"voices\") which are most commonly found in serious mental illnesses such as schizophrenia. In around 30% of patients these symptoms are not alleviated by anti-psychotic medication. This work is designed to tackle the problem created by the inaccessibility of the patients' experience of voices to the clinician. Patients are invited to create an external representation of their dominant voice hallucination using computer speech and animation technology. Customised graphics software is used to create an avatar that gives a face to the voice, while voice morphing software realises it in audio, in real time. The therapist then conducts a dialogue between the avatar and the patient, with a view to gradually bringing the avatar, and ultimately the hallucinatory voice, under the patient's control. Results of a pilot study reported elsewhere indicate that the approach has potential for dramatic improvements in patient control of the voices after a series of only six short sessions. The focus of this paper is on the audio-visual speech technology which delivers the central aspects of the therapy.", "title": "Avatar therapy: an audio-visual dialogue system for treating auditory hallucinations"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/muthukumar13_interspeech.html", "abstract": "Every parametric speech synthesizer requires a good excitation model to produce speech that sounds natural. In this paper, we describe efforts toward building one such model using the Liljencrants-Fant (LF) model. We used the Iterative Adaptive Inverse Filtering technique to derive an initial estimate of the glottal flow derivative (GFD). Candidate pitch periods in the estimated GFD were then located and LF model parameters estimated using a gradient descent optimization algorithm. Residual energy in the GFD, after subtracting the fitted LF signal, was then modeled by a 4-term LPC model plus energy term to extend the excitation model and account for source information not captured by the LF model. The ClusterGen speech synthesizer was then trained to predict these excitation parameters from text so that the excitation model could be used for speech synthesis. ClusterGen excitation predictions were further used to reinitialize the excitation fitting process and iteratively improve the fit by including modeled voicing and segmental influences on the LF parameters. The results of all of these methods have been confirmed both using listening tests and objective metrics.", "title": "Optimizations and fitting procedures for the liljencrants-fant model for statistical parametric speech synthesis"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/hovy13_interspeech.html", "abstract": "This paper uses a crowd-sourced definition of a speech phenomenon we have called \"focus\". Given sentences, text and speech, in isolation and in context, we asked annotators to identify what we term the \"focus\" word. We present their consistency in identifying the focused word, when presented with text or speech stimuli. We then build models to show how well we predict that focus word from lexical (and higher) level features. Also, using spectral and prosodic information, we show the differences in these focus words when spoken with and without context. Finally, we show how we can improve speech synthesis of these utterances given focus information.", "title": "Analysis and modeling of \u201cfocus\u201d in context"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/ishihara13_interspeech.html", "abstract": "We have previously proposed a generative model of speech F0 contours, based on the discrete-time version of the Fujisaki model (a model of the mechanism for controlling F0s through laryngeal muscles). One advantage of this model is that it allows us to apply statistical methods to estimate the Fujisaki-model parameters from speech F0 contours. This paper proposes a new generative model of speech F0 contours incorporating a vocabulary model of intonation patterns. A parameter inference algorithm for the present model is derived. We quantitatively evaluated the performance of our parameter inference algorithm.", "title": "Probabilistic speech F0 contour model incorporating statistical vocabulary model of phrase-accent command sequence"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/mcloughlin13_interspeech.html", "abstract": "Whispers are an important secondary vocal communications mechanism, that can be necessary for communicating private information and which are an integral aspect of natural humanto- human dialogue. Furthermore, they may be the primary communications method of those suffering from certain forms of aphonia, such as laryngectomees. This paper considers the conversion of continuous whispers to natural-sounding speech, and proposes a new reconstruction method based upon the synthesis of individual formants as excitation source, followed by artificial glottal modulation. Early results show that the proposed method can improve quality and intelligibility over the original whispers when evaluated using continuous speech. It requires neither a priori nor speaker-dependent information, is of relatively low-complexity and suitable for real-time processing.", "title": "Reconstruction of continuous voiced speech from whispers"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/niekerk13_interspeech.html", "abstract": "We present methods for modelling and synthesising fundamental frequency (F0) contours suitable for application in text-to-speech (TTS) synthesis of Yoruba (an African tone language). These methods are discussed and compared with a baseline approach using the HMM-based speech synthesis system HTS. Evaluation is done by comparing ten-fold cross validation squared errors on a small corpus of four speakers. We show that the proposed methods are relatively effective at modelling and generating F0 contours in this context, achieving lower error rates than the baseline. These results suggest that our methods will be useful for the generation of improved synthesis of tone in African languages, which has been a challenge to date.", "title": "Generating fundamental frequency contours for speech synthesis in yor\u00f9b\u00e1"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/azarov13b_interspeech.html", "abstract": "This paper presents an approach to parametric voice conversion that can be used in real-time entertainment applications. The approach is based on spectral mapping using an artificial neural network (ANN) with rectified linear units (ReLU). To overcome the oversmoothing problem a special network configuration is proposed that utilizes temporal states of the speaker. The speech is represented using the harmonic plus noise model. The parameters of the model are estimated using instantaneous harmonic parameters. Using objective and subjective measures the proposed voice conversion technique is compared to the main alternative approaches.", "title": "Real-time voice conversion using artificial neural networks with rectified linear units"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/krityakien13_interspeech.html", "abstract": "As classic and intrinsic requirements, synthetic speech need to convey correct information with good quality of naturalness to listeners. Fundamental frequency (F0) contours need to be controlled to meet these requirements. Additional challenges have been introduced to tonal languages because the F0 contour reflects both intelligibility and naturalness of the speech. According to the fact that the F0 contour in a syllable conveys information asymmetrically, Tone nucleus model has been successfully established. In this study, Tone nucleus model is applied in order to generate F0 contours for Thai speech synthesis. This is among the first that has introduced the model to other tonal languages other than Mandarin. All tone nuclei for five distinctive tones are defined according to the underlying targets. The full process of F0 contour generation is presented from the nucleus extraction until the F0 contour generation for continuous speech. The efficiency and adaptability of the model in Thai language were confirmed by the objective and subjective tests. The model outperformed a baseline without applying the model. The generated F0 contours showed less distortion, more tone intelligibility and more naturalness. The modified method is also introduced for enhancement. The results showed significant improvement on the generated F0 contours.", "title": "Generation of fundamental frequency contours for Thai speech synthesis using tone nucleus model"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/chen13_interspeech.html", "abstract": "This work aims to improve expressive speech synthesis of ebooks for multiple speakers by using training data from many audiobooks. Audiobooks contain a wide variety of expressive speaking styles which are often impractical to annotate. However, the speaker-expression factorization (SEF) framework, which has been proven to be a powerful tool in speaker and expression modelling usually requires the (supervised) information about expressions in the training data. This work presents an unsupervised SEF method which implements the SEF on unlabelled training data in the framework of cluster adaptive training (CAT). The proposed method integrates the expression clustering and parameter estimation in a single process to maximize the likelihood of the training data. Experimental results indicate that it outperforms the cascade system of expression clustering and supervised SEF, and significantly improves the expressiveness of the synthetic speech of different speakers.", "title": "Unsupervised speaker and expression factorization for multi-speaker expressive synthesis of ebooks"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/nakajima13_interspeech.html", "abstract": "To establish Expressive Text-to-speech synthesis, current research studies both the processing of input text and the rendering of natural expressive speech. Focusing on the former as a front-end task in the production of synthetic speech, this paper investigates a novel feature for predicting phrase boundary tone labels which transcribe local fundamental frequency (F0) changes frequently appearing at phrase end positions in expressive speech. To this end, we examined a kind of distribution-based semantic features consisting of i) word surface strings, ii) their part-of-speech tags taken from a phrase and iii) the pause existence/non-existence at the final position of the phrase, which are different from conventional numerically-expressed stylistic features such as positions and lengths and distances of the phrase. Through experiments on Japanese expressive speech such as conversational speech and advertisement speech, we confirmed that the proposed features attain performance equal to or better than conventional features. These results suggest that the distribution-based semantic features might be useful to predict phrase boundary rise labels for conversational speech and might be useful equal to conventional numerically-expressed stylistic feature for advertisement speech.", "title": "Which resemblance is useful to predict phrase boundary rise labels for Japanese expressive text-to-speech synthesis, numerically-expressed stylistic or distribution-based semantic?"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/ni13_interspeech.html", "abstract": "Superpositional model of fundamental frequency (F0) contours as suggested by the Fujisaki model can well represent F0 movements of speech keeping a clear relation with linguistic information of utterances. Therefore, improvement of HMM-based speech synthesis is expected by using the merit of superpositional model. In this paper, a targets-based superpositional model is proposed in the light of the Fujisaki model. Here, both accent and phrase components are parameterized by respectively defined low and high targets which allow flexible interaction between accent and phrase components. Due to the flexible interaction, the new method consistently treats such complex F0 movements as low digging, varying declination, and final lowering by simply adjusting parameter values. This facilitates extraction of the model parameters from observed F0 contours, which is one of major problems preventing the use of the Fujisaki model. Extraction of the target parameters is evaluated for a Japanese speech corpus and the F0 contours generated by the model are used for HMM training instead of the original. Listening test of synthetic speech indicates significant improvements in speech quality. Micro-prosodic effects are also investigated. Results show that adding the micro-prosody to the generated F0 contours does not significantly improve speech quality.", "title": "A targets-based superpositional model of fundamental frequency contours applied to HMM-based speech synthesis"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/kobayashi13_interspeech.html", "abstract": "In this paper, we investigate the acoustic features that can be modified to control the perceptual age of a singing voice. Singers can sing expressively by controlling prosody and vocal timbre, but the varieties of voices that singers can produce are limited by physical constraints. Previous work has attempted to overcome this limitation through the use of statistical voice conversion. This technique makes it possible to convert singing voice characteristics of an arbitrary source singer into those of an arbitrary target singer. However, it is still difficult to intuitively control singing voice characteristics by manipulating parameters corresponding to specific physical traits, such as gender and age. In this paper, we focus on controlling the perceived age of the singer and, as a first step, perform an investigation of the factors that play a part in the listener's perception of the singer's age. The experimental results demonstrate that 1) the perceptual age of singing voices corresponds relatively well to the actual age of the singer, 2) speech analysis/synthesis processing and statistical voice conversion processing don\u0081ft cause adverse effects on the perceptual age of singing voices, and 3) prosodic features have a larger effect on the perceptual age than spectral features.", "title": "An investigation of acoustic features for singing voice conversion based on perceptual age"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/bollepalli13_interspeech.html", "abstract": "In this paper, the effect of MPEG audio compression on HMM-based speech synthesis is studied. Speech signals are encoded with various compression rates and analyzed using the GlottHMM vocoder. Objective evaluation results show that the vocoder parameters start to degrade from encoding with bit-rates of 32 kbit/s or less, which is also confirmed by the subjective evaluation of the vocoder analysis-synthesis quality. Experiments with HMM-based speech synthesis show that the subjective quality of a synthetic voice trained with 32 kbit/s speech is comparable to a voice trained with uncompressed speech, but lower bit rates induce clear degradation in quality.", "title": "Effect of MPEG audio compression on HMM-based speech synthesis"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/doi13_interspeech.html", "abstract": "In this paper, we evaluate our proposed singing voice conversion method from various perspectives. To enable singers to freely control their voice timbre of singing voice, we have proposed a singing voice conversion method based on many-to-many eigenvoice conversion (EVC) that enables to convert the voice timbre of an arbitrary source singer into that of another arbitrary target singer using a probabilistic model. Furthermore, to easily develop training data consisting of multiple parallel data sets between a single reference singer and many other singers, a technique for efficiently and effectively generating the parallel data sets from nonparallel singing voice data sets of many singers using a singingto- singing synthesis system have been proposed. However, we have never conducted sufficient investigations into the effectiveness of these proposed methods. In this paper, we conduct both objective and subjective evaluations to carefully investigate the effectiveness of proposed methods. Moreover, the differences between singing voice conversion and speaking voice conversion are also analyzed. Experimental results show that our proposed method succeeds in enabling people to control their own voice timbre by using only an extremely small amount of the target singing voice.", "title": "Evaluation of a singing voice conversion method based on many-to-many eigenvoice conversion"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/koriyama13_interspeech.html", "abstract": "This paper proposes a statistical nonparametric speech synthesis technique based on a sparse Gaussian process regression (GPR). In our previous study, we proposed GPR-based speech synthesis where each frame of synthesis units is modeled by a regression of Gaussian processes. Preliminary experiments of synthesizing several phones including both vowels and consonants showed a potential of the technique. In this paper, the previous work is extended to full-sentence speech synthesis using sparse GPs and context modification. Specifically, cluster-based sparse Gaussian processes such as local GPs and partially independent conditional (PIC) approximation are examined as a computationally feasible approach. Moreover, frame-level context is extended to include not only a position context from a current phone but also adjacent phones to generate smoothly changing speech parameters. Objective and subjective evaluation results show that the proposed technique outperforms the HMM-based speech synthesis with minimum generation error training.", "title": "Statistical nonparametric speech synthesis using sparse Gaussian processes"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/mohammadi13_interspeech.html", "abstract": "Statistical speech synthesis (SSS) approach has become one of the most popular methods in the speech synthesis field. An advantage of the SSS approach is the ability to adapt to a target speaker with a couple of minutes of adaptation data. However, many applications, especially in consumer electronics, require adaptation with only a few seconds of data which can be done using eigenvoice adaptation techniques. Although such techniques work well in speech recognition, they are known to generate perceptual artifacts in statistical speech synthesis. Here, we propose two methods to both alleviate those quality problems and improve the speaker similarity obtained with the baseline eigenvoice adaptation algorithm. Our first method is based on using a Bayesian approach for constraining the eigenvoice adaptation algorithm to move in realistic directions in the speaker space to reduce artifacts. Our second method is based on finding a reference speaker that is close to the target speaker, and using that reference speaker as the seed model in a second eigenvoice adaptation step. Both techniques performed significantly better than the baseline eigenvoice method in the subjective quality and similarity tests.", "title": "Hybrid nearest-neighbor/cluster adaptive training for rapid speaker adaptation in statistical speech synthesis systems"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/cabral13_interspeech.html", "abstract": "In general, speech synthesis using the source-filter model of speech production requires the classification of speech into two classes (voiced and unvoiced) which is prone to errors. For voiced speech, the input of the synthesis filter is an approximately periodic excitation, whereas it is a noise signal for unvoiced. This paper proposes an excitation model which can be used to synthesise both voiced and unvoiced speech, thus overcoming the problem of degradation in speech quality caused by those classification errors. Basically this model consists of representing two contiguous segments of the residual signal pitch-synchronously. The first segment is represented by the original residual in a fraction of the period around the pitch-mark (obtained using an epoch detector), in order to capture the most important aspects of the residual during voiced speech. Instead, the remaining part of the period is modelled by a set of parameters of the amplitude envelope of the residual waveform and its energy. The technique for synthesising the excitation combines these shaping parameters with a novel method for regeneration of the residual waveform and a method to mix a periodic signal with noise based on the Harmonic plus Noise model. Besides producing high-quality speech, this technique is computationally fast.", "title": "Uniform concatenative excitation model for synthesising speech without voiced/unvoiced classification"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/tran13_interspeech.html", "abstract": "Vowels are generally described with static articulatory configurations represented by targets in the acoustic space: typically, formant frequencies in the F1-F2 and F2-F3 planes. Plosive consonants can be described in terms of places of articulation, represented by locus or locus equations in an acoustic plane. But how are a given vowel and a given consonant identified, when produced with different acoustic characteristics and in different environments? To which extent do listeners use contextual information? To which extent do they use normalization, and of which kind? These questions lead to studying both vowels and consonants from a dynamic point of view. At this level, what are the respective roles of static targets and dynamics information? Previous studies reveal that synthesized transitions situated on a F1-F2 plane but beyond the values observed in natural speech can be perceived as V1V2: that is, vowel-to-vowel transitions can be characterized simply by the direction and rate of the transitions, even when absolute frequency values are outside of the vowel triangle. The present paper extends the investigation to consonants: it reports new experiments showing that perception of pseudo-V1CV2 can also be obtained with formant transitions situated outside the vowel triangle.", "title": "Production and perception of pseudo-V1CV2 outside the vowel triangle: speech illusion effects"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/candea13_interspeech.html", "abstract": "This paper investigates sociophonetic questions about global tendencies in contemporaneous European spoken French. The authors argue that automatic alignment allowing targeted variants can provide evidence for current hypotheses about possible ongoing sound changes or about destandardization even in formal contexts as broadcast news. This study focused on the evolution over a decade, in radio or TV news, of three \u0081enon- standard\u0081f consonantal variants: consonant cluster reduction, affrication/palatalization of dental stops and voiceless fricative epithesis. Measures obtained by this method showed that the first variant remains almost absent in journalists' speech, exactly as affrication of /d/. In contrast, affrication of /t/ is increasing and the fricative epithesis, partially unpredictable, becomes longer. Our findings support the use of automatic alignment as an aid to validate sociolinguistic hypotheses and to develop pattern-driven studies, gathering more variables.", "title": "Recent evolution of non-standard consonantal variants in French broadcast news"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/zimmerer13_interspeech.html", "abstract": "In Tokyo Japanese, vowel devoicing is a common process leading to the reduction of high, unstressed vowels, mostly when they occur between unvoiced consonants. Native Japanese speakers learning German show a strong tendency to produce these devoiced vowels in the foreign language, too, despite the lack of this regular process in that language, here in German. This study examines the extent to which this reduction process leads to perception problems by German listeners, who are confronted only rarely with devoiced vowels in their native language. Results of a phoneme detection task indicate that devoiced vowels may indeed lead to perceptual difficulties. German listeners seem to refrain from reconstructing the vowel completely, which also can add to a perceived foreign accent of Japanese productions by German listeners.", "title": "Architekt or archtekt? perception of devoiced vowels produced by Japanese speakers of German"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/plummer13_interspeech.html", "abstract": "We investigate vowel category perception within and across languages by proffering a statistical methodology for creating vowel category response surfaces over maximal vowel spaces based on the responses of subjects from five different language communities to vowel stimuli generated by an age-varying articulatory synthesizer. The methodology is based on an additive modeling approach to surface regression within the general smoothing spline approach to statistical modeling. We also put forward a simple method for the comparison of surfaces and demonstrate its basic utility by comparing response surfaces derived from Greek and Japanese subjects. We discuss the results of the comparison with attention to the potential of the approach to reveal meaningful differences between and within the vowel systems of different language communities.", "title": "Comparing vowel category response surfaces over age-varying maximal vowel spaces within and across language communities"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/babel13_interspeech.html", "abstract": "This study reports on three populations' ratings of vocal attractiveness for 30 male and 30 female voices producing isolated words. Equal numbers of male and female listeners were recruited from three dialect areas: northern California, western Canada, and Minnesota. Attractiveness ratings across dialects were highly correlated, particularly for female voices. To determine the acoustic features which influenced listener ratings, detailed acoustic analyses of vowel quality and voice quality were conducted. These measures were entered into separate principal component analyses to reduce the dimensionality. Principal components and additional measures of duration and F0 were entered into models to assess which acoustic features predict attractiveness ratings across dialects. The results indicate that despite the highly correlated ratings across dialects, listener populations differed slightly in the phonetic features used to make attractiveness judgments. Listeners from the more similar dialect groups (California and western Canada) used similar acoustic features in their judgments, supporting the hypothesis that vocal attractiveness involves community-specific preferences. These results support a theory of vocal attractiveness which considers community-specific norms in assessing vocal preferences.", "title": "Perceived vocal attractiveness across dialects is similar but not uniform"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/wang13_interspeech.html", "abstract": "This paper investigates the mutual intelligibility of Chinese, Dutch (both foreign-language learners) and American (native language) speakers of English using SUS (Semantically Unpredictable Sentences) and SPIN (Speech in Noise) materials. We test the hypothesis that speakers and listeners who share the same native-language background have an advantage (interlanguage speech intelligibility benefit).", "title": "Mutual intelligibility of American, Chinese and Dutch-accented speakers of English tested by SUS and SPIN sentences"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/lu13_interspeech.html", "abstract": "We previously have applied deep autoencoder (DAE) for noise reduction and speech enhancement. However, the DAE was trained using only clean speech. In this study, by using noisy-clean training pairs, we further introduce a denoising process in learning the DAE. In training the DAE, we still adopt greedy layer-wised pretraining plus fine tuning strategy. In pretraining, each layer is trained as a one-hidden-layer neural autoencoder (AE) using noisy-clean speech pairs as input and output (or transformed noisy-clean speech pairs by preceding AEs). Fine tuning was done by stacking all AEs with pretrained parameters for initialization. The trained DAE is used as a filter for speech estimation when noisy speech is given. Speech enhancement experiments were done to examine the performance of the trained denoising DAE. Noise reduction, speech distortion, and perceptual evaluation of speech quality (PESQ) criteria are used in the performance evaluations. Experimental results show that adding depth of the DAE consistently increase the performance when a large training data set is given. In addition, compared with a minimum mean square error based speech enhancement algorithm, our proposed denoising DAE provided superior performance on the three objective evaluations.", "title": "Speech enhancement based on deep denoising autoencoder"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/saruwatari13_interspeech.html", "abstract": "In this study, we perform a theoretical analysis of the amount of musical noise generated in Bayesian minimum mean-square error speech amplitude estimators. In our previous study, a musical noise assessment based on kurtosis has been successfully applied to spectral subtraction. However, it is difficult to apply this approach to the methods with a decision-directed a priori SNR estimator because it corresponds to a nonlinear recursive process for noise power spectral sequences. Therefore, in this paper, we analyze musical noise generation by combining Breithaupt-Martin\u0081fs approximation and our higher-order-statistics analysis. We also compare the result of theoretical analysis and that of objective experimental evaluation to indicate the validity of the proposed closed-form analysis.", "title": "Musical noise analysis for Bayesian minimum mean-square error speech amplitude estimators based on higher-order statistics"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/lyubimov13_interspeech.html", "abstract": "This paper investigates a non-negative matrix factorization (NMF)- based approach to the semi-supervised single-channel speech enhancement problem where only non-stationary additive noise signals are given. The proposed method relies on sinusoidal model of speech production which is integrated inside NMF framework using linear constraints on dictionary atoms. This method is further developed to regularize harmonic amplitudes. Simple multiplicative algorithms are presented. The experimental evaluation was made on TIMIT corpus mixed with various types of noise. It has been shown that the proposed method outperforms some of the state-of-the-art noise suppression techniques in terms of signal-to-noise ratio.", "title": "Non-negative matrix factorization with linear constraints for single-channel speech enhancement"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/tseng13_interspeech.html", "abstract": "In this paper, we consider the single-channel speech enhancement problem, in which a clean speech signal needs to be estimated from a noisy observation. To capture the characteristics of both the noise and speech signals, we combine the well-known Short-Time- Spectrum-Amplitude (STSA) estimator with a machine learning based technique called Multi-frame Sparse Dictionary Learning (MSDL). The former utilizes statistical information for denoising, while the latter helps better preserve speech, especially its temporal structure. The proposed algorithm, named STSA-MSDL, outperforms standard statistical algorithms such as the Wiener filter, STSA estimator, as well as dictionary based algorithms when applied to the TIMIT database, using four different objective metrics that measure speech intelligibility, speech distortion, background noise reduction, and the overall quality.", "title": "A single channel speech enhancement approach by combining statistical criterion and multi-frame sparse dictionary learning"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/mirbagheri13_interspeech.html", "abstract": "A novel method for speech enhancement based on Convolutive Non-negative Matrix Factorization (CNMF) is presented in this paper. The sparsity of activation matrix for speech components has already been utilized in NMF-based enhancement methods. However such methods do not usually take into account prior knowledge about occurrence relations between different speech components. By introducing the notion of cosparsity, we demonstrate how such relations can be characterized from available speech data and enforced when recovering speech from noisy mixtures. Through objective evaluations we show our proposed regularization improves sparse reconstruction of speech, especially in low SNR conditions.", "title": "Speech enhancement using convolutive nonnegative matrix factorization with cosparsity regularization"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/mccallum13_interspeech.html", "abstract": "Stochastic-deterministic (SD) speech modelling exploits the predictability of speech components that may be regarded deterministic. This has recently been employed in speech enhancement resulting in an improved recovery of deterministic speech components, although the improvement achieved is largely dependant on how these components are estimated. In this paper we propose a joint SD Wiener filtering scheme that exploits the predictability of sinusoidal components in speech. Estimation of sinusoidal speech components is approached in the recursive Bayesian context, where the linearity of the joint SD Wiener filter and Gaussian assumptions suggest a Kalman filtering scheme for the estimation of sinusoidal components. A further refinement also imposes a restriction of a smooth spectral envelope on sinusoidal magnitude estimates. The resulting joint SD Wiener filtering scheme improves speech quality in terms of the perceptual evaluation of speech quality (PESQ) metric when compared to both the traditional Wiener filter and the proposed Wiener filter based on alternative estimates of deterministic speech components.", "title": "Joint stochastic-deterministic wiener filtering with recursive Bayesian estimation of deterministic speech"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/knuuttila13_interspeech.html", "abstract": "Discovery of statistically significant patterns from data and learning of associative links between qualitatively different data streams is becoming increasingly important in dealing with the so-called Big Data problem of the modern society. In this work, a methodological framework for automatic discovery of statistical associations between a high bit-rate and noisy sensory signal (speech) and temporally discrete categorical data with different temporal granularity (text) is presented. The proposed approach does not utilize any phonetic or linguistic knowledge in the analysis, but simply learns the meaningful units of text and speech and their mutual mappings in an unsupervised manner. The first experiments with a limited vocabulary of child-directed speech show that, after a period of learning, the method is successful in the generation of a textual representation of continuous speech.", "title": "Automatic self-supervised learning of associations between speech and text"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/daubigney13_interspeech.html", "abstract": "Dialogue management optimisation has been cast into a planning under uncertainty problem for long. Some methods such as Reinforcement Learning (RL) are now part of the state of the art. Whatever the solving method, strong assumptions are made about the dialogue system properties. For instance, RL assumes that the dialogue state space is Markovian. Such constraints may involve important engineering work. This paper introduces a more general approach, based on fewer modelling assumptions. A Black Box Optimisation (BBO) method and more precisely a Particle Swarm Optimisation (PSO) is used to solve the control problem. In addition, PSO allows taking advantage of the parallel aspect of the problem of optimising a system online with many users calling at the same time. Some preliminary results are presented.", "title": "Particle swarm optimisation of spoken dialogue system strategies"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/lison13_interspeech.html", "abstract": "Reinforcement learning methods are increasingly used to optimise dialogue policies from experience. Most current techniques are model-free: they directly estimate the utility of various actions, without explicit model of the interaction dynamics. In this paper, we investigate an alternative strategy grounded in model-based Bayesian reinforcement learning. Bayesian inference is used to maintain a posterior distribution over the model parameters, reflecting the model uncertainty. This parameter distribution is gradually refined as more data is collected and simultaneously used to plan the agent's actions. Within this learning framework, we carried out experiments with two alternative formalisations of the transition model, one encoded with standard multinomial distributions, and one structured with probabilistic rules. We demonstrate the potential of our approach with empirical results on a user simulator constructed from Wizard-of-Oz data in a human-robot interaction scenario. The results illustrate in particular the benefits of capturing prior domain knowledge with high-level rules.", "title": "Model-based Bayesian reinforcement learning for dialogue management"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/ghigi13_interspeech.html", "abstract": "The new Interactive Pattern Recognition (IPR) framework has been proposed to deal with human-machine interaction. In this context a new formulation has been recently defined to represent a Spoken Dialogue System as an IPR problem. In this work this formulation is applied to define graphical models that deal with Spoken Dialogue Systems. The definition of both a Dialogue Manager and a User Model are shown and the estimation of the parameters and smoothing techniques are presented in the paper. These models were evaluated in a dialogue generation task on two very different corpora: Dihana corpus consisting of Spanish spoken dialogues acquired with the Wizard of Oz technique and Let's Go corpus consisting of spoken dialogues in English between real users and the Ravenclaw dialogue manager developed by CMU. The results obtained show that original and simulated dialogues exhibited very similar behaviours, thus demonstrating the learning capacity of the proposed models in both a controlled Wizard of Oz task and a spoken dialogue system that interacts with real users. This formulation can then be considered as a promising framework to deal with Spoken Dialogue Systems.", "title": "Evaluating spoken dialogue models under the interactive pattern recognition framework"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/chen13b_interspeech.html", "abstract": "This paper proposes an improved approach of summarization for spoken multi-party interaction, in which a multi-layer graph with hidden parameters is constructed. The graph includes utterance-to-utterance relation, utterance-to-parameter weight, and speaker-to-parameter weight. Each utterance and each speaker are represented as a node in the utterance-layer and speaker-layer of the graph respectively. We use terms/ topics as hidden parameters for estimating utterance-to-parameter and speaker-to-parameter weight, and compute topical similarity between utterances as the utterance-to-utterance relation. By within- and between-layer propagation in the graph, the scores from different layers can be mutually reinforced so that utterances can automatically share the scores with the utterances from the speakers who focus on similar terms/ topics. For both ASR output and manual transcripts, experiments confirmed the efficacy of including hidden parameters and involving speaker information in the multi-layer graph for summarization. We find that choosing latent topics as hidden parameters significantly reduces computational complexity and does not hurt the performance.", "title": "Multi-layer mutually reinforced random walk with hidden parameters for improved multi-party meeting summarization"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/su13_interspeech.html", "abstract": "This paper introduces a new recursive dialogue game framework for personalized computer-assisted language learning. A series of sub-dialogue trees are cascaded into a loop as the script for the game. At each dialogue turn there are a number of training sentences to be selected. The dialogue policy is optimized to offer the most appropriate training sentence for an individual learner at each dialogue turn considering the learning status, such that the learner can have the scores for all pronunciation units exceeding a pre-defined threshold in minimum number of turns. The policy is modeled as a Markov Decision Process (MDP) with high dimensional continuous state space. Experiments demonstrate promising results for the approach.", "title": "A recursive dialogue game framework with optimal Policy offering personalized computer-assisted language learning"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/hahn13_interspeech.html", "abstract": "In virtually every state-of-the-art large vocabulary continuous speech recognition (LVCSR) system, grapheme-to-phoneme (G2P) conversion is applied to generalize beyond a fixed set of words given by a background lexicon. The overall performance of the G2P system has a strong effect on the recognition quality. Typically, generative models based on joint-n-grams are used, although some discriminative models have a competitive performance but the training time may be quite large. In this work, the effect of using discriminative G2P modeling based on hidden conditional random fields (HCRFs) is analyzed. Besides measuring and comparing the G2P qualities on a textual level, one focus is the performance of LVCSR systems. Although the HCRF model does not outperform the generative one on text data, we could improve our English QUAERO ASR system by 1.3% relative on a couple of test corpora over a strong baseline by only replacing the G2P strategy.", "title": "Improving LVCSR with hidden conditional random fields for grapheme-to-phoneme conversion"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/do13_interspeech.html", "abstract": "This paper presents a context-dependent phone mapping approach for acoustic modeling of large vocabulary speech recognition for under-resourced languages by leveraging on well trained models of other languages. Generally speaking, phone mapping can be considered as a hybrid HMM/MLP (Hidden Markov Model / Multilayer Perceptron) model where the input of the MLP is phone acoustic scores, e.g. likelihood or posterior scores. In this paper, we use deep neural networks trained with a lot of Malay training data to generate bottleneck and posterior features for the target English acoustic models. We extend the concept of phone mapping by using not only posteriors but also bottleneck feature as the input for phone mapping. Experiments show that the phone mapping technique outperforms the cross-lingual tandem approach significantly. In addition, we also show that bottleneck and posterior features contain complementary information. A consistent improvement is obtained by combining these two feature streams to form the input for phone mapping.", "title": "Context-dependent phone mapping for LVCSR of under-resourced languages"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/rasipuram13_interspeech.html", "abstract": "There is growing interest in using graphemes as subword units, especially in the context of the rapid development of hidden Markov model (HMM) based automatic speech recognition (ASR) system, as it eliminates the need to build a phoneme pronunciation lexicon. However, directly modeling the relationship between acoustic feature observations and grapheme states may not be always trivial. It usually depends upon the grapheme-to-phoneme relationship within the language. This paper builds upon our recent interpretation of Kullback-Leibler divergence based HMM (KL-HMM) as a probabilistic lexical modeling approach to propose a novel grapheme-based ASR approach where, first a set of acoustic units are derived by modeling context-dependent graphemes in the framework of conventional HMM/Gaussian mixture model (HMM/GMM) system, and then the probabilistic relationship between the derived acoustic units and the lexical units representing graphemes is modeled in the framework of KL-HMM. Through experimental studies on English, where the grapheme-to-phoneme relationship is irregular, we show that the proposed graphemebased ASR approach (without using any phoneme information) can achieve performance comparable to standard phoneme-based ASR approach.", "title": "Improving grapheme-based ASR by probabilistic lexical modeling approach"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/motlicek13_interspeech.html", "abstract": "Recent studies have shown that speech recognizers may benefit from data in languages other than the target language through efficient acoustic model- or feature-level adaptation. Crosslingual Tandem-Subspace Gaussian Mixture Models (SGMM) are successfully able to combine acoustic model- and feature-level adaptation techniques. More specifically, we focus on under-resourced languages (Afrikaans in our case) and perform feature-level adaptation through the estimation of phone class posterior features with a Multilayer Perceptron that was trained on data from a similar language with large amounts of available speech data (Dutch in our case). The same Dutch data can also be exploited on an acoustic model-level by training globally-shared SGMM parameters in a crosslingual way. The two adaptation techniques are indeed complementary and result in a crosslingual Tandem-SGMM system that yields relative improvement of about 22% compared to a standard speech recognizer on an Afrikaans phoneme recognition task. Interestingly, eventual score-level combination of the individual SGMM systems yields additional 3% relative improvement.", "title": "Crosslingual tandem-SGMM: exploiting out-of-language data for acoustic model and feature level adaptation"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/vu13_interspeech.html", "abstract": "In this paper, we present our latest investigations of multilingual Multilayer Perceptrons (MLPs) for rapid language adaptation between and across language families. We explore the impact of the amount of languages and data used for the multilingual MLP training process. We show that the overall system performance on the target language is significantly improved by initializing it with a multilingual MLP. Our experiments indicate that the more languages we use to train a multilingual MLP, the better is the initialization for MLP training. As a result, the ASR performance is improved, even if the target language and the source languages are not in the same language family. Our best results show an error rate improvement of up to 22.9% relative for different target languages (Czech, Hausa and Vietnamese) by using a multilingual MLP which has been trained with many different languages from the GlobalPhone corpus. In the case of very few training or adaptation data, an improvement of up to 24% relative in terms of error rate is observed.", "title": "Multilingual multilayer perceptron for rapid language adaptation between and across language families"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/rosenberg13_interspeech.html", "abstract": "In this paper we describe two unsupervised representations of prosodic sequences based on k-means and Dirichlet Process Gaussian Mixture Model (DPGMM) clustering. The clustering algorithms are used to infer an inventory of prosodic categories over automatically segmented syllables. A tri-gram model is trained over these sequences to characterize speech. We find that DPGMM clusters show a greater correspondence with manual ToBI labels than k-means clusters. However, sequence models trained on k-means clusters significantly outperform DPGMM sequences in classifying speaking style, nativeness and speakers. We also investigate the use of these sequence models in the detection of outliers regarding these three tasks. Non-parametric Bayesian techniques have the advantage of being able to learn a clustering solution and infer the number of clusters directly from data. While it is attractive to avoid specifying k before clustering, on the tasks of characterizing prosodic sequences we find that effective use of DPGMMs still requires a significant amount of parameter tuning, and performance fails to reach the level of k-means.", "title": "Modeling prosodic sequences with k-means and dirichlet process GMMs"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/schweitzer13_interspeech.html", "abstract": "This paper presents results from a project on phonetic convergence in German spontaneous speech. We used linear mixed models to examine 22 unimodal and 24 multimodal dialogs for articulation rate. We show that speakers' local articulation rates are influenced by the preceding rates of their interlocutors, and that the direction of this influence (i.e., divergence or convergence) depends on social factors, viz. interactants' mutual likeability scores. More specifically, we found that in general there was a \"default\" effect of divergence in articulation rates which was not mediated by social factors. However, this effect was weakened or reversed for higher mutual liking scores, i.e. the degree of convergence increased with the liking scores. Furthermore, while it has recently been suggested that convergence may be enhanced in multimodal settings, we did not find an effect of modality on convergence. However, there was an effect of modality on articulation rate in general.", "title": "Convergence of articulation rate in spontaneous speech"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/pardo13_interspeech.html", "abstract": "Phonetic convergence is highly variable across studies, measures, and analyses. The current paper describes a study that examined multiple acoustic measures in concert with a perceptual measure of phonetic convergence. The study employed a shadowing task in which multiple talkers shadowed words from a set of models. Across different scales of analysis, the acoustic measures were highly variable, yielding inconsistent results. Perceptual assessment of phonetic convergence provided a measure that was more stable, reliable, and valid than any single acoustic attribute. Mixedeffects regression modeling assessed the relative contributions of each acoustic attribute to perceived phonetic convergence on a word-by-word basis. This study demonstrates the utility of an approach that combines acoustic and perceptual measures of phonetic convergence.", "title": "Phonetic convergence in shadowed speech: a comparison of perceptual and acoustic measures"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/wodarczak13_interspeech.html", "abstract": "The present paper reports on the impact of pitch accents and duration on temporal organisation of overlapping speech onsets in spontaneous dialogue. We observe a non-random pattern of overlap initiations within intervals between consecutive pitch accents, thus extending our earlier reports of a similar effect within vowel-to-vowel intervals. The latter finding was interpreted as a tendency to start overlapped speech directly before perceptually prominent vocalic onsets. In an attempt to reconcile these results, we investigate whether the effect observed on vowel-to-vowel intervals is influenced by presence of pitch accents and lengthening, both of which are known to be correlated with perceptual prominence. We find a strong effect of duration, which, however, does not on its own account fully for the observed pattern, indicating that other correlates of prominence might be involved in guiding the timing of overlap onsets.", "title": "Pitch and duration as a basis for entrainment of overlapped speech onsets"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/bonin13_interspeech.html", "abstract": "Conversational interaction is a dynamic activity in which participants engage in the construction of meaning and in establishing and maintaining social relationships. Lexical and prosodic accommodation have been observed in many studies as contributing importantly to these dimensions of social interaction. However, while previous works have considered accommodation mechanisms at global levels (for whole conversations, halves and thirds of conversations), this work investigates their evolution through repeated analysis at time intervals of increasing granularity to analyze the dynamics of alignment in a spoken language corpus. Results show that the levels of both prosodic and lexical accommodation fluctuate several times over the course of a conversation.", "title": "Investigating fine temporal dynamics of prosodic and lexical accommodation"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/kim13_interspeech.html", "abstract": "The single word speech shadowing task typically produces spontaneous imitation of the speech of the shadowed talker (the model). This task has been used as a tool for examining speech convergence in a non-social setting and has provided data for claims that the mental lexicon is constituted from instance-based exemplars. We examined whether the speech of participants who shadowed or explicitly imitated a model talker would converge on similar properties of the model's speech. The model talker produced speech in two styles (normal and clear speech) that differed in word duration, intensity and F0. Participants produced immediate and delayed naming responses. The results suggested that spontaneous and explicit imitation tap different processes. For spontaneous imitation only word duration showed convergence with model speech and this effect was reduced with delayed naming. Explicit imitation showed an association with model speech both for duration and intensity and this effect was unaffected by the delayed naming. The pattern of partial correlations between the imitation conditions and the model speech provided further evidence that the spontaneous imitation was based upon different processes than those used in explicit imitation.", "title": "Spontaneous and explicit speech imitation"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/podlipsky13_interspeech.html", "abstract": "This study explored effects of simultaneous use of late bilinguals\u0081f languages on their second-language (L2) pronunciation. We tested (1) if bilinguals effectively inhibit the first language (L1) when simultaneously processing L1 and L2, (2) if bilinguals, like natives, imitate subphonemic variation, (3) if bilinguals' imitation operates cross-linguistically, and (4) if imitation interacts with phonological structure. Sixteen L1-Czech L2-English speakers heard stimuli with two factors manipulated: language (Czech, English) and Voice Onset Time (VOT) in /p, t, k/ (short, long). They subsequently pronounced English /t/- and /d/-initial words. Speakers' VOTs in the Czech-Short-VOT, Czech-Extended-VOT, and English-Reduced- VOT conditions were comparable, but VOTs were more English-like after exposure to English-Long-VOT, which applied to both /t/ and /d/. The conclusions are as follows. (1) Bilinguals' potentially ineffective L1 inhibition did not affect their L2 production, since exposure to Czech did not lead to VOT reduction. (2) Imitation is not limited to native speech, since bilinguals increased their VOTs following exposure to English-Long-VOT. (3) Imitation did not operate cross-linguistically, since bilinguals' English productions following Czech-Short-VOT and Czech-Extended-VOT did not differ. Finally, (4) imitation does interact with phonology, since exposure to English long-VOT /t/ resulted in a reduction in prevoicing of its voiced counterpart, /d/.", "title": "Imitation interacts with one's second-language phonology but it does not operate cross-linguistically"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/hsieh13_interspeech.html", "abstract": "This paper investigated the effect of semantic predictability on acoustic realizations in Taiwan Mandarin. Most previous studies have been guided by the assumption that in high predictability conditions, speech units tend to be reduced in the temporal and spectral qualities. This paper focuses on a tone language in an attempt to not only examine previous findings about vowels but also look into possible effect of semantic predictability on F0. The results showed that words were shorter in vowel duration, lower in the intensity, and more centralized within the vowel space when they were embedded in high predictability contexts. Male and female speakers both performed the same reduction process systematically. F0 excursion was also found strongly affected by semantic predictability; specifically, it was much smaller in high predictability conditions. Moreover, gender difference in the extent of F0 excursion was mediated in high predictability conditions. This paper provides additional evidence for the acoustic correlates of semantic predictability in speech production and sheds light on the prosodic encoding of semantic structures in tone languages.", "title": "Prosodic markings of semantic predictability in taiwan Mandarin"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/hoffmann13_interspeech.html", "abstract": "The early history in experimental phonetics and speech technology is featured by numerous sophisticated devices which we can study today in a number of historic collections which are located mainly at universities. In many cases, however, it is not easy to understand how these devices have been applied in an optimal way. Illustrations and historic photographs are especially helpful. The historic collection of the TU Dresden, which also includes the former devices of the Hamburg phonetician Giulio Panconcelli-Calcia, benefits from the fact that he prepared a lot of photographs which are still available in the collection. They show selected phonetic experimental equipment in function. This paper informs about the photographic part of the Dresden collection and presents some examples.", "title": "How did it work? historic phonetic devices explained by coeval photographs"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/kohtz13_interspeech.html", "abstract": "We show on the basis of German that prosodic patterns change in the course of a traditional sentence-list elicitation. Two frequent methods are analyzed: sentence-frame and syntax-frame elicitations. While only the sentences of the sentence-frame elicitation show an increase in speaking rate, both elicitation methods cause a drastic reduction in the alignment variability of nuclear pitch-accent rises. So, the starting point for the idea of segmental anchoring, i.e. the characteristic stable alignment of L and H targets, could primarily be due to a training effect based on the continuous production of analogously constructed or identical carrier sentences. Detailed pitch-accent analyses also offer alternative interpretations for anchoring patterns. Methodologically, in order to avoid training effects in pitch-accent production, our findings suggest using the syntax-frame method and short sentence lists of 40 items or less.", "title": "Eliciting speech with sentence lists \u2014 a critical evaluation with special emphasis on segmental anchoring"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/wang13b_interspeech.html", "abstract": "This study aims at exploring detailed acoustic characteristics of Mandarin sustained vowels based on magnetic resonance imaging (MRI). Cross-sectional area functions of the vocal tract for Mandarin vowels were extracted from volumetric MRI images obtained from eight Chinese speakers (six-male and two-female subjects). Acoustic analysis was performed for the vowels recorded from each speaker. The acoustic analysis includes comparisons between measured and calculated formants, vowel space, simplified models for Mandarin vowels, and correlation between vocal tract length and formant frequencies. Mean absolute errors of calculated formants across 10 Mandarin vowels over eight subjects ranged from 4.6% to 11%. Simple-model also offers good approximations for Mandarin vowels. Since the simple model can give relatively clear relation between acoustic characteristics and vocal tract properties, it is useful for intuitively understanding the mechanism of speech production. The correlation analysis suggests that there exist negative correlations between vocal tract length and first four formants for Mandarin vowels in general, which showed a clearer relation than a recent study by another research group.", "title": "An MRI-based acoustic study of Mandarin vowels"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/hirst13_interspeech.html", "abstract": "In recent years there have been a number of proposals for objective paradigms for establishing prosodic typologies among languages. This paper compares the results of melody metrics calculated on just over two hours of read speech for each of three languages. Pitch movements in Chinese, a lexical tone language, were found to be significantly more ample and more varied than those obtained for English, characterised as a language with lexical stress and French, characterised as a language with no lexical prosody. Moreover, a gender difference, observed in both English and French was not observed in Chinese. It is conjectured that the pressure from the lexical use of tone in Chinese may have the effect of restricting the use of pitch for other functions.", "title": "Melody metrics for prosodic typology: comparing English, French and Chinese"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/proctor13_interspeech.html", "abstract": "Production of nasal vowels in French, and nasal consonants in French and English, was examined using real-time magnetic resonance imaging (rtMRI). The coordination of velic and lingual gestures was found to be tightly controlled across different prosodic contexts in French nasals. Velum lowering in English nasal consonants did not show the same control, although the timing of the corresponding lingual gestures varied with prosodic context in the same way as for French nasals, suggesting a coordinative relationship in which oral and velic articulators are consistently phased in French nasal production. These findings illustrate the utility of real-time MRI as a method for studying velic activity and articulatory coordination in vocalic and nasal phonology.", "title": "Velic coordination in French nasals: a real-time magnetic resonance imaging study"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/huckvale13b_interspeech.html", "abstract": "Pre-linguistic infants need to learn how to produce spoken word forms that have the appropriate intentional effect on adult carers. One proposed imitation strategy is based on the idea that infants are innately able to match the sounds of their own babble to sounds of adults, while another proposed strategy requires only reinforcement signals from adults to improve random imitations. Here we demonstrate that knowledge gained from interactions between infants and adults can provide useful normalizing data that improves the recognisability of infant imitations. We use the KLAIR virtual infant toolkit to collect spoken interactions with adults, exploit the collected data to learn adult-to-infant mappings, and construct imitations of adult utterances using KLAIR's articulatory synthesizer. We show that speakers reinterpret and reformulate KLAIR's productions in terms of standard phonological forms, and that these reformulations can be used to train a system that generates infant imitations that are more recognisable to adults than a system based on babbling alone.", "title": "Learning to imitate adult speech with the KLAIR virtual infant"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/lucero13_interspeech.html", "abstract": "The purpose of this work is the development of a synthesizer of disordered voices, i.e., a generator of sounds that mimic the vocal quality of speakers that suffer from a voice disorder or laryngeal pathology. A physics-based modeling strategy is followed for physiological fidelity and a direct relation between the physiology and the perception of vocal quality. The synthesizer is based on a smooth and asymmetric version of a lumped mucosal wave model of the vocal folds, coupled to a wave-reflexion analog of the vocal tract. In this paper, we report our progress and present results of synthetic voices with various levels of vocal frequency jitter, additive pulsatile noise at the glottis due to airflow turbulence, and tension imbalance between the left and right vocal folds.", "title": "Physics-based synthesis of disordered voices"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/dapolito13_interspeech.html", "abstract": "This study focuses on how French heterosyllabic sibilant clusters are produced by one French native speaker and by three Italian learners of French-L2. In French, these clusters are frequent and are reported to show place assimilation; in Italian, on the contrary, they are very rare and speakers are expected to repair the phonotactically marked sequences by epenthesizing a schwa. In the current study, acoustic and articulatory (AG500) data have been collected in order to observe the production of clusters in French L1 and L2, under the influence of two factors . that is speech rate and presence of prosodic boundaries . that may affect the realization of assimilation or repairs by acting on coarticulation. Results reveal that place assimilation occurs only at faster speech rate and it favors the realization of the postalveolar over the alveolar fricative; as expected, the presence of prosodic boundaries interferes with this process. Place assimilation is realized by the French speaker and one Italian speaker, although the latter produces it in fewer cases and using a different articulatory strategy.", "title": "Place assimilation and articulatory strategies: the case of sibilant sequences in French as L1 and L2"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/samlowski13_interspeech.html", "abstract": "German demonstrative pronouns, relative pronouns, and definite articles are segmentally identical but differ strongly in the frequency with which they appear. We examined the production of five such particles in a reading task. In a comparison of orthographically identical word pairs belonging to different lexical classes we found small but significant differences in word and vowel duration, prominence, and spectral similarity. Three of the particles in particular tended to be longer and more prominent when they occurred as demonstrative or relative articles than when they were assigned their usual role as definite articles.", "title": "Effects of lexical class and lemma frequency on German homographs"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/lancia13_interspeech.html", "abstract": "The study of glottalization is usually based on methods whose limited reliability in the analysis of non sustained phonation is widely acknowledged. In this study we propose a method to measure the degree of glottalization which is robust to variation of fundamental frequency and which, applied to the electroglottographic signal, does not depend on changes in vocal tract resonances. The method, based on an original variant of recurrence analysis, is used to study the interaction between phonological glottalization of vowels and contrastive tones in Yalalag Zapotec. In this American Indian language, both modal and laryngeal vowels can occur with three different contrastive tones (low, high and falling). However, the rapid modulation of fundamental frequency needed for the production of contrastive tones is expected to be strongly limited by the presence of laryngealization. We study how speakers resolve the competition between the constraints on the articulation of these two features of Zapotec phonology. Through wavelet based functional mixed regression we could model the changes in the degree of glottalization over the duration of both modal and laryngealized vowels produced with the three different tones.", "title": "Measuring laryngealization in running speech: interaction with contrastive tones in yal\u00e1lag zapotec"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/rusaw13_interspeech.html", "abstract": "This paper introduces the Neural Oscillator Model of Speech Timing and Rhythm (NOMSTR), which is designed to be a flexible tool for investigating the systems which affect speech rhythm and timing through simulation. NOMSTR is an artificial neural network (ANN) model which incorporates oscillators, inspired by central pattern generators (CPGs), a type of neural circuit which underlies other types of patterned motor behavior in animals. NOMSTR uses three oscillators paired with thresholded nodes to model three levels of prosodic structure (e.g. syllables, accents, and phrases). In addition to setting the periods and phases of the oscillators to represent syllable and phrase durations, the weights between the thresholded nodes can be adjusted to model interactions between prosodic levels and their durational effects (e.g. pre-boundary lengthening). In this paper I demonstrate NOMSTR's ability to simulate the prosodic structure of spontaneous utterances in English and French, languages with disparate prosodic systems. The accuracy of NOMSTR's simulated prosodic structures is tested through its ability to simulate syllable durations, the locations of accents and phrase boundaries, and the influence of accenting and boundaries on syllable durations.", "title": "A neural oscillator model of speech timing and rhythm"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/wong13_interspeech.html", "abstract": "Recent advances in real-time magnetic resonance imaging (rt-MRI) make possible the following study of post-dorsal coarticulatory effects on lateral approximants in running speech. As the MRI is particularly well-suited for investigating pharyngeal articulations, it was capable of capturing perseverative coarticulation from a low central vowel to the immediately following syllable-initial lateral approximant. The lowered and retracted tongue body position associated with a low central vowel is articulatorily equivalent to a pharyngeal constriction. We show here that rt-MRI techniques can track coarticulation direction by calculating the average pixel intensity within a region of interest in MR images as a function of time.", "title": "Observations of perseverative coarticulation in lateral approximants using MRI"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/fujimoto13_interspeech.html", "abstract": "Laryngeal and supralaryngeal articulators coordinately work to produce speech sounds. In order to study differences in supralaryngeal manifestations of voiced and voiceless consonants, we compared the tongue movement during a minimal pair /agise/ and /akise/ using the fast scanning techniques of MRI movies. The result showed that the tongue displacement starts earlier in /k/ than in /g/ for many of the speakers of Tokyo Japanese. This agrees with our previous findings using other dialect speakers. These results suggest that many Japanese actively differentiate supralaryngeal articulation according to the voicing of the consonants, raising the tongue earlier in voiceless ones. This movement is presumably to ensure the voicelessness of the consonant. The present study also supplies evidence for the usefulness of a constructive approach for physical modeling.", "title": "Timing differences in articulation between voiced and voiceless stop consonants: an analysis of cine-MRI data"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/lammert13_interspeech.html", "abstract": "Real-Time Magnetic Resonance Imaging affords speech articulation data with good spatial and temporal resolution and complete midsagittal views of the moving vocal tract, but also brings many challenges in the domain of image processing and analysis. Region-of-interest analysis has previously been proposed for simple, efficient and robust extraction of linguistically-meaningful constriction degree information. However, the accuracy of such methods has not been rigorously evaluated, and no method has been proposed to calibrate the pixel intensity values or convert them into absolute measurements of length. This work provides such an evaluation, as well as insights into the placement of regions in the image plane and calibration of the resultant pixel intensity measurements. Measurement errors are shown to be generally at or below the spatial resolution of the imaging protocol with a high degree of consistency across time and overall vocal tract configuration, validating the utility of this method of image analysis.", "title": "Vocal tract cross-distance estimation from real-time MRI using region-of-interest analysis"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/arrabothu13_interspeech.html", "abstract": "Speech can be segmented into syllables by identifying the syllable nuclei, which are points of high sonority. The excitation peaks in the linear prediction (LP) residual and the formant peaks can be interpreted as perceptually significant point features which contribute to the loudness of speech. In this paper, the use of these two point features is described for the use of detecting syllable nuclei. Each of these evidences contain information about different aspects of speech production, namely the glottal vibrations and the time varying vocal tract system. Thus it is possible that they contain complementary information about the syllable nuclei. Performance of the proposed syllable nuclei detection algorithm is evaluated for the TIMIT, Switchboard and the NTIMIT corpus. The proposed method performs comparably against two other state of the art syllable nuclei detection methods, and is shown to perform better for conversational speech. It is very fast and requires no training.", "title": "Syllable nuclei detection using perceptually significant features"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/hsieh13b_interspeech.html", "abstract": "It is well acknowledged that [a] in English diphthongs (e.g. [a] in \"pi'd\") has a different formant structure from its closest corresponding monophthong (e.g. [a] in \"pod\"). The current study proposes that these two sounds share the same cognitive unit, i.e. the pharyngeal constriction gesture that produces [a], and the surface difference can be modeled as a consequence of truncating the same articulatory movement in time by the following palatal glide in the diphthongal environment. Formation of pharyngeal constriction gesture during the production of [a] in a diphthong and in its corresponding monophthong was observed in various timing contexts using Realtime MRI; and the collected production data were quantitatively analyzed using the direct image analysis (DIA) technique, which infers tissue movement by tracking pixel intensity change over time in regions of interest. Results support our truncation account in that: (1) formation time of pharyngeal constriction is significantly longer in monophthongs than in diphthongs; (2) this duration correlates with the resulting constriction degree; and (3) the resulting constriction degree predicts the acoustic difference in the F2 dimension as predicted by our hypothesis.", "title": "Truncation of pharyngeal gesture in English diphthong [a\u026a]"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/yang13_interspeech.html", "abstract": "Word frequency and lexical class distinction between function and content words have been shown to significantly influence word production. In this paper, we use real-time magnetic resonance imaging to investigate the effect of word frequency and lexical class on articulatory characteristics (the articulator speed) as well as acoustic characteristics (F0 and short-term energy) in word production. Multiple regression analyses showed that word frequency exhibits significantly higher correlation with articulatory and acoustic factors for content words compared to function words. A Granger causality analysis uncovered a causal relationship from articulatory speed to F0/energy for low-frequency content words. We further observed, using functional canonical correlation analysis, a tight coupling of articulatory and acoustic characteristics for low-frequency content words. These results support the view that word frequency distinctly influences the production of function and content words as manifested in their articulation and acoustics, as well as the dynamic coupling of these temporal streams.", "title": "The effect of word frequency and lexical class on articulatory-acoustic coupling"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/yamakawa13_interspeech.html", "abstract": "Previous studies on Japanese fricatives and affricates revealed that [s] and [ts] are classified by variables in the time domain (Yamakawa et al., 2012), whereas [ts] and [tC] as well as [s] and [tC] are classified by variables in the spectral domain (Yamakawa & Amano, 2011). To gain an integrated perspective on these findings, this study examined whether [s] [ts], and [tC] can be classified by a single discriminant model. Using variables in the time and spectral domains, canonical discriminant analysis was performed on word materials with [s], [ts], and [tC] pronounced by both single and multiple Japanese speakers. The time domain variables were a combination of the rise duration and sum of steady and decay durations of the three consonants. The spectral domain variable was their intensity obtained from a one-third-octave bandpass filter with a center frequency of 3150 Hz. The results showed that [s], [ts], and [tC] were successfully classified with high discriminant ratios of 92.6% and 90.2% for single and multi-speaker materials, respectively. This means that, in both cases, the single discriminant model can discriminate [s], [ts], and [tC] with the variables in the time and spectral domains.", "title": "Discrimination between fricative and affricate in Japanese using time and spectral domain variables"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/drozdova13_interspeech.html", "abstract": "The purpose of the present study was to investigate the effect of modality-specific practice through a CALL system on the acquisition of an L2 syntactic phenomenon: subject-verb inversion. Two groups of learners of Dutch participated in the study: one group worked with a version of the CALL system that allows drag-anddrop exercises while another one practiced with a different version of the system that makes use of Automatic Speech Recognition (ASR) to provide practice and feedback on spoken utterances. Progress was measured by comparing the results of the groups on pre- and post-tests with discourse completion tasks in written and oral forms. The CALL system performed well, and statistical analyses revealed a significant difference between the results of all participants in pre- and post-tests, demonstrating the positive effect of practice with both versions of the program on the acquisition of the syntactic feature addressed in the training. Larger improvements were observed for oral training, but no significant effect of practice modality on the progress of the participants or their appreciation of the program was found. The implications of these findings are discussed.", "title": "L2 syntax acquisition: the effect of oral and written computer assisted practice"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/signorello13_interspeech.html", "abstract": "Voice is one of the orators perceivable behaviors that convey traits of charisma. Contemporary leaders, such as politicians, use their voices to shape their message and persuade audiences. The present study analyzes the physiological use of the \"charismatic voice\" in Italian, French, and Brazilian Portuguese political speech. We investigated the \"discourse level\" aspects of voice by conducting acoustic analyses on the voice production of political leaders speaking in different communication contexts (formal versus informal) and while addressing different kinds of audiences (interviewers, colleagues, voters). Politicians' physiological speech range profiles were found to be dependent on the communication context, the audience being addressed and the persuasive goal. There thus seems to be a cross-language and cross-cultural trend in common for voice usage in different communication contexts. Orators adapt voice behavior to the context, the audience, and the persuasive goal of communication. The range of acoustic voice correlates will be wider when the discourse must be at its most persuasive.", "title": "The physiological use of the charismatic voice in Political speech"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/rose13_interspeech.html", "abstract": "There is a growing consensus that there is a need to evaluate second language speech performance with respect to first language speech behavior. To support this need, the Crosslinguistic Corpus of Hesitation Phenomena was developed. This freely available corpus is designed to investigate the crosslinguistic influence of speech patterns and consists of recordings of speakers producing first and second language speech samples in response to parallel elicitation tasks in each language. Preliminary results from the corpus are consistent with other findings that second language performance is sometimes correlated with first language speech behavior. In particular, findings show that silent pause rate and duration as well as other hesitation phenomena correlate with first language performance while speech rate does not. Interestingly, repeats also differ from first language production. Results show that the corpus may be a useful tool for researchers who wish to investigate the correspondence between first and second language speech, particularly with respect to the use of hesitation phenomena.", "title": "Crosslinguistic corpus of hesitation phenomena: a corpus for investigating first and second language speech performance"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/preu13_interspeech.html", "abstract": "This paper presents an animated 2D articulation model of the tongue and the lips for biofeedback applications. The model is controlled by real-time optopalatographic measurements of the positions of the upper lip and the tongue in the anterior oral cavity. The measurement system is an improvement on a previous prototype with increased spatial resolution and an enhanced close-range behavior. The posterior part of the tongue was added to the model by linear prediction. The prediction coefficients were determined and evaluated using a corpus of vocal tract traces of 25 sustained phonemes. The model represents the tongue motion and the lip opening physiologically plausible during articulation in real-time.", "title": "Real-time control of a 2d animation model of the vocal tract using optopalatography"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/siddins13_interspeech.html", "abstract": "The aim of this study was to investigate the relationship between compensatory shortening and coarticulation in German tense and lax vowels in trochees and iambs and to determine whether this relationship was influenced by prosodic accentuation. Speakers produced near minimal pairs differing in vowel tensity in monosyllabic and disyllabic words (both trochees and iambs) in accented and deaccented contexts. We found significant effects of polysyllabic shortening, but only in tense vowels of nuclearaccented target words. Both stress patterns (trochaic and iambic) showed equal effects of polysyllabic shortening. Thus, while the duration of tense vowels in this study depended on accentuation and syllabicity, perhaps in order to provide perceptual cues for the listener, lax vowels were immune from lengthening and shortening phenomena. As a result, the durational difference between tense and lax vowels appears to lessen in prosodically weak contexts. The greater overlap of acoustic duration in deaccented contexts may contribute to the origin of the diachronic merger of tense and lax vowels in some languages.", "title": "The influence of accentuation and polysyllabicity on compensatory shortening in German"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/ding13_interspeech.html", "abstract": "The present study investigates the influence of phonetic factors on the frequency of vowel epenthesis in the German speech of Chinese learners. The subjects were intermediate learners of German who entered Germany within five months of their study. Descriptive statistics were performed on the data collected from reading tasks, and phonetic analysis was provided to explain the phenomenon of epenthesis. In the main experiment, eighteen Chinese students were recruited to read 50 phonetically rich sentences with various sentence modes after one month residence in Germany. Results indicate that these learners employed the epenthesis strategy more or less in producing consonant codas and consonant onset clusters in German. An investigation in the frequency of epenthesis in relation to various factors demonstrates that consonant cluster length, L1 transfer, markedness, sonority, and articulatory timing influence the occurrences of epenthesis simultaneously. An additional experiment was conducted after a time span of three months, ten of these subjects were requested to read the same text, the result shows that the amount of epenthesis decreases with the increase of the length of residence and German language learning experience. These findings might shed some light on the acquisition process of consonant codas in foreign languages.", "title": "An investigation of vowel epenthesis in Chinese learners' production of German consonants"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/richmond13_interspeech.html", "abstract": "The two measures typically used to assess the performance of an inversion mapping method, where the aim is to estimate what articulator movements gave rise to a given acoustic signal, are root mean squared (RMS) error and correlation. In this paper, we investigate whether \"task-based\" evaluation using an articulatory-controllable HMM-based speech synthesis system can give useful additional information to complement these measures. To assess the usefulness of this evaluation approach, we use articulator trajectories estimated by a range of different inversion mapping methods as input to the synthesiser, and measure their performance in the acoustic domain in terms of RMS error of the generated acoustic parameters and with a listening test involving 30 participants. We then compare these results with the standard RMS error and correlation measures calculated in the articulatory domain. Interestingly, in the acoustic evaluation we observe one method performs with no statistically significant difference from measured articulatory data, and cases where statistically significant differences between methods exist which are not reflected in the results of the two standard measures. From our results, we conclude such task-based evaluation can indeed provide interesting extra information, and gives a useful way to compare inversion methods.", "title": "On the evaluation of inversion mapping performance in the acoustic domain"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/gupta13b_interspeech.html", "abstract": "In this paper we look at real-time computing issues in large vocabulary speech recognition. We use the French broadcast audio transcription task from ETAPE 2011 for this evaluation. We compare word error rate (WER) versus overall computing time for hidden Markov models with Gaussian mixtures (GMM-HMM) and deep neural networks (DNN-HMM). We show that for a similar computing during recognition, the DNN-HMM combination is superior to the GMM-HMM. For a real-time computing scenario, the error rate for the ETAPE dev set is 23.5% for DNN-HMM versus 27.9% for the GMM-HMM: a significant difference in accuracy for comparable computing. Rescoring lattices (generated by DNN-HMM acoustic model) with a quadgram language model (LM), and then with a neural net LM reduces the WER to 22.0% while still providing real-time computing.", "title": "Comparing computation in Gaussian mixture and neural network based large-vocabulary speech recognition"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/stein13_interspeech.html", "abstract": "While both the acoustic model and the language model in automatic speech recognition are typically well-trained on the target domain, the free parameters of the decoder itself are often set manually. In this paper, we investigate in how far a stochastic approximation algorithm can be employed to automatically determine the best parameters, especially if additional time-constraints are given on unknown machine architectures. We offer our findings on the German Difficult Speech Corpus, and present significant improvements over both the spontaneous and planned clean speech task.", "title": "Simultaneous perturbation stochastic approximation for automatic speech recognition"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/sheffield13_interspeech.html", "abstract": "In this paper, we explore high performance software and hardware implementations of an automatic speech recognition system that can run locally on a mobile device. We automate the generation of key components of our speech recognition system using Three Fingered Jack, a tool for hardware/software codesign that maps computation to CPUs, data parallel processors, and custom hardware. We use Three Fingered Jack to explore energy and performance for two key kernels in our speech recognizer, the observation probability evaluation and across-word traversal. Through detailed hardware simulation and measurement, we produce accurate estimates for energy and area and show a significant energy improvement over a conventional mobile CPU.", "title": "Hardware/software codesign for mobile speech recognition"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/shi13_interspeech.html", "abstract": "In automatic speech recognition, conventional language models recognize the current word using only information from preceding words. Recently, Recurrent Neural Network Language Models (RNNLMs) have drawn increased research attention because of their ability to outperform conventional n-gram language models. The superiority of RNNLMs is based in their ability to capture longdistance word dependencies. RNNLMs are, in practice, applied in an N-best rescoring framework, which offers new possibilities for information integration. In particular, it becomes interesting to extend the ability of RNNLMs to capture long distance information by also allowing them to exploit information from succeeding words during the rescoring process. This paper proposes three approaches for exploiting succeeding word information in RNNLMs. The first is a forward-backward model that combines RNNLMs exploiting preceding and succeeding words. The second is an extension of a Maximum Entropy RNNLM (RNNME) that incorporates succeeding word information. The third is an approach that combines language models using two-pass alternating rescoring. Experimental results demonstrate the ability of succeeding word information to improve RNNLM performance, both in terms of perplexity and Word Error Rate (WER). The best performance is achieved by a combined model that exploits the three words succeeding the current word.", "title": "Exploiting the succeeding words in recurrent neural network language models"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/torbati13_interspeech.html", "abstract": "Speech recognition systems have historically used contextdependent phones as acoustic units because these units allow linguistic information, such as a pronunciation lexicon, to be leveraged. However, when dealing with a new language for which minimal linguistic resources exist, it is desirable to automatically discover acoustic units. The process of discovering acoustic units usually consists of two stages: segmentation and clustering. In this paper, we focus on the segmentation portion of this problem. We introduce a nonparametric Bayesian approach for segmentation, based on Hierarchical Dirichlet Processes (HDP), in which a hidden Markov model (HMM) with an unbounded number of states is used to segment the utterance. This model is referred to as an HDP-HMM. We compare this algorithm to several popular heuristic methods and demonstrate an 11% improvement in finding boundaries on the TIMIT Corpus. A self-similarity measure over segments shows an 88% improvement compared to manual segmentation with comparable segment length. This work represents the first step in the development of a speech recognition system that is entirely based on nonparametric Bayesian models.", "title": "Speech acoustic unit segmentation using hierarchical dirichlet processes"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/georges13_interspeech.html", "abstract": "In this paper, a method is proposed which embeds regular grammars into an N-gram Markov language model. This allows accurate speech recognition even for N-gram models estimated on sparse grammatical word sequences. Moreover, it allows explicit userdependent modelling of word sequences, such as phone numbers, email addresses or US ZIP codes, separately from the Markov model. The method is theoretically described along with a feasible implementation overview. More precisely, a language model preprocessing step generalizes the enclosed grammatical word sequences during language model learning. These grammars are embedded during speech decoding by using a novel transducer nesting technique. The Wall Street Journal corpus was used to evaluate the proposed method. We achieved a word error rate reduction of 31.1%. A computational environment was used, which is typical for car head units or mobile devices.", "title": "Transducer-based speech recognition with dynamic language models"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/kubo13_interspeech.html", "abstract": "Weighted finite-state transducers (WFSTs) are widely used as a fundamental data structure in several spoken language processing systems since they can provide a unified representation of many types of probabilistic models. Even though the use of accurate WFSTs is important in many spoken language systems, WFSTs are conventionally obtained by transforming probabilistic models that are not estimated in terms of WFST accuracy. Several recent techniques have enabled the direct optimization of weight parameters in WFSTs; however, these techniques do not optimize the structures of WFSTs directly. In this paper, with the goal of achieving a direct estimation of WFST structures from a dataset, we introduce a Bayesian method for structure inference. The proposed method employs the hierarchical Dirichlet process (HDP) as a prior process of generative processes of arcs in the WFSTs. Thanks to the flexibility of the HDP that enables the handling of countably infinite entities, the proposed method can potentially generate the infinite number of arcs in the WFSTs. The efficiency of the proposed method is verified by estimating WFSTs for grapheme-tophoneme (G2P) conversion. We confirmed that the WFST obtained by the proposed method realized a compact representation of G2P conversion compared with the conventional N-gram-based G2P models.", "title": "A method for structure estimation of weighted finite-state transducers and its application to grapheme-to-phoneme conversion"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/jouvet13_interspeech.html", "abstract": "Combining outputs of speech recognizers is a known way of increasing speech recognition performance. The ROVER approach handles efficiently such combinations. In this paper we show that the best performance is not achieved by combining the outputs of the best set of recognizers, but rather by combining outputs of recognizers that rely on different processing components, and in particular on a different order (backward vs. forward) for processing speech frames. Indeed, much better speech recognition results were obtained by combining outputs of sphinx-based recognizers with outputs of Julius-based recognizers than by combining the same number of outputs from only sphinx-based recognizers, even if the individual sphinx-based systems led to better results than the individual Julius-based recognizers. Further experiments have also been conducted using sphinx-based tools for processing speech frames in reverse order (i.e. backward in time). The results clearly show that combining forward-based and backward-based decoders provide significant improvement with respect to a combination of forward only or backward only decoders. Experiments have been conducted on the ESTER2 and ETAPE speech corpora. Overall, combining sphinx-based and Julius-based systems led to 18.6% word error rate on ESTER2 test data, and 24.5% word error rate on ETAPE test data.", "title": "Combining forward-based and backward-based decoders for improved speech recognition performance"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/siohan13_interspeech.html", "abstract": "This paper presents a data selection approach where spoken utterances are selected in a sequential fashion from a large out-of-domain data set to match the utterance distribution of an in-domain data set. We propose to represent each utterance by its iVector, a low dimensional vector indicating the coordinate of that utterance in a subspace acoustic model. We show that the distribution of iVectors can characterize a data set and enables distinguishing subsets of utterances from different domains. Last, we present experimental speech recognition results based on a system trained on a data set constructed by the proposed algorithm and a comparison with random data selection.", "title": "ivector-based acoustic data selection"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/lei13_interspeech.html", "abstract": "In this paper we describe the development of an accurate, smallfootprint, large vocabulary speech recognizer for mobile devices. To achieve the best recognition accuracy, state-of-the-art deep neural networks (DNNs) are adopted as acoustic models. A variety of speedup techniques for DNN score computation are used to enable real-time operation on mobile devices. To reduce the memory and disk usage, on-the-fly language model (LM) rescoring is performed with a compressed n-gram LM. We were able to build an accurate and compact system that runs well below real-time on a Nexus 4 Android phone.", "title": "Accurate and compact large vocabulary speech recognition on mobile devices"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/allauzen13_interspeech.html", "abstract": "This paper describes a modified composition algorithm that is used for combining two finite-state transducers, representing the context-dependent lexicon and the language model respectively, in large vocabulary speech recognition. This algorithm is a hybrid between the static and dynamic expansion of the resultant transducer, which maps from context-dependent phones to words and is searched during decoding. The approach is to pre-compute part of the recognition transducer and leave the balance to be expanded during decoding. This method allows for a fine-grained trade-off between space and time in recognition. For example, the time overhead of purely dynamic expansion can be reduced by over six-fold with only a 20% increase in memory in a collection of large-vocabulary recognition tasks available on the Google Android platform.", "title": "Pre-initialized composition for large-vocabulary speech recognition"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/kurniawati13_interspeech.html", "abstract": "In this paper, we present a new method for isolated keyword detection that is meant to activate a personal device from standby state. Instead of using the common method for speech recognition such as Hidden Markov Model (HMM) or Dynamic Time Warping (DTW), we modify a GMM-UBM (Gaussian Mixture Model . Universal Background Model) scheme that is better known in speaker recognition field. Since only one adapted Gaussian mixture is used to represent the keyword, a second layer of check is employed to ensure the right sequence of occurrence within the keyword. This is done by comparing it with the Longest Common Subsequence (LCS) of the highest performing GMM component obtained during the registration phase. Results for a subset of the SpeechDat-Car database are presented to validate the benefit of this modeling against moderate noise level.", "title": "Speaker dependent activation keyword detector based on GMM-UBM"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/sak13_interspeech.html", "abstract": "Language modeling for automatic speech recognition (ASR) systems has been traditionally in the verbal domain. In this paper, we present finite-state modeling techniques that we developed for language modeling in the written domain. The first technique we describe is for the verbalization of written-domain vocabulary items, which include lexical and non-lexical entities. The second technique is the decomposition-recomposition approach to address the out-of-vocabulary (OOV) and the data sparsity problems with non-lexical entities such as URLs, email addresses, phone numbers, and dollar amounts. We evaluate the proposed written-domain language modeling approaches on a very large vocabulary speech recognition system for English. We show that the written-domain language modeling improves the speech recognition and the ASR transcript rendering accuracy in the written domain over a baseline system using a verbal-domain language model. In addition, the written-domain system is much simpler since it does not require complex and error-prone text normalization and denormalization rules, which are generally required for verbal-domain language modeling.", "title": "Written-domain language modeling for automatic speech recognition"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/versteegh13_interspeech.html", "abstract": "This paper studies the properties of the Histograms of Acoustic Co-occurrences (HAC) approach to acoustic modeling. While HACvectors have been predominantly used with matrix decomposition algorithms, we show that the additivity and sparseness constraints inherent in HAC lead to a representational space in which utterances are linearly separable with respect to the words they contain. The method implies that it is possible to detect and locate words in test utterances by using a classifier that is trained without any information about word order or word location during training. We demonstrate this by showing that an ensemble of linear classifiers can reach excellent detection scores on the TIDIGITS dataset. We further explore the usefulness of the linear separability in the HAC space by demonstrating the use of a sliding window decoder for continuous speech recognition.", "title": "Detecting words in speech using linear separability in a bag-of-events vector space"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/burlick13_interspeech.html", "abstract": "As mobile devices, intelligent displays, and home entertainment systems permeate digital markets, the desire for users to interact through spoken and visual modalities similarly grows. Previous interactive systems limit voice activity detection (VAD) to the acoustic domain alone, but the incorporation of visual features has shown great improvement in performance accuracy. When employing both acoustic and visual (AV) information the central recurring question becomes \"how does one efficiently fuse modalities\". This work investigates the effects of different features (from multiple modalities), different classifiers, and different fusion techniques for the task of AV-VAD on data with varying acoustic noise. Furthermore we present a novel multi-tier classifier that combines traditional approaches, feature fusion and decision fusion, with independent modality classifiers and combined intermediary decisions with raw features as inputs to a second stage classifier. Our augmented multi-tier classification system concatenates the output of a set of base classifiers with the original fused features for a final classifier. Experiments over various noise conditions show average relative improvements of 5.0% and 4.1% on the CUAVE dataset and 2.6% and 11.1% on the MOBIO [2] dataset over majority voters and LDA respectively.", "title": "On the improvement of multimodal voice activity detection"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/geiger13b_interspeech.html", "abstract": "Overlapping speech is still a major cause of error in many speech processing applications, currently without any satisfactory solution. This paper considers the problem of detecting segments of overlapping speech within meeting recordings. Using an HMM-based framework recordings are segmented into intervals containing non-speech, speech and overlapping speech. New to this contribution is the use of linguistic information, where spoken content is used to improve overlap detection. Using language models for speech and overlap, an overlap score is created for every spoken word and used as an additional feature within the HMM framework. Experiments conducted on the AMI corpus demonstrate the potential of the proposed linguistic features.", "title": "Using linguistic information to detect overlapping speech"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/ye13_interspeech.html", "abstract": "This paper presents novel voice activity detection (VAD) approach based on incremental subspace learning using harmonicity-based features. Harmonic structure is well known as noise robust speech feature. We develop novel harmonicity-based feature based on temporal-spectral co-occurrence patterns. At statistical decision stage, many conventional statistical VAD methods rely on Gaussian model; however, owing to the non-Gaussian nature in speech, Gaussian model becomes faulty and produces incorrect VAD results. We reformulate the VAD by incremental subspace learning. The candid covariance-free incremental PCA (CCIPCA) subspace method is employed to adaptively model the input sound by a subspace. Subsequently, a speech activity measure can be established based on the distance from input sound to the adaptive subspace. Notably, the CCIPCA subspace update interval is set to 0.5 second in this work and the deviation distance is computed afterwards. In such short time scale, environmental sound present more Gaussian-like/stationary pattern and therefore can be well accommodated by adaptive subspace, conversely, speech always exhibit non-stationary characteristic which lead to distinct deviation to the adaptive acoustic subspace, and thus, can be effectively distinguished. We experimentally compared our scheme with various VAD methods over real-world data. The results validate the effectiveness of the proposed approach.", "title": "Incremental acoustic subspace learning for voice activity detection using harmonicity-based features"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/chung13_interspeech.html", "abstract": "In this paper, we discuss the possibility of applying weighted finite state transducer (WFST) as a unified framework to solve endpoint detection problem. In general, endpoint detection is composed of two cascaded decision processes. The first process is voice activity detection (VAD) which makes frame-level speech/non-speech classification. The second process is utterance-level detection which makes final decision with state transition control and heuristic knowledge. In recent, statistical model-based approach is common on VAD but rule-based logic is dominant on utterance-level detection. However, such an approach can cause some problems. First, it requires expert knowledge to define rules and it also requires sophisticate implementation to avoid confliction among them. Second, it can yield suboptimal performance because each process has to be dealt with independently. Therefore, in order to handle these problems by integrating the two processes, we propose WFST-based endpoint detection framework. The experimental result shows that the endpoint detection problem can be solved in a straightforward way under the proposed framework.", "title": "Endpoint detection using weighted finite state transducer"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/segbroeck13_interspeech.html", "abstract": "Reliable automatic detection of speech/non-speech activity in degraded, noisy audio signals is a fundamental and challenging task in robust signal processing. As various speech technology applications rely on the accuracy of a Voice Activity Detection (VAD) system for their effectiveness and robustness, the problem has gained considerable research interest over the years. It has been shown that in highly distorted conditions, an accurate segmentation of the target speech can be achieved by combining multiple feature streams. In this paper, we extract four one-dimensional streams each attempting to separate speech from the disturbing background by exploiting a different speech-related characteristic, i.e. (i) the spectral shape, (ii) spectro-temporal modulations, (iii) the periodicity structure due to the presence of pitch harmonics, and (iv) the long-term spectral variability profile. The information from these streams is then expanded over long duration context windows and applied to the input layer of a standard Multilayer Perceptron classifier. The proposed VAD was evaluated on the DARPA RATS corpora and shows to be very competitive to current state-of-the art systems.", "title": "A robust frontend for VAD: exploiting contextual, discriminative and spectral cues of human voice"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/graciarena13_interspeech.html", "abstract": "Speech activity detection (SAD) on channel transmissions is a critical preprocessing task for speech, speaker and language recognition or for further human analysis. This paper presents a feature combination approach to improve SAD on highly channel degraded speech as part of the Defense Advanced Research Projects Agency's (DARPA) Robust Automatic Transcription of Speech (RATS) program. The key contribution is the feature combination exploration of different novel SAD features based on pitch and spectro-temporal processing and the standard Mel Frequency Cepstral Coefficients (MFCC) acoustic feature. The SAD features are: (1) a GABOR feature representation, followed by a multilayer perceptron (MLP); (2) a feature that combines multiple voicing features and spectral flux measures (Combo); (3) a feature based on subband autocorrelation (SAcC) and MLP postprocessing and (4) a multiband comb-filter F0 (MBCombF0) voicing measure. We present single, pairwise and all feature combinations, show high error reductions from pairwise feature level combination over the MFCC baseline and show that the best performance is achieved by the combination of all features.", "title": "All for one: feature combination for highly channel-degraded speech activity detection"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/coz13_interspeech.html", "abstract": "On this paper we present a new approach for the localisation of superposed speech areas. The system is based on the frequency tracking of speech segments following the evolution of the main amplitude frequencies and uses no learning of acoustic or prosodic models. The set of trackings of the frequencies are then grouped together using a distance based on the harmonicity, each group being the production of a single speaker. The co-occurrence of different harmonic groups is then used as a consequence of the presence of multiple speakers. Our method has been evaluated on the data of the French ANR evaluation campaign ETAPE, showing the usability of this approach.", "title": "Superposed speech localisation using frequency tracking"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/tsiartas13_interspeech.html", "abstract": "In this paper, we propose robust features for the problem of voice activity detection (VAD). In particular, we extend the long term signal variability (LTSV) feature to accommodate multiple spectral bands. The motivation of the multiband approach stems from the non-uniform frequency scale of speech phonemes and noise characteristics. Our analysis shows that the multi-band approach offers advantages over the single band LTSV for voice activity detection. In terms of classification accuracy, we show 0.3%.61.2% relative improvement over the best accuracy of the baselines considered for 7 out 8 different noisy channels. Experimental results, and error analysis, are reported on the DARPA RATS corpora of noisy speech.", "title": "Multi-band long-term signal variability features for robust voice activity detection"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/lezzoum13_interspeech.html", "abstract": "In this paper, a Voice Activity Detector (VAD) is proposed for smart hearing protection applications where speech is to get through the hearing protector while ambient noise is to be blocked out. The VAD calculates a short-term statistical assessment of the temporal envelopes within different frequency bands. This assessment uses the Inter-Quartile Range (IQR) and reflects the dispersion of the envelopes' magnitudes. The VAD's decision is made using two threshold comparison rules and a hangover scheme triggered after a given number of observations. These four parameters have been optimized off-line using a genetic algorithm approach. The performance of the proposed VAD is compared to Sohn's VAD using a database of 90 speech signals corrupted by five real-world noise environments at Signal-to-Noise ratios (SNR) varying from 0 to +10 dB. Results show that the proposed VAD performs better than Sohn's VAD with an 85.9% (compared to 77.5%) F1 score averaged across all SNRs and also minimizes by a factor of three the mid-speech clipping rate. In addition, the evaluation of the proposed VAD's computational cost shows that its implementation on-board a low-power low-consumption DSP is very feasible and would enable smart hearing protection for hypersensitive persons.", "title": "A low-complexity voice activity detector for smart hearing protection of hyperacusic persons"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/ryant13_interspeech.html", "abstract": "Speech activity detection (SAD) is an important first step in speech processing. Commonly used methods (e.g., frame-level classification using gaussian mixture models (GMMs)) work well under stationary noise conditions, but do not generalize well to domains such as YouTube, where videos may exhibit a diverse range of environmental conditions. One solution is to augment the conventional cepstral features with additional, hand-engineered features (e.g., spectral flux, spectral centroid, multiband spectral entropies) which are robust to changes in environment and record- ing condition. An alternative approach, explored here, is to learn robust features during the course of training using an appropriate architecture such as deep neural networks (DNNs). In this paper we demonstrate that a DNN with input consisting of multiple frames of mel frequency cepstral coefficients (MFCCs) yields drastically lower frame-wise error rates (19.6%) on YouTube videos compared to a conventional GMM based system (40%).", "title": "Speech activity detection on youtube using deep neural networks"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/germain13_interspeech.html", "abstract": "Voice activity detection (VAD) in the presence of heavy, nonstationary noise is a challenging problem that has attracted attention in recent years. Most modern VAD systems require training on highly specialized data: either labeled mixtures of speech and noise that are matched to the application, or, at the very least, noise data similar to that encountered in the application. Because obtaining labeled data can be a laborious task in practical applications, it is desirable for a voice activity detector to be able to perform well in the presence of any type of noise without the need for matched training data. In this paper, we propose a VAD method based on non-negative matrix factorization. We train a universal speech model from a corpus of clean speech but do not train a noise model. Rather, the universal speech model is sufficient to detect the presence of speech in noisy signals. Our experimental results show that our technique is robust to a variety of non-stationary noises mixed at a wide range of signal-to-noise ratios and significantly outperforms baseline algorithms.", "title": "Speaker and noise independent voice activity detection"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/tsai13_interspeech.html", "abstract": "This paper uses an unconventional analysis as a tool to diagnose the problems with three different speech activity detection systems. The unconventional analysis is to score the frames in an audio file in order of confidence, starting with the frame that we have the most confidence in and progressing towards less and less confident frames. By keeping track of the cumulative number of errors, we can determine how the errors are distributed across the data. Using speech activity detection on highly degraded audio as a case example, we show how this simple analysis can yield useful insight into both system performance and the data itself. In our case example, we use the analysis to establish three main points. First, a small percentage of the frames account for a lion\u0081fs share of the errors. Second, three different systems perform very poorly on the same small subset of data . despite the fact that the systems adopt very different decoding algorithms and features. In other words, three very different systems agree on which data is \u0081ehard\u0081f. Third, the \u0081ehard\u0081f data is primarily characterized by its proximity to speech-nonspeech boundaries. Through follow-up analyses, we show that this phenomenon is not merely an artifact of ground truth inaccuracy, but rather a steady progression of the data becoming harder and harder to classify correctly as one moves closer to the boundaries. Through this case example, we demonstrate the utility of confidence-based scoring as a general diagnostic tool for detection tasks on time-series data.", "title": "Confidence-based scoring: a useful diagnostic tool for detection tasks"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/kanai13_interspeech.html", "abstract": "Voice activity detection (VAD) is mainly used to detect speech/nonspeech periods in observed noisy signals. The detected periods are used to reduce noise components or enhance speech components in noisy speech. However, current VAD techniques have serious problems in that the accuracy of detection of speech/non-speech periods drastically reduces if they are used for noisy speech and/or for mixtures of non-speech such as those in musical and environmental sounds. Thus, VAD needs to be robust to enable speech periods to be accurately detected in these situations. This paper proposes concurrent processing of VAD and noise reduction (NR) using empirical mode decomposition (EMD) and modulation spectrum analysis (MSA) to simultaneously resolve these problems. The proposed method effectively works on reducing stationary background noise by using EMD without estimating SNR (noise conditions), and then on reducing non-stationary noise including non-speech components by using MSA while this is determining speech/non-speech periods by thresholding the noise-reduced speech. Three experiments on VAD/NR in real environments were conducted to evaluate the proposed method by comparing it with typical methods (Otsu's method, G.729B, and AMR) and our previous methods. The results demonstrated that the proposed method could accurately detect speech/non-speech periods and effectively reduce noise components simultaneously.", "title": "Concurrent processing of voice activity detection and noise reduction using empirical mode decomposition and modulation spectrum analysis"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/almoubayed13_interspeech.html", "abstract": "In this demonstrator we present the Furhat robot head. Furhat is a highly human-like robot head in terms of dynamics, thanks to its use of back-projected facial animation. Furhat also takes advantage of a complex and advanced dialogue toolkits designed to facilitate rich and fluent multimodal multiparty human-machine situated and spoken dialogue. The demonstrator will present a social dialogue system with Furhat that allows for several simultaneous interlocutors, and takes advantage of several verbal and nonverbal input signals such as speech input, real-time multi-face tracking, and facial analysis, and communicates with its users in a mixed initiative dialogue, using state of the art speech synthesis, with rich prosody, lip animated facial synthesis, eye and head movements, and gestures.", "title": "The furhat social companion talking head"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/gelin13_interspeech.html", "abstract": "In this paper, we introduce NAO, the humanoid robot developed by Aldebaran Robotics. For this robot, dedicated to be a companion, the interaction with humans is crucial and the audition appears to be a very important sense while the robotic community has been focusing on vision for a long time. Some of the developments on audio are presented.", "title": "Audition: the most important sense for humanoid robots?"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/hueber13_interspeech.html", "abstract": "This paper introduces Ultraspeech-player, a software dedicated to the visualization of ultrasound and video sequences of the tongue and lips. This software is designed for speech therapy and pronunciation training applications and aims at increasing the articulatory awareness of the learner. Ultraspeech-player includes an audiovisual time-stretching algorithm allowing the user to slow-down in real-time both the articulatory gesture and its corresponding acoustic realization. This rendering technique aims at improving the way a naive speaker perceives and understands an articulatory gesture.", "title": "Ultraspeech-player: intuitive visualization of ultrasound articulatory data for speech therapy and pronunciation training"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/oh13b_interspeech.html", "abstract": "Laughing while speaking, also referred to as speech-laugh, occurs frequently in social conversations. In order to understand how laughter influences the acoustics of its co-occurring speech signal, we take a synthesis approach in designing an interactive system for artificial \"laughter modulation\": users input an arbitrary speech signal, and the system processes the signal to yield acoustic patterns characteristic of a speech-laugh. Implemented using the ChucK audio programming language, our prototype allows for real-time manipulation of modulation parameters for rapid experimentation. This paper describes key components of our prototype, to be demonstrated at the 2013 Interspeech Show & Tell. This synthesis-based approach to speech-laughs may serve as a starting point for the future of affective machine-generated speech.", "title": "Laughter modulation: from speech to speech-laugh"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/bikel13_interspeech.html", "abstract": "ReFr (http://refr.googlecode.com) is a software architecture for specifying, training and using reranking models, which take the n-best output of some existing system and produce new scores for each of the n hypotheses that potentially induce a different ranking, ideally yielding better results than the original system. The Reranker Framework has some special support for building discriminative language models, but can be applied to any reranking problem. The framework is designed with parallelism and scalability in mind, being able to run on any Hadoop cluster out of the box. While extremely efficient, ReFr is also quite flexible, allowing researchers to explore a wide variety of features and learning methods. ReFr has been used for building state-of-the-art discriminative LM's for both speech recognition and machine translation systems.", "title": "Refr: an open-source reranker framework"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/sosi13_interspeech.html", "abstract": "This demo demonstrates a small and portable system embedding microphone array processing and robust speech recognition for distant-speech interaction to control a floor lamp. The system is entirely contained inside the lamp and operates in real-time, \"always-listening\" mode. It runs on a small, low-power, fanless board and acts as the light control interface. The prototype shows the feasibility, potential and limits of the integration of speech technology in devices of everyday use.", "title": "Embedding speech recognition to control lights"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/meltzner13_interspeech.html", "abstract": "sEMG based silent speech recognition has become a desirable communication modality because it has the potential to provide natural, covert, hands-free communication in acoustically challenging environments. To enable this capability, we have developed a portable, self-contained, Android based Mouthed-speech Understanding and Transcription Engine (MUTE) system. To demonstrate the MUTE system's ability to recognize a continuous vocabulary of 210 words we propose to conduct map task based demonstration, in which a MUTE user guides a \"listener\" around a schematized map. The listener draws out a map based on the received instructions; a comparison of the original and drawn map then illustrates the MUTE system's recognition performance.", "title": "The MUTE silent speech recognition system"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/scobbie13_interspeech.html", "abstract": "The DoubleTalk articulatory corpus was collected at the Edinburgh Speech Production Facility (ESPF) using two synchronized Carstens AG500 electromagnetic articulometers. The first release of the corpus comprises orthographic transcriptions aligned at phrasal level to EMA and audio data for each of 6 mixed-dialect speaker pairs. It is available from the ESPF online archive. A variety of tasks were used to elicit a wide range of speech styles, including monologue (a modified Comma Gets a Cure and spontaneous story-telling), structured spontaneous dialogue (Map Task and Diapix), a wordlist task, a memory-recall task, and a shadowing task. In this session we will demo the corpus with various examples.", "title": "The edinburgh speech production facility doubletalk corpus"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/sityaev13_interspeech.html", "abstract": "This paper describes a new platform called Lexee developed as part of a cloud-based Angel offering which allows to rapidly develop voice-enabled mobile applications. The solution allows to rapidly build application flows without any significant investments into software or extensive knowledge of programming. The Lexee SDK described in this paper gives developers a starting point to create their own multi-modal mobile applications.", "title": "Lexee: a cloud-based platform for building and deploying voice-enabled mobile applications"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/ouni13_interspeech.html", "abstract": "During this show & Tell event, we present the latest version of our new software VisArtico. The software allows visualizing electro- magnetic articulography (EMA) data. It is intended for researchers that need to visualize the data acquired from an articulograph with no excessive processing. This software is very useful for the speech science community, makes the use of articulatory data more accessible and improves the understanding of speech production.", "title": "Visualizing articulatory data with VisArtico"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/soury13_interspeech.html", "abstract": "We present the implementation of a data collection tool of multicultural and multi-modal laughter for the 14th Interspeech conference. The application will automatically record and analyze audio and video stream to provide real-time feedback. Using this tool, we expect to collect multimodal cues of different kind of laughers elicited in participants with funny videos, as well as jokes and tongue-twisters games with the Nao robot. The collected corpus will be used for paralinguistic challenges.", "title": "A tool to elicit and collect multicultural and multimodal laughter"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/schleicher13_interspeech.html", "abstract": "In this paper, we describe the development of a mobile phone app for supporting participants and organizers of Interspeech conferences. Based on a survey amongst future organizers and attendees, we identified the most relevant functionalities and implemented an initial set of them on two popular platforms, iOS and Android. The app is meant as an open tool to be handled under the auspices of ISCA, and can be extended with speech and language functionalities in the future. This way, we hope to turn it into a community platform which can be used for experimenting with new speech technologies on site. A first version of this app will be presented at Interspeech 2013.", "title": "Design of a mobile app for interspeech conferences: towards an open tool for the spoken language community"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/metze13_interspeech.html", "abstract": "This paper describes the \"Speech Recognition Virtual Kitchen\" environment which has the goals to promote community sharing of research techniques, foster innovative experimentation, and provide solid reference systems as a tool for education, research, and evaluation with a focus on, but not restricted to, speech and language research. The core of the research infrastructure is the use of Virtual Machines (VMs) that provide a consistent environment for experimentation. We liken the virtual machines to a \"kitchen\" because they provide the infrastructure into which one can install \"appliances\" (e.g., speech recognition tool-kits), \"recipes\" (scripts for creating state-of-the art systems), and \"ingredients\" (language data). A web-based community platform complements the VMs, to allow physically disconnected users to jointly explore VMs, learn from each other, and collaborate in research. In this demo, we present initial VMs that were mostly used for teaching classes at Carnegie Mellon and Ohio State University, and solicit feedback for an initial \"hub\"-style web-site.", "title": "The speech recognition virtual kitchen"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/chen13c_interspeech.html", "abstract": "It is now commonplace to use web conferencing technology in order to hold meetings between participants situated in different physical locations. A drawback of this technology is that nearly all of the interaction between these participants is monolingual. Here, we demonstrate a novel form of this technology that enables cross-lingual speech-to-speech communication between conference participants in real time. We model this translation problem as a combination of incremental speech recognition and segmentation, addressing the question of finding which segmentation strategy maximizes translation accuracy while minimizing latency. Our demonstration takes the form of a web conferencing scenario where a presenter speaks in one language while talk participants listen to or read the speaker's translated texts in real time. This system is flexible enough to allow real-time translation of technical talks or speeches covering broad topics.", "title": "Multilingual web conferencing using speech-to-speech translation"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/ferragne13_interspeech.html", "abstract": "ROCme! . Recording of Oral Corpora Made Easy . has been designed to allow a sensible, autonomous, and dematerialized management of speech recordings. Users can create forms for the collection of metadata. Speakers autonomously fill in questionnaires, record, play, and save audio; and browse sentences (or other types of corpora) by clicking on buttons or using keyboard shortcuts. ROCme! can display text, optionally with HTML and CSS formatting, images, and play sounds or videos.", "title": "ROCme! software for the recording and management of speech corpora"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/burkhardt13_interspeech.html", "abstract": "We present several Android apps that deal with voice based access to internet content on mobile apps, namely AskWiki, AutoScout24 search and the TV-guide app. They highlight several aspects of voice search strategies to match user query with vocabularies based on controlled, semi-controlled and linked open databases.", "title": "Voice search in mobile applications with the rootvole framework"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/novak13_interspeech.html", "abstract": "On-line audio dilation is a technique that time stretches, or slows down the tempo of audio signals as they are generated. This paper presents an on-line dilation technique and our ongoing research to assess the effects of audio dilation on speech communication using an interactive DIAPIX problem solving task.", "title": "On-line audio dilation for human interaction"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/mowlaee13_interspeech.html", "abstract": "Many short-time Fourier transform (STFT) based single-channel speech enhancement algorithms are focused on estimating clean speech spectral amplitude from the noisy observed signal in order to suppress the additive noise. To this end, the state-of-the-art speech enhancement algorithms, employ noisy amplitude information and correspondingly a priori and a posteriori SNRs while they use no information about the speech phase spectrum. In this show and tell we demonstrate our recent progress on developing novel ideas towards incorporating phase information in solving single-channel speech enhancement problem.", "title": "Phase-aware single-channel speech enhancement"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/hirano13_interspeech.html", "abstract": "We have developed the very first free online and free framework for teaching and learning Japanese prosody with features for word accent and phrase intonation. This framework is called OJAD (Online Japanese Accent Dictionary), which provides three functions. 1) Visual, auditory, systematic, and comprehensive illustration of patterns of accent change (accent sandhi) of verbs and adjectives. Here only the changes resulting from twelve kinds of fundamental conjugation are focused upon. 2) Visual illustration of the accent pattern of a given verbal expression, which is a combination of a verb and its postpositional auxiliary words. 3) Visual illustration of the pitch pattern of an any given sentence and the expected positions of accent nuclei in the sentence. The third function is implemented by using an accent change prediction module that we developed for Japanese text-to-speech (TTS) synthesizers. Subjective assessment by teachers shows very high pedagogical effectiveness of the framework.", "title": "A free online accent and intonation dictionary for teachers and learners of Japanese"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/astrinaki13_interspeech.html", "abstract": "In this paper, we present our first prototype system for interactive accent control using HMM-based speech synthesis. In this application, voices in various English accents including American, Canadian and British, are controlled and interpolated by the user using gestures acquired via an interactive geographical map in real time. Users can choose the gender of the voices to manipulate and the accent interpolation strategy: either interpolation of all available speakers within an area or the N-nearest speakers around a point.", "title": "Reactive accent interpolation through an interactive map application"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/berkling13_interspeech.html", "abstract": "In Germany, international and national comparative studies such as PISA or IGLU have shown that some 20% of school children do not reach the minimal competence level as defined by PISA literacy stages by the age of 15. Such a large number of students can no longer be supported by a single teacher in a classroom. The proposed submission demonstrates a web application that allows non-experts to interact with highly complex underlying algorithms in order to obtain personalized spelling diagnosis in minutes and keep track of error profiles over time. Furthermore, usage of the application will result in a large data collection to support research on long-term studies of written language acquisition that do not yet exist in that form. Even though there is no companion paper at this conference, previous published work has lead up to this demonstration. The underlying algorithms were published at SLaTE 2011 and preliminary studies regarding effective data display were published at WOCCI 2012.", "title": "A non-experts user interface for obtaining automatic diagnostic spelling evaluations for learners of the German writing system"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/clark13_interspeech.html", "abstract": "The Simple4All project enables the building of speech synthesis systems automatically from data with little or no expert supervision, thus creating speech synthesisers for new languages or domains in a quick, easy and pain-free way. Whilst substantial effort has been invested over the years into developing novel and progressively more natural sounding speech synthesis techniques, there has always been a high entry barrier to building a speech synthesis system in a new language, because conventional methods require expensive linguistic resources, detailed expert knowledge, and a high degree of skill in system \u0081etuning\u0081f. The result is that only a very few of the world's languages have speech synthesis systems. For those less widely-spoken languages that do have a system available, the quality of the synthetic speech is not very good, because of the lack of resources available and the lack of skills to \u0081etune\u0081f such systems. The main objectives of Simple4All are to address this imbalance by providing Open Source tools to: . enable the creation of a voice for any domain in any language, . provide an end-to-end framework in which these voices can be automatically built and improved, . produce natural and expressive speech synthesis, . provide feedback-driven learning to improve systems.", "title": "Simple4all"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/pointeau13_interspeech.html", "abstract": "In order to be able to understand a conversation in interaction, a robot, has to first understand the language used by his interlocutor. A central aspect of language learning is adaptability. Individuals can learn new words and new grammatical structures. We have developed learning methods that allow the humanoid robot iCub to robot can learn new lexical items by interaction with the human and consolidation of its autobiographical memory. Then, based on these open class words, the robot can bootstrap the acquisition of novel grammatical structures in real-time. Finally, we demonstrate how human gaze can be monitored, and could be used in order to reduce referential ambiguity inherent in such learning conditions. These learning capabilities are demonstrated in a collection of videos.", "title": "On-line learning of lexical items and grammatical constructions via speech, gaze and action-based human-robot interaction"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/miyakoda13_interspeech.html", "abstract": "In this demonstration, we present the auditory-visual pronunciation system that we have developed. One of the key features of this system is that it employs easy-to-understand visuals of the speech organ that can be seen from different angles. In addition, the internal organs in movement can also be presented by changing the mode to transparent. Furthermore, unlike most systems that can present only the ideal model movements of the speech organs, our system allows users to freely adjust the tongue and jaw movements by controllers. This allows instructors, for example, to visually indicate and point out the deviant movement(s) of the learners so that the learners themselves can understand their present state (i.e. problems) with the help of visual information and feedback.", "title": "Development of a pronunciation training system based on auditory-visual elements"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/azarov13c_interspeech.html", "abstract": "Two speech processing systems have been developed for real-time and non-real-time voice conversion. Using the real-time processing the user can apply conversion during voice over IP (VoIP) calls imitating identity of a specified target speaker. Non-real-time processing system converts prerecorded audio books read by a professional reader imitating voice of the user. Both systems require some speech samples of the user for training. The training procedures are similar for both systems however the user is considered as a source speaker in the first case and as a target speaker in the second. For parametric representation of speech we use a speech model based on instantaneous harmonic parameters with multicomponent sinusoidal excitation. The voice conversion itself is made using artificial neural networks (ANN) with rectified linear units. Here we demonstrate implementations of the voice conversion systems with dedicated web interfaces and iPhone application.", "title": "Real-time and non-real-time voice conversion systems with web interfaces"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/csala13_interspeech.html", "abstract": "This demo presents some acts of a potential application of NAO humanoid robots under special circumstances in a hospital, including results of experiments performed at the Children's Hematology and Stem Cell Transplantation Unit of Szent Laszlo Hospital in Budapest, Hungary. Children in this unit are forced to live in small 2\u0081~3 m sterile boxes where NAO can be a good companion to cheer them up and break their usual daily routine with performances and exercise.", "title": "Application of the NAO humanoid robot in the treatment of bone marrow-transplanted children (demo)"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/wan13_interspeech.html", "abstract": "A controllable computer animated avatar that could be used as a natural user interface for computers is demonstrated. Driven by text and emotion input, it generates expressive speech with corresponding facial movements. To create the avatar, HMM-based text-to-speech synthesis is combined with active appearance model (AAM)-based facial animation. The novelty is the degree of control achieved over the expressiveness of both the speech and the face while keeping the controls simple. Controllability is achieved by training both the speech and facial parameters within a cluster adaptive training (CAT) framework. CAT creates a continuous, low dimensional eigenspace of expressions, which allows the creation of expressions of different intensity (including ones more intense than those in the original recordings) and combining different expressions to create new ones. Results on an emotion-recognition task show that recognition rates given the synthetic output are comparable to those given the original videos of the speaker.", "title": "Photo-realistic expressive text to talking head synthesis"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/maddieson13_interspeech.html", "abstract": "LAPSyD, the Lyon-Albuquerque Phonological Systems Database, is an online phonological database equipped with powerful query, mapping and visualization tools. It stems from the UPSID and WALS databases, enhanced with newly validated data not only covering segmental inventories but also syllable structures, stress and tonal systems. In its current version it covers around 700 languages and it is accessible at http://www.lapsyd.ddl.ish-lyon.cnrs.fr. This Show & Tell aims at providing an overview of the database content and features and demonstrating the richness of the queries and visualization tools available to the user.", "title": "Demonstration of LAPSyd: lyon-albuquerque phonological systems database"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/boyce13_interspeech.html", "abstract": "One area of voice research that has historically been understudied is the interaction between voice pathology and acoustic aspects of the speech signal that affect intelligibility. Landmark-based software tools are particularly suited to fast, automatic analysis of small, non-lexical differences in the acoustic signal reflecting the production of speech. We are building a tool set that provides fast, automatic summary statistics for measures of speech acoustics based on Stevens' paradigm of landmarks, points in an utterance around which information about articulatory events can be extracted. This paper explores the use of landmark analysis for evaluation of intelligibility-based measures of vocal pathology.", "title": "Speechmark acoustic landmark tool: application to voice pathology"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/catanese13_interspeech.html", "abstract": "MODIS is a free speech and audio motif discovery software developed at IRISA Rennes. Motif discovery is the task of discovering and collecting occurrences of repeating patterns in the absence of prior knowledge, or training material. MODIS is based on a generic approach to mine repeating audio sequences, with tolerance to motif variability. The algorithm implementation allows to process large audio streams at a reasonable speed where motif discovery often requires huge amount of time.", "title": "MODIS: an audio motif discovery software"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/eriksson13_interspeech.html", "abstract": "The study presented here is one in a series of studies intended to describe the acoustics of word stress for several typologically different languages in a common framework. The idea is that, when fully developed the methodology should be applicable to any language in the same way regardless of prosodic type. The languages included in the present round of data collection and analyzes are Brazilian Portuguese, English, Estonian, French, Italian and Swedish. The acoustic variables examined here are F0-level, F0-variation, Duration, and Spectral Emphasis for all vowels in the data. All parameters are tested with respect to their correlation with stress level (primary, secondary, unstressed), speaking style (wordlist reading, phrase reading, spontaneous speech) and tonal word accent. The most robust results concerning stress level are found for Duration and F0-variation. Speaking style turned out to play a minimal role. The only robust effect was found for duration which was longer in word list reading. Word accent had a significant effect on F0-variation, and Duration.", "title": "The acoustics of word stress in Swedish: a function of stress level, speaking style and word accent"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/michelas13_interspeech.html", "abstract": "While recent crosslinguistic studies have shown that the degree of speaker's commitment or certainty is encoded intonationally either in a gradient or categorical fashion, our understanding of how French speakers use Intonational-Phrase (IP) final contours to signal their degree of certainty is limited. This paper investigates the contribution of a penultimate peak contour in French to convey speakers' uncertainty. Participants read target sentences in a neutral vs. incredulity declarative question context. Prosodic annotation revealed that incredulity declarative questions consistently exhibited the presence of an additional F0 peak in the penultimate syllable of the IP which was unaccented. The acoustic analyses showed that the H tone of the unaccented penultimate peak was not downstepped and thus approximately scaled to the same height of the last pitch accent. The findings of this study provide the first quantitative description of a phonological contrast between H*H% and H+ H*H% to signal speakers' certainty in declarative questions in French.", "title": "Intonational contrasts encode speaker's certainty in neutral vs. incredulity declarative questions in French"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/ishimoto13_interspeech.html", "abstract": "In this paper we aim to clarify that participants of conversation can predict whether an utterance includes grammatical terminal elements, which have been referred to as the utterance-final elements (UFEs) in Japanese. We carried out perceptual experiments with Japanese utterances missing a part close to their end. The results showed that subjects distinguished whether an UFE follows at the verb before the appearance of the UFE, and they noticed that the end of the utterance arrives sooner or has already arrived even if the last mora is missing when the utterance includes the UFEs. Then, we analyzed the prosodic differences between the utterances with/without the UFE. The results presented the following information. The F0 declines gradually toward the end-of-utterance, the final lowering of the F0s remarkably occurs at the UFE, the power falls at the verb if the utterance does not have the UFE, and the power falls at the UFE if the utterance has the UFEs. That is, the F0 declination towards the end-of-utterance and the power falling at the verb are pre-announcing the syntactic completion point in the utterances to the hearers.", "title": "Prosodic changes pre-announcing a syntactic completion point in Japanese utterance"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/simard13_interspeech.html", "abstract": "This paper examines the prosodic encoding of sentence types in Jaminjung, a language of Northern Australia. Analyses cover the description of the contours as well as a systematic acoustic analysis, comprising the measurements of F0, duration, pitch excursion and velocity for each syllable in datasets carefully selected from spontaneous speech. Results show that declaratives and imperatives receive a falling contour; interrogatives, either polar or wh questions, can have one of three contours: falling, fall-rise, marked by a rise on the last syllable, or rising. A test on the F0 measurements of each sentence type with a falling contour reveals that they are in effect distinguished by pitch register, ranging from higher to lower, from imperatives to polar questions, to wh questions and statements. Hence, contour shape alone is not sufficient to describe the encoding of sentence types in Jaminjung: overall pitch register is also used. We will argue for the usefulness of instrumental phonetic investigations in describing lesser-known languages and also to enhance our understanding of sentence type characterization in a typological perspective.", "title": "Prosodic encoding of declarative, interrogative and imperative sentences in jaminjung, a language of australia"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/vullinghs13_interspeech.html", "abstract": "Bilingual participants offer a unique opportunity to study how concepts and their relations are stored in the brain. We focus on this in the context of the spoken production of referring expressions. Previous work on reference in interaction showed that speakers adapt their descriptions to those produced earlier in the interaction. For example, when participants hear a description that contains a dispreferred attribute (such as orientation in \"the chair seen from the side\"), they were more likely to use that attribute in future references. The adaptation found here is claimed to take place at the conceptual level (i.e., participants adapt to the attribute \"orientation\" and not to the value \"seen from the side\"). However, so far, convincing evidence of this claim has been lacking, because it has proven difficult to rule out adaptation at the lexical or syntactic level. A crosslinguistic study was set up to provide evidence for conceptual adaptation. In our study, Spanish/Dutch bilinguals listened to Spanish descriptions that all used postmodifiers and referred in Dutch using premodifiers. The results showed that, even without syntactic or lexical cues, speakers adapted their Dutch descriptions with the (Spanish) attributes they had listened to, providing evidence for adaptation at the conceptual level.", "title": "Crosslinguistic priming in interactive reference: evidence for conceptual alignment in speech production"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/kousidis13_interspeech.html", "abstract": "That speakers take turns in interaction is a fundamental fact across languages and speaker communities. How this taking of turns is organised is less clearly established. We have looked at interactions recorded in the field using the same task, in a set of three genetically and regionally diverse languages: Georgian, Cabecar, and Fongbe. As in previous studies, we find evidence for avoidance of gaps and overlaps in floor transitions in all languages, but also find contrasting differences between them on these features. Further, we observe that interlocutors align on these temporal features in all three languages. (We show this by correlating speaker averages of temporal features, which has been done before, and further ground it by ruling out potential alternative explanations, which is novel and a minor methodological contribution.) The universality of smooth turn-taking and alignment despite potentially relevant grammatical differences suggests that the different resources that each of these languages make available are nevertheless used to achieve the same effects. This finding has potential consequences both from a theoretical point of view as well as for modeling such phenomena in conversational agents.", "title": "A cross-linguistic study on turn-taking and temporal alignment in verbal interaction"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/grais13_interspeech.html", "abstract": "In this work, we introduce a new discriminative training method for nonnegative dictionary learning. The new method can be used in single channel source separation (SCSS) applications. In SCSS, nonnegative matrix factorization (NMF) is used to learn a dictionary (a set of basis vectors) for each source in the magnitude spectrum domain. The trained dictionaries are then used in decomposing the mixed signal to find the estimate for each source. Learning discriminative dictionaries for the source signals can improve the separation performance. To achieve discriminative dictionaries, we try to avoid the bases set of one source dictionary from representing the other source signals. We propose to minimize cross-coherence between the dictionaries of all sources in the mixed signal. We incorporate a simplified cross-coherence penalty using a regularized NMF cost function to simultaneously learn discriminative and reconstructive dictionaries. The new regularized NMF update rules that are used to discriminatively train the dictionaries are introduced in this work. Experimental results show that using discriminative training gives better separation results than using conventional NMF.", "title": "Discriminative nonnegative dictionary learning using cross-coherence penalties for single channel source separation"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/kim13b_interspeech.html", "abstract": "We propose a novel method of pitch track correction that uses an ensemble Kalman filter to improve the performance of monaural speech segregation. The proposed method considers all reliable pitch streaks for pitch track correction, whereas the conventional segregation approach relies on only the longest streak in a given speech stream. In addition, unreliable pitch streaks are corrected with an ensemble Kalman filter that uses autocorrelation functions as noisy observations for the hidden true pitch values. Our proposed approach provides more accurate pitch estimation, thus improving speech segregation performance for various types of noises, in particular, colored noise. In speech segregation experiments on mixtures of speech and various competing noises, the proposed method demonstrated superior performance to the conventional approach.", "title": "Monaural speech segregation based on pitch track correction using an ensemble kalman filter"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/tran13b_interspeech.html", "abstract": "A simple and low computational complexity system for bi-speaker speech separation is proposed in this paper. The system is constructed of a voice activity classification (VAC) module and an adaptive bi-beamformer module for speech separation using a microphone array. The first module identifies active speaker(s) and allows the system to control the adaptation of the second module automatically. The VAC is based on a novel classification method containing two steps. The first step uses a robust VAC method based on our previous work on beamformer-output-ratio of a bi-beamforming system. The second step refines the VAC results using a novel method derived from an analytical result on the output power of an adaptive beamformer. The system is tested in reverberant environments with both synthesized and real recordings. The synthesized recordings contain two speakers, a background speech and noises. The real recording contains two speakers speaking spontaneously. The VAC results satisfy a conservative classification scheme to avoid the signal cancellation problem. The final separation outputs are compared with the ideal outputs provided by genie-aided adaptive beamformers which have perfect VAC knowledge. The results show that the propose automatic system achieves high performance close to the ideal system.", "title": "Voice activity classification for automatic bi-speaker adaptive beamforming in speech separation"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/kinoshita13_interspeech.html", "abstract": "Distributed microphone array (DMA) processing has recently been gathering increasing research interest due to its various applications and diverse challenges. In many conventional multi-channel speech enhancement algorithms that use co-located microphones, such as the multi-channel Wiener filtering and mask-based blind source separation (BSS) approaches, statistics of the target and interference signals are required if we are to design an optimal enhancement filter. To obtain such statistics, we estimate activity information regarding source and interference signals (hereafter, source activity information), that is generally assumed to be common to all the microphones. However, in DMA scenarios, the source activities observable at any given microphone may be significantly different from those of others when the microphones are spatially distributed to a great degree, and the level of each signal at each microphone varies significantly. Thus, to capture such source activity information appropriately and thereby achieve optimal speech enhancement in DMA environments, in this paper we propose an approach for estimating microphone-dependent source activity, and for performing blind source separation based on such information. The proposed method estimates the activity of each source signal at each microphone, which can be explained by the microphone-independent speech log power spectra and microphone-location dependent source gains. We introduce a probabilistic formulation of the proposed method, and an efficient algorithm for model parameter estimation. We show the efficacy of the proposed method experimentally in comparison with conventional methods in various DMA scenarios.", "title": "Blind source separation using spatially distributed microphones based on microphone-location dependent source activities"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/barker13_interspeech.html", "abstract": "This paper proposes an algorithm for separating monaural audio signals by non-negative tensor factorisation of modulation spectrograms. The modulation spectrogram is able to represent redundant patterns across frequency with similar features, and the tensor factorisation is able to isolate these patterns in an unsupervised way. The method overcomes the limitation of conventional nonnegative matrix factorisation algorithms to utilise the redundancy of sounds in frequency. In the proposed method, separated sounds are synthesised by filtering the mixture signal with a Wiener-like filter generated from the estimated tensor factors. The proposed method was compared to conventional algorithms in unsupervised separation of mixtures of speech and music. Improved signal to distortion ratios were obtained compared to standard non-negative matrix factorisation and non-negative matrix deconvolution.", "title": "Non-negative tensor factorisation of modulation spectrograms for monaural sound source separation"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/watanabe13_interspeech.html", "abstract": "Partial phase reconstruction based on a confidence domain has recently been shown to provide improved signal reconstruction performance in a single-channel source separation scenario. In this paper, we replace the previous binarized fixed-threshold confidence domain with a new signal-dependent one estimated by employing a sinusoidal model to be applied on the estimated magnitude spectrum of the underlying sources in the mixture. We also extend the sinusoidal-based confidence domain into Multiple Input Spectrogram Inversion (MISI) framework, and we propose to re-distribute the remixing error at each iteration on the sinusoidal-signal components. Our experiments on both oracle and estimated spectra show that the proposed method achieves improved separation results at a lower number of iterations, making it as a favorable choice for faster phase estimation.", "title": "Iterative sinusoidal-based partial phase reconstruction in single-channel source separation"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/yao13_interspeech.html", "abstract": "We focus on variations in the aerodynamics of airflow patterns in the laryngeal ventricle and the false vocal folds based on a physical model for the classification of neutral and stressed speech. We modify the two-mass model to include the laryngeal ventricle, and the physical parameters characterizing airflow variations in the laryngeal ventricle under psychological stress are explored. The two-mass model is fitted to real speech by estimating the physical parameters representing stiffness of the vocal folds and effective area of laryngeal ventricle. The estimated parameters can be used to separate stressed speech from neutral speech because these parameters represent the mechanisms of the vocal folds and airflow variation in the glottis under stress. Experimental evaluations show that the area of laryngeal ventricle has a modulating effect on speech production, and is effective for the classification of stressed speech.", "title": "Classification of speech under stress by modeling the aerodynamics of the laryngeal ventricle"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/rakov13_interspeech.html", "abstract": "While a fair amount of work has been done on automatically detecting emotion in human speech, there has been little research on sarcasm detection. Although sarcastic speech acts are inherently subjective, humans have relatively clear intuitions as to what constitutes sarcastic speech. In this paper, we present a system for automatic sarcasm detection. Using a new acted speech corpus that is annotated for sarcastic and sincere speech, we examine a number of features that are indicative of sarcasm. The first set of features looks at a baseline of basic acoustic features that have been found to be helpful in human sarcasm identification. We then present an effective way of modeling and applying prosodic contours to the task of automatic sarcasm detection. This approach applies sequential modeling to categorical representations of pitch and intensity contours obtained via k-means clustering. Using a SimpleLogistic (LogitBoost) classifier, we are able to predict sarcasm with 81.57% accuracy. This result suggests that certain pitch and intensity contours are predictive of sarcastic speech.", "title": "\u201csure, i did the right thing\u201d: a system for sarcasm detection in speech"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/scherer13_interspeech.html", "abstract": "We seek to investigate voice quality characteristics, in particular on a breathy to tense dimension, as an indicator for psychological distress, i.e. depression and post-traumatic stress disorder (PTSD), within semi-structured virtual human interviews. Our evaluation identifies significant differences between the voice quality of psychologically distressed participants and not-distressed participants within this limited corpus. We investigate the capability of automatic algorithms to classify psychologically distressed speech in speaker-independent experiments. Additionally, we examine the impact of the posed questions' affective polarity, as motivated by findings in the literature on positive stimulus attenuation and negative stimulus potentiation in emotional reactivity of psychologically distressed participants. The experiments yield promising results using standard machine learning algorithms and solely four distinct features capturing the tenseness of the speaker's voice.", "title": "Investigating voice quality as a speaker-independent indicator of depression and PTSD"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/pellegrini13_interspeech.html", "abstract": "This paper presents a study of European Portuguese elderly speech, in which the acoustic characteristics of two groups of elderly speakers (aged 60.75 and over 75) are compared with those of young adult speakers (aged 19-30). The correlation between age and a set of 14 acoustic features was investigated, and decision trees were used to establish the relative importance of the features. A greater use of pauses characterized speakers aged 60 and over. For female speakers, speech rate also appeared to correlate with age. For male speakers, jitter distinguished between speakers aged 60.75 and older. The correlation between the features and speech recognition performance was also investigated. Word error rate correlated mostly with the use of pauses, speech rate, and the ratio of long phone realizations. Finally, by comparing the phone sequences used by the recognizer on the most frequent words, we observed that the young adult speakers reduced schwas more than the elderly speakers. This result seems to confirm the common idea that young speakers reduce articulation more than older speakers. Further investigation is needed to confirm this result by determining whether this is due to ageing or to the generation gap.", "title": "A corpus-based study of elderly and young speakers of European Portuguese: acoustic correlates and their impact on speech recognition performance"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/cummins13_interspeech.html", "abstract": "Quantifying how the spectral content of speech relates to changes in mental state may be crucial in building an objective speechbased depression classification system with clinical utility. This paper investigates the hypothesis that important depression based information can be captured within the covariance structure of a Gaussian Mixture Model (GMM) of recorded speech. Significant negative correlations found between a speaker's average weighted variance . a GMM-based indicator of speaker variability . and their level of depression support this hypothesis. Further evidence is provided by the comparison of classification accuracies from seven different GMM-UBM systems, each formed by varying different parameter combinations during MAP adaption. This analysis shows that variance-only adaptation either outperforms or matches the de facto standard mean-only adaptation when classifying both the presence and severity of depression. This result is perhaps the first of its kind seen in GMM-UBM speech classification.", "title": "Modeling spectral variability for the classification of depressed speech"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/perezrosas13_interspeech.html", "abstract": "This paper describes several experiments in building a sentiment analysis classifier for spoken reviews. We specifically focus on the linguistic component of these reviews, with the goal of understanding the difference in sentiment classification performance when using manual versus automatic transcriptions, as well as the difference between spoken and written reviews. We introduce a novel dataset, consisting of video reviews for two different domains (cellular phones and fiction books), and we show that using only the linguistic component of these reviews we can obtain sentiment classifiers with accuracies in the range of 65.75%.", "title": "Sentiment analysis of online spoken reviews"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/shepstone13_interspeech.html", "abstract": "In this paper we show a new method of using automatic age and gender recognition to recommend a sequence of multimedia items to a home TV audience comprising multiple viewers. Instead of relying on explicitly provided demographic data for each user, we define an audio-based demographic group profile that captures the age and gender for all members of the audience. A 7-class age and gender classifier employing a fusion of acoustic and prosodic features determines the probability of each speaker belonging to each class. The information for all speakers is then combined to form the group profile, which itself is the input to a recommender system. The recommender system finds the content items whose demographics best match the group profile. We tested the effectiveness of the system for several typical home audience configurations. In a survey, users were given a configuration and asked to rate a set of advertisements on how well each advertisement matched the configuration. Unbeknown to the subjects, half of the adverts were recommended using the derived audio demographics and the other half were randomly chosen. The recommended adverts received a significantly higher median rating of 7.75, as opposed to 4.25 for the randomly selected adverts.", "title": "Demographic recommendation by means of group profile elicitation using speaker age and gender recognition"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/malandrakis13_interspeech.html", "abstract": "We investigate acoustic modeling, feature extraction and feature selection for the problem of affective content recognition of generic, non-speech, non-music sounds. We annotate and analyze a database of generic sounds containing a subset of the BBC sound effects library. We use regression models, long-term features and wrapper-based feature selection to model affect in the continuous 3-D (arousal, valence, dominance) emotional space. The frame-level features for modeling are extracted from each audio clip and combined with functionals to estimate long term temporal patterns over the duration of the clip. Experimental results show that the regression models provide similar categorical performance as the more popular Gaussian Mixture Models. They are also capable of predicting accurate affective ratings on continuous scales, achieving 62.67% 3-class accuracy and 0.69.0.75 correlation with human ratings, higher than comparable numbers in literature.", "title": "Affective classification of generic audio clips using regression models"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/jeon13_interspeech.html", "abstract": "The aim of this study is to investigate the effect of cross-lingual data on human perception and automatic classification of emotion from speech. We use four different databases from three languages (English, Chinese, and German) and two types (acted and improvised). For automatic classification, there is a significant degradation using cross-corpus than within-corpus setup. For human perception, we observe differences between native and non-native speakers when judging emotions for a language, and there is less performance loss in cross-language setup compared to automatic classification. In addition, we find that the automatic approaches work well in classifying the emotional activation category: positive and negative activated emotions, but are not good at classifying instances within the same activation category, which is different from the confusion patterns of the human perception experiment. This study provides insights to better understanding of cross-lingual human emotion perception and development of robust automatic emotion recognition systems.", "title": "A preliminary study of cross-lingual emotion recognition from speech: automatic classification versus human perception"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/han13_interspeech.html", "abstract": "State-of-the-art dimensional speech emotion recognition systems are trained using continuously labelled instances. The data labelling process is labour intensive and time-consuming. In this paper, we propose to apply active learning to reduce according efforts: The unlabelled instances are evaluated automatically, and only the most informative ones are intelligently picked by an informativeness measure function for a human to label. Specifically, we estimate the informativeness of each unlabelled instance based on a binary-classification confidence score for an emotion being predicted to be negative or positive on a given emotional dimension. For verification, we consider a pool-based and a stream-based scenario run on part of the continuous AVEC 2012 task to demonstrate the feasibility of the proposed approach in practice. In the result, our approach requires significantly less human labelled data instances to reach a given performance than passive learning does in both scenarios.", "title": "Active learning for dimensional speech emotion recognition"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/kelly13_interspeech.html", "abstract": "The comparison of non-contemporary speech samples is common in forensic speaker recognition cases. It has yet to be established however, to what extent the time interval between non-contemporary samples can increase before a problem is created for forensic automatic speaker recognition. This paper presents results of a human listener test designed to evaluate the detectability of vocal ageing over increasing intervals of up to 30 years. Subsequently, a forensic automatic speaker recognition evaluation of 15 ageing males at increasing intervals of up to 60 years is presented. It is shown that at intervals of around 10 years, the average detectability of vocal ageing by humans is just above chance. As the interval rises to 30 years, vocal ageing is detected 90% of the time. In the automatic system, vocal ageing is manifested as a drop in intra-speaker likelihood ratios (LRs) as the time interval between non-contemporary samples increases. At an interval of 30 years, LRs for the vast majority of intra-speaker comparisons fall below a value of 100 . commonly interpreted as \u0081emoderate support\u0081f on a verbal LR scale. Our findings indicate that at a time-lapse of 30 years, vocal ageing creates significant problems for forensic automatic speaker recognition.", "title": "Auditory detectability of vocal ageing and its effect on forensic automatic speaker recognition"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/alam13_interspeech.html", "abstract": "Natural human-computer interaction requires, in addition to understand what the speaker is saying, recognition of behavioral descriptors, such as speaker's personality traits (SPTs). The complexity of this problem depends on the high variability and dimensionality of the acoustic, lexical and situational context manifestations of the SPTs. In this paper, we present a comparative study of automatic speaker personality trait recognition from speech corpora that differ in the source speaking style (broadcast news vs. conversational) and experimental context. We evaluated different feature selection algorithms such as information gain, relief and ensemble classification methods to address the high dimensionality issues. We trained and evaluated ensemble methods to leverage base learners, using three different algorithms such as SMO (Sequential Minimal Optimization for Support Vector Machine), RF (Random Forest) and Adaboost. After that, we combined them using majority voting and stacking methods. Our study shows that, performance of the system greatly benefits from feature selection and ensemble methods across corpora.", "title": "Comparative study of speaker personality traits recognition in conversational and broadcast news speech"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/zhang13_interspeech.html", "abstract": "Speech data is in principle available in large amounts for the training of acoustic emotion recognisers. However, emotional labelling is usually not given and the distribution is heavily unbalanced, as most data is \u0081erather neutral\u0081f than truly \u0081eemotional\u0081f. In the \u0081ehay stack\u0081f of speech data, Active Learning automatically identifies the \u0081eneedles\u0081f, i.e., the more informative instances to reduce human labelling effort when building a classifier, e.g., for acoustic emotion recognition. The critical issue thus is the determination and quantification of informativeness. To this end, we suggest to exploit the reliability of the usual ambiguity of emotional labels, i.e., we propose a novel approach based on label uncertainty. By building a certainty model and predicting the candidate instances, informativeness is thus based on labeller agreement. In addition, we consider class sparseness. The results of extensive test runs under well standardised conditions show the method's great potential in reducing labelling costs while boosting performance.", "title": "Active learning by label uncertainty for acoustic emotion recognition"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/xiao13_interspeech.html", "abstract": "Empathy is considered a key curative aspect of interactive counseling based psychotherapy. In this present work, it is deemed an interpersonal behavior whereby one person communicates attention and understanding to another. The process of empathy involves \"trying on\" the thoughts or feelings of another person. Thus it is hypothesized to involve entrainment, wherein interlocutors become more alike in behaviors such as speech, gestures, emotions, etc. We extend previous algorithms on vocal similarity, measured through temporal weighting on speech features, and principal components constructed from these features. In addition, we approximate entrainment via turn-based differences in weighted pitch between speakers and turn-taking statistics. Results show these cues are significantly correlated with human ratings of empathy, and can predict therapist empathy significantly better than chance. This work establishes a link between empathy and entrainment, and proposes computational approaches to infer therapist empathy.", "title": "Modeling therapist empathy and vocal entrainment in drug addiction counseling"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/miyazaki13_interspeech.html", "abstract": "In call centers, since callers range from novices to experts with respect to what they are asking about, callers should be treated differently depending on their knowledge levels. To extract dialogues of callers with a certain knowledge level for an analysis, we propose a method to estimate callers' levels of knowledge. We focus on features related to vocabulary, utterance timing and duration, and information exchange and use a machine learning technique to learn a classifier that distinguishes the knowledge levels. Experimental results show that our method achieves a precision of 0.8 or better while retaining a moderate recall of around 0.5 in extracting the dialogues of novice callers.", "title": "Estimating callers' levels of knowledge in call center dialogues"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/arias13_interspeech.html", "abstract": "This paper proposes the use of reference models to detect emotional prominence in the energy and F0 contours. The proposed framework aims to model the intrinsic variability of these prosodic features. We present a novel approach based on Functional Data Analysis (FDA) to build reference models using a family of energy and F0 contours, which are implemented with lexicon-independent models. The neutral models are represented by bases of functions and the testing energy and F0 contours are characterized by their projections onto the corresponding bases. The proposed system can lead to accuracies as high as 80.4% in binary emotion classification in the EMO-DB corpus, which is 17.6% higher than the one achieved by a benchmark classifier trained with sentence level prosodic features. The approach is also evaluated with the SEMAINE corpus, showing that it can be effectively used in real applications.", "title": "Energy and F0 contour modeling with functional data analysis for emotional speech detection"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/mishra13_interspeech.html", "abstract": "Most emotion recognition systems do not perform real-time emotion recognition due to latencies caused by phrase segmentation and resource-intensive feature acquisition, etc. To address this issue, we present an emotion recognition approach that can estimate speaker emotions with much lower latency. The proposed approach does not rely on phrase-level features to recognize speaker emotion; rather, it estimates the speaker's emotional state over the course of the utterance incrementally, using a shifting n-word window on the basis of easily computable features. These features are obtained from three information streams, i.e. cepstral, prosodic and textual, at the word-level and combined at decision-level using a statistical framework. Our work shows that combining the three information streams yields higher emotion recognition accuracy than any single information stream. Using features extracted from n-word sequences rather than phrases provides for the low-latency capabilities of the proposed system, without any loss in utterance-level emotion recognition accuracy. The performance of the proposed system on a binary utterance-level emotion recognition task using an in-house database shows a relative improvement of 41% over chance, compared to a relative improvement of 31.82% shown by the baseline phrase-level emotion recognition approach.", "title": "Incremental emotion recognition"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/hanilci13_interspeech.html", "abstract": "We study the problem of vocal effort mismatch in speaker verification. Changes in speaker's vocal effort induce changes in fundamental frequency (F0) and formant structure which introduce unwanted intra-speaker variations to features. We compare seven alternative spectrum estimators in the context of mel-frequency cepstral coefficient (MFCC) extraction for speaker verification. The compared variants include traditional FFT spectrum and six parametric all-pole models. Experimental results on the NIST 2010 speaker recognition evaluation (SRE) corpus utilizing both GMM-UBM and more recent GMM supervector classifier indicate that spectrum estimation has a considerable impact on speaker verification accuracy under mismatched vocal effort conditions. The highest recognition accuracy was achieved using a particular variant of temporally weighted all-pole model, stabilized weighted linear prediction (SWLP).", "title": "Comparison of spectrum estimators in speaker verification: mismatch conditions induced by vocal effort"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/xia13_interspeech.html", "abstract": "In this paper, we propose to use the denoising autoencoder to generate robust feature representations for emotion recognition. In our method, the input of the denoising autoencoder is the normalized static feature set (state-of-the-art features for emotion recognition). This input is mapped to two hidden representations: one is to capture the neutral information from the input, and the other one is used to extract emotional information. Model parameters are learned by minimizing the squared error between the original and the reconstructed input. After pre-training and fine-tuning, we use the hidden representation as features in the SVM model for emotion classification. Our experimental results show significant performance improvement compared to using the static features.", "title": "Using denoising autoencoder for emotion recognition"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/abdelaziz13_interspeech.html", "abstract": "In this paper we propose the use of the recently introduced twin-HMM-based audio-visual speech enhancement algorithm as a front-end for audio-visual speech recognition systems. This algorithm determines the clean speech statistics in the recognition domain based on the audio-visual observations and transforms these statistics to the synthesis domain through the so-called twin HMMs. The adopted front-end is used together with back-end methods like the conventional maximum likelihood decoding or the newly introduced significance decoding. The proposed combination of the front- and back-end is applied to acoustically corrupted signals of the Grid audio-visual corpus and results in statistically significant improvements of the audio-visual recognition accuracy compared to using the ETSI advanced front-end.", "title": "Using twin-HMM-based audio-visual speech enhancement as a front-end for robust audio-visual speech recognition"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/gibson13_interspeech.html", "abstract": "We introduce a novel spectro-temporal representation of speech by applying directional derivative filters to the Mel-spectrogram, with the aim of improving the robustness of automatic speech recognition. Previous studies have shown that two-dimensional wavelet functions, when tuned to appropriate spectral scales and temporal rates, are able to accurately capture the acoustic modulations of speech, even in high noise conditions. Therefore, spectro-temporal features extracted from the wavelet transformation of the spectrogram, offer additional noise robustness to important signal processing tasks, such as voice activity detection and speech recognition. In this paper, we explore the use of the steerable pyramid, a directional wavelet transform that is common in image processing, to derive a spectro-temporal feature representation of speech that can serve as an alternative to cepstral derivatives and Gabor filter-bank features. We discuss their application for the task of robust automatic speech recognition. Experiments conducted on the Aurora-2 database demonstrate their competitive robustness to other state-of-the-art speech features, especially in low signal-to-noise ratio conditions.", "title": "Spectro-temporal directional derivative features for automatic speech recognition"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/xiao13b_interspeech.html", "abstract": "Histogram equalization (HEQ) is a simple and effective feature normalization technique for robust speech recognition. Recently, we proposed to adapt HEQ transform to each test utterance using a maximum likelihood (ML) criterion and observed improved performance. In this paper, we further the study by applying attribute-based HEQ and its ML adaptation. Instead of applying a global HEQ transform to the test utterance, we propose to apply different HEQ transforms to the 6 manners of speech, e.g. vowel and fricative. We also developed the ML adaptation algorithm of the attribute-based HEQ. Experimental results show that the attribute-based HEQ adaptation obtained 21.8% and 19.5% relative error rate reduction over the global HEQ baseline on the Aurora-2 and Aurora-4 benchmarking tasks, respectively.", "title": "Attribute-based histogram equalization (HEQ) and its adaptation for robust speech recognition"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/joshi13_interspeech.html", "abstract": "Cepstral Mean Normalization (CMN) is a widely used technique for channel compensation and for noise robustness. CMN compensates for noise by transforming both train and test utterances to zero mean, thus matching first-order moment of train and test conditions. Since all utterances are normalized to zero mean, CMN could lead to loss of discriminative speech information, especially for short utterances. In this paper, we modify CMN to reduce this loss by transforming every noisy test utterance to the estimate of clean utterance mean (mean estimate of the given utterance if noise was not present) and not to zero mean. A look-up table based approach is proposed to estimate the clean-mean of the noisy utterance. The proposed method is particularly relevant for IVR-based applications, where the utterances are usually short and noisy. In such cases, techniques like Histogram Equalization (HEQ) do not perform well and a simple approach like CMN leads to loss of discrimination. We obtain a 12% relative improvement over CMN in WER for Aurora-2 database; and when we analyze only short utterances, we obtain a relative improvement of 5% and 25% in WER over CMN and HEQ respectively.", "title": "Modified cepstral mean normalization \u2014 transforming to utterance specific non-zero mean"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/mitra13_interspeech.html", "abstract": "This paper presents a new signal-processing technique motivated by the physiology of human auditory system. In this approach, auditory hair cells are modeled as damped oscillators that are stimulated by bandlimited time domain speech signals acting as forcing functions. Oscillation synchrony is induced by time aligning and three-way coupling of the forcing functions across the individual bands such that a given oscillator is induced not only by its critical band's forcing function but also by its two neighboring functions. We present two separate features; one which uses the damped oscillator response to the forcing functions without synchrony which we name as the Damped Oscillator Cepstral Coefficient (DOCC) and the other which uses the damped oscillator response to a time synchronized forcing function and we name it as the Synchronized Damped Oscillator Cepstral Coefficient (SyDOCC). The proposed features are used in an Aurora4 noiseand channel-degraded speech recognition task, and the results indicate that they improved speech-recognition performance in all conditions compared to the baseline mel-cepstral feature and other published noise robust features", "title": "Damped oscillator cepstral coefficients for robust speech recognition"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/alam13b_interspeech.html", "abstract": "In this paper, we present two robust feature extractors that use a regularized minimum variance distortionless response (RMVDR) spectrum estimator instead of the discrete Fourier transform-based direct spectrum estimator, used in many front-ends including the conventional MFCC, for estimating the speech power spectrum. Direct spectrum estimators, e.g., single tapered periodogram, have high variance and they perform poorly under noisy and adverse conditions. RMVDR spectrum estimator has low spectral variance and are robust to mismatch conditions. Based on RMVDR spectrum estimator two robust feature extractors, robust RMVDR cepstral coefficients (RRMCC) and normalized RMVDR cepstral coefficients (NRMCC), are proposed that incorporate an auditory domain spectrum enhancement (ASE) method and a medium duration power bias subtraction (MDPBS) technique, respectively, for enhancement of the speech spectrum. Experimental speech recognition results are conducted on the AURORA-4 corpus and performances are compared with the MFCC, PLP, MVDR-MFCC, RMVDR-MFCC, PMVDR, ETSI advancement front-end (ETSI-AFE), PNCC, CFCC, and the robust feature extractor (RFE) of [6]. Experimental results demonstrate that the proposed robust feature extractors outperformed the other robust front-ends in terms of percentage word accuracy on the AURORA-4 large vocabulary continuous speech recognition (LVCSR) task under different mismatch conditions.", "title": "Regularized MVDR spectrum estimation-based robust feature extractors for speech recognition"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/lu13b_interspeech.html", "abstract": "Noise adaptive training (NAT) is an effective approach to normalise environmental distortions when training a speech recogniser on noise-corrupted speech. This paper investigates the model-based NAT scheme using joint uncertainty decoding (JUD) for subspace Gaussian mixture models (SGMMs). A typical SGMM acoustic model has much larger number of surface Gaussian components, which makes it computationally infeasible to compensate each Gaussian explicitly. JUD tackles this problem by sharing the compensation parameters among the Gaussians and hence reduces the computational and memory demands. For noise adaptive training, JUD is reformulated into a generative model, which leads to an efficient expectation-maximisation (EM) based algorithm to update the SGMM acoustic model parameters. We evaluated the SGMMs with NAT on the Aurora 4 database, and obtained higher recognition accuracy compared to systems without adaptive training.", "title": "Noise adaptive training for subspace Gaussian mixture models"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/saon13_interspeech.html", "abstract": "We present the IBM speech activity detection system that was fielded in the phase 2 evaluation of the DARPA RATS (robust automatic transcription of speech) program. Key ingredients of the system are: multi-pass HMM Viterbi segmentation, fusion of multiple feature streams, file-based and speech-based normalization schemes, the use of regular and convolutional deep neural networks, and model fusion through frame-level score combination of channel-dependent models. These techniques were instrumental in achieving a 1.4% equal error rate on the RATS phase 2 evaluation data.", "title": "The IBM speech activity detection system for the DARPA RATS program"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/sehr13_interspeech.html", "abstract": "A novel framework based on conditional emission densities for hiddenMarkov models (HMMs) is proposed in this contribution to integrate speech enhancement systems with automatic speech recognition systems. In the training phase, the observed feature vectors, corrupted by background noise and reverberation, together with estimates for the interference as provided by the speech enhancement system are used for training joint densities of the observations and the interference estimates. In the decoding phase, the joint densities are transformed to conditional densities of the observed features given the interference estimates. Thus, front end processing can be exploited for obtaining interference estimates, and the estimation errors can be modeled very effectively in a data-driven way. Connected digit recognition experiments in a simulated reverberant environment show the potential of the proposed approach: HMMs with the proposed conditional densities outperform various configurations of conventional HMMs in the logarithmic melspectral domain. This is a first step towards using conditional densities for creating synergies between front end and back end.", "title": "Conditional emission densities for combining speech enhancement and recognition systems"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/wolf13_interspeech.html", "abstract": "If speech is captured by several arbitrarily-located microphones in a room, the degree of distortion by noise and reverberation may vary strongly from one channel to another. Channel selection for automatic speech recognition aims to rank the signals according to their quality, and, in particular, to select the best one for further processing in the recognition system. To create this ranking, we propose here to use posterior probabilities estimated from the N-best hypothesis of each channel. When evaluated experimentally, this new channel selection technique outperforms the methods published so far. We also propose the combination of different channel selection techniques to further increase the recognition accuracy and to reduce the computational load without significant performance loss.", "title": "Channel selection using n-best hypothesis for multi-microphone ASR"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/ishii13_interspeech.html", "abstract": "Denoising autoencoder is applied to reverberant speech recognition as a noise robust front-end to reconstruct clean speech spectrum from noisy input. In order to capture context effects of speech sounds, a window of multiple short-windowed spectral frames are concatenated to form a single input vector. Additionally, a combination of short and long-term spectra is investigated to properly handle long impulse response of reverberation while keeping necessary time resolution for speech recognition. Experiments are performed using the CENSREC-4 dataset that is designed as an evaluation framework for distant-talking speech recognition. Experimental results show that the proposed denoising autoencoder based front-end using the short-windowed spectra gives better results than conventional methods. By combining the long-term spectra, further improvement is obtained. The recognition accuracy by the proposed method using the short and long-term spectra is 97.0% for the open condition test set of the dataset, whereas it is 87.8% when a multi-condition training based baseline is used. As a supplemental experiment, large vocabulary speech recognition is also performed and the effectiveness of the proposed method has been confirmed.", "title": "Reverberant speech recognition based on denoising autoencoder"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/maymon13_interspeech.html", "abstract": "Stereo-based stochastic mapping (SSM) is a technique based on constructing a Gaussian mixture model for the joint distribution of stereo data. This paper considers the use of SSM for noise robust speech recognition, in which clean and noisy speech features form the stereo data. The Gaussian mixture model, whose parameters are estimated from the observed stereo features during training time, is then used in test time to predict the clean speech from its noisy observation. This paper proposes to leverage the noisy speech observation for updating the model parameters during test time, and thus improve the prediction of the clean speech from its noisy observation. Specifically, an expectation-maximization procedure is developed for adapting the model parameters during test time. This adaptation is especially important when there is a mismatch between the training and testing sets, or when the size of the training set is relatively small, resulting in a poor estimation of the parameters. The proposed method is tested on a noise robustness task and is shown to improve the performance achieved by SSM.", "title": "Adaptive stereo-based stochastic mapping"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/kao13_interspeech.html", "abstract": "Recently, histogram equalization (HEQ) of speech features has received considerable attention in the area of robust speech recognition because of its relative simplicity and good empirical performance. In this paper, we present a novel extension to the conventional HEQ approach in two significant aspects. First, polynomial regression of various orders is employed to efficiently perform feature normalization building up the notion of HEQ. Second, not only the contextual distributional statistics but also the dynamics of feature values are taken as the input to the presented regression functions for better normalization performance. By doing so, we can to some extent relax the dimension-independence and bag-of-frames assumptions made by the conventional HEQ approach. All experiments were carried out on the Aurora-2 database and task and further verified on the Aurora-4 database and task. The corresponding results demonstrate that our proposed methods can achieve considerable word error rate reductions over the baseline systems and offer additional performance gains for the AFE-processed features.", "title": "Distribution-based feature normalization for robust speech recognition leveraging context and dynamics cues"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/liu13b_interspeech.html", "abstract": "In this paper, recently proposed Temporally Varying Weight Regression (TVWR) is investigated in two ways for noise robust speech recognition. Firstly, since typical model compensation approaches assume that the noise feature is independent and identically distributed, non-stationary noise environment can be poorly compensated using conventional model compensation approaches in the standard Hidden Markov Model (HMM) framework. TVWR, however, maintains both the basic HMM structure and additional time-varying property, therefore, model compensation for TVWR is proposed such that i.i.d. noise assumption can be relaxed. Secondly, although Noise Adaptive Training NAT has been proposed to optimize the \"pseudo-clean\" HMM model for a better performance by maximizing the likelihood of multi-condition data, NAT heavily depends on the simplicity of Vector Taylor Series (VTS) formulation. Hence, other advanced compensation approaches, such as Trajectory-based Parallel Model Combination (TPMC), have difficulties benefiting from this powerful training schema. This paper exploits the time-varying attribute of TVWR to approximate NAT such that any compensation technique can be applied during noise adaptive training. Experiments on the Aurora 4 corpus show that significant improvements over the standard HMM or NAT system can be obtained by compensating TVWR either trained using clean data or adaptively trained using multi-condition data.", "title": "An investigation of temporally varying weight regression for noise robust speech recognition"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/li13b_interspeech.html", "abstract": "Handling variable ambient noise is a challenging task for automatic speech recognition (ASR) systems. To address this issue, multi-style training using speech data collected in diverse noise environments, noise adaptive training or uncertainty decoding techniques can be used. An alternative approach is to explicitly approximate the continuous trajectory of Gaussian component or model space linear transform parameters against the varying noise, for example, using generalized variable parameter HMMs (GVP-HMM). In order to reduce the computational cost of conventional GVP-HMMs when model parameter update against the varying noise condition is required, this paper investigates a novel and more efficient extension of GVP-HMMs that can also model the trajectories of feature space linear transforms. Significant error rate reductions of 9.3% and 18.5% relative were obtained over the multi-style training baseline system on Aurora 2 and a medium vocabulary Mandarin Chinese speech recognition task respectively.", "title": "Feature space generalized variable parameter HMMs for noise robust recognition"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/brakel13_interspeech.html", "abstract": "We propose a bidirectional truncated recurrent neural network architecture for speech denoising. Recent work showed that deep recurrent neural networks perform well at speech denoising tasks and outperform feed forward architectures. However, recurrent neural networks are difficult to train and their simulation does not allow for much parallelization. Given the increasing availability of parallel computing architectures like GPUs this is disadvantageous. The architecture we propose aims to retain the positive properties of recurrent neural networks and deep learning while remaining highly parallelizable. Unlike a standard recurrent neural network, it processes information from both past and future time steps. We evaluate two variants of this architecture on the Aurora2 task for robust ASR where they show promising results. The models outperform the ETSI2 advanced front end and the SPLICE algorithm under matching noise conditions.", "title": "Bidirectional truncated recurrent neural networks for efficient speech denoising"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/variani13_interspeech.html", "abstract": "A prototype multi-stream system with a performance monitor for stream selection is proposed to recognize speech in unknown noise. The speech signal is decomposed into seven band-limited streams. Posterior probabilities of phonemes are estimated by a multi-layer perceptron (MLP) in each of these band-limited streams. Estimated posterior vectors of all 127 combinations (processing streams) of the seven band-limited streams form inputs to a second-stage MLP that estimates posterior probabilities of phonemes in each processing stream. A performance monitor is designed to predict the reliability of individual processing streams based on the outputs from these streams. The top N streams that are least affected by noise are selected and their outputs are averaged to yield the final posterior probability vector used in Viterbi search for the best phoneme sequence. Experimental results show that the proposed technique is effective in dealing with noise.", "title": "Multi-stream recognition of noisy speech with performance monitoring"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/fujimoto13b_interspeech.html", "abstract": "Although typical model-based noise suppression including the vector Taylor series-based approach employs a single Gaussian distribution for the noise model, it is insufficient for non-stationary noises which have a complex structured distribution. As a solution to this problem, we have already proposed a method for estimating a Gaussian mixture model (GMM)-based noise model by using a minimum mean squared error (MMSE) estimate of the noise. However, the state transition process of the non-stationary noise is not modeled in the noise GMM. In this paper, we propose a way of modeling the noise with a hidden Markov model (HMM) as an extension of our previous method. The proposed method proves that the HMM-based noise model outperforms a GMM-based noise model composed of the same number of Gaussian components. In addition, we discuss the appropriate topology for the noise HMM, i.e., a left-to-right HMM and an ergodic HMM.", "title": "Model-based noise suppression using unsupervised estimation of hidden Markov model for non-stationary noise"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/nathwani13_interspeech.html", "abstract": "Speech acquired from an array of distant microphones is affected by ambient noise and reverberation. Single channel linearly constrained minimum variance (LCMV) filters have been proposed to remove ambient noise. In this paper, an algorithm for joint noise cancellation and dereverberation using a multi channel LCMV filter in the frequency domain is proposed. A single channel LCMV filter which accounts for the inter frame correlation is applied on each channel to remove the early reverberation component. A modified spectral subtraction method is also proposed to remove the late reverberation component present in the speech signal. Experimental results on joint noise cancellation and dereverberation indicate a reasonable improvement over conventional speech enhancement methods. Additional experiments on distant speech recognition are also conducted to illustrate the significance of the method.", "title": "Joint noise cancellation and dereverberation using multi-channel linearly constrained minimum variance filter"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/delcroix13_interspeech.html", "abstract": "Using deep neural networks (DNNs) for automatic speech recognition (ASR) has recently attracted much attention due to the large performance improvement they provide for a variety of tasks. DNNs are known to be robust to overfitting and to be able to remove speaker variability. Another important cause of variability in speech is the presence of noise. A lot of research has been undertaken on noise robust ASR including front-end and back-end approaches. However most approaches have been developed or evaluated on traditional ASR systems based on Gaussian mixture models (GMMs). The question we try to address in this paper is whether conventional noise robust approaches can still be competitive when using recent DNN-based ASR systems. To this end, we compare experimentally the performance of DNN-based ASR systems in a distant speech recognition task, for DNNs trained with noise-free, noisy and enhanced speech. We confirm that DNNs are powerful when the training and testing conditions are well matched. However, the performance degrades in the presence of noise. The use of a speech enhancement pre-processor to reduce the noise variability significantly improves performance with performance improvement comparable to that observed with conventional GMM-based ASR systems.", "title": "Is speech enhancement pre-processing still relevant when using deep neural networks for acoustic modeling?"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/hsieh13c_interspeech.html", "abstract": "Histogram equalization (HEQ) of acoustic features has received considerable attention in the area of robust speech recognition because of its relative simplicity and good empirical performance. This paper presents a novel HEQ-based feature extraction approach that performs equalization in both acoustic frequency and modulation frequency domains for obtaining better noise-robust features. In particular, the real and imaginary acoustic spectra are first individually transformed to the modulation domain via discrete Fourier transform (DFT). The HEQ process is then carried on the corresponding magnitude modulation spectra so as to compensate for the noise distortions. Finally, the equalized modulation spectra are converted back to form the real and imaginary acoustic spectra, respectively. By doing so, we can enhance not only the magnitude but also the phase components of the acoustic spectra, and thereby create more noise-robust cepstral features. The experiments conducted on the Aurora-2 clean-condition database and task reveal that the presented approach delivers superior recognition accuracy in comparison with some other HEQ-related methods and the well-known advanced front-end (AFE) extraction scheme, which supports the potential utility of this novel approach.", "title": "Histogram equalization of real and imaginary modulation spectra for noise-robust speech recognition"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/li13c_interspeech.html", "abstract": "Deep Neural Networks (DNNs) are becoming widely accepted in automatic speech recognition (ASR) systems. The deep structured nonlinear processing greatly improves the model's generalization capability, but the performance under adverse environments is still unsatisfactory. In the literature, there have been many techniques successfully developed to improve Gaussian mixture models\u0081f robustness. Investigating the effectiveness of these techniques for the DNN is an important step to thoroughly understand its superiority, pinpoint its limitations and most importantly to further improve it towards the ultimate human-level robustness. In this paper, we investigate the effectiveness of speech enhancement using spectral restoration algorithms for DNNs. Four approaches are evaluated, namely minimum mean-square error spectral estimator (MMSE), maximum likelihood spectral amplitude estimator (MLSA), maximum a posteriori spectral amplitude estimator (MAPA), and generalized maximum a posteriori spectral amplitude algorithm (GMAPA). The preliminary experimental results on the Aurora 2 speech database show that with multi-condition training data the DNN itself is capable of learning robust representations. However, if only clean data is available, the MLSA algorithm is the best spectral restoration training method for DNNs.", "title": "An investigation of spectral restoration algorithms for deep neural networks based noise robust speech recognition"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/remes13_interspeech.html", "abstract": "Missing-feature imputation or reconstruction is used in noiserobust automatic speech recognition to recover the unobserved clean speech information. Reconstruction methods often use the noise-corrupted observations and a clean speech prior to calculate a point estimate for the unobserved clean speech features, whereas the approach proposed in this work associates the unobserved clean speech features with a full posterior distribution. The posterior mean can be used as a clean speech estimate in bounded conditional mean imputation and the posterior variance can be included as observation uncertainties. The proposed method is evaluated in a large-vocabulary noise-robust speech recognition task with speech data recorded in real noisy environments.", "title": "Bounded conditional mean imputation with an approximate posterior"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/cui13_interspeech.html", "abstract": "This paper investigates a noise robust approach to automatic speech recognition based on a mixture of Bayesian joint factor analyzers. In this approach, noisy features are modeled by two joint groups of factors accounting for speaker and noise variabilities which are estimated by clean and noisy speech respectively. The factors form an overcomplete dictionary with a redundant representation. Automatic relevance determination (ARD) is carried out by the relevance vector machine (RVM) where sparsity-promoting priors are applied on two factor loading matrices. Experiments on large vocabulary continuous speech recognition (LVCSR) tasks show good improvements by this approach.", "title": "Mixtures of Bayesian joint factor analyzers for noise robust automatic speech recognition"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/liu13c_interspeech.html", "abstract": "In the current ASR systems the presence of competing speakers greatly degrades the recognition performance. This phenomenon is getting even more prominent in the case of hands-free, far-field ASR systems like the \"Smart-TV\" systems, where reverberation and non-stationary noise pose additional challenges. Furthermore, speakers are, most often, not standing still while speaking. To address these issues, we propose a cascaded system that includes Time Differences of Arrival estimation, multi-channel Wiener Filtering, non-negative matrix factorization (NMF), multi-condition training, and robust feature extraction, whereas each of them additively improves the overall performance. The final cascaded system presents an average of 50% and 45% relative improvement in ASR word accuracy for the CHiME 2011 (non-stationary noise) and CHiME 2012 (non-stationary noise plus speaker head movement) tasks, respectively.", "title": "Robust speech enhancement techniques for ASR in non-stationary noise and dynamic environments"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/poblete13_interspeech.html", "abstract": "This paper describes the development of an optimal sigmoidal rate-level function that is a part of many models of the peripheral auditory system. The optimization makes use of a set of criteria defined on the basis of physical attributes of the input sound that are inspired by physiological evidence. These criteria attempt to discriminate between a degraded speech signal and noise to preserve the maximum information in the linear region of the sigmoidal curve, and to minimize the effects of distortion in the saturating regions. The performance of the proposed approach is validated by text-independent speaker-verification experiments with signals corrupted by additive noise at different SNRs. Experimental results suggest that the approach presented in combination with CVN can lead to relative reductions in EER as great as 30% when compared with the use of baseline MFCC coefficients for some SNRs.", "title": "Optimization of sigmoidal rate-level function based on acoustic features"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/sadakata13_interspeech.html", "abstract": "Two series of Electroencephalogram (EEG) measurements indicated that average auditory event-related potentials (ERPs) elicited by the disyllabic sound /asu/ were different between Japanese and English native listeners. Significant differences were observed in the time window where the P1-N1-P2 complex for /a/ is expected. This difference may be due to the absence/presence of the Auditory Change Complex (ACC) elicited by /s/. Furthermore, by combining the P1-N1-P2 complex elicited by each component of /asu/ (/a//s//u/) recorded individually, it was possible to compose ERPs similar to those elicited by /asu/. Interestingly, the optimized weights of /s/ were significantly lower for Japanese than for native- English listeners, suggesting that the trace of the ACC associated with /s/ was less visible in the actual ERP response to /asu/ for Japanese native listeners. These results may together suggest that Japanese and English native individuals process the /s/ in /asu/ differently and that the ACC is sensitive to language-specific perceptual categories.", "title": "Composing auditory ERPs: cross-linguistic comparison of auditory change complex for Japanese fricative consonants"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/bedoin13_interspeech.html", "abstract": "The experimental investigation of response inhibition and the neuropsychological assessment of impulsivity are classically conducted with Go/Nogo tasks, where the participant presses a key for standard (Go) stimuli and withholds the response for deviant (Nogo) ones. However, auditory Go/Nogo tasks frequently fail to elicit the typical ERP correlates of response inhibition (N2, P3). We elaborated an auditory Go/Nogo experiment with speech stimuli (VCV) and sufficient difficulty level (Go and Nogo stimuli differed by one phonetic feature only) to strongly involve response inhibition. An N2 wave . the earlier correlate of inhibition . was recorded in 15 healthy adults. This result encourages the use of auditory Go/Nogo tasks to assess impulsivity, which results in decreased N2 amplitude. Additionally, a substantial P3 was observed as a secondary correlate of inhibition. Its amplitude was clearly modulated by the perceptual salience of the phonetic difference: P3 was highest for manner of articulation, then voicing, and it was smallest for place differences. ERP indices of the right-hemisphere involvement in voicing processing are also reported. This auditory Go/Nogo task therefore appears useful as a clinical tool for impulsivity assessment and an experimental way to address phonetic issues.", "title": "How voicing, place and manner of articulation differently modulate event-related potentials associated with response inhibition"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/bellier13_interspeech.html", "abstract": "Categorical perception is a pervasive phenomenon in phonemic identification and has been demonstrated in the last decades through behavioral experiments and, more recently, in neurophysiological studies. However, the precise neural mechanisms underlying the transformation of continuous physical properties into discrete phonemic units is still largely unknown. This paper presents a study that aims at investigating the neurological correlates of phonemic categorization at a very early stage of auditory processing. We recorded the auditory evoke potentials (AEP) of twelve subjects using scalp electrodes placed between the mastoid bone and the vertex of the head. The subjects passively heard four stimuli in the continuum /ba/-/pa/ with varying voicing onset times (VOT). Those stimuli were chosen individually for each subject according to her psychometric response to the identification task run previously. The individual AEPs were represented using the coefficients of the discrete wavelet transform. Using linear discriminant analysis, we were able to show an early categorization of the responses in terms of VOT, which mimics the behavioral responses of the subjects. The category signatures in the AEPs arose during the vocalic part of the syllable, which suggests that phonemic categorization may take place at the lower level stages of the auditory pathway.", "title": "Categorization of speech in early auditory evoked responses"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/manca13_interspeech.html", "abstract": "In the present study, cortical responses of two sustained Italian vowels, /a/ and /i/, were analyzed in perception and production tasks by using the event-related potential (ERP) technique. First, we showed that the earlier auditory waveform complex, N1/P2, was elicited in both experimental tasks by acoustically presented vowels. More interestingly, the cortical activity of auditory cortex seems to reflect different spectro-acoustical items of Italian vowels in the production task. Specifically, the vowel /a/ generated a weaker activity than /i/ as revealed by different neuronal modulations; moreover, during speech production, the auditory cortex showed a dampening and delaying of its activity in the left hemisphere.", "title": "Perception and production of Italian vowels: an ERP study"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/grohe13_interspeech.html", "abstract": "Previous studies have shown that speech processing is accelerated for familiar voices in contrast to unfamiliar ones, and for familiar intonation in contrast to unfamiliar intonation. The present experiments probed these effects in a single experiment and tested whether they also occur with short, implicit familiarization. Results of two auditory lexical decision tasks (Experiment 1 with a task-based familiarization phase and Experiment 2 with a passive listening familiarization phase), showed that familiarity with the intonation (rise vs. fall) affected reaction times but that familiarity with the voice (speaker A vs. B) did not. Our results suggest that intonation (which contributes to utterance interpretation) is stored in the mental lexicon, but voice information is not.", "title": "Implicit learning leads to familiarity effects for intonation but not for voice"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/evans13_interspeech.html", "abstract": "It is widely acknowledged that most biometric systems are vulnerable to spoofing, also known as imposture. While vulnerabilities and countermeasures for other biometric modalities have been widely studied, e.g. face verification, speaker verification systems remain vulnerable. This paper describes some specific vulnerabilities studied in the literature and presents a brief survey of recent work to develop spoofing countermeasures. The paper concludes with a discussion on the need for standard datasets, metrics and formal evaluations which are needed to assess vulnerabilities to spoofing in realistic scenarios without prior knowledge.", "title": "Spoofing and countermeasures for automatic speaker verification"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/hautamaki13_interspeech.html", "abstract": "Voice imitation is mimicry of another speaker's voice characteristics and speech behavior. Professional voice mimicry can create entertaining, yet realistic sounding target speaker renditions. As mimicry tends to exaggerate prosodic, idiosyncratic and lexical behavior, it is unclear how modern spectral-feature automatic speaker verification systems respond to mimicry \"attacks\". We study the vulnerability of two well-known speaker recognition systems, traditional Gaussian mixture model . universal background model (GMM-UBM) and a state-of-the-art i-vector classifier with cosine scoring. The material consists of one professional Finnish imitator impersonating five well-known Finnish public figures. In a carefully controlled setting, mimicry attack does slightly increase the false acceptance rate for the i-vector system, but generally this is not alarmingly large in comparison to voice conversion or playback attacks.", "title": "I-vectors meet imitators: on vulnerability of speaker verification systems against voice mimicry"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/gomezbarrero13_interspeech.html", "abstract": "This work studies the vulnerabilities of i-vector based speaker verification systems against indirect attacks. Particularly, we exploit the one-to-one representation of speakers via their corresponding i-vectors to perform Hill-Climbing based attacks; under the hypothesis that the inherent low-dimensionality of i-vectors might represent a potential security breach to fraudulently access the system. The conducted attacks followed a standard experimental protocol already applied to other biometric systems based on face or signature; and they were tested against a state-of-art PLDA speaker verification system in the framework of the NIST SRE 2010 evaluation campaign. Specifically, up to 200 speakers, 100 female and 100 male, were attacked supplanting their corresponding i-vectors by those derived with the Hill-Climbing approach. Experiments show the success of the proposed attack compared with those based on brute force, achieving high Success Rates (up to 100%) and needing half as many comparisons as the brute force access attempts. These results evidence the vulnerability of i-vector based speaker verification systems in those scenarios where access to the matcher score is granted multiple times. As a countermeasure to minimize the effect of the attack score quantization is also evaluated.", "title": "Security evaluation of i-vector based speaker verification systems against hill-climbing attacks"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/alegre13_interspeech.html", "abstract": "This paper presents a new countermeasure for the protection of automatic speaker verification systems from spoofed, converted voice signals. The new countermeasure is based on the analysis of a sequence of acoustic feature vectors using Local Binary Patterns (LBPs). Compared to existing approaches the new countermeasure is less reliant on prior knowledge and affords robust protection from not only voice conversion, for which it is optimised, but also spoofing attacks from speech synthesis and artificial signals, all of which otherwise provoke significant increases in false acceptance. The work highlights the difficulty in detecting converted voice and also discusses the need for formal evaluations to develop new countermeasures which are less reliant on prior knowledge and thus more reflective of practical use cases.", "title": "A new speaker verification spoofing countermeasure based on local binary patterns"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/kons13_interspeech.html", "abstract": "In the past few years state-of-the-art text-dependent speaker verification technology has improved significantly in terms of the ability to accept target speakers and reject imposters. As a result, the use of speaker verification systems for real world security is increasing. Real world usage of speaker verification technology raises the issue of spoofing attacks. As part of our efforts for developing countermeasures for such attacks, we describe in this paper a spoofing attack based on simple voice transformation and report an analysis of the vulnerability of several state-of-the-art text-dependent algorithms to such an attack.", "title": "Voice transformation-based spoofing of text-dependent speaker verification systems"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/wu13_interspeech.html", "abstract": "Voice conversion, a technique to change one's voice to sound like that of another, poses a threat to even high performance speaker verification system. Vulnerability of text-independent speaker verification systems under spoofing attack, using statistical voice conversion technique, was evaluated and confirmed in our previous work. In this paper, we further extend the study to text-dependent speaker verification systems. In particular, we compare both joint density Gaussian mixture model (JD-GMM) and unit-selection (US) spoofing methods and, for the first time, the performances of text-independent and text-dependent speaker verification systems in a single study. We conduct the experiments using RSR2015 database which is recorded using multiple mobile devices. The experimental results indicate that text-dependent speaker verification system tolerates spoofing attacks better than the text-independent counterpart.", "title": "Vulnerability evaluation of speaker verification under voice conversion spoofing: the effect of text constraints"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/sperber13_interspeech.html", "abstract": "We propose a method for efficient off-line speech transcription through respeaking. Speech is segmented into smaller utterances using an initial automatic transcript. Respeaking is performed segment by segment, while confidence filtering helps save supervision effort. We conduct detailed experiments comparing speaking vs. typing, sequential vs. confidence-ordered supervision, and examine the effect of the respeaking word error rate on correction efficiency. Our results demonstrate that the proposed method can not only outperform typing in terms of correction efficiency, but is also much less demanding for the respeakers than traditional respeaking methods, consequently helping to keep costs down. Annotation and Classification of Political Advertisements", "title": "Efficient speech transcription through respeaking"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/kim13c_interspeech.html", "abstract": "Political advertising has changed drastically over the last several decades with video advertising becoming a major force in all outlets from the traditional TV medium to online media. In this work, we attempt to automatically classify the political advertisements along various dimensions such as purpose, content and emotion. First, we use a crowd-sourcing method to annotate the political advertisements in terms of how viewers perceive them along the above mentioned aspects. Then, we use audio-based features and machine learning algorithms for automatic classification tasks. In particular, we deploy speech-related features along with support vector machine (SVM) and music-related features along with k-nearest neighbor (KNN). The analysis of crowd-sourced annotations shows that the same advertisements are often used to serve multiple purpose and that certain content categories such as speech clips from the candidate and other public figures are more prevalent. The experimental results using speech/audio features on advertisements aired during the U.S. presidential campaign of 2012 show promising classification performance.", "title": "Annotation and classification of Political advertisements"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/higashinaka13_interspeech.html", "abstract": "Chatbot-like dialogue agents typically use character-anchored question-answer pairs for response generation. However, creating such pairs requires manual effort and is very costly. We propose using role play to efficiently collect such character-anchored question-answer pairs. In our role-play-based scheme, users play the roles of certain characters and respond to questions. Since multiple users can play the role of the same character, questions addressed to one character can be quickly answered by any of the role-players of that character. This leads to the efficient collection of character-anchored question-answer pairs. In addition, role play has a certain entertaining nature, which could encourage the active participation of users. From the analysis of a trial run of a website based on our scheme, we found that users were highly motivated to provide data and that appropriate question-answer pairs were collected, which suggests the effectiveness of role play for dialogue data collection.", "title": "Using role play for collecting question-answer pairs for dialogue agents"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/arimoto13_interspeech.html", "abstract": "The goal of this study is to elucidate differences in speakers\u0081f emotional expressions in behavioral and autonomic responses. Verbal and non-verbal emotional behaviors of interlocutors were recorded during two types of dialogs (competitive and cooperative). Autonomic nervous system (ANS) activity (heart rate and skin conductance level) was also recorded as an internal measure of emotional reactions toward an interlocutor. To annotate the emotional states of speakers, the speakers who participated in the recording evaluated their own emotional states (arousal, valence and positivity) and their interlocutor\u0081fs states along with the time course of the dialogs. The behavioral and autonomic emotional reactions were used as independent variables for speaker-independent and speaker-specific models to predict a speaker's emotion. The results demonstrate that speaker-independent models could explain each emotional state in a certain degree; in contrast, some speaker-specific models could explain each emotional state with moderate or high accuracy. Moreover, a comparison of the absolute standard partial regression coefficients of each variable of the models revealed that there are two types of emotional expression styles, one in which emotional behavioral expression is dominant and another in which emotional autonomic reaction is dominant.", "title": "Individual differences of emotional expression in speaker's behavioral and autonomic responses"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/wechsung13_interspeech.html", "abstract": "The gulf between user and system can be minimized by adapting the system to the user's natural characteristics. So-called anthropomorphic interfaces represent one strategy of such an adaption as they are assumed to provide a more human-like and therefore more natural interaction. However, regarding the evaluation of anthropomorphic interfaces, the well-known and empirically tested instruments are limited to educational contexts. Hence, this paper describes the first steps towards the development of an evaluation instrument applicable to a wide range of such interfaces.", "title": "Development and validation of the conversational agents scale (CAS)"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/riccardi13_interspeech.html", "abstract": "A widely used strategy in human and machine performance enhancement is achieved through feedback. In this paper we investigate the effect of live motivational feedback on motivating crowds and improving the performance of the crowdsourcing computational model. The provided feedback allows workers to react in real-time and review past actions (e.g. word deletions); thus, to improve their performance on the current and future (sub) tasks. The feedback signal can be controlled via clean (e.g. expert) supervision or noisy supervision in order to trade-off between cost and target performance of the crowd-sourced task. The feedback signal is designed to enable crowd workers to improve their performance at the (sub) task level. The type and performance of feedback signal is evaluated in the context of a speech transcription task. Amazon Mechanical Turk (AMT) platform is used to transcribe speech utterances from different corpora. We show that in both clean (expert) and noisy (worker/turker) real-time feedback conditions the crowd workers are able to provide significantly more accurate transcriptions in a shorter time.", "title": "Motivational feedback in crowdsourcing: a case study in speech transcription"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/fox13b_interspeech.html", "abstract": "Recognition of speech in natural environments is a challenging task, even more so if this involves conversations between several speakers. Work on meeting recognition has addressed some of the significant challenges, mostly targeting formal, business style meetings where people are mostly in a static position in a room. Only limited data is available that contains high quality near and far field data from real interactions between participants. In this paper we present a new corpus for research on speech recognition, speaker tracking and diarisation, based on recordings of native speakers of English playing a table-top wargame. The Sheffield Wargames Corpus comprises 7 hours of data from 10 recording sessions, obtained from 96 microphones, 3 video cameras and, most importantly, 3D location data provided by a sensor tracking system. The corpus represents a unique resource, that provides for the first time location tracks (1.3Hz) of speakers that are constantly moving and talking. The corpus is available for research purposes, and includes annotated development and evaluation test sets. Baseline results for close-talking and far field sets are included in this paper.", "title": "The sheffield wargames corpus"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/kumar13_interspeech.html", "abstract": "The expertise required to develop a speech recognition system with reasonable accuracy for a given task is quite significant, and precludes most non-speech experts from integrating speech recognition into their own research. While an initial baseline recognizer may readily be available or relatively simple to acquire, identifying the necessary accuracy optimizations require an expert understanding of the application domain as well as significant experience in building speech recognition systems. This paper describes our efforts and experiments in formalizing knowledge from speech experts that would help novices by automatically analyzing an acoustic context and recommending appropriate techniques for accuracy gains. Through two recognition experiments, we show that it is possible to model experts' understanding of developing accurate speech recognition systems in a rule-based knowledge base, and that this knowledge base can accurately predict successful optimization techniques for previously seen acoustic situations, both in seen and unseen datasets. We argue that such a knowledge base, once fully developed, will be of tremendous value for boosting the use of speech recognition in research and development on non-mainstream languages and acoustic conditions.", "title": "Formalizing expert knowledge for developing accurate speech recognizers"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/moubayed13_interspeech.html", "abstract": "In order to understand and model the dynamics between interaction phenomena such as gaze and speech in face-to-face multiparty interaction between humans, we need large quantities of reliable, objective data of such interactions. To date, this type of data is in short supply. We present a data collection setup using automated, objective techniques in which we capture the gaze and speech patterns of triads deeply engaged in a high-stakes quiz game. The resulting corpus consists of five one-hour recordings, and is unique in that it makes use of three state-of-the-art gaze trackers (one per subject) in combination with a state-of-the-art conical microphone array designed to capture roundtable meetings. Several video channels are also included. In this paper we present the obstacles we encountered and the possibilities afforded by a synchronised, reliable combination of large-scale multi-party speech and gaze data, and an overview of the first analyses of the data.", "title": "Analysis of gaze and speech patterns in three-party quiz game interaction"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/galibert13_interspeech.html", "abstract": "Speaker Diarization and Automatic Speech Recognition have been a topic of research for decades. Evaluating the developed systems has been required for almost as long. Following the NIST initiatives a number of metrics have become standard to handle these evaluations, namely the Diarization Error Rate and the Word Error Rate. The initial definitions of these metrics and, more importantly, their implementations, were designed for single-speaker speech. One of the aims of the OSEO Quaero and the ANR ETAPE projects was to investigate the capabilities of Diarization and ASR systems in the presence of overlapping speech. Evaluating said systems required extending the metrics definitions and adapting the algorithmic approaches required for their implementation. This paper presents these extensions and adaptations and the open tools that provide them.", "title": "Methodologies for the evaluation of speaker diarization and automatic speech recognition in the presence of overlapping speech"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/sangwan13_interspeech.html", "abstract": "NASA's Apollo program stands as one of mankind's greatest achievements in the 20th century. During a span of 4 years (from 1968 to 1972), a total of 9 lunar missions were launched and 12 astronauts walked on the surface of the moon. It was one the most complex operations executed from scientific, technological and operational perspectives. In this paper, we describe our recent efforts in gathering and organizing the Apollo program data. It is important to note that the audio content captured during the 7.10 day missions represent the coordinated efforts of hundreds of individuals within NASA Mission Control, resulting in well over 100k hours of data for the entire program. It is our intention to make the material stemming from this effort available to the research community to further research advancements in speech and language processing. Particularly, we describe the speech and text aspects of the Apollo data while pointing out its applicability to several classical speech processing and natural language processing problems such as audio processing, speech and speaker recognition, information retrieval, document linking and a range of other processing tasks which enable knowledge search, retrieval, and understanding. We also highlight some of the outstanding opportunities and challenges associated with this dataset. Finally, we also present initial results for speech recognition, document linking, and audio processing systems.", "title": "'houston, we have a solution': using NASA apollo program to advance speech and language processing technology"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/matousek13_interspeech.html", "abstract": "We investigate the problem of automatic detection of annotation errors in single-speaker read-speech corpora used for text-to-speech (TTS) synthesis. Various word-level feature sets were used, and the performance of several detection methods based on support vector machines, extremely randomized trees, k-nearest neighbors, and the performance of novelty and outlier detection are evaluated. We show that both word- and utterance-level annotation error detections perform very well with both high precision and recall scores and with F1 measure being almost 90%, or 97%, respectively.", "title": "Annotation errors detection in TTS corpora"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/ahmed13_interspeech.html", "abstract": "A frugal approach to construct speech corpora, specially for resource deficient languages, is to exploit collections of speech and corresponding text data available in audio books, news, lectures. However, using these resources for building speech corpora require an alignment of the long duration speech data with the accompanying text data. Existing techniques for automatic speech-text alignment of long audio files assume availability of a basic speech recognition engine and hence cannot be directly used for resource deficient languages. In this paper, we propose a novel technique for sentence level alignment of long speech-text data by exploiting the syllable information in speech and text data. The proposed technique does not depend on the availability of any speech recognition models and hence can be used for resource deficient languages.", "title": "Technique for automatic sentence level alignment of long speech and transcripts"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/hoffmann13b_interspeech.html", "abstract": "Large recordings like radio broadcasts or audio books are an interesting additional resource for speech research but often have nothing but an orthographic transcription available in terms of annotation. The alignment between text and speech is an important first step for further processing. Conventionally, this is done by using automatic speech recognition (ASR) on the speech corpus and then aligning recognition result and transcription. This has the drawback that an ASR system needs to be available for the target language. In this paper, we introduce an approach based on forced alignment with hidden Markov models (HMM) normally applied only to shorter utterances. We show that by using a set of generalized phone models computed over phonetic groups, forced alignment is able to reliably align text and speech while being robust against transcription errors. In contrast to ASR methods, the alignment models can be used in a language-independent way.", "title": "Text-to-speech alignment of long recordings using universal phone models"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/stan13_interspeech.html", "abstract": "This paper introduces a method for lightly supervised discriminative training using MMI to improve the alignment of speech and text data for use in training HMM-based TTS systems for low-resource languages. In TTS applications, due to the use of long-span contexts, it is important to select training utterances which have wholly correct transcriptions. In a low-resource setting, when using poorly trained grapheme models, we show that the use of MMI discriminative training at the grapheme-level enables us to increase the amount of correctly aligned data by 40%, while maintaining a 7% sentence error rate and 0.8% word error rate. We present the procedure for lightly supervised discriminative training with regard to the objective of minimising sentence error rate.", "title": "Lightly supervised discriminative training of grapheme models for improved sentence-level alignment of speech and text data"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/sapru13_interspeech.html", "abstract": "Social roles characterize relation between participants in a conversation and, in turn, influence their interaction patterns. This paper investigates automatic social role recognition in professional meetings using a completely discriminative framework based on conditional random fields. We present a novel approach which combines information from multiple layers of data. The conversation layer models the influence of social roles on turn taking patterns of participants present in multiparty interactions. A conditional random field augmented with hidden state sequences is used to estimate the posterior distribution of social roles in this layer. The other novelty of our approach consists in modeling statistical dependencies between roles across adjacent segments of meeting. The posterior distribution estimated in conversation layer is combined with role transition information to improve the model. Experiments conducted on more than 40 hours of data reveal that the proposed approach reaches a recognition accuracy of 67% in classifying four social roles using information from conversation layer. Moreover, recognition accuracy increases to 70% when information from multiple layers is taken into consideration.", "title": "Automatic social role recognition in professional meetings using conditional random fields"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/draxler13_interspeech.html", "abstract": "In this paper we present a comparison of the performance of the automatic phonetic segmentation and labeling system MAUS for two different signal qualities. For a forensic study on the similarity of voices within a family [2], eight speakers from four families were recorded simultaneously in both high bandwidth and mobile phone quality. The recordings were then automatically segmented and labeled using MAUS. The results show marked effects on the segment counts and durations between the two signal qualities: for the mobile phone quality, the segment counts for fricatives were much lower than for high quality recordings, whereas the segment counts for plosives and vowels increased. The segment duration of fricatives was much lower for mobile phone recordings, slightly lower for the front vowels, but quite much longer for the back and low vowels.", "title": "Same same but different \u2014 an acoustical comparison of the automatic segmentation of high quality and mobile telephone speech"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/hofe13_interspeech.html", "abstract": "This paper investigates the performance of a silent speech interface (SSI) based on permanent-magnetic articulography (PMA) across several speakers. In a previously published study, the SSI was shown to be capable of distinguishing between voiced and unvoiced plosives ([b,p] and [d,t]) in data recorded from a single speaker; a surprising result in a system without access to speech acoustics. The study presented in this paper verifies this finding for three male native English speakers. Furthermore, the speech data used in this investigation was designed to shed light on the capability of the SSI to differentiate between a wider range of phones. A particularly interesting result is that it is possible to distinguish between voiced and unvoiced fricatives ([s,z] and [f,v]) as well as plosives, using PMA data.", "title": "Performance of the MVOCA silent speech interface across multiple speakers"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/andrademiranda13_interspeech.html", "abstract": "The present work describes a new approach for the automatic tracking of the glottal area from high-speed digital images of the larynx. This approach involves three processes: Firstly, the frame with the maximal glottal opening is found automatically, by detecting the frame for which the sum of pixel intensities is minimum. Secondly, a segmentation algorithm based on active contours is used to detect the glottal space and build an initial template. Finally, using the normalized cross correlation the surface that represents the best matching between the initial template and the next frame is obtained. The surface area corresponds, at the same time, to the glottal space and to the new template. The process is repeated for each frame in order to obtain the new template and the new best cross correlation surface. This process is done iteratively until the last frame of the sequence has been reached. The performance, effectiveness and validation of the approach is demonstrated even in high-speed recordings in which the images present an inappropriate closure of the vocal folds.", "title": "Automatic glottal tracking from high-speed digital images using a continuous normalized cross correlation"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/bocklet13_interspeech.html", "abstract": "Articulation and phonation is affected in 70% to 90% of patients with Parkinson's disease (PD). This study focuses on the question whether speech carries information about 1. PD being present at a speaker or not, and 2. estimating the severity of PD (if present). We first perform classification experiments focusing on the automatic detection of PD as a 2-class problem (PD vs. healthy speakers). The detection of severity is described as a 3-class task based on the Unified Parkinson's Disease Rating Scale (UPDRS) ratings. We employ acoustic, prosodic and glottal features on different kinds of speech tests: various syllable repetition tasks, read sentences and texts, and monologues. Classification is performed in either case by SVMs. We report recognition results of 81.9% when trying to differentiate between normally speaking persons and speakers with PD. With system fusion we achieved a recognition results of 59.1% on the task of UPDRS classification.", "title": "Automatic evaluation of parkinson's speech \u2014 acoustic, prosodic and voice related cues"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/orosanu13_interspeech.html", "abstract": "This article analyzes the phonetic decoding performance obtained with different choices of linguistic units. The context is to later use such an approach as a support for helping communication with deaf people, and to run it on an embedded decoder on a portable terminal, which introduces constrains on the model size. As a first step, this paper presents and analyses the performance of various approaches. Two baseline systems are considered, one relying on a large vocabulary speech recognizer, and another one relying on a phonetic n-gram language model. Then syllable-based lexicons and language models are investigated. Various lexicon sizes are studied by setting thresholds on their frequency of occurrences in the training data. Evaluations are conducted on the ESTER and ETAPE speech corpora. Keeping only the most frequent syllables leads to a limited-size lexicon and language model, which nevertheless provides good phonetic decoding performance. The phone error rate is only 4% worse (absolute) than the phone error rate obtained with the large vocabulary recognizer, and much better than the phone error rate obtained with the phone n-gram language model.", "title": "Comparison of approaches for an efficient phonetic decoding"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/christensen13_interspeech.html", "abstract": "One of the main clinical applications of speech technology is in voice-enabled assistive technology for people with disordered speech. Progress in this area is hampered by a sparseness in suitable data and recent research have focused on ways of incorporating knowledge about typical (i.e., un-impaired) speech through the use of e.g., deep belief neural networks. This paper presents a new way of using deep belief neural networks trained on typical speech, namely to improve pronunciations for individual speakers. Analysis of the posterior probabilities show a clear correlation between measured pronunciation \u0081edisorderedness' and the overall speech recognition performance of the full system. Based on this, we propose a method to use deep belief network outputs to i) identify which words are pronounced differently than what would be expected from a typical pronunciation, and ii) subsequently generate new pronunciations. We investigate different methods for pronunciation generation as well as what is the best way of using the modified pronunciations to inform the system development stages. Using the UAspeech database of disordered speech, we demonstrate improvement in average accuracy of 69.76% to 70.51%, with some speakers showing individual improvements of up to 10%.", "title": "Learning speaker-specific pronunciations of disordered speech"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/lopezludena13_interspeech.html", "abstract": "This paper presents a methodology for adapting an advanced communication system for deaf people in a new domain. This methodology is a user-centered design approach consisting of four main steps: requirement analysis, parallel corpus generation, technology adaptation to the new domain, and finally, system evaluation. In this paper, the new considered domain has been the dialogues in a hotel reception. With this methodology, it was possible to develop the system in a few months, obtaining very good performance: good speech recognition and translation rates (around 90%) with small processing times.", "title": "Adapting a speech into sign language translation system to a new domain"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/vaerenberg13_interspeech.html", "abstract": "The clinical assessment of speech discrimination by professional audiologists is resource intensive. Yet discrepancies in language or dialect between the test subject and the audiologist may cause a significant bias in the test result. To address these issues, a speech audiometric test (SAT) has been designed to be language/dialect independent and to allow automated scoring by means of an MFCCbased Dynamic Time Warping alignment measure. A Pearson correlation of 0.83 was found between the automatic scores and human phoneme scoring. Normative data were obtained and compared to conventional SATs which revealed differences in speech reception thresholds within 2 dB.", "title": "Language-universal speech audiometry with automated scoring"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/hammer13_interspeech.html", "abstract": "This paper describes a distance measure which estimates the distance between a language sample and a reference corpus with regard to graphemes, phonemes and the relation between them. The underlying assumption of this approach is that a language\u0081fs phoneme distribution can be partially accessed via graphemes. The advantage of using such a measure in speech audiometry is twofold: (i) it may be applied to determine how representative existing word lists are with respect to the distribution of speech sounds in the target language of the test subject; (ii) it enables the audiologist to generate highly representative lists based on large corpora of languages for which broad phonetic transcription is lacking. In this paper the development of the de novo distance measure is described and demonstrated for Dutch. The technique itself however, is language-independent and has been applied successfully to 10 other EU-languages. As such, it paves the way to generating representative word lists as part of speech audiometric test batteries for any given language.", "title": "Balancing word lists in speech audiometry through large spoken language corpora"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/lopezludena13b_interspeech.html", "abstract": "This paper presents the SAILSE Project (Sistema Avanzado de Informacion en Lengua de Signos Espanola . Spanish Sign Language Advanced Information System). This project aims to develop an interactive system for facilitating the communication between a hearing and a deaf person. The first step has been the linguistic study, including a sentence collection, its translation into LSE (Lengua de Signos Espanola.Spanish Sign Language), and sign generation. After this analysis, the paper describes the interactive system that integrates an avatar to represent the signs, a text to speech converter and several translation technologies. Finally, this paper presents the set up carried out with deaf people and the main conclusions extracted from it.", "title": "Developing an information system for deaf"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/kim13d_interspeech.html", "abstract": "Dysarthria is a motor speech disorder that impairs the physical production of speech. Modern automatic speech recognition for normal speech is ineffective for dysarthric speech due to the large mismatch of acoustic characteristics. In this paper, a new speaker adap- tation scheme is proposed to reduce the mismatch. First, a speaker with dysarthria is classified into one of the pre-defined severitylevels, and then an initial model to be adapted is selected depending on their severity-level. The candidates of an initial model are generated using dysarthric speech associated with their labeled severitylevel in the training phase. Finally, speaker adaptation is applied to the selected initial model. Evaluation of the proposed method on a database of several hundred words for 31 speakers with moderate to mild dysarthria showed that the proposed approach provides substantial improvement over the conventional speaker-adaptive system when a small amount of adaptation data is available.", "title": "Dysarthric speech recognition using dysarthria-severity-dependent and speaker-adaptive models"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/muhammad13_interspeech.html", "abstract": "In this paper, a new pathological voice detection and pathology classification method based on MPEG-7 audio low-level features is proposed. MPEG-7 features are originally used for multimedia indexing, which includes both video and audio. Indexing is related to event detection, and as pathological voice is a separate event than normal voice, we show that MPEG-7 audio low-level features can do very well in detecting pathological voices, as well as classifying the pathologies. The experiments are done on a subset of sustained vowel (namely, \"AH\") recordings from healthy and voice pathological subjects, from the MEEI database. For classification, support vector machine (SVM) with 10-fold cross-validation is applied. The proposed method with MPEG-7 audio features and SVM classification is evaluated on voice pathology detection, as well as pathology classification. The experiment results show that the proposed method outperforms some recent methods in the literature both in detection and in classification. The proposed method is able to achieve an accuracy of 99.994 \u0081} 0.0105% for detecting pathological voices and an accuracy of 100% for binary pathologies classifying.", "title": "Voice pathology detection and classification using MPEG-7 audio low-level features"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/kacha13_interspeech.html", "abstract": "Cepstral-based acoustic cues for disordered voices analysis have been investigated in a number of studies. It has been shown that cepstral-based acoustic cues such as the harmonics-to-noise ratio (HNR), the amplitude of the first rhamonic (R1A) provide acoustic correlates for hoarse voice quality. The aim of this presentation is to investigate an acoustic analysis of speech by means of spectral acoustic cues obtained via empirical mode decomposition (EMD) of the log of the magnitude spectrum of the speech signal as an alternative to the cepstral-based acoustic cues. The spectral acoustic cues investigated in this article are the harmonics-to-noise ratio (HNR) and the amplitude of the first harmonic (H1A). The EMDbased spectral acoustic cues are evaluated on a corpus of synthetic stimuli generated by a synthesizer of disordered voices as well as on a corpus of natural sustained vowels comprising 251 normophonic and dysphonic speakers. The performances of the EMD-based spectral acoustic cues (HNREMD and H1A) are compared to those of cepstral-based acoustic cues (HNRceps and R1A). Experimental results show that the EMD-based spectral acoustic cues outperform the cepstral-based acoustic measures in terms of the correlation with the perceived degree of hoarseness defined as the global quality of the voice and provided by the grade (G) in the GRBAS scale.", "title": "Empirical mode decomposition-based spectral acoustic cues for disordered voices analysis"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/aihara13_interspeech.html", "abstract": "We present in this paper a noise robust voice conversion (VC) method for a person with an articulation disorder resulting from athetoid cerebral palsy. The movements of such speakers are limited by their athetoid symptoms, and their consonants are often unstable or unclear, which makes it difficult for them to communicate. In this paper, exemplar-based spectral conversion using Nonnegative Matrix Factorization (NMF) is applied to a voice with an articulation disorder in real noisy environments. In this paper, in order to deal with background noise, an input noisy source signal is decomposed into the clean source exemplars and noise exemplars by NMF. Also, to preserve the speaker's individuality, we use a combined dictionary that was constructed from the source speaker\u0081fs vowels and target speaker's consonants. The effectiveness of this method was confirmed by comparing its effectiveness with that of a conventional Gaussian Mixture Model (GMM)-based method.", "title": "Exemplar-based individuality-preserving voice conversion for articulation disorders in noisy environments"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/christensen13b_interspeech.html", "abstract": "Recently there has been increasing interest in ways of using out-ofdomain (OOD) data to improve automatic speech recognition performance in domains where only limited data is available. This paper focuses on one such domain, namely that of disordered speech for which only very small databases exist, but where normal speech can be considered OOD. Standard approaches for handling small data domains use adaptation from OOD models into the target domain, but here we investigate an alternative approach with its focus on the feature extraction stage: OOD data is used to train featuregenerating deep belief neural networks. Using AMI meeting and TED talk datasets, we investigate various tandem-based speaker independent systems as well as maximum a posteriori adapted speaker dependent systems. Results on the UAspeech isolated word task of disordered speech are very promising with our overall best system (using a combination of AMI and TED data) giving a correctness of 62.5%; an increase of 15% on previously best published results based on conventional model adaptation. We show that the relative benefit of using OOD data varies considerably from speaker to speaker and is only loosely correlated with the severity of a speaker's impairments.", "title": "Combining in-domain and out-of-domain speech data for automatic recognition of disordered speech"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/mai13_interspeech.html", "abstract": "In cochlear implants, limited spectral and temporal information is provided. Previous studies argued for different effects of temporal information on speech identification in adverse environments. Particularly, it is unclear how speech intelligibility is influenced by the low-pass cutoff frequency of temporal envelope extractors in noise. The current study explored this issue with Mandarin noisevocoder simulation in babble noise. Mandarin spoken sentences were mixed with multi-talker babble at different SNRs (4-, 6- and 8-dB) and then noise-vocoded. The vocoders had three numbers of logarithmic-spaced frequency channels (6, 8 and 10) with lowpass filtering the Hilbert envelope at 50 or 500 Hz in each channel. We measured the sentence intelligibility with native normal-hearing subjects and found that intelligibility for the high frequency cutoff condition (500-Hz) is significantly higher at 8-dB SNR with 6 and 8 channels, but significantly lower at 4-dB SNR with 6 channels than the low frequency cutoff condition (50-Hz). This finding suggests that high cutoff frequency of the envelope extractor in noise-vocoders can either improve or impair speech intelligibility in babble noise, hinging on the number of channels and the SNR level. Potential implications of the current finding for cochlear implant designs were further discussed in the paper.", "title": "Effects of envelope filter cutoff frequency on the intelligibility of Mandarin noise-vocoded speech in babble noise: implications for cochlear implants"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/schwenk13_interspeech.html", "abstract": "Language models play a very important role in many natural language processing applications, in particular large vocabulary speech recognition and statistical machine translation. For a long time, back-off n-gram language models were considered to be the state-of-art when large amounts of training data are available. Recently, so called continuous space methods or neural network language models have shown to systematically outperform these models and they are getting increasingly popular. This article describes an open-source toolkit that implements these models in a very efficient way, including support for GPU cards. The modular architecture makes it very easy to work with different data formats and to support various alternative models. Using data selection, resampling techniques and a highly optimized code, training on more than five billions words takes less than 24 hours. The resulting models achieve reductions in the perplexity of almost 20%. This toolkit has been very successfully applied to various languages for large vocabulary speech recognition and statistical machine translation. By making available this toolkit we hope that many more researchers will be able to work on this very promising technique, and by these means, quickly advance the field.", "title": "CSLM \u2014 a modular open-source continuous space language modeling toolkit"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/shi13b_interspeech.html", "abstract": "Recurrent neural network based language models (RNNLM) have been demonstrated to outperform traditional n-gram language models in automatic speech recognition. However, the superior performance is obtained at the cost of expensive model training. In this paper, we propose a sentence-independent subsampling stochastic gradient descent algorithm (SIS-SGD) to speed up the training of RNNLM using parallel processing techniques under the sentence independent condition. The approach maps the process of training the overall model into stochastic gradient descent training of submodels. The update directions of the submodels are aggregated and used as the weight update for the whole model. In the experiments, synchronous and asynchronous SIS-SGD are implemented and compared. Using a multi-thread technique, the synchronous SIS-SGD can achieve a 3-fold speed up without losing performance in terms of word error rate (WER). When multi-processors are used, a nearly 11-fold speed up can be attained with a relative WER increase of only 3%.", "title": "Speed up of recurrent neural network language models with sentence independent subsampling stochastic gradient descent"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/chang13b_interspeech.html", "abstract": "In this paper we propose a method for improving unsupervised language model (LM) adaptation by discriminatively filtering the adaptation training material. Two main issues are addressed in this solution: first, how to automatically identify recognition errors and more correct alternatives without manual transcription; second, how to update the model parameters based on the recognition error cues. Within the adaptation framework, we address the first issue by predicting regression pairs between recognition results from the baseline LM and an initial adapted LM, using features such as language model score difference. For the second issue, we adopted a data filtering approach to penalize potent error attractors introduced by the unsupervised adaptation data, using Ngram set difference statistics computed on the predicted regression pairs. Experimental results on a large real-world application of voice catalog search demonstrated that the proposed solution provides significant recognition error reduction over an initial adapted LM.", "title": "Improving unsupervised language model adaptation with discriminative data filtering"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/kobayashi13b_interspeech.html", "abstract": "We propose a lightly supervised training method for a discriminative language model (DLM) based on risk minimization criteria. In lightly supervised training, pseudo labels generated by automatic speech recognition (ASR) are used as references. However, as these labels usually include recognition errors, the discriminative models estimated from such faulty reference labels may degrade ASR performance. Therefore, an approach to prevent performance degradation is necessary for discriminative language modeling. In our proposed lightly supervised training, the DLM is estimated from a \"fused\" risk, which is a relaxed version of the conventional Bayes risk. The fused risk is computed in a supervised manner when pseudo labels are accepted as references with high confidence while computed in an unsupervised manner when the labels are rejected due to low confidence. Accordingly, minimizing the fused risk for the training lattices results in a DLM with smoothed model parameters. The experimental results show that our proposed lightly supervised training method significantly reduced the word error rate compared with DLMs trained in conventional lightly supervised manners.", "title": "Lightly supervised training for risk-based discriminative language models"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/dikici13_interspeech.html", "abstract": "Semi-supervised discriminative language modeling uses simulated N-best lists instead of real ASR outputs as its training examples. In this study we apply two techniques in which artificial examples are generated using a WFST and an MT system trained on pairs of reference text and ASR output. We compare the performance of these techniques with the structured prediction and ranking variants of the WER-sensitive perceptron algorithm, and contrast with the supervised case where real ASR outputs are given as input. Choosing Turkish statistical morphs as n-gram features, we analyze the similarities between the hypotheses of these three setups and the number of utilized features. We show that the MT-based system yields the lowest WER, not only because the examples generated by this technique are more effective, but also because the ranking perceptron generalizes better with this setup. When trained on a combination of artificial WFST and MT data, the structured perceptron performs as well on an unseen test set as it does when trained on real ASR output.", "title": "Investigation of MT-based ASR confusion models for semi-supervised discriminative language modeling"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/oba13_interspeech.html", "abstract": "Discriminative language modeling is a successful approach to improving speech recognition accuracy. However, it requires a large amount of spoken data and manually transcribed reference text for training. This paper proposes an unsupervised training method to overcome this handicap. The key idea is to use an error rate estimator, instead of calculating the true error rate from the reference. In standard supervised approaches, the true error rate is used only for finding the Oracle, the minimum error rate hypothesis, and for prioritizing the competing hypotheses for weighted learning. Namely, we really need the error rate, not the reference. In our proposed method, estimates of the error rate are used instead, and so the references are not necessary. Our experiments show that our proposed method can generate a model that performs to the same level of accuracy as supervised methods.", "title": "Unsupervised discriminative language modeling using error rate estimator"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/rath13b_interspeech.html", "abstract": "In this paper, we present an in-depth analysis of a recently proposed method for speaker adaptation. The method involves a region-specific feature-space transformation, which we refer to as soft R-FMLLR. We argue that the method has certain difficulties, the most significant being the fact that it is non-invertible. An analysis that pertains to the singularity of the Jacobian matrix is presented, from which we note that the matrix becomes nearsingular at certain points in the feature space. It indicates that the transformation is non-invertible. We observe that under this case maximum likelihood estimation adversely affects the speech recognition performance. Moreover, sufficient statistics do not exist that makes the estimation procedure computationally very expensive. The concerns outlined above render the method to be unattractive. We propose a simple yet important modification, hard R-FMLLR, and show that the associated Jacobian matrix is assured to be full-rank, and it is computationally efficient. On a large vocabulary continuous speech recognition task the performance of the proposed method is shown to be better than soft R-FMLLR. Further, it is comparable to the widely used CMLLR with regression classes, especially when higher number of transforms are used.", "title": "A region-specific feature-space transformation for speaker adaptation and singularity analysis of jacobian matrix"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/wang13c_interspeech.html", "abstract": "Speech signals are usually affected by multiple acoustic factors, such as speaker characteristics and environment differences. Usually, the combined effect of these factors is modelled by a single transform. Acoustic factorisation splits the transform into several factor transforms, each modelling only one factor. This allows, for example, estimating a speaker transform in a noise condition and applying the same speaker transform in a different noise condition. To achieve this factorisation, it is crucial to keep factor transforms independent of each other. Previous work on acoustic factorisation relies on using different forms of factor transforms and/or the attribute of the data to enforce this independence. In this work, the independence is formulated in mathematically, and an explicit constraint is derived to enforce the independence. Using factorised cluster adaptive training (fCAT) as an application, experimental results demonstrates that the proposed explicit independence constraint helps factorisation when imbalanced adaptation data is used.", "title": "An explicit independence constraint for factorised adaptation in speech recognition"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/saz13_interspeech.html", "abstract": "This paper presents a novel approach to separate the effects of speaker and background conditions by application of featuretransform based adaptation for Automatic Speech Recognition (ASR). So far factorisation has been shown to yield improvements in the case of utterance-synchronous environments. In this paper we show successful separation of conditions asynchronous with speech, such as background music. Our work takes account of the asynchronous nature of the background, by estimation of condition-specific Constrained Maximum Likelihood Linear Regression (CMLLR) transforms. In addition, speaker adaptation is performed, allowing to factorise speaker and background effects. Equally, background transforms are used asynchronously in the decoding process, using a modified Hidden Markov Model (HMM) topology which applies the optimal transform for each frame. Experimental results are presented on the WSJCAM0 corpus of British English speech, modified to contain controlled sections of background music. This addition of music degrades the baseline Word Error Rate (WER) from 10.1% to 26.4%. While synchronous factorisation with CMLLR transforms provides 28% relative improvement in WER over the baseline, our asynchronous approach increases this reduction to 33%.", "title": "Asynchronous factorisation of speaker and background with feature transforms in speech recognition"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/yu13_interspeech.html", "abstract": "Cluster adaptive training (CAT) is a popular approach to train multiple-cluster HMMs for fast speaker adaptation in speech recognition. Traditionally, a cluster-independent decision tree is shared among all clusters, which could limit the modelling power of multiple-cluster HMMs. In this paper, each cluster is allowed to have its own decision tree. The intersections between the triphones subsets, corresponding to the leaf nodes of each cluster-dependent trees, are used to define a finer state sharing structure. The parameters of these intersections are constructed from the parameters of the leaf nodes of each individual decision tree. This is referred to as CAT with factorized decision trees (FD-CAT). FD-CAT significantly increases the modelling power without introducing additional free parameters. A novel iterative mean cluster update approach and a robust covariance matrix update method with united statistics are proposed to efficiently train FD-CAT. Experiments showed that using multiple decision trees can yield better performance than single decision tree. Furthermore, FD-CAT significantly outperformed traditional CAT system.", "title": "Cluster adaptive training with factorized decision trees for speech recognition"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/abdelhamid13_interspeech.html", "abstract": "Recently, we have proposed a novel fast adaptation method for the hybrid DNN-HMM models in speech recognition. This method relies on learning an adaptation NN that is capable of transforming input speech features for a certain speaker into a more speaker independent space given a suitable speaker code. Speaker codes are learned for each speaker during adaptation. The whole multispeaker training dataset is used to learn the adaptation NN weights. Our previous work has shown that this method is quite effective in adapting DNNs even when only a very small amount of adaptation data is available. However, the proposed method does not work well in the case of convolutional neural network (CNN). In this paper, we investigate the fast adaptation of CNN models. We first modify the speaker code based adaptation method to better suit to the CNN structure. Moreover, we investigate a new adaptation scheme using speaker specific adaptive nodes output weights. These weights scale different nodes outputs to optimize the model for new speakers. Experimental results on the TIMIT dataset demonstrates that both methods are quite effective in terms of adapting CNN based acoustic models and we can achieve even better performance by combining these two methods together.", "title": "Rapid and effective speaker adaptation of convolutional neural network based models for speech recognition"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/kintzley13_interspeech.html", "abstract": "In the construction of whole-word acoustic models, we have previously demonstrated substantial gains by using MAP estimation to introduce a simple prior model of phonetic timing. Based solely on the word's phonetic (dictionary) pronunciation, this simple model included no information about the individual durations of constituent phones. However, the problem of modeling segmental duration has long been studied in the text-to-speech (TTS) community. We draw upon this work to develop a classification and regression tree (CART) approach for constructing prior models of phonetic timing which considers factors such as syllable stress, syllable position, adjacent phone class and voicing. This improved prior model closes 33% of the gap in keyword spotting performance between highly supervised whole-word models and those estimated without any examples.", "title": "Text-to-speech inspired duration modeling for improved whole-word acoustic models"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/gregory13_interspeech.html", "abstract": "The duration of three infants' vocalisations were examined during a six-month longitudinal study. In contrast to most other infant research, this study included in its analysis all vocalisations including those deemed vegetative or those having non-modal voice quality. All three infants produced vocalisations which decreased in duration in the initial months. However between the 3rd and 5th month a significant increase (p<0.001) in the duration of vocalisations was found (from mean 207ms to 431ms). When vocalisations were analysed using perceptual voice quality categories, all were found to have significant differences (p<0.05) in duration relative to modal voice. Non-modal voice qualities showed initial decreases in duration before increasing in duration in later months. In contrast those vocalisations produced using modal voice showed a positive linear trend and had the greatest linear rate of change across the study. These findings highlight the importance of including a wide variety of infant vocalisations including those with non-modal voice quality in infant linguistic developmental studies.", "title": "Duration of early vocalisations"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/yang13b_interspeech.html", "abstract": "Children's vowel acquisition has long been examined on the basis of transcription-based evaluations of the accuracy rate of the vowel production in children before 5 years of age. This study examines the development of static and dynamic acoustic features in children between 3 and 7 years of age by comparing the acoustic features of children with those of adults. All acoustic analyses were based on the normalized formant frequency values to exclude the effect of different vocal tract size. The increasing compactness of individual vowel categories in the acoustic space evidenced the refinement of phonetic features in children in this age range. In addition, the vowel dispersion pattern of certain vowel plotted on the basis of formant frequency values at 5 temporal locations demonstrated positional change as well as differences in terms of the trajectory length. Results demonstrate that the acoustical development of vowels from children to adult norms is likely a long-term, graduate but not necessarily continuous process.", "title": "Acoustic development of vowel production in American English children"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/moulinfrier13_interspeech.html", "abstract": "Learning complex mappings between various modalities (typically articulatory, somatosensory and auditory) is a central issue in computationally modeling speech acquisition. These mappings are generally nonlinear and redundant, involving high dimensional sensorimotor spaces. Classical approaches consider two separate phases: a relatively pre-determined exploration phase analogous to infant babbling followed by an exploitation phase involving higher level communicative motivations. In this paper, we consider the problem as a developmental robotics one, in which an agent actively learns sensorimotor mappings of an articulatory vocal model. More specifically, we show how intrinsic motivations can allow the emergence of efficient exploration strategies, driving the way a learning agent will interact with its environment to collect an adequate learning set.", "title": "The role of intrinsic motivations in learning sensorimotor vocal mappings: a developmental robotics study"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/hazan13_interspeech.html", "abstract": "This study investigated the development of clear speech strategies in children. Groups of 20 talkers aged 9.10 and 13.14 years were recorded in pairs while they carried out \u0081espot the difference\u0081f picture tasks, either while hearing each other normally (\u0081eno barrier\u0081f) or when one talker heard the other via a noise vocoder (VOC), which led their interlocutor (\u0081etalker A\u0081f) to clarify their speech to maintain effective communication. Data were compared to those previously collected for 20 adults. Mean word duration, number and duration of pauses were calculated for talker A. Strategies used in response to direct requests for clarification were also classified. Children spoke at a slower rate than teens and adults, who did not differ. Relative to \u0081eno barrier\u0081f, all groups significantly reduced their speech rate in VOC and used longer pauses, but the relative change in pause rate across conditions was greater for adults than children or teens. In response to a direct clarification request, children and teens used a higher rate of repetitions than adults who used more varied strategies. These results suggest that although children use some strategies to clarify their speech in difficult conditions, other strategies continue to develop until late adolescence.", "title": "Children's timing and repair strategies for communication in adverse listening conditions"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/barbier13_interspeech.html", "abstract": "This paper investigates speech motor control maturity in 4-yearold Canadian French children. Acoustic and ultrasound data recorded from four children, and for comparison, from four adults, are presented and analyzed. Maturity of speech motor control is assessed by measuring two characteristics: token-totoken variability of isolated vowels, as a measure of motor control accuracy, and extra-syllabic anticipatory coarticulation within V1-C-V2 sequences. In line with theories of optimal motor control, anticipatory coarticulation is assumed to be based on the use of internal models of the speech apparatus and its efficiency is considered to reflect the maturity of these representations. In agreement with former studies, token-to-token variability is larger in children than in adults. An anticipation of V2 in V1 was found in all adults but in none of the children studied so far. These results indicate that children's speech motor control is immature from two perspectives: insufficiently accurate motor control patterns for vowel production, and inability to anticipate forthcoming gestures. Both aspects are discussed and interpreted in the context of the immaturity of the internal representations of the speech motor apparatus in 4-year-old children.", "title": "Speech planning as an index of speech motor control maturity"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/kinsman13_interspeech.html", "abstract": "The acoustics of the sound /s/ have been shown to vary as a function of speaker gender in previous research, but the origin of this gender dichotomy remains controversial. The present study aimed to investigate the age at which gender-specific /s/ production begins to appear, and whether this dichotomy was related to gender role formation. Thirty normally developing children aged 4 and 5 participated in a series of experiments which examined their speech production, physical development and gender role behaviour. Word-initial /s/ and /S/ were elicited and recorded for acoustic analysis. The mean spectral frequency of the fricative noise was calculated over the middle 40-ms window. Gender role behaviour was measured through both parental report and a free-play activity in the lab for the children. Our results revealed that even very young children have clearly distinguished gender role behaviour, as well as gender differentiated /s/ productions. No such differentiation was found for the production of /S/. Importantly, correlation between the acoustic variation in /s/ and gender role behaviour began to emerge in these subjects, as evidenced by productions of the word \"suitcase\". This relationship was discussed in regards to articulatory strategies, gender stereotypes and adult role models.", "title": "The relationship between gender-differentiated productions of /s/ and gender role behaviour in young children"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/berry13_interspeech.html", "abstract": "Articulatory data offers promising developments in our understanding of speech production and advances in speech technologies. However, it is more expensive and difficult to obtain than audio data, which means data collection must be carefully planned. This paper presents a method for designing an articulatory speech corpus comparable to the widely-used TIMIT corpus, for languages other than English, using Italian as a case study. This data-driven method searches freely-available online text corpora for a set of sentences that provide broad phonetic coverage, while still being small enough to be read in a single session, which is important given the often invasive nature of articulatory data collection. Sentences are first phonemically transcribed and scored based on negative log-likelihood of triphones, with sentences that have many rare triphones scoring higher. The search algorithm then finds sentences that have high scores, but also contain the most frequent triphones. Experiments show that the distribution of triphones in the automatically selected sentences is similar to that found in hand-constructed sentence sets for English, such as TIMIT, and offers broader phonetic coverage than selecting random sets of sentences.", "title": "Data-driven design of a sentence list for an articulatory speech corpus"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/zhu13_interspeech.html", "abstract": "Real-time magnetic resonance imaging (rtMRI) is a valuable emerging tool for studying the dynamics of vocal production. Conventional 2D rtMRI typically images the midsagittal plane of the vocal tract, acquiring data from all the important articulators. Dynamic 3D MRI would be a major advance, as it would provide 3D visualization of the vocal tract shaping dynamics, especially for the modeling of complex vocal tract geometries, such as liquid and fricative consonants, which is not easily available from 2D rtMRI. In this paper, we present an approach to directly acquire data in 3D using a highly temporally-undersampled stack-of-spirals imaging sequence, and perform reconstruction using a partially separable model. We demonstrate visualization of vocal tract dynamics from midsagittal and coronal scan planes in the articulation of fricative consonants.", "title": "Faster 3d vocal tract real-time MRI using constrained reconstruction"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/canevari13_interspeech.html", "abstract": "We present a strategy for learning Deep-Neural-Network (DNN)- based Acoustic-to-Articulatory Mapping (AAM) functions where the contribution of an articulatory feature (AF) to the global recon- struction error is weighted by its relevance. We first empirically show that when an articulator is more crucial for the production of a given phone it is less variable, confirming previous findings. We then compute the relevance of an articulatory feature as a function of its frame-wise variance dependent on the acoustic evidence which is estimated through a Mixture Density Network (MDN). Finally we combine acoustic and recovered articulatory features in a hybrid DNN-HMM phone recognizer. Tested on the MOCHATIMIT corpus, articulatory features reconstructed by a standardly trained DNN lead to a 8.4% relative phone error reduction (w.r.t. a recognizer that only uses MFCCs), whereas when the articulatory features are reconstructed taking into account their relevance the relative phone error reduction increased to 10.9%.", "title": "Relevance-weighted-reconstruction of articulatory features in deep-neural-network-based acoustic-to-articulatory mapping"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/tomaschek13_interspeech.html", "abstract": "A frequently replicated finding is that higher frequency words tend to be shorter and contain more strongly reduced vowels. However, little is known about potential differences in the articulatory gestures for high vs. low frequency words. The present study made use of electromagnetic articulography to investigate the production of two German vowels, [i] and [a], embedded in high and low frequency words. We found that word frequency differently affected the production of [i] and [a] at the temporal as well as the gestural level. Higher frequency of use predicted greater acoustic durations for long vowels; reduced durations for short vowels; articulatory trajectories with greater tongue height for [i] and more pronounced downward articulatory trajectories for [a]. These results show that the phonological contrast between short and long vowels is learned better with experience, and challenge both the Smooth Signal Redundancy Hypothesis and current theories of German phonology.", "title": "Word frequency, vowel length and vowel quality in speech production: an EMA study of the importance of experience"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/silva13_interspeech.html", "abstract": "Articulatory data can nowadays be obtained using a wide range of techniques, such as real-time magnetic resonance (RT-MRI), enabling acquisitions of large amounts of data. A major challenge arises: analysing these new large data sets to extract meaningful information regarding speech production in an expedite and replicable way. Traditional approaches such as superimposing vocal tract profiles and qualitatively characterizing relevant properties and differences, although providing valuable information, are rather inefficient and subjective. Therefore, analysis must evolve towards a more automated, quantitative approach. To tackle this issue we propose the use of objective measures to compare the configurations assumed by the vocal tract during the production of different sounds. The proposed framework provides quantitative data regarding differences pertaining meaningful regions under the influence of various articulators. Visual representation of such data is a key part of the proposal and some concrete forms of visualization are proposed to depict the differences found and corresponding direction of change. Application examples concerning the articulatory characterization of EP vowels are presented with promising results, paving the way towards automated and objective analyses of articulatory data.", "title": "Towards a systematic and quantitative analysis of vocal tract data"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/vaz13_interspeech.html", "abstract": "We present a method for speech enhancement of data collected in extremely noisy environments, such as those found during magnetic resonance imaging (MRI) scans. We propose a two-step algorithm to perform this noise suppression. First, we use probabilistic latent component analysis to learn dictionaries of the noise and speech+noise portions of the data and use these to factor the noisy spectrum into estimated speech and noise components. Second, we apply a wavelet packet analysis in conjunction with a wavelet threshold that minimizes the KL divergence between the estimated speech and noise to achieve further noise suppression. Based on both objective and subjective assessments, we find that our algorithm significantly outperforms traditional techniques such as nLMS, while not requiring prior knowledge or periodicity of the noise waveforms that current state-of-the-art algorithms require.", "title": "A two-step technique for MRI audio enhancement using dictionary learning and wavelet packet analysis"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/stella13_interspeech.html", "abstract": "The AG500 (Carstens Medizinelektronik GmbH) is widely used in electromagnetic articulography, as it allows the recording in 3D of the movements of the articulatory organs. However some anomalies in sensor position tracking were recognized and analyzed in previous works, which ultimately attributed them to numerical issues and not to external interferences. Recently a new upgraded model, the AG501, has been introduced. By means of both speech experiments and numerical measurements, we analyzed the reliability of the AG501 and compared it to that of the previous model, the AG500. While, in some scenarios, the positions obtained via the AG500 are afflicted by external perturbations, the AG501 showed a greater degree of precision in all the numerical experiments and in the speech analyses that have been performed so far.", "title": "Electromagnetic articulography with AG500 and AG501"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/badin13_interspeech.html", "abstract": "MRI allows characterizing the shape and position of speech articulators, but not tracking flesh points, since there are no physiological landmarks reliably associated with the highly deformable tissues of these articulators. This information is, however, interesting for studying the biomechanical properties of these organs as well as for modelling the relations between measurement modalities such as MRI and electromagnetic articulography. We have therefore attached fiducial markers made of MRI-visible polymers to a speaker's articulators, and recorded a corpus of MRI midsagittal images. We then determined and analyzed the articulators' contours and the markers' centre coordinates. We found that the \"apparent sliding\" of markers with respect to lips and tongue contours ranges between 0.6 and 1.5 cm. Specifically, the curvilinear distances between two tongue flesh points relative to the total length varied up to about \u0081}20%, confirming a non longitudinal elasticity of the contours. However, we have also found that the markers and jaw coordinates can predict the articulators\u0081f contours with a variance explanation of about 85%, and an RMS reconstruction error between 0.08 and 0.15 cm, compared with 74 to 95% of variance and 0.07 to 0.14 cm of RMS error for the original articulatory models.", "title": "Development and implementation of fiducial markers for vocal tract MRI imaging and speech articulatory modelling"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/schotz13_interspeech.html", "abstract": "This study used Functional Data Analysis (FDA) to analyse tongue articulation dynamics, more specifically the height and frontness of the tongue body and tip in the palatal vowels /i:, y:, 0ff :/ of two varieties of Swedish. Articulatory data were collected from nine speakers each of Gothenburg and Malmohus Swedish. Standard z-score transformations were used for speaker normalisation. Results showed that the tongue articulation for /i:/ and /y:/ is generally similar, and significantly different from /0ff :/ in both Malmohus and Gothenburg Swedish. We also found a subdivision of Gothenburg Swedish into two subtypes, where type 1 resembled Malmohus Swedish more. Significant differences in tongue body height were found between all varieties for all of the vowels, except for /y:/ between Gothenburg type 1 and Malmohus Swedish.", "title": "Functional data analysis of tongue articulation in palatal vowels: gothenburg and malm\u00f6hus Swedish /i\u02d0, y\u02d0, \u031f\u0289\u02d0/"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/green13_interspeech.html", "abstract": "Recent innovations in 3D motion capture technology such as electromagnetic articulography (EMA) are providing unprecedented access to the intricate movements of the articulators during speech production. Although these technological advances afford exciting opportunities for advancing the assessment and treatment of speech, they have presented new challenges associated with data collection, processing, and analysis. To address these challenges, we have standardized our EMA data collection protocols and developed a Matlab-based software tool, SMASH, for processing, visualizing, and analyzing speech movement data. The goal of the software is to advance research on speech production by improving the efficiency and reliability of speech movement analyses.", "title": "SMASH: a tool for articulatory data processing and analysis"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/lin13_interspeech.html", "abstract": "In a natural conversation, a complete emotional expression is typically composed of a complex temporal course representing temporal phases of onset, apex, and offset. In this study, subemotional states are defined to model the temporal course of an emotional expression in natural conversation. Hidden Markov Models (HMMs) are adopted to characterize the sub-emotional states; each represents one temporal phase. A sub-emotion language model, which considers the temporal transition between sub-emotional states (HMMs), is further constructed to provide a constraint on allowable temporal structures to determine an optimal emotional state. Experimental results show that the proposed approach yielded satisfactory results on the MHMC conversationbased affective speech corpus, and confirmed that considering the complex temporal structure in natural conversation is useful for improving the emotion recognition performance from speech.", "title": "Emotion recognition of conversational affective speech using temporal course modeling"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/altrov13_interspeech.html", "abstract": "The study addresses the role of empathy in the recognition of vocal emotions (joy, anger, sadness) when they are moderately expressed in speech which is listened to without the speaker being seen. The test was taken by 67 adults (29 male and 38 female subjects aged 30.60), whose empathy level had previously been measured by Baron-Cohen & Wheelwright's self-report questionnaire, the Empathy Quotient (EQ). In the test group only men (n=14) had a low empathy level (EQ<32), and only women (n=16) had a high empathy level (EQ>51). A comparison of the men (n=15) and women (n=22) who had a medium empathy quotient showed that gender was a factor in the recognition of emotions. To eliminate the possible gender influence, the recognition of emotions was tested by comparing the results of the men with a low EQ with those of the men with a medium EQ and the results of the women with a medium EQ with those of the women with a high EQ. According to the results, personal empathic ability does not affect the recognition of emotions from the voice.", "title": "The role of empathy in the recognition of vocal emotions"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/brunelliere13_interspeech.html", "abstract": "This event-related potential study examined the impact of imitating an unfamiliar accent on the processing of spoken words embedded in sentential contexts produced in that accent. The cloze probability effect in two groups of southern French speakers after they had to either listen to or imitate sentences spoken by a Belgian French speaker was tested. Speakers who did not imitate the unfamiliar accent showed a cloze probability effect on the phonological N200 wave, while those who did imitate the accent showed no effect on this component. Over a later time window, both groups showed a cloze probability effect on the N400, which is associated with lexical and semantic processing. Taken together, these results give clear evidence for processing benefits from the imitation of speech patterns, particularly at an acoustic/phonological level of processing.", "title": "Electrophysiological evidence for benefits of imitation during the processing of spoken words embedded in sentential contexts"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/ogane13_interspeech.html", "abstract": "In this experiment, we examined speech compensation to time-scale altered auditory feedback during the transition of the semivowel for a target utterance of /ija/. We examined speech compensation and the after effect in terms of three acoustic features, the maximum velocities on the F1 and F2 trajectories (VF1 and VF2) and the F1-F2 onset time difference (TD) during the transition, as well as listening test on the feedback speech. As the result, speech compensation was observed in VF1, VF2 and TD. The magnitude of speech compensation in VF1 and TD monotonically increased as amount of the time-scale perturbation. Speech compensation for time-scale altered auditory feedback is carried out primarily by changing VF1 and secondarily by adjusting VF2 and TD. Also, it is activated mainly by detecting the speed change in altered feedback speech sound.", "title": "Compensatory speech response to time-scale altered auditory feedback"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/nwe13_interspeech.html", "abstract": "Emotion classification is essential for understanding human interactions and hence is an important module in Human-Computer Interaction (HCI) systems. Well-performed emotion classification systems have potential to integrate into the HCI systems to provide additional user state details. This paper presents an emotion classification system that employs Emotional Dissimilarity (ED) measure. Instead of measuring ED in Single Dimensional (SD) space, we propose an approach to measure ED in Multi-Dimensional (MD) space. Proposed approach measures ED between emotions using GMM-SVM kernel with Bhattacharyya based GMM distance. In addition, we also formulate ED measure using GMM-SVM kernel with Kullback Leibler (KL) based GMM distance. We observe the effectiveness of employing ED measure in MD space over that in SD space using different kernels. Experiments were conducted using SVM classifier to classify emotions of anger, happiness, neutral and sadness. We achieve average accuracy of 81.25% for speaker independent emotion classification.", "title": "Bhattacharyya distance based emotional dissimilarity measure in multi-dimensional space for emotion classification"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/prego13_interspeech.html", "abstract": "This paper describes an optimization strategy based on a perceptual assessment criterion for dereverberation algorithms. The complete procedure is applied to the adaptive inverse-filtering (AIF) and spectral subtraction (SS) stages of a given dereverberation algorithm using the so-called QAreverb quality measure. Experimental results, using a 204-signal speech database, indicate that the associated algorithm can be greatly simplified (in about 97% of the overall computational complexity) by removing the AIF stage. In addition, a fine tuning of the SS stage is able to improve in 6% the algorithm's QAreverb score, resulting in a much simpler and more efficient algorithm in a perceptual point of view.", "title": "On the enhancement of dereverberation algorithms based on a perceptual evaluation criterion"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/gussenhoven13_interspeech.html", "abstract": "The shape of pitch contours has been shown to have an effect on the perceived duration of vowels. For instance, vowels with high level pitch and vowels with falling contours sound longer than vowels with low level pitch. Depending on whether the comparison is between level pitches or between level and dynamic contours, these findings have been interpreted in two ways. For inter-level comparisons, where the duration results are the reverse of production results, a hypercorrection strategy in production has been proposed. By contrast, for comparisons between level pitches and dynamic contours, the longer production data for dynamic contours have been held responsible. We report an experiment with Dutch and Chinese listeners which aimed to show that production data and perception data are each other's opposites for high, low, falling and rising contours. We explain the results, which are consistent with earlier findings, in terms of the compensatory listening strategy of [1], arguing that the perception effects are due to a perceptual compensation of articulatory strategies and constraints, rather than that differences in production compensate for psycho-acoustic perception effects.", "title": "Revisiting pitch slope and height effects on perceived duration"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/guiraud13_interspeech.html", "abstract": "Adaptation to artificially time-compressed speech and natural fast speech has been previously shown in adult listeners, with significant improvement of performance within 5.10 sentences. In the present study, we investigated whether typically developing children also adapt to such variations in speech rate. Eighteen children performed a semantic judgment task on normal speed sentences, natural fast sentences and time-compressed sentences. The three speech rate conditions were presented in separate blocks to examine adaptation over exposure time. Analysis of response times broken down into miniblocks of 5 sentences of the same rate reveals that whereas performance for normal sentences remains stable over time, response times become significantly shorter after listening to the first 5 sentences, both in the natural fast and time-compressed conditions. Therefore, children find it more difficult to understand natural fast and time-compressed sentences as revealed by increased response times, but after listening to 5 sentences, their performance improves and becomes comparable to that for normal sentences. These preliminary results suggest that children adapt to speech rate changes as rapidly as adults and that they adapt to both types of speech distortion (natural fast and time-compressed) in the same way.", "title": "Adaptation to natural fast speech and time-compressed speech in children"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/windmann13_interspeech.html", "abstract": "We show how incompressibility, a well-described property of some prosodic timing effects, can be accounted for in an optimizationbased model of speech timing. Preliminary results of a corpus study are presented, replicating and generalizing previous findings on incompressibility as a function of increasing speaking rate. We then introduce the architecture of our model and present results of simulation experiments that reproduce the results of the corpus analysis. Results suggest that incompressibility can be interpreted as a consequence of trade-offs between competing requirements of production efficiency and communicative efficacy.", "title": "Modeling durational incompressibility"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/emond13_interspeech.html", "abstract": "Smiling is a visible expression and also an audible one when it is synchronized with speech. Very few studies have documented the perceptual prosodic cues associated with perceived smiled speech, and there have been especially few studies using data from spontaneous speech. The aim of this study was to identify a combination of prosodic parameters that would allow a phonetic description of perceived smiled speech. A total of 85 utterances were extracted from spontaneous-speech data (Montreal 1995 corpus) and used as stimuli for a perception test administrated to 40 listeners (20 men, 20 women) of Quebec French. Perceived prosodic parameters of pitch height, pitch range, speech rate, and rhythm related to smiled speech are discussed.", "title": "Perceived prosodic correlates of smiled speech in spontaneous data"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/raake13_interspeech.html", "abstract": "A new model of speech quality under delay is presented that includes conversational interactivity. It is based on two previously reported narrowband telephony conversation tests involving different delays, with subject-pairs judging overall quality after each conversation. The tests were conducted with different conversation scenarios targeting different levels of interactivity. The instructions given prior to the tests were varied in their emphasis on speed of task completion. Based on the test results, the paper proposes an extension of a widely used conversational speech quality model, the so-called E-model (ITU-T Rec. G.107), to cover the joint effect of interactivity and delay. To this aim, two new parameters are introduced, one of which represents the minimum perceivable delay, and the other expresses in how far users will attribute the delay-effect to the conversational quality of the line. Based on the analysis of the recorded test conversations in terms of its surface structure (turns, speaker activities, etc.), prominent differences and delay-dependencies of a number of conversation parameters were found that characterize the impact of delay on the conversational flow and on perceived quality.", "title": "Predicting speech quality based on interactivity and delay"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/kouklia13_interspeech.html", "abstract": "This paper presents an experimental study of 136 utterances of aggressive attitudes in French: sarcastic irony, cold anger, hot anger, and a neutral control condition. Two male actors following a predefined scenario produced utterances on a controlled corpus including logatomes. Perceptual ratings by 28 listeners of aggressiveness, dominancy, and control among a subset of 24 utterances are compared to measurements of F0 and open quotient from EGG, vowel duration and formants values. More extreme forms of anger are rated as less controlled, more aggressive and dominant. Results indicate that the gradient of activation (sarcastic irony\n\n\n\n      doi: 10.21437/Interspeech.2013-365\n     Cite as: Kouklia, C., Audibert, N. (2013) Perceptual, acoustic and electroglottographic correlates of 3 aggressive attitudes in French: a pilot study. Proc. Interspeech 2013, 1389-1393, doi: 10.21437/Interspeech.2013-365\n@inproceedings{kouklia13_interspeech,\n  author={Charlotte Kouklia and Nicolas Audibert},\n  title={{Perceptual, acoustic and electroglottographic correlates of 3 aggressive attitudes in French: a pilot study}},\n  year=2013,\n  booktitle={Proc. Interspeech 2013},\n  pages={1389--1393},\n  doi={10.21437/Interspeech.2013-365}\n}", "title": "Perceptual, acoustic and electroglottographic correlates of 3 aggressive attitudes in French: a pilot study"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/morchid13_interspeech.html", "abstract": "The paper introduces new features for describing possible focus variation in a human/human conversation. The application considered is a real-life telephone customer care service. The purpose is to hypothesize the dominant theme of conversations between a casual customer calling. Conversations are processed by an automatic speech recognition system that provides hypotheses used for extracting word frequency. Features are extracted in different, broadly defined and partially overlapped, time segments. Combinations of each feature in different segments are represented in a quaternion algebra framework. The advantage of the proposed approach is made evident by the statistically significant improvements in theme classification accuracy.", "title": "Theme identification in telephone service conversations using quaternions of speech features"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/rao13_interspeech.html", "abstract": "Laughter is an important para-linguistic cue that can be useful in gauging the affective state of the speaker. In this paper, we present an approach to detecting laughter in children's speech using acoustic features in the spectral and prosodic domains. Feature selection was performed using the information gain-based technique and a speaker-independent validation using a support vector machine (SVM), an accuracy of 94.43% was observed, which was a 12.48% absolute improvement over the baseline result of 81.95%. For us to explore generalization properties, the models of speech and laughter were tested on a completely different database of adult-child interactions known as the Multimodal Dyadic Behavior Dataset (MDBD). The accuracy using the earlier trained models was 70.58%. Even though the children in this database were toddlers (less than three years old), the results suggest that the predictive power of the selected features generalizes well to different forms of children's laughter.", "title": "Detection of laughter in children's speech using spectral and prosodic acoustic features"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/truong13_interspeech.html", "abstract": "One of the major properties of overlapping speech is that it can be perceived as competitive or cooperative. For the development of real-time spoken dialog systems and the analysis of affective and social human behavior in conversations, it is important to (automatically) distinguish between these two types of overlap. We investigate acoustic characteristics of cooperative and competitive overlaps with the aim to develop automatic classifiers for the classification of overlaps. In addition to acoustic features, we also use information from gaze and head movement annotations. Contexts preceding and during the overlap are taken into account, as well as the behaviors of both the overlapper and the overlappee. We compare various feature sets in classification experiments that are performed on the AMI corpus. The best performances obtained lie around 27%.30% EER.", "title": "Classification of cooperative and competitive overlaps in speech using cues from the context, overlapper, and overlappee"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/kim13e_interspeech.html", "abstract": "Conflict escalation in multi-party conversations refers to an increase in the intensity of conflict during conversations. Here we study annotation and detection of conflict escalation in broadcast political debates towards a machine-mediated conflict management system. In this regard, we label conflict escalation using crowdsourced annotations and predict it with automatically extracted conversational and prosodic features. In particular, to annotate the conflict escalation we deploy two different strategies, i.e., indirect inference and direct assessment; the direct assessment method refers to a way that annotators watch and compare two consecutive clips during the annotation process, while the indirect inference method indicates that each clip is independently annotated with respect to the level of conflict then the level conflict escalation is inferred by comparing annotations of two consecutive clips. Empirical results with 792 pairs of consecutive clips in classifying three types of conflict escalation, i.e., escalation, de-escalation, and constant, show that labels from direct assessment yield higher classification performance (45.3% unweighted accuracy (UA)) than the one from indirect inference (39.7% UA), although the annotations from both methods are highly correlated (\u0192\u00cf= 0.74 in continuous values and 63% agreement in ternary classes).", "title": "Annotation and detection of conflict escalation in Political debates"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/schiel13_interspeech.html", "abstract": "A blending of phonological concepts and technical analysis is proposed to yield a better modeling and understanding of phonological processes. Based on the manual segmentation and labeling of the Italian CLIPS corpus we automatically derive a probabilistic set of phonological pronunciation rules: a new alignment technique is used to map the phonological form of spontaneous sentences onto the phonetic surface form. A machine-learning algorithm then calculates a set of phonological replacement rules together with their conditional probabilities. A critical analysis of the resulting probabilistic rule set is presented and discussed with regard to regional Italian accents. The rule set presented here is also applied in the newly published web-serviceWebMAUS that allows a user to segment and phonetically label Italian speech via a simple web-interface.", "title": "Machine learning of probabilistic phonological pronunciation rules from the Italian CLIPS corpus"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/baumeister13_interspeech.html", "abstract": "In this paper we describe a perception experiment on intoxicated and sober speech of 161 speakers recorded in the German Alcohol Language Corpus. 72 listeners achieved an average discrimination rate of 63.1% when asked to choose from pairs of stimuli which one sounded intoxicated. Perception results were not genderdependent and no hidden effects were found in a control group test. Since earlier studies reported higher fundamental frequency for intoxicated speakers, the influence of fundamental frequency as a potential acoustic cue in human perception of intoxication was analyzed. Results show a significantly higher detection rate for speakers who produce a higher fundamental frequency when being intoxicated, and a higher success rate for listeners who show a general preference for choosing the stimulus with higher fundamental frequency. However, human listeners do not consistently exploit this acoustic cue, since a simple algorithm which always classifies the stimulus with higher fundamental frequency as intoxicated would lead to a better performance of 82% discrimination rate.", "title": "Human perception of alcoholic intoxication in speech"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/hou13_interspeech.html", "abstract": "The present paper conducts a pioneering study on the phonetic manifestation and influence of zero anaphora in Chinese reading texts. The stress degree of the boundary syllable and the duration of the pause at the anaphoric position are examined. The results show: i) the boundary syllable after zero anaphoric form is more accented; for the two types of zero anaphora concerned in this study, the boundary syllable after distant zero anaphoric form is more accented than that after immediate zero anaphoric form; ii) the boundary syllable before immediate anaphoric form is more accented than that before distant anaphoric form; iii) the pause before immediate zero anaphoric form is shorter than that before other types of anaphoric forms. Based on these results, the study further proposes that the syllable weight of the underlying anaphoric form is projected to the following syllable in the surface representation. Moreover, semantic distance and relation can account for the differences between distant and immediate zero anaphora.", "title": "Phonetic manifestation and influence of zero anaphora in Chinese reading texts"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/harrat13_interspeech.html", "abstract": "In this paper we present a statistical approach for automatic diacritization of Algiers dialectal texts. This approach is based on statistical machine translation. We first investigate this approach on Modern Standard Arabic (MSA) texts using several data sources and extrapolated the results on available dialectal texts. For evaluation we used word and diacritization error rates and also precision and recall.", "title": "Diacritics restoration for Arabic dialect texts"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/wodarczak13b_interspeech.html", "abstract": "Faced with lack of objective and easily applicable criteria for segmentation of speech into dialogue turns, many authors resort instead to units defined in terms of stretches of speech minimally bounded by silence of some predefined duration. There is, however, no consensus concerning silence thresholds employed. While such thresholds can be established on perceptual grounds, in practice a wide range of values is used. As this has a direct impact on the reported frequencies of silences and overlaps, the discrepancies make comparisons of results across different studies difficult. In an attempt to overcome these problems in the present paper we use the Switchboard corpus to evaluate the expected variability in distributions of inter- and intra-speaker intervals when silence boundary thresholds of inter-pausal units are manipulated.", "title": "Effects of talk-spurt silence boundary thresholds on distribution of gaps and overlaps"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/kachkovskaia13_interspeech.html", "abstract": "The paper presents the results of a corpus-based study of final lengthening in Russian. The Corpus of Russian Professionally Read Speech (CORPRES) was used to investigate the degree of word and vowel lengthening as a function of prosodic factors, including prosodic boundary type and phrase or utterance pitch pattern. The results show that the degree of both word and vowel lengthening depends on the boundary depth and the location of the word within the intonation contour, as well as on the presence of a pause. Our data are only partly in line with the data previously obtained in a controlled experiment.", "title": "Final lengthening in Russian: a corpus-based study"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/reichel13b_interspeech.html", "abstract": "The mapping of a raw phonetic transcription to an orthographic word sequence is carried out in three steps: First, a syllable segmentation of the transcription is bootstrapped, based on unsupervised subtractive learning. Then, the syllables are grouped to word entities guided by non-linguistic distributional properties. Finally, the phonetic word segmentations are mapped onto entries of a canonic pronunciation dictionary by means of a co-occurrence based aligner. For syllable segmentation accuracies between 89 and 96% are obtained, and for word segmentation accuracies between 92 and 98%. The transcription to word conversion performance amounts 77%.", "title": "From segmentation bootstrapping to transcription-to-word conversion"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/caelenhaumont13_interspeech.html", "abstract": "The goal of our study is twofold. First, the results of two approaches in a tone annotation task are presented: a manual approach provided by the system MISTRAL+ and an automatic approach provided by the system PROSOTRAN. Second, an attempt is made here to determine the number and the patterns of the Mo Piu language tones with respect to the pitch slope and the tone level. It is found that both approaches of the tone annotation yield similar results on the speech material studied: tone patterns detected by the two systems are mainly falling (68% in the manual detection and 55% in the automatic detection) and flat tones (30% in manual detection and 43% in automatic detection) while rising tones are detected very seldom by the two systems (only 2% of the patterns are considered as rising ones). The agreement between the two systems is quite high: 70% of the detected tones have the same pitch movement (rising or flat) and only 4% of the tones detected by the two systems are completely different (neither the slope direction nor the tone levels are identical). The main tone patterns detected in our study are /41, 43, 54/ as falling slopes and /33/ as a plateau.", "title": "Manual and automatic tone annotation: the case of an endangered language from north vietnam \u201cmo piu\u201d"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/leonarduzzi13_interspeech.html", "abstract": "This paper presents a corpus study of four non-canonical English structures used for information packaging . extrapositions, right dislocations, it-clefts, wh-clefts. We study the relations between the information structure and the prosodic patterns (tonality, tonicity and tones) and show that the canonical (expected) prosody of these structures is not the most frequent one in semi-spontaneous speech, meaning that the canonical use of these structures might not be canonical at all in terms of frequency. It is argued that in discourse the pragmatic functions of the non-canonical structures studied derive from the complex interactions between syntax and prosody. Tonality reveals the informativeness (relevance at this point of discourse) of each part of the structure, adding meaning to that of the syntactic structure. Inside the intonation phrase, tonicity indicates what is old or new information, sometimes countering the canonical use of syntax, and can be used for highlighting or contrastive purposes. As for tones, their function is to mark contrast, emphasis or implication on the part of the speaker.", "title": "Non-canonical syntactic structures in discourse: tonality, tonicity and tones in English (semi-)spontaneous speech"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/nouri13_interspeech.html", "abstract": "Negotiations can be characterized by the strategy participants adopt to achieve their ends (e.g., individualistic strategies are based on self-interest, cooperative strategies are used when participants try to maximize the joint gain, while competitive strategies focus on maximizing each participant's score against the other) and the outcomes that each participant achieves in the negotiation. This paper investigates the process and the result of predicting the outcome and strategy of participants throughout the progress of the negotiation by using basic, easy to extract, linguistic and acoustic features. We evaluate our approach on a face-to-face negotiation dataset consisting of 41 dyadic interactions and show that it's possible to significantly improve over a majority-class baseline in tasks of predicting the strategy and outcome of the interaction by analyzing only basic low level features of the negotiation.", "title": "Prediction of strategy and outcome as negotiation unfolds by using basic verbal and behavioral features"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/poignant13_interspeech.html", "abstract": "Persons identification in video from TV broadcast is a valuable tool for indexing them. However, the use of biometric models is not a very sustainable option without a priori knowledge of people present in the videos. The pronounced names (PN) or written names (WN) on the screen can provide hypotheses names for speakers. We propose an experimental comparison of the potential of these two modalities (names pronounced or written) to extract the true names of the speakers. The names pronounced offer many instances of citation but transcription and named-entity detection errors halved the potential of this modality. On the contrary, the written names detection benefits of the video quality improvement and is nowadays rather robust and efficient to name speakers. Oracle experiments presented for the mapping between written names and speakers also show the complementarity of both PN and WN modalities.", "title": "Unsupervised naming of speakers in broadcast TV: using written names, pronounced names or both?"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/bredin13_interspeech.html", "abstract": "Most state-of-the-art approaches address speaker diarization as a hierarchical agglomerative clustering problem in the audio domain. In this paper, we propose to revisit one of them: speech turns clustering based on the Bayesian Information Criterion (a.k.a. BIC clustering). First, we show how to model it as an integer linear programming (ILP) problem. Its resolution leads to the same overall diarization error rate as standard BIC clustering but generates significantly purer speaker clusters. Then, we describe how this approach can easily be extended to the audiovisual domain and TV broadcast in particular. The straightforward integration of detected overlaid names (used to introduce guests or journalists, and obtained via video OCR) into a multimodal ILP problem yields significantly better speaker diarization results. Finally, we explain how this novel paradigm can incidentally be used for unsupervised speaker identification (i.e. not relying on any prior acoustic speaker models). Experiments on the REPERE TV broadcast corpus show that it achieves performance close to that of an oracle capable of identifying any speaker as long as their name appears on screen at least once in the video.", "title": "Integer linear programming for speaker diarization and cross-modal identification in TV broadcast"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/demarco13_interspeech.html", "abstract": "We present a comprehensive analysis of the use of I-vector based classifiers for the classification of unlabelled acoustic data as native British accents. We demonstrate the different behaviours of various popular dimensionality reduction techniques that have been previously used in problems such as speaker and language classification. Our results show that a fusion of I-vector based systems gives state-of-the-art performance for unlabelled classification of British accent speech data, reaching .81% accuracy.", "title": "Native accent classification via i-vectors and speaker compensation fusion"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/rouvier13_interspeech.html", "abstract": "This paper presents the LIUM open-source speaker diarization toolbox, mostly dedicated to broadcast news. This tool includes both Hierarchical Agglomerative Clustering using well-known measures such as BIC and CLR, and the new ILP clustering algorithm using i-vectors. Diarization systems are tested on the French evaluation data from ESTER, ETAPE and REPERE campaigns.", "title": "An open-source state-of-the-art toolbox for broadcast news diarization"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/kons13b_interspeech.html", "abstract": "We present in this paper our work on audio event classification for outdoor events. As the main classification method we employ a deep neural network (DNN) and compare this to other classification methods. We propose a novel improvement to the pre-training process of the network which is useful when training with Gaussian data. Our experimental results are based on an audio corpus extracted from the FreeSound.org website repository. We show that the DNN has some advantage over other classification methods and that fusion of two methods can produce the best results.", "title": "Audio event classification using deep neural networks"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/liang13_interspeech.html", "abstract": "This paper presents a new paradigm for code-switching event detection based on delta Bayesian Information Criterion (\u0192\u00a2BIC). First, an automatic speech recognizer (ASR) and an articulatory feature (AF) detector are constructed. The inter-syllable boundaries obtained from the ASR are regarded as the potential code-switching boundaries. To estimate the language likelihood, eigenvoice models (EVMs) are employed to model the relationship between the senones/articulatory attributes and their corresponding eigenvoices constructed from the training data for different languages. The Euclidean distance and the inner product-based direction between the eigenvoice vector of the input sentence and the eigenvoice vector of a senone or an articulatory attribute in the EVMs for different languages are calculated for \u0192\u00a2BIC-based language likelihood estimation. Then, an n syllable Bayesian mask centered at each potential boundary is then employed to output the likelihood of language change for the potential boundary. Finally, the dynamic programming algorithm is employed to search the best language sequence given the inter-syllable boundaries from the ASR. The proposed approach was evaluated on a Chinese-English code-switching speech database and the results show that 71.93% accuracy for code-switching event detection can be obtained.", "title": "Code-Switching event detection based on delta-BIC using phonetic eigenvoice models"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/hirayama13_interspeech.html", "abstract": "This paper proposes methods for determining an appropriate mixing ratio of dialects in automatic speech recognition (ASR) for dialects. To handle ASR for various dialects, it has been reported to be effective to train a language model using a dialect-mixed corpus. One reason behind this is geographical continuity of spoken dialect; we regard spoken dialect as a mixture of various dialects. This mixing ratio changes at every moment as well as depends on a speaker. We can improve recognition accuracy by giving an appropriate dialect mixing ratio for a speaker's dialect. The mixing ratio is generally unknown and requires to be estimated and updated referring to input utterances. We handle two methods for updating it based on recognition results; one is to compute contribution of dialects for each recognized word, and the other is to predict mixture information referring to a whole recognized sentence based on topic modeling. The experimental result shows that the mixing ratio estimated by these methods realized higher recognition accuracy than a fixed mixing ratio.", "title": "Automatic estimation of dialect mixing ratio for dialect speech recognition"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/rodriguezfuentes13_interspeech.html", "abstract": "The Albayzin 2012 Language Recognition Evaluation (LRE), carried out from June to October 2012, was the third effort made by the Spanish/Portuguese community for benchmarking language recognition technology. As in previous Albayzin 2008 and 2010 evaluations, the task consisted on deciding whether or not a target language was spoken in a test utterance. The primary condition involved 6 target languages for which there was plenty of training data: English, Portuguese and the four official languages in Spain (Basque, Catalan, Galician and Spanish). A new challenging condition was defined involving 4 target languages for which no training data were available: French, German, Greek and Italian. In both cases, other (Out-Of-Set) languages were also recorded to allow open-set verification tests. An innovative feature of this evaluation, not common to other evaluations, was that audio data for system development and evaluation were extracted from YouTube videos. Also, a new performance metric was proposed, the so called Multiclass Cross-Entropy, summarizing in a single figure the information provided by system scores, without the need to take hard decisions. This paper presents the main features of the evaluation and analyses the performance of the submitted systems on the different conditions, including the confusion among target languages.", "title": "The albayzin 2012 language recognition evaluation"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/han13b_interspeech.html", "abstract": "Automatic language identification or detection of audio data has become an important preprocessing step for speech/speaker recognition and audio data mining. In many surveillance applications, language detection has to be performed on highly degraded audio inputs. In this paper, we present our work on language detection in highly degraded radio channel scenarios. We provide a brief description of the Targeted Robust Audio Processing (TRAP) language detection system built for the Phase II Evaluation of the Robust Automatic Transcription of Speech (RATS) program. This system is a combination of 15 systems with different frontends and speech activity decisions. We also analyze the usefulness of multi-layer perceptron (MLP) based non-linear projection of i-vectors before SVM classification. The proposed backend reduces the Equal Error Rate (EER) by 11%.25% relative compared to the baseline PCA-based feature representation for SVM classification, on the RATS test data consisting of data from eight high-frequency radio communication channels.", "title": "TRAP language identification system for RATS phase II evaluation"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/lawson13_interspeech.html", "abstract": "We describe a language identification system developed for robustess to noise conditions such as those encountered under the DARPA RATS program, which is focused on multi-channel audio collected in high noise conditions. Work presented here includes novel approaches to scoring iVectors, the introduction of several new acoustic and prosodic features for language identification, and discriminative file selection approaches to score calibration. Further, we explore the use of Discrete Cosine Transforms (DCT) as a supplement to traditional context modeling with Shifted Delta Cepstrum (SDC) and fusion of multiple iVector systems based on Gaussian backends, neural networks, and adaptive Gaussian backend modeling.", "title": "Improving language identification robustness to highly channel-degraded speech through multiple system fusion"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/kang13_interspeech.html", "abstract": "A novel method is proposed to improve the duration prediction for HMM based speech synthesis. Based on the decision tree trained by the conventional HTS training method, the duration instances of every leaf node are further clustered into several classes by the K-means clustering method, and the mapping functions between the context features and class labels are trained by CRF. Instead of using the mean value of the Gaussian distribution of a leaf node in the decision tree as the predicted duration, the weighted summation of the multi-centroids from these several clustered classes is used to predict the phoneme duration. The weights are given by the output probability provided by CRF according to input context features and the prior probability from the clustering results. Compared with conventional HTS method, experiments show that the proposed method can significantly reduce RMSE in objective evaluations and achieves better preference scores in the subjective evaluations.", "title": "Multi-centroidal duration generation algorithm for HMM-based TTS"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/raitio13_interspeech.html", "abstract": "In this study, the acoustic properties of shouted speech are analyzed in relation to normal speech, and various synthesis techniques for shouting are investigated. The analysis shows large differences between the two styles, which induces difficulties in synthesis. Analysis-synthesis experiments show that the use of spectral estimation methods that are not biased by the sparse harmonics of shouted speech is beneficial. The synthesis of shouting is performed through adaptation and voice conversion. Subjective evaluation of synthesis reveals that, despite quality degradation, the impression of shouting and use of vocal effort is fairly well preserved. In addition, the use of specific spectral estimation methods is found to be beneficial also in adaptation.", "title": "Analysis and synthesis of shouted speech"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/nagata13_interspeech.html", "abstract": "This paper describes spontaneous dialogue speech synthesis based on multiple-regression hidden semi-Markov model (MRHSMM), which enables users to specify paralinguistic information of synthesized speech with a dimensional representation. Paralinguistic aspects of synthesized speech are controlled by multiple regression models whose explanatory variables are abstract dimensions such as pleasant-unpleasant and aroused-sleepy. For robust estimation of the regression matrices of the MRHSMM with unbalanced spontaneous dialogue speech samples, the re-estimation formulae were derived in the framework of the maximum a posteriori (MAP) estimation. The result of a perceptual experiment confirmed that the naturalness of synthesized speech was improved by applying the MAP estimation for regression matrices. In addition a high correlation (R &# 3; 0.7) was observed between given and perceived paralinguistic information, which implies that the proposed method could successfully reflect intended paralinguistic messages on the synthesized speech.", "title": "Robust estimation of multiple-regression HMM parameters for dimension-based expressive dialogue speech synthesis"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/brognaux13_interspeech.html", "abstract": "This paper proposes a new prosody annotation protocol specific to live sports commentaries. Two levels of annotation are defined with HMM-based speech synthesis in view. Local labels are assigned to all syllables and refer to accentual phenomena. Global labels classify sequences of words into five distinct sub-genres, defined in terms of valence and arousal. The objective of the study is to provide a set of labels both related to a specific function and characterized by a distinct acoustic realization. The consideration of these constraints should allow for an automatic prediction of the labels both from the text or from the speech signal. Reasonable inter-annotator scores are achieved for both annotation levels. A prosodic analysis of all labels also shows that they can usually be distinguished by specific acoustic realizations. The integration of this new annotation protocol within HMM-based speech synthesis shows promising results.", "title": "A new prosody annotation protocol for live sports commentaries"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/mehrabani13_interspeech.html", "abstract": "We propose an unsupervised prominence prediction method for expressive speech synthesis. Prominence patterns are learned by statistical analysis of prosodic features extracted from speech data. The advantages of our unsupervised data-driven prominence prediction include: easy adaptation to new speakers, speech styles, and even languages without requiring expert knowledge or complicated linguistic rules. In this approach, first, prominence predictive prosodic features are extracted at the foot level. Next, the extracted prosodic features are clustered, each cluster representing a prominence level. Based on just-noticeable-differences of prosodic features, the optimal number of perceptually distinct prominence levels is determined. Finally, the proposed prominence prediction is applied to prosody prediction for unit selection speech synthesis. Perceptual evaluation results show a preference for a 4-level unsupervised prominence prediction over a rule-based baseline in terms of naturalness and expressiveness of synthesized speech.", "title": "Unsupervised prominence prediction for speech synthesis"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/charfuelan13_interspeech.html", "abstract": "This paper describes a framework for synthesis of expressive speech based on MARY TTS and Emotion Markup Language (EmotionML). We describe the creation of expressive unit selection and HMM-based voices using audiobook data labelled according to voice styles. Audiobook data is labelled/split according to voice styles by principal component analysis (PCA) of acoustic features extracted from segmented sentences. We introduce the implementation of EmotionML in MARY TTS and explain how it is used to represent and control expressivity in terms of discrete emotions or emotion dimensions. Preliminary results on perception of different voice styles are presented.", "title": "Expressive speech synthesis in MARY TTS using audiobook data and emotionML"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/ward13_interspeech.html", "abstract": "We want to enable users to locate desired information in spoken audio documents using not only the words, but also dialog activities. Following previous research, we infer this information from prosodic features, however, instead of retrieval by matching to a predefined finite set of activities, we estimate similarity using a vector space representation. Utterances close in this vector space are frequently similar not only pragmatically, but also topically. Using this we implemented a dialog-based query-by-example function and built it into an interface for use in combination with normal lexical search. In an experiment searchers used the new feature and considered it helpful, but only for some search tasks.", "title": "Using dialog-activity similarity for spoken information retrieval"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/chen13d_interspeech.html", "abstract": "An HMM/DNN framework is proposed to address the issues of short-word detection. The first-stage keyword hypothesizer is redesigned with a context-aware keyword model and a 9-state filler model to reduce the miss rate from 80% to 6% and increase the figure-of-merit (FOM) from 6.08% to 21.88% for short words. The hypothesizer is followed by a MLP-based second-stage keyword verifier to further reduce its putative hits. To enhance short word detection, three new techniques, including an HMM-based feature transformation for the MLPs, knowledge-based features, and deep neural networks, are incorporated into redesigning the verifier. With a set of nine short keywords from the TIMIT set the best FOM we had achieved for the proposed KWS system was 42.79%, which is comparable with that of 42.6% for long content words and much better than the FOM of 18.4% for short keywords reported in previous research.", "title": "A hybrid HMM/DNN approach to keyword spotting of short words"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/wintrode13_interspeech.html", "abstract": "We evaluate the limitations of the bag-of-words assumption for topic identification of conversational discourse by examining whether topic-dependent word occurrence statistics are also position-independent. We demonstrate where the assumption is violated in conversational speech corpora and show how the relevance of words to the classification task decreases over the length of the document. We seek to improve topic identification by modeling this topic drift phenomenon and weight word counts according to a decay function over the length of the document. By applying a global decay rate for all words we observe reduction in error rates of 23.47% relative on conversational corpora. Furthermore, we apply a minimum classification error (MCE) training procedure to learn per-word decay rates, and reduce error rates by up to an additional 27%.", "title": "Leveraging locality for topic identification of conversational speech"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/senay13_interspeech.html", "abstract": "In this article, we are interested in spoken term detection task, with a particular focus on Person Name (PN) spotting in automatic speech recognition (ASR) system outputs. We propose a two-step method that combines an acoustic matching based on a Phoneme Confusion Network (PCN) with a semantic rescoring based on the Latent Dirichlet Allocation (LDA) models. The first module allows to find, in the PCN, potential PN candidates in speech segments, while the second is in charge of ranking the competing PN, according to a LDA topic model. The proposed LDA-based approach outperforms significantly the baseline system based on a search in the ASR phoneme lattice, obtaining a F-measure score of 77.04% on PN detection.", "title": "Person name spotting by combining acoustic matching and LDA topic models"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/szaszak13_interspeech.html", "abstract": "This paper investigates the usage of prosody for the improvement of keyword spotting, focusing on the highly agglutinating Hungarian language, where keyword spotting cannot be effectively performed using LVCSR, as such systems are either unavailable or hard to operate due to high OOV rates and poor N-gram language modelling capabilities. Therefore, the applied keyword spotting system is based on confidence scores computed as a ratio of acoustic scores obtained in two ways: firstly, by decoding with an universal background model; and secondly, by decoding with a keyword model embedded into filler models. Prosody is used to perform an automatic phonological phrase alignment for speech, proven to be useful for automatic partial word boundary detection in fixed stress languages. Several features deduced from the phonological phrase alignment are investigated to rescore baseline confidence scores both in a rule-based and in a data-driven manner. Results show that in relevant operating points of the system, a false alarm reduction of 10%.40% can be reached by the same miss probability rates.", "title": "Using phonological phrase segmentation to improve automatic keyword spotting for the highly agglutinating Hungarian language"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/heck13_interspeech.html", "abstract": "The past decade has seen the emergence of web-scale structured and linked semantic knowledge resources (e.g., Freebase, DB-Pedia). These semantic knowledge graphs provide a scalable \"schema for the web\", representing a significant opportunity for the spoken language understanding (SLU) research community. This paper leverages these resources to bootstrap a web-scale semantic parser with no requirement for semantic schema design, no data collection, and no manual annotations. Our approach is based on an iterative graph crawl algorithm. From an initial seed node (entitytype), the method learns the related entity-types from the graph structure, and automatically annotates documents that can be linked to the node (e.g., Wikipedia articles, web search documents). Following the branches, the graph is crawled and the procedure is repeated. The resulting collection of annotated documents is used to bootstrap web-scale conditional random field (CRF) semantic parsers. Finally, we use a maximum-a-posteriori (MAP) unsupervised adaptation technique on sample data from a specific domain to refine the parsers. The scale of the unsupervised parsers is on the order of thousands of domains and entity-types, millions of entities, and hundreds of millions of relations. The precision-recall of the semantic parsers trained with our unsupervised method approaches those trained with supervised annotations.", "title": "Leveraging knowledge graphs for web-scale unsupervised semantic parsing"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/cumani13_interspeech.html", "abstract": "Most of the state-of-the-art speaker recognition systems use a compact representation of spoken utterances referred to as i-vectors. Since the \"standard\" i-vector extraction procedure requires large memory structures and is relatively slow, new approaches have recently been proposed that are able to obtain either accurate solutions at the expense of an increase of the computational load, or fast approximate solutions, which are traded for lower memory costs. We propose a new approach particularly useful for applications that need to minimize their memory requirements. Our solution not only dramatically reduces the storage needs for i-vector extraction, but is also fast. Tested on the female part of the tel-tel extended NIST 2010 evaluation trials, our approach substantially improves the performance with respect to the fastest but inaccurate eigen-decomposition approach, using much less memory than any other known method.", "title": "Fast and memory effective i-vector extraction using a factorized sub-space"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/simonchik13_interspeech.html", "abstract": "The paper deals with the problem of estimation an optimal i-vector based speaker voice model using several sessions of his or her voice recordings, each of which has different signal parameters: speech duration and SNR. Our aim is to minimize inter-session variability so as to achieve minimal EER in the task of speaker recognition. We examine the influence of the main signal parameters on intersession variability and propose a model for multi-session i-vector estimation based on minimizing inter-session variability.", "title": "Effective estimation of a multi-session speaker model using information on signal parameters"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/hautamaki13b_interspeech.html", "abstract": "In this paper we study automatic regularization techniques for the fusion of automatic speaker recognition systems. Parameter regularization could dramatically reduce the fusion training time. In addition, there will not be any need for splitting the development set into different folds for cross- validation. We utilize majorization-minimization approach to automatic ridge regression learning and design a similar way to learn LASSO regularization parameter automatically. By experiments we show improvement in using automatic regularization.", "title": "Automatic regularization of cross-entropy cost for speaker recognition fusion"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/li13d_interspeech.html", "abstract": "We propose a practical, feature-level fusion approach for combining acoustic and articulatory information in speaker verification task. We find that concatenating articulation features obtained from the measured speech production data with conventional Mel-frequency cepstral coefficients (MFCCs) improves the overall speaker verification performance. However, since access to the measured articulatory data is impractical for real world speaker verification applications, we also experiment with estimated articulatory features obtained using acoustic-to-articulatory inversion technique. Specifically, we show that augmenting MFCCs with articulatory features obtained from subject-independent acousticto- articulatory inversion technique also significantly enhances the speaker verification performance. This performance boost could be due to the information about inter-speaker variation present in the estimated articulatory features, especially at the mean and variance level. Experimental results on the Wisconsin X-Ray Microbeam database show that the proposed acoustic-estimated-articulatory fusion approach significantly outperforms the traditional acousticonly baseline, providing up to 10% relative reduction in Equal Error Rate (EER). We further show that we can achieve an additional 5% relative reduction in EER after score-level fusion.", "title": "Speaker verification based on fusion of acoustic and articulatory information"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/leeuwen13_interspeech.html", "abstract": "This paper studies properties of the score distributions of calibrated log-likelihood-ratios that are used in automatic speaker recognition. We derive the essential condition for calibration that the log likelihood ratio of the log-likelihood-ratio is the log-likelihood-ratio. We then investigate what the consequence of this condition is to the probability density functions (PDFs) of the log-likelihood-ratio score. We show that if the PDF of the non-target distribution is Gaussian, then the PDF of the target distribution must be Gaussian as well. The means and variances of these two PDFs are interrelated, and determined completely by the discrimination performance of the recognizer characterized by the equal error rate. These relations allow for a new way of computing the offset and scaling parameters for linear calibration, and we derive closed-form expressions for these and show that for modern i-vector systems with PLDA scoring this leads to good calibration, comparable to traditional logistic regression, over a wide range of system performance.", "title": "The distribution of calibrated likelihood-ratios in speaker recognition"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/kelly13b_interspeech.html", "abstract": "Dealing with the effect of vocal ageing on speaker verification is an important challenge. In this paper, a new approach to improving speaker verification performance in the presence of long-term ageing is presented. Analogous to eigenchannel compensation, the proposed eigenageing compensation method operates by adapting a speaker model to a test sample based on a predetermined ageing subspace. An experimental evaluation of the new method, using the Trinity College Dublin Speaker Ageing database, demonstrates it to be very effective at reducing long-term speaker verification error rates, and shows it to compare favourably with our previous stacked classifier technique.", "title": "Eigenageing compensation for speaker verification"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/sarkar13_interspeech.html", "abstract": "In this paper, we propose two techniques to extend the recently introduced global Maximum Likelihood Linear Regression (MLLR) transformation (i.e. super-vector) based m-vector system for speaker verification into a multi-class MLLR m-vector system in the Universal Background Model (UBM) framework. In the first method, Gaussian mean vectors of the UBM are first grouped into several classes using conventional K-means and a proposed clustering algorithm based on Expectation Maximization (EM) and Maximum Likelihood (ML) concepts. Then, MLLR transformations are calculated for a given speech data with respect to each class, which are used in the form of super-vector for speaker representation by their m-vectors. In the second approach, several MLLR transformations are estimated with respect to pre-defined models called anchors. The proposed systems show better performance than the conventional system. Furthermore, the proposed UBMbased system does not require additional alignment of speech data with respect to the UBM for estimation of multiple MLLR transformations. We also further show that the proposed EM & ML clustering algorithm is robust to random initialization and provides equal or comparable system performance compared to K-means. The experimental results are shown on NIST 2008 SRE core condition over various tasks.", "title": "Anchor and UBM-based multi-class MLLR m-vector system for speaker verification"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/perera13_interspeech.html", "abstract": "The speech signal is a combination of attributes that contain information of the speaker, channel and noise. Conventional speaker verification systems train a single generic model for all cases, and handle all variations from these attributes either by factor analysis, or by not considering the variations explicitly. We propose a new methodology to partition the data space according to these factors and train separate models for each partition. The partitions may be obtained according to any attribute. We train models for the partitions discriminatively to maximize the separation between them. For classification we suggest multiple ways of combining scores from partitions. Experiments performed on the database NIST2008 show that our method improves the performance with respect to conventional methods when partitions are formed according to speakers. On noisy speech, partitions by noise result in the best performance.", "title": "Ensemble approach in speaker verification"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/wang13d_interspeech.html", "abstract": "GMM-UBM-based speaker verification heavily relies on well-trained UBMs. In practice, it is not often easy to obtain a UBM that fully matches the acoustic channel in operation. In a previous study, we proposed to address this problem by a novel sequential UBM adaptation approach based on MAP. This work extends the study by applying the sequential approach to speaker model adaptation. In addition, we investigate a new feature-space sequential adaptation approach based on feature MAP linear regression (fMAPLR) and compare it with the previously proposed model-space MAP approach. We find that these two approaches are complementary and can be combined to deliver additional performance gains. The experiments conducted on a time-varying speech database demonstrate that the proposed MAP-fMAPLR approach leads to significant EER reduction with two mismatched UBMs (25% and 39% respectively).", "title": "Sequential model adaptation for speaker verification"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/kanagasundaram13_interspeech.html", "abstract": "A significant amount of speech is typically required for speaker verification system development and evaluation, especially in the presence of large intersession variability. This paper introduces a source and utterance-duration normalized linear discriminant analysis (SUN-LDA) approaches to compensate session variability in short-utterance i-vector speaker verification systems. Two variations of SUN-LDA are proposed where normalization techniques are used to capture source variation from both short and full-length development i-vectors, one based upon pooling (SUNLDA- pooled) and the other on concatenation (SUN-LDA-concat) across the duration and source-dependent session variation. Both the SUN-LDA-pooled and SUN-LDA-concat techniques are shown to provide improvement over traditional LDA on NIST 08 truncated 10sec-10sec evaluation conditions, with the highest improvement obtained with the SUN-LDA-concat technique achieving a relative improvement of 8% in EER for mis-matched conditions and over 3% for matched conditions over traditional LDA approaches.", "title": "Improving short utterance based i-vector speaker recognition using source and utterance-duration normalization techniques"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/aronowitz13_interspeech.html", "abstract": "Recently we have investigated the use of state-of-the-art textindependent and text-dependent speaker verification algorithms for a text-dependent user authentication task and obtained satisfactory results mainly by using a fair amount of text-dependent development data. In our study, best results were obtained using the NAP framework rather than using the more advanced JFA and i-vector-based frameworks. In this work we investigate the ability to build high accuracy i-vector-based systems by leveraging widely available conversational data. We explore various techniques for transforming conversational sessions in such a way that attributes which are more relevant to the text-dependent task are enhanced. Using these techniques we managed to reduce verification error significantly.", "title": "On leveraging conversational data for building a text dependent speaker verification system"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/zhang13b_interspeech.html", "abstract": "This paper introduces the system fusion of Thu-EE for the NIST 2012 Speaker Recognition Evaluation (SRE12). In our approach, mean Z-norm, pseudo post probability T-norm and bi-criterion optimization are used. These methods mainly concern the changes of evaluation rules and limitation of our development data, which are the new problems for SRE12. Through post evaluation of SRE12 core test, the effectiveness of our approach is validated.", "title": "THU-EE system fusion for the NIST 2012 speaker recognition evaluation"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/garciaromero13_interspeech.html", "abstract": "In this paper, we consider speaker supervectors as observed variables and model them with a supervector probabilistic linear discriminant analysis model (SV-PLDA). By constraining the speaker and channel variability to lie in a common low-dimensional subspace, the model parameters and verification log likelihood ratios (LLR) can be computed in this low-dimensional subspace. Unlike the standard i-vector framework, SV-PLDA does not ignore the uncertainty arising from the variable length of a speech cut (observation noise). Moreover, the SV-PLDA model can be equivalently formulated in terms of an intermediate low-dimensional representation denoted as projected i-vectors (\u0192\u00ce-vectors). This intermediate representation facilitates the use of techniques that are important in practice such as length normalization and multi-cut enrollment averaging. We validate the proposed model on a subset of the NIST extended-SRE12 telephone dataset for which test segments of nominal durations of 300, 100, and 30 seconds are available. We show significant improvements over the standard i-vector system for the short-duration test cuts and also compare SV-PLDA with recently proposed extensions of the i-vector framework that also include the observation noise.", "title": "Subspace-constrained supervector PLDA for speaker verification"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/do13b_interspeech.html", "abstract": "Short-term cepstral features have long been chosen as standard features for speaker recognition thanks to their relevance and effectiveness. In contrast, discriminative features, calculated by a multi-layer perceptron (MLP) from much longer stretches of time, have been gradually adopted in automatic speech recognition (ASR). It has been shown that augmenting short-term cepstral features with long-term MLP (multi-layer perceptron) features makes it possible to improve significantly the performance of ASR. In this work, we investigate the possibility of augmenting short-term cepstral features with MLP features in order to improve the performance of text-independent speaker verification. We show, that, even though augmenting cepstral features with MLP features does not directly improve speaker verification performance, reducing the dimension of the augmented features, using principal component analysis (PCA), makes it possible to reduce, relatively, around 12% of the equal error rate (EER). Experiments are performed on telephone data of the 2008 NIST SRE (speaker recognition evaluation) database.", "title": "Augmenting short-term cepstral features with long-term discriminative features for speaker verification of telephone data"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/rajan13_interspeech.html", "abstract": "Popular features for speech processing, such as mel-frequency cepstral coefficients (MFCCs), are derived from the short-term magnitude spectrum, whereas the phase spectrum remains unused. While the common argument to use only the magnitude spectrum is that the human ear is phase-deaf, phase-based features have remained less explored due to additional signal processing difficulties they introduce. A useful representation of the phase is the group delay function, but its robust computation remains difficult. This paper advocates the use of group delay functions derived from parametric all-pole models instead of their direct computation from the discrete Fourier transform. Using a subset of the vocal effort data in the NIST 2010 speaker recognition evaluation (SRE) corpus, we show that group delay features derived via parametric all-pole models improve recognition accuracy, especially under high vocal effort. Additionally, the group delay features provide comparable or improved accuracy over conventional magnitude-based MFCC features. Thus, the use of group delay functions derived from all-pole models provide an effective way to utilize information from the phase spectrum of speech signals.", "title": "Using group delay functions from all-pole models for speaker recognition"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/portelo13_interspeech.html", "abstract": "Remote speaker verification services typically rely on the system to have access to the users recordings, or features derived from them, and also a model of the users voice. This conventional scheme raises several privacy concerns. In this work, we address this privacy problem in the context of a speaker verification system using a factor analysis based front-end extractor, the so-called i-vectors. Speaker verification without exposing speaker data is achieved by transforming speaker i-vectors to bit strings in a way that allows the computation of approximate distances, instead of exact ones. The key to the transformation uses a hashing scheme known as Secure Binary Embeddings. Then, a modified SVM kernel permits operating on the i-vector hashes. Experiments on sub-sets of NIST SRE 2008 showed that the secure system yielded similar results as its non-private counterpart.", "title": "Secure binary embeddings of front-end factor analysis for privacy preserving speaker verification"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/taghia13_interspeech.html", "abstract": "This paper addresses text-independent speaker identification (SI) based on line spectral frequencies (LSFs). The LSFs are transformed to differential LSFs (\u0192\u00a2LSF) in order to exploit their boundary and ordering properties. We show that the square root of \u0192\u00a2LSF has interesting directional characteristics implying that their distribution can be modeled by a mixture of von-Mises Fisher (vMF) distributions. We analytically estimate the mixture model parameters in a fully Bayesian treatment by using variational inference. In the Bayesian inference, we can potentially determine the model complexity and avoid overfitting problem associated with conventional approaches based on the expectation maximization. The experimental results confirm the effectiveness of the proposed SI system.", "title": "On von-mises fisher mixture model in text-independent speaker identification"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/diez13b_interspeech.html", "abstract": "The so called Phone Log-Likelihood Ratio (PLLR) features, computed on phone posterior probabilities provided by phonetic decoders, convey acoustic-phonetic information in a sequence of frame-level vectors. Thus, PLLRs can be easily plugged into traditional acoustic systems just by replacing MFCCs, PLPs or whatever other representation. PLLR features were used under an iVector-PLDA approach in our submission to the NIST 2012 Speaker Recognition Evaluation (SRE). In this work, we present a report of the goodness of these features for speaker recognition. Results on the telephone clean speech condition of the NIST 2010 and 2012 SRE show that, although the system based on PLLR features does not reach state-of-the-art performance, including it in a fusion with a traditional acoustic based system (trained on MFCC features) provides remarkable gains in performance (among the best reported in the NIST 2012 SRE telephone without added noise condition), revealing a fruitful way of using acoustic-phonetic information for speaker recognition.", "title": "Using phone log-likelihood ratios as features for speaker recognition"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/villalba13_interspeech.html", "abstract": "In some speaker recognition scenarios we find conversations recorded simultaneously over multiple channels. That is the case of the interviews in the NIST SRE dataset. To take advantage of that, we propose a modification of the PLDA model that considers two different inter-session variability terms. The first term is tied between all the recordings belonging to the same conversation whereas the second is not. Thus, the former mainly intends to capture the variability due to the phonetic content of the conversation while the latter tries to capture the channel variability. We test this approach on the NIST SRE12 core condition using multiple channels per interview to enroll the speakers. The proposed approach improves the minimum DCF by 26.29% on telephone speech and by 1.8% on interviews compared to the standard PLDA (scored by the book).", "title": "Handling recordings acquired simultaneously over multiple channels with PLDA"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/fang13_interspeech.html", "abstract": "This paper presents a new speaker verification system based on i-vector modeling as a feature extractor. In this modeling, we explore the distance constraints between i-vector pairs from the same speaker and different speakers. With an approximation of the distance metric as a weighted covariance matrix of the top eigenvectors from the data covariance matrix, variational inference is used to estimate a posterior distribution for the distance metric. Given speaker labels, we select different-speaker data pairs with the highest cosine scores to form a different-speaker constraint set. This set captures the most discriminative between-speaker variability in the training data. This Bayesian distance metric learning approach achieves better performance than state-of-the-art method. Furthermore, this approach is insensitive to score normalization, as compared to cosine scoring. Without the requirement of the number of labeled examples, this approach performs very well in the context of limited training data.", "title": "Bayesian distance metric learning on i-vector for speaker verification"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/hautamaki13c_interspeech.html", "abstract": "Human judgment is the final authority in forensic speaker recognition, but the use of modern speaker verification systems with accurate algorithms to perform the task under various circumstances has a huge potential to help the expert. The ultimate goal is to improve the accuracy of automatic systems when challenging data is provided and find a methodology for human-aided speaker recognition systems. This work presents an evaluation of speaker recognition carried out by human listeners and a gender dependent i-vector recognizer with a strategy for fusion of the decision process. Our experiments with HASR 2010 and HASR 2012 data indicate complementarity in the performance of the automatic system and the naive listeners decisions.", "title": "Merging human and automatic system decisions to improve speaker recognition performance"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/erjavec13_interspeech.html", "abstract": "The goal of the present study was to establish the nature of visual input (featural vs holistic) and the mode of its presentation that facilitates best audio-visual speech perception. Sixteen participants were asked to repeat acoustically strongly and mildly degraded syllables, presented in auditory and three audio-visual conditions, within which one contained holistic and two contained featural visual information. The featural audio-visual conditions differed in characteristics of talker's mouth presentation. Data on correct repetitions and participants fixations duration in talker\u0081fs mouth area were collected. The results showed that the facilitative effect of visual information on speech perception depended upon both auditory input degradation level and the visual presentation format, while eye-movement behavior was only affected by the visual input format. Featural information, when presented in a format containing no high contrast elements, was overall the most efficient visual aid for speech perception. It was also in this format that the fixations duration on talker's mouth was the longest. The results are interpreted with a stress on differences in attentional and perceptual processes that the different visual input formats most likely induced.", "title": "Effects of mouth-only and whole-face displays on audio-visual speech perception in noise: is the vision of a talker's full face truly the most efficient solution?"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/tiippana13_interspeech.html", "abstract": "Information from the acoustic speech signal and the talking face is integrated into a unified percept. This is demonstrated in the McGurk effect, in which discrepant visual articulation changes the auditory perception of a consonant. We studied acoustic (A) and visual (V) phonetic features that contribute to audiovisual speech perception by measuring the McGurk effect in two vowel contexts, [a] and [e], at various levels of acoustic noise. The McGurk stimuli consisted of an acoustic [p] presented with a visual [k]. This combination is generally heard as [t] (called fusion) or as [k] (visually dominant percept). The stimulus A[apa]V[aka] was most often heard as [aka], and these percepts increased with noise level. The stimulus A[epe]V[eke] was heard mostly as a fusion [ete], but in high noise also as [eke]. A phonetic analysis showed that, in [e] context, A[p] and V[k] stimulus features were close to those of [t], explaining why fusions were frequent. In [a] context, the visual stimulus had clear features of [k], while the features of the acoustic component were less distinctive, resulting in visual dominance particularly in noise. These results show how audiovisual integration depends on the features of acoustic and visual speech.", "title": "Acoustic and visual phonetic features in the mcgurk effect \u2014 an audiovisual speech illusion"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/davis13_interspeech.html", "abstract": "Auditory speech processing is facilitated when the talker's face and head movements (visual speech) can be seen. This effect occurs over a range of spoken word tasks, e.g., for spoken word identification (determining which word was presented) and for speech detection (determining whether speech was presented). This study examined the effect of providing two types of visual cue on the speed of determining whether a speech or non-speech sound was presented. Speech stimuli consisted of spoken nonwords and non-speech stimuli which were the spectrally inverted-versions of these. These stimuli were presented paired with either the talker\u0081fs static or moving face. Two types of moving face stimuli were used: full-face versions where both spoken form and timing cues were available and modified face versions where only the timing cues provided by peri-oral motion were available (i.e., the mouth area was obscured). The results showed that the peri-oral timing cues facilitated response time for both speech and non-speech stimuli (compared to the static face condition). An additional facilitatory effect was found for the full-face versions (compared to the peri-oral timing cue condition) but this effect only occurred for the speech stimuli. The different roles these cues play in speech processing are discussed.", "title": "The effect of visual speech timing and form cues on the processing of speech and nonspeech"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/chandrashekara13_interspeech.html", "abstract": "In a previous set of experiments we showed that audio-visual fusion during the McGurk effect may be modulated by context. A short context (2 to 4 syllables) composed of incoherent auditory and visual material significantly decreases the McGurk effect. We interpreted this as showing the existence of an audiovisual \"binding\" stage controlling the fusion process, incoherence producing \"unbinding\", and we also showed the existence of a \"rebinding\" process when an incoherent material is followed by a short coherent material. In this work we evaluate the role of acoustic noise superimposed to the context and to the rebinding material. We use either a coherent or incoherent context, followed, if incoherent, by a variable amount of coherent \"rebinding\" material, with two conditions, either silent or with superimposed speech-shaped noise. The McGurk target is presented with no acoustic noise. We confirm the existence of unbinding (lower McGurk effect with incoherent context) and rebinding (the McGurk effect is recovered with coherent rebinding). Noise uniformly increases the rate of McGurk responses compared to the silent condition. We conclude on the role of audiovisual coherence and noise in the binding process, in the framework of audiovisual speech scene analysis and the cocktail party effect.", "title": "Effect of context, rebinding and noise, on audiovisual speech fusion"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/rilliard13_interspeech.html", "abstract": "This paper presents the recording paradigm and the perceptual evaluation of a corpus of 16 prosodic social affects performed by a set of 8 native American English speakers (5f, 3m). The social affects are defined according to given communication goals in predefined social contexts, such as varying the relative hierarchical relation between the speaker and the interlocutor. The prosodic and facial strategies are evaluated by native listeners, rating their performance for achieving the targeted communication goal. Variations in the prosodic and facial strategies observed are then described and discussed in light of Ohala's frequency code. By selecting the best performances, 15 social affects were analyzed. Given the dimension of dominance as a main aspect related to the observed pitch level, the complexity of expressions is reflected in the multiparametric nature of the prosody. Voicing strength seems to be an important part of the acoustic information. In addition, the visual expressions allow an efficient interpretation of prosodic communication goals. Individual strategies to perform these social affects are observed in the prosodic variations, and may be related to factors such as the extraversion of speakers, their gender, and intrinsic pitch.", "title": "Social face to face communication \u2014 American English attitudinal prosody"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/bailly13_interspeech.html", "abstract": "Speech and variation of respiratory chest circumferences of eight French dyads were monitored while reading texts with increasing constraints on mutual synchrony. In line with previous research, we find that speakers mutually adapt their respiratory patterns. However a significant alignment is observed only when speakers need to perform together, i.e. when reading in alternation or synchronously. From quiet breathing to listening, to speech reading, we didn\u0081ft find the gradual asymmetric shaping of respiratory cycles generally assumed in literature (e.g. from symmetric inhalation and exhalation phases towards short inhalation and long exhalation). In contrast, the control of breathing seems to switch abruptly between two systems: vital vs. speech production. We also find that the syllabic and the respiratory cycles are strongly phased at speech onsets. This phenomenon is in agreement with the quantal nature of speech rhythm beyond the utterance, previously observed via pause durations.", "title": "Adaptation of respiratory patterns in collaborative reading"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/toth13_interspeech.html", "abstract": "Rectifier neurons differ from standard ones only in that the sigmoid activation function is replaced by the rectifier function, max(0,x). Several recent studies suggest that rectifier units may be more suitable building units for deep nets. For example, we found that with deep rectifier networks one can attain a similar speech recognition performance than that with sigmoid nets, but without the need for the time-consuming pre-training procedure. Here, we extend the previous results by modifying the rectifier network so that it has a convolutional structure. As convolutional networks are inherently deep, rectifier neurons seem to be an ideal choice as their building units. Indeed, on the TIMIT phone recognition task we report a 6% relative error reduction compared to our earlier results, giving an 18.6% error rate on the core test set. Then, with the application of the recently proposed \u0081edropout\u0081f training method we reduce the error rate further to 17.8%, which, to our knowledge, is the best result to date on this database.", "title": "Convolutional deep rectifier neural nets for phone recognition"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/hirsch13_interspeech.html", "abstract": "Humans use the pitch of their conversational partner as an important feature for improving the communication and the understanding especially in noisy situations. This knowledge is taken to investigate the idea of a pitch synchronous spectral analysis and a pitch dependent recognition of voiced speech segments. A first approach is presented for realizing this pitch dependent processing. Its applicability is shown for recognizing the voiced segments of the Timit database.", "title": "Pitch synchronous spectral analysis for a pitch dependent recognition of voiced phonemes \u2014 PISAR"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/rodriguez13_interspeech.html", "abstract": "Undoubtedly the compact representation by a set of Mel Frequency Cepstrum Coefficients (MFCC) has been used satisfactorily for ASR [9]. The cochlea is an organ, in humans or mammalians that converts the frequency perceived by the ear in punctual stimulation to excite the nerve auditory that receives a set of stimulus that comes from speech sound pressure. A new approach is proposed that considers this phenomenon to construct the bank filter in our parametric representation. Then we substitute the distribution of the bank filter in the Mel scale function for a different distribution that depends of the inner ear response to the stimulus that it receives. The place theory is used which achieves a 99.8% performance. Finally, this paper compares the performance of different acoustic representations in Continuous Automatic Speech Recognition system (CASRs) based on words. The cochlea operation is explained that permits obtaining a model and we will show that one alternative solution to the model based on fluid mechanical proposed by Lesser and Berkley, can be obtained if resonance analysis weather Fourier series is used as a solution.", "title": "New parameters for automatic speech recognition based on the mammalian cochlea model using resonance analysis"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/jaitly13_interspeech.html", "abstract": "In this paper we show how we can discover non-linear features of frames of spectrograms using a novel autoencoder. The autoencoder uses a neural network encoder that predicts how a set of prototypes called templates need to be transformed to reconstruct the data, and a decoder that is a function that performs this operation of transforming prototypes and reconstructing the input. We demonstrate this method on spectrograms from the TIMIT database. The features are used in a Deep Neural Network - Hidden Markov Model (DNN-HMM) hybrid system for automatic speech recognition. On the TIMIT monophone recognition task we were able to achieve gains of 0.5% over Mel log spectra, by augmenting traditional the spectra with the predicted transformation parameters. Further, using the recently discovered \u0081edropout\u0081f training, we were able to achieve a phone error rate (PER) of 17.9% on the dev set and 19.5% on the test set, which, to our knowledge is the best reported number on this task using a hybrid system. Speaking Rate Normalization with Lattice-Based Context-Dependent Phoneme Duration Modeling for Personalized Speech Recognizers on Mobile Devices", "title": "Using an autoencoder with deformable templates to discover features for automated speech recognition"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/yeh13_interspeech.html", "abstract": "Voice access of cloud applications including social networks using mobile devices becomes attractive today. And personalized speech recognizers over mobile devices become feasible because most mobile devices have only a single user. Speaking rate variation is known to be an important source of performance degradation for spontaneous speech recognition. Speaking rate is speaker dependent, it changes from time to time for every speaker. Furthermore, the speaking rate variation pattern is unique for each speaker. An approach of continuous frame rate normalization (CFRN) was recently proposed to take care of the speaking rate variation problem. In this paper, we further proposed an extended version of CFRN for personalized speech recognizers on mobile platforms. In this approach, we use context-dependent phoneme duration models adapted to each speaker to estimate the speaking rate utterance by utterance based on lattices obtained with a first-pass recognizer. The proposed approach was evaluated on both read speech and spontaneous recordings from mobile platforms and significant improvement were observed in the experimental result.", "title": "Speaking rate normalization with lattice-based context-dependent phoneme duration modeling for personalized speech recognizers on mobile devices"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/qi13_interspeech.html", "abstract": "The bottleneck (BN) feature, particularly based on deep structures, has gained significant success in automatic speech recognition (ASR). However, applying the BN feature to small/medium-scale tasks is nontrivial. An obvious reason is that the limited training data prevent from training a complicated deep network; another reason, which is more subtle, is that the BN feature tends to possess high inter-dimensional correlation, thus being inappropriate to be modeled by the conventional diagonal Gaussian mixture model (GMM). This difficulty can be mitigated by increasing the number of Gaussian components and/or employing full covariance matrices. These approaches, however, are not applicable for small/mediumscale tasks for which only a limited amount of training data is available. In this paper, we study the subspace Gaussian mixture model (SGMM) for BN features. The SGMM assumes full but shared covariance matrices, and hence can address the inter-dimensional correlation in a parsimonious way. This is particularly attractive for the BN feature, especially on small/medium-scale tasks, where the inter-dimensional correlation is high but the full covariance modeling is not affordable due to the limited training data. Our preliminary experiments on the Resource Management (RM) database demonstrate that the SGMM can deliver significant performance improvement for ASR systems based on BN features.", "title": "Subspace models for bottleneck features"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/qi13b_interspeech.html", "abstract": "Recent work demonstrates impressive success of the bottleneck (BN) feature in speech recognition, particularly with deep networks plus appropriate pre-training. A widely admitted advantage associated with the BN feature is that the network structure can learn multiple environmental conditions with abundant training data. For tasks with limited training data, however, this multi-condition training is unavailable, and so the networks tend to be over-fitted and sensitive to acoustic condition changes. A possible solution is to base the BN features on a channel-robust primary feature. In this paper, we propose to derive the BN feature based on Gammatone frequency cepstral coefficients (GFCCs). The GFCC feature has shown nice robustness against acoustic change, due to its capability of simulating the auditory system of humans. The idea is to integrate the advantage of the GFCC feature in acoustic robustness and the advantage of the BN feature in signal representation, so that the BN feature can be improved in the condition of mismatched training/test channels. This is particularly useful for small-scale tasks for which the training data are often limited. The experiments are conducted on the WSJCAM0 database, where the test utterances are mixed with noises at various SNR levels to simulate the channel change. The results confirm that the GFCC-based BN feature is much more robust than the BN features based on the MFCC and the PLP. Furthermore, the primary GFCC feature and the GFCC-based BN feature can be concatenated, leading to a more robust combined feature which provides considerable performance gains in all the tested noise conditions.", "title": "Bottleneck features based on gammatone frequency cepstral coefficients"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/golik13_interspeech.html", "abstract": "In this paper we investigate the error criteria that are optimized during the training of artificial neural networks (ANN). We compare the bounds of the squared error (SE) and the cross-entropy (CE) criteria being the most popular choices in state-of-the-art implementations. The evaluation is performed on automatic speech recognition (ASR) and handwriting recognition (HWR) tasks using a hybrid HMM-ANN model. We find that with randomly initialized weights, the squared error based ANN does not converge to a good local optimum. However, with a good initialization by pre-training, the word error rate of our best CE trained system could be reduced from 30.9% to 30.5% on the ASR, and from 22.7% to 21.9% on the HWR task by performing a few additional \"fine-tuning\" iterations with the SE criterion.", "title": "Cross-entropy vs. squared error training: a theoretical and experimental comparison"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/patil13b_interspeech.html", "abstract": "Plosives in Indo-Aryan languages such as Hindi and Marathi display a 4-way contrast involving the two dimensions of voicing and aspiration. While many studies are available on the acoustics of aspiration in unvoiced stops due to their more universal presence in the world's languages, voiced aspirated plosives have been less studied. Rather than the release duration cue of aspiration in unvoiced stops, the acoustic realization of aspiration in voiced plosives is marked by the coarticulatory breathiness of the following vowel. We consider the automatic detection of aspiration in Marathi word-initial voiced stops and affricates via several features relating to extent and timing of breathiness of the following vowel. The effectiveness of the features is evaluated by classification performance on a database of Marathi words. A practical application of this work to the detection of non-native pronunciation of voiced obstruents is presented.", "title": "Acoustic features for detection of phonemic aspiration in voiced plosives"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/palaz13_interspeech.html", "abstract": "In hybrid hidden Markov model/artificial neural networks (HMM/ANN) automatic speech recognition (ASR) system, the phoneme class conditional probabilities are estimated by first extracting acoustic features from the speech signal based on prior knowledge such as, speech perception or/and speech production knowledge, and, then modeling the acoustic features with an ANN. Recent advances in machine learning techniques, more specifically in the field of image processing and text processing, have shown that such divide and conquer strategy (i.e., separating feature extraction and modeling steps) may not be necessary. Motivated from these studies, in the framework of convolutional neural networks (CNNs), this paper investigates a novel approach, where the input to the ANN is raw speech signal and the output is phoneme class conditional probability estimates. On TIMIT phoneme recognition task, we study different ANN architectures to show the benefit of CNNs and compare the proposed approach against conventional approach where, spectral-based feature MFCC is extracted and modeled by a multilayer perceptron. Our studies show that the proposed approach can yield comparable or better phoneme recognition performance when compared to the conventional approach. It indicates that CNNs can learn features relevant for phoneme classification automatically from the raw speech signal.", "title": "Estimating phoneme class conditional probabilities from raw speech signal using convolutional neural networks"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/olaso13_interspeech.html", "abstract": "Phonological feature space has been proposed to represent acoustic models for automatic speech recognition (ASR) tasks. The most successful methods to detect articulatory gestures from the speech signal are based on Time Delay Neural Networks (TDNN). Stochastic Finite-State Automata have been effectively used in many speech-input natural language tasks. They are versatile models with well established learning algorithms that can easily be combined with other models. A two-level finite state model has also been proposed to classify articulatory features. However in this case a strong discretization procedure was required. In this work we propose a hierarchical finite-state model that considers two space of representations based on phonological features and on acoustic parameters, respectively. This model was evaluated in a phonological features identification task over a Spanish corpus. Experimental results show better frame classification accuracy than discrete models. Moreover, some specific articulations are better identified by the proposed models than by TDNN, leading to higher phone identification rates at frame level.", "title": "Hierarchical models based on a continuous acoustic space to identify phonological features"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/tomar13_interspeech.html", "abstract": "Manifold learning based techniques have been found to be useful for feature space transformations and semi-supervised learning in speech processing. However, the immense computational requirements in building neighborhood graphs have hindered the application of these techniques to large speech corpora. This paper presents an approach for fast computation of neighborhood graphs in the context of manifold learning. The approach, known as locality sensitive hashing (LSH), has been applied to a discriminative manifold learning based feature space transformation technique that utilizes a cosine-correlation based distance measure. Performance is evaluated first in terms computational savings at a given level of ASR performance. The results demonstrate that LSH provides a factor of 9 reduction in the computational complexity with minimal impact on speech recognition performance. A study is also performed comparing the efficiency of the LSH algorithm presented here and other LSH approaches in identifying nearest neighbors.", "title": "Locality sensitive hashing for fast computation of correlational manifold learning based feature space transformations"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/schatz13_interspeech.html", "abstract": "We present a new framework for the evaluation of speech representations in zero-resource settings, that extends and complements previous work by Carlin, Jansen and Hermansky. In particular, we replace their Same/Different discrimination task by several Minimal-Pair ABX (MP-ABX) tasks. We explain the analytical advantages of this new framework and apply it to decompose the standard signal processing pipelines for computing PLP and MFC coefficients. This method enables us to confirm and quantify a variety of well-known and not-so-well-known results in a single framework.", "title": "Evaluating speech features with the minimal-pair ABX task: analysis of the classical MFC/PLP pipeline"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/chiang13_interspeech.html", "abstract": "This paper presents a knowledge integration framework to improve performance in large vocabulary continuous speech recognition. Two types of knowledge sources, manner attribute and prosodic structure, are incorporated. For manner of articulation, six attribute detectors trained with an American English corpus (WSJ0) are utilized to rescore hypothesized phones in word lattices obtained by a baseline ASR system. For the prosodic structure, models trained with an unsupervised joint prosody labeling and modeling (PLM) technique using WSJ0 are used in lattice rescoring. Experimental results on the American English WSJ word recognition task of the Nov92 test set show that the proposed approach significantly outperforms the baseline system that does not use articulatory and prosodic information. The results also demonstrate the effectiveness and usefulness of the PLM technique in constructing prosodic models for American English ASR.", "title": "Knowledge integration for improving performance in LVCSR"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/heckmann13_interspeech.html", "abstract": "In this paper we present results for the audio-visual discrimination of prominent from non-prominent words on a dataset with 16 speakers and more than 5000 utterances. We collected data in an experiment where users were interacting via speech in a small game, designed as a Wizard-of-Oz experiment, with a computer. Following misunderstandings of one single word of the system, users were instructed to correct this word using prosodic cues only. Hence we obtain a dataset which contains the same word with normal and with high prominence. We extract an extensive range of features from the acoustic and visual channel. Thereby we also introduce fundamental frequency curvature as a measure. The analysis shows that there is a large variation from speaker to speaker in respect to the discrimination accuracy between prominent and non-prominent words as well to which features yield the best results. In particular we show that the visual channel is very informative for many of the speakers and that overall the feature capturing the mouth shape is the best individual feature. Furthermore, we show that a combination of the acoustic and visual features improves the performance for many of the speakers.", "title": "Inter-speaker variability in audio-visual classification of word prominence"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/liu13d_interspeech.html", "abstract": "Recently, an implicit trajectory model using temporally varying weight regression (TVWR) was proposed and achieved promising gains using ML training criteria. In the original TVWR, each component weight is modelled as a constrained linear regression function with respect to the monophone posterior feature. Due to the high dimensionality of the posterior feature, many free parameters were introduced into the TVWR system. Compared to the standard HMM system, such increment of system complexity could potentially cause two issues: over-training and slow decoding. In order to avoid these two issues, parameter clustering for TVWR is proposed to estimate cluster specific instead of original component specific regression parameters. In this paper, both knowledge-driven and data-driven approaches are introduced to define the cluster. Parameter re-estimation of clustered regression parameters is also derived. Experiments are conducted on the clean data of Aurora 4 corpus and systems are evaluated on Nov\u0081f92 5k closed vocabulary recognition task. Results show that comparable performance can be obtained and decoding time improves by more than 20% after significant reduction of system complexity.", "title": "Parameter clustering for temporally varying weight regression for automatic speech recognition"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/alumae13_interspeech.html", "abstract": "This paper describes a phone duration model applied to speech recognition. The model is based on a decision tree that finds clusters of phones in various contexts that tend to have similar durations. Wide contexts with rich linguistic and phonetic features are used. To better model varying and non-stationary speaking rates, the contextual features also include the observed duration values of previous phones. For each resulting phone cluster, a log-normal distribution of duration is estimated. The resulting decision tree and the log-normal distributions are used to calculate likelihoods of phone durations in N-best lists. Experiments on two Estonian recognition tasks show a small but significant improvement in speech recognition accuracy.", "title": "Phone duration modeling using clustering of rich contexts"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/ahmadi13_interspeech.html", "abstract": "for speech processing applications which is intrinsically robust to audible noise. Current solutions tend to rely on Doppler frequency shift to estimate movement of facial components during speech production. However, these approaches do not consider special attributes of LFUS propagation inside the vocal tract and consequently they do not discriminate between speech related and other forms of facial movements. This paper investigates the LFUS resonance characteristics of the vocal tract and uses these characteristics to develop a novel approach to determine the status of the mouth (open/partiallyopen/ closed). The proposed system provides high precision in detecting mouth status and provides a significant improvement to Doppler systems by being robust to variations of the direction of the face towards the ultrasonic source. The underlying paradigm is highly applicable in silent speech interfaces, multimodal voice activity detection, and hands-free control of rehabilitation devices for the disabled.", "title": "Human mouth state detection using low frequency ultrasound"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/li13e_interspeech.html", "abstract": "This paper investigates lexical stress detection for L2 English speech using Deep Belief Networks (DBNs). The features of the DBN used in this work include the syllable-based prosodic features (assumed to have Gaussian distribution) and their expected lexical stress (assumed to have Bernoulli distribution). As stressed syllables are more prominent than their neighbors, the two preceding and two following syllables are taken into consideration. Experimental results show that the DBN achieves an accuracy of about 80% in syllable stress classification (primary/secondary/no stress) for words with three or more syllables. It outperforms the conventional Gaussian Mixture Model and our previous Prominence Model by an absolute accuracy of about 8% and 4%, respectively.", "title": "Lexical stress detection for L2 English speech using deep belief networks"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/qian13_interspeech.html", "abstract": "This paper focuses on speech recognition applications where there is a limited amount of manually labelled training data in the target language, but plentiful unlabelled data. We investigate approaches based on unsupervised training: following the traditional method, we proposed a more effective and efficient data selection principle considering confidence scores as well as phone frequency. In addition, we transfer the HMM-based unsupervised training to MLP feature level at the first time, and obtain much more robust MLP-based features. Taking into account that HMM or MLP based unsupervised trainings are focused on model or feature level of speech recognition systems, we combined these two approaches finally, and proposed a more optimized strategy to get further improved unsupervised trained system in the low-resource applications. In our experiments, we get significant improvements of about 12% relative versus a conventional baseline in this lowresource scenario.", "title": "MLP-HMM two-stage unsupervised training for low-resource languages on conversational telephone speech recognition"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/novak13b_interspeech.html", "abstract": "This work investigates two related issues in the area of WFST-based G2P conversion. The first is the impact that the approach utilized to convert a target word to an equivalent finite-state machine has on downstream decoding efficiency. The second issue considered is the impact that the approach utilized to represent the joint n-gram model via the WFST framework has on the speed and accuracy of the system. In the latter case two novel algorithms are proposed, which extend the work from [1] to enable the use of failure-transitions with joint n-gram models. All solutions presented in this work are available as part of the open-source, BSD-licensed Phonetisaurus G2P toolkit [2].", "title": "Failure transitions for joint n-gram models and G2p conversion"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/kameoka13_interspeech.html", "abstract": "This paper introduces our ongoing work on generative modeling of speech fundamental frequency (F0) contours for estimating prosodic features from raw speech data. The present F0 contour model is formulated by translating the Fujisaki model, a wellfounded mathematical model representing the control mechanism of vocal fold vibration, into a probabilistic model described as a discrete-time stochastic process. The motivation behind this formulation is two fold. One is to derive a general parameter estimation framework for the Fujisaki model, allowing for the introduction of powerful statistical methods. The other is to construct an automatically trainable version of the Fujisaki model so that in future it can be used to develop a statistical speaking style conversion system or incorporated into existing text-to-speech synthesis systems to improve the naturalness and intelligibility of computer-generated speech. We also briefly introduce a generative model of F0 contours of singing voice developed under the same spirit.", "title": "Generative modeling of speech F0 contours"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/davel13_interspeech.html", "abstract": "Introducing pronunciation variants into a lexicon is a balancing act: incorporating necessary variants can improve automatic speech recognition (ASR) and spoken term detection (STD) performance by capturing some of the variability that occurs naturally; introducing superfluous variants can lead to increased confusability and a decrease in performance. We experiment with two very different grapheme-to-phoneme variant prediction techniques and analyze the variants generated, as well as their effect when used within fairly standard ASR and STD systems with unweighted lexicons. Specifically, we compare the variants generated by joint sequence models, which use probabilistic information to generate as many or as few variants as required, with a more discrete approach: the use of pseudo-phonemes within the default-and-refine algorithm. We evaluate results using three of the 2013 Babel evaluation languages with quite different variant characteristics . Tagalog, Pashto and Turkish . and find that there are clear trends in how the number and type of variants influence performance, and that the implications for lexicon creation for ASR and STD are different.", "title": "G2p variant prediction techniques for ASR and STD"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/jin13_interspeech.html", "abstract": "Rhythm patterns play an important role in the perception of second-language (L2) speech. This paper presents a novel approach to evaluating L2 speech rhythm using low-frequency spectral features inspired by the rhythmogram auditory model. In this paper we investigate several new feature sets for use in training rhythm-centric acoustic models. By capturing information over suprasegmental linguistic units appropriate for rhythmic analysis (including syllables and prosodic feet), these novel features can outperform traditional features in detecting rhythm errors on the ISLE corpus of learner English by 5.15% absolute.", "title": "Rhythm analysis of second-language speech through low-frequency auditory features"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/liu13e_interspeech.html", "abstract": "This paper presents several novel contributions to the emerging framework of graph-based semi-supervised learning for speech processing. First, we apply graph-based learning to variable-length segments rather than to the fixed-length vector representations that have been used previously. As part of this work we compare various graph-based learners, and we utilize an efficient feature selection technique for high-dimensional feature spaces that alleviates computational costs and improves the performance of graph-based learners. Finally, we present a method to improve regularization during the learning process. Experimental evaluation on the TIMIT frame and segment classification tasks demonstrates that the graph-based classifiers outperform standard baseline classifiers; furthermore, we find that the best learning algorithms are those that can incorporate prior knowledge.", "title": "Graph-based semi-supervised learning for phone and segment classification"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/shen13_interspeech.html", "abstract": "Using information from a person's gaze has potential to improve ASR performance in acoustically noisy environments. However, previous work has resulted in relatively minor improvements. A cache-based language model adaptation framework is presented where the cache contains a sequence of gaze events, classes represent visual context and task, and the relative importance of gaze events is considered. An implementation in a full ASR system is described and evaluated on a set of gaze-speech data recorded in both a quiet and acoustically noisy environment. Results demonstrate that selectively using gaze events based on measured characteristics significantly increases the performance improvements in WER on speech recorded in the noisy environment from 6.34% to 10.58%. This work highlights: the need to selectively use information from gaze, to constrain the redistribution of probability mass between words during adaptation via classes, and to evaluate the system with gaze and speech collected in environments that represent the real-world utility.", "title": "Selective use of gaze information to improve ASR performance in noisy environments by cache-based class language model adaptation"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/abdelhamid13b_interspeech.html", "abstract": "Hybrid systems which integrate the deep neural network (DNN) and hidden Markov model (HMM) have recently achieved remarkable performance in many large vocabulary speech recognition tasks. These systems, however, remain to rely on the HMM and assume the acoustic scores for the (windowed) frames are independent given the state, suffering from the same difficulty as in the previous GMM-HMM systems. In this paper, we propose the deep segmental neural network (DSNN), a segmental model that uses DNNs to estimate the acoustic scores of phonemic or sub-phonemic segments with variable lengths. This allows the DSNN to represent each segment as a single unit, in which frames are made dependent on each other. We describe the architecture of the DSNN, as well as its learning and decoding algorithms. Our evaluation experiments demonstrate that the DSNN can outperform the DNN/HMM hybrid systems and two existing segmental models including the segmental conditional random field and the shallow segmental neural network.", "title": "Deep segmental neural networks for speech recognition"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/coene13_interspeech.html", "abstract": "In the literature, languages have been identified as having more or less transparent orthographies, depending on the degree of predictability of their spelling-to-sound correspondences. Quantitative measures based on large-scaled language corpora which are capable to objectively assess such cross-linguistic variation are rather scarce. The quantitative assessment method presented here builds on the correlation between distances of phonemic and graphemic frequency distributions of a given sample and similar distances obtained from large corpora of the same language. The metric itself may be used as a research tool to investigate the potential effect of orthographic transparency on the development and performance of reading in different populations.", "title": "Quantifying cross-linguistic variation in grapheme-to-phoneme mapping"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/kawahara13b_interspeech.html", "abstract": "We address the estimation of the interest and comprehension level of an audience in poster sessions. Compared to lecture presentations, the audience's behaviors such as gazing and backchannels are more observable in poster presentations. These multi-modal behaviors are presumably related with their interest and comprehension level. We also assume that the interest and comprehension level can be judged by particular speech acts of the audience such as questions and reactive tokens. First, we make a preliminary analysis on their correlation. Next, we investigate the relationship between the audience's behaviors and the question type. Then, we conduct prediction of questions and their type based on the multi-modal behaviors during the relevant topic segment. Experimental results show that verbal backchannels and eye-gaze patterns are good predictors to this task, and also the combination of the multi-modal features is effective.", "title": "Estimation of interest and comprehension level of audience through multi-modal behaviors in poster conversations"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/hu13_interspeech.html", "abstract": "In this paper, we propose to use Deep Neural Net (DNN), which has been recently shown to reduce speech recognition errors significantly, in Computer-Aided Language Learning (CALL) to evaluate English learners' pronunciations. Multi-layer, stacked Restricted Boltzman Machines (RBMs), are first trained as nonlinear basis functions to represent speech signals succinctly, and the output layer is discriminatively trained to optimize the posterior probabilities of correct, sub-phonemic \"senone\" states. Three Goodness of Pronunciation (GOP) scores, including: the likelihood-based posterior probability, averaged frame-level posteriors of the DNN output layer \"senone\" nodes, and log likelihood ratio of correct and competing models, are tested with recordings of both native and non-native speakers, along with manual grading of pronunciation quality. The experimental results show that the GOP estimated by averaged frame-level posteriors of \"senones\" correlate with human scores the best. Comparing with GOPs estimated with non-DNN, i.e. GMM-HMM, based models, the new approach can improve the correlations relatively by 22.0% or 15.6%, at word or sentence levels, respectively. In addition, the frame-level posteriors, which doesn\u0081ft need a decoding lattice and its corresponding forwardbackward computations, is suitable for supporting fast, on-line, multi-channel applications.", "title": "A new DNN-based high quality pronunciation evaluation for computer-aided language learning (CALL)"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/planells13_interspeech.html", "abstract": "In this paper, we present an architecture to create a multi-domain spoken dialog system with minimum effort by composing heterogeneous pre-existent spoken dialog systems into a new system able to perform richer interactions. A Task Manager acts as a proxy for the different sub-domains and activates one of the systems each turn. The different sub-systems are not aware that they are used in a multi-domain scenario and believe that they are speaking directly to the user. This allows us to add new domains leaving most of the underlying models unmodified and reducing the amount of time and money needed to deploy the application. The proposed architecture has been applied to create a multidomain system combining three heterogeneous spoken dialog systems (sport facilities booking, weather service and personal calendar) in Spanish. The evaluation with naive real users shows that this is an appropriate approach to develop multi-domain spoken dialog systems.", "title": "A multi-domain dialog system to integrate heterogeneous spoken dialog systems"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/todo13_interspeech.html", "abstract": "Almost all current spoken dialog systems treat dialog as that where a single user talks to an agent. We, on the other hand, set out to investigate a multiparty dialog system that deals with two agents and a single user. We developed a three person (one user and two agents) and a two person (one user and one agent) dialog system to consider the same dialog task, that is, \"Which do you prefer, udon or ramen (Japanese noodle or Chinese noodle)?\" and compared them with respect to user behavior and satisfaction. According to the results of the experiments, the three person dialog system performed better in terms of lively conversation, and user can talk with the agents more like chatting.", "title": "Development and evaluation of spoken dialog systems with one or two agents"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/skantze13_interspeech.html", "abstract": "This paper investigates forms and functions of user feedback in a map task dialogue between a human and a robot, where the robot is the instruction-giver and the human is the instruction-follower. First, we investigate how user acknowledgements in task-oriented dialogue signal whether an activity is about to be initiated or has been completed. The parameters analysed include the users\u0081f lexical and prosodic realisation as well as gaze direction and response timing. Second, we investigate the relation between these parameters and the perception of uncertainty.", "title": "User feedback in human-robot interaction: prosody, gaze and timing"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/xi13_interspeech.html", "abstract": "This paper introduces KPCatcher (keyphrase catcher). The value of our work lies in providing concrete solutions to building a real keyphrase extraction product for enterprise videos. KPCatcher has been designed to robustly extract a ranked list of keyphrases from enterprise videos, independent of the domain. It treats noun phrases in the transcript as candidate keyphrases and scores them by aggregating word-level scores. By using confidence-based and counting-based rules, KPCatcher handles transcription errors to prevent incorrect keyphrases to be surfaced to end users. Different from previous work, we focus our experiments on automatic transcriptions of real enterprise videos from various domains. We thoroughly evaluate several well-known keyword ranking features and the denoising rules, using enterprise videos from several domains at various word error rates. We find term frequency to be the best feature and show that our denoising rules are very effective in both rejecting incorrect keyphrases and increasing the overlap between top keyphrases and human provided keyphrases. We also show that KPCatcher compares favorably to existing research systems on ICSI meeting data.", "title": "KPCatcher \u2014 a keyphrase extraction system for enterprise videos"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/song13_interspeech.html", "abstract": "Introducing pronunciation models into decoding has proven beneficial for LVCSR. As Minimum Phone Error (MPE) training has almost become a standard scheme for acoustic modeling, a discriminative pronunciation modeling method is investigated under the framework of MPE training. In order to bring the pronunciation models into MPE training, the auxiliary function of MPE training is rewritten at word level, and decomposes into two parts. One is for co-training the acoustic models, and the other is for discriminatively training the pronunciation models. On Mandarin conversational telephone speech recognition task, compared to the baseline using a canonical lexicon, the discriminative pronunciation models reduced the absolute Character Error Rate (CER) by 0.7% on LDC test set, and with the acoustic model co-training, about 1% additional CER decrease had been achieved.", "title": "Discriminative pronunciation modeling based on minimum phone error training"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/kubo13b_interspeech.html", "abstract": "The current state-of-the-art approach in grapheme-to-phoneme (g2p) conversion is structured learning based on the Margin Infused Relaxed Algorithm (MIRA), which is an online discriminative training method for multiclass classification. However, it is known that the aggressive weight update method of MIRA is prone to overfitting, even if the current example is an outlier or noisy. Adaptive Regularization of Weight Vectors (AROW) has been proposed to resolve this problem for binary classification. In addition, AROW's update rule is simpler and more efficient than that of MIRA, allowing for more efficient training. Although AROW has these advantages, it has not been applied to g2p conversion yet. In this paper, we first apply AROW to g2p conversion which is structured learning problem. In an evaluation that employed a dataset including noisy data our proposed approach achieves a 5.3% error reduction rate compared to MIRA implemented in DirecTL+ in terms of phoneme error rate while requiring only 78% the training time.", "title": "Grapheme-to-phoneme conversion based on adaptive regularization of weight vectors"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/naghibi13_interspeech.html", "abstract": "Given K utterances of a word and a set of sub-word units one may need a generalization of the conventional one-dimensional Viterbi algorithm to jointly decode them in order to derive their underlying word model (pronunciation). This extension is called k-dimensional Viterbi. However, as the number of utterances increases, the complexity of the k-dimensional Viterbi algorithm exponentially increases causing prohibitive computational burden. Here, we propose an approximation algorithm for the k-dimensional Viterbi which efficiently uses the available utterances to estimate the pronunciation. In addition to automatic dictionary generation, it can be used in computationally expensive applications such as lexicon-free training and joint pattern alignment.", "title": "An efficient method to estimate pronunciation from multiple utterances"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/basson13_interspeech.html", "abstract": "Grapheme-based speech recognition systems are faster to develop but typically do not reach the same level of performance as phoneme-based systems. In this paper we introduce a technique for improving the performance of standard grapheme-based systems. We find that by handling a relatively small number of irregular words through phoneme-to-grapheme (P2G) transliteration . transforming the original orthography of irregular words to an \u0081eidealised\u0081f orthography . grapheme-based accuracy can be improved. An analysis of speech recognition accuracy based on word categories shows that P2G transliteration succeeds in improving certain word categories in which grapheme-based systems typically perform poorly, and that the problematic categories can be identified prior to system development. We evaluate when category-based P2G transliteration is beneficial and discuss how the technique can be implemented in practice.", "title": "Category-based phoneme-to-grapheme transliteration"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/jyothi13_interspeech.html", "abstract": "One of the most popular speech recognition architectures consists of multiple components (like the acoustic, pronunciation and language models) that are modeled as weighted finite state transducer (WFST) factors in a cascade. These factor WFSTs are typically trained in isolation and combined efficiently for decoding. Recent work has explored jointly estimating parameters for these models using considerable amounts of training data. We propose an alternative approach to selectively train factor WFSTs in such an architecture, while still leveraging information from the entire cascade. This technique allows us to effectively estimate parameters of a factor WFST using relatively small amounts of data, if the factor is small. Our approach involves an online training paradigm for linear models adapted for discriminatively training one or more WFSTs in a cascade. We apply this method to train a pronunciation model for recognition on conversational speech, resulting in significant improvements in recognition performance over the baseline model.", "title": "Discriminative training of WFST factors with application to pronunciation modeling"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/karanasou13_interspeech.html", "abstract": "To enhance the recognition lexicon, it is important to be able to add pronunciation variants while keeping the confusability introduced by the extra phonemic variation low. However, this confusability is not easily correlated with the ASR performance, as it is an inherent phenomenon of speech. This paper proposes a method to construct a multiple pronunciation lexicon with a high discriminability. To do so, a phoneme confusion model is used to expand the phonemic search space of pronunciation variants during ASR decoding and a discriminative framework is adopted for the training of the weights of the phoneme confusions. For the parameter estimation, two training algorithms are implemented, the perceptron and the CRF model, using finite state transducers. Experiments on English data were conducted using a large stateof- the-art ASR system of continuous speech.", "title": "Discriminative training of a phoneme confusion model for a dynamic lexicon in ASR"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/greenberg13_interspeech.html", "abstract": "In 2012 NIST held the latest in an ongoing series of text-independent speaker recognition evaluations (SRE\u0081fs). The 2012 NIST Speaker Recognition Evaluation (SRE12) was the largest and most complex SRE to date, including over 100 million trials. Several aspects of SRE12 were new; most significantly, NIST released in advance of the evaluation target speaker training data from six preexisting corpora, and systems were permitted to utilize joint information from target speakers for speaker modeling and test segment scoring. Results from the evaluation suggest that systems found it easier to reject non-target trials where the test speaker was among the target speakers.", "title": "The 2012 NIST speaker recognition evaluation"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/brummer13_interspeech.html", "abstract": "Prior-weighted logistic regression has become a standard tool for calibration in speaker recognition. Logistic regression is the optimization of the expected value of the logarithmic scoring rule. We generalize this via a parametric family of proper scoring rules. Our theoretical analysis shows how different members of this family induce different relative weightings over a spectrum of applications of which the decision thresholds range from low to high. Special attention is given to the interaction between prior weighting and proper scoring rule parameters. Experiments on NIST SRE\u0081f12 suggest that for applications with low false-alarm rate requirements, scoring rules tailored to emphasize higher score thresholds may give better accuracy than logistic regression.", "title": "Likelihood-ratio calibration using prior-weighted proper scoring rules"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/ferrer13_interspeech.html", "abstract": "The National Institute of Standards and Technology (NIST) 2012 speaker recognition evaluation posed several new challenges including noisy data, varying test-sample length and number of enrollment samples, and a new metric. Target speakers were known during system development and could be used for model training and score normalization. For the evaluation, SRI International (SRI) submitted a system consisting of six subsystems that use different low- and high-level features, some specifically designed for noise robustness, fused at the score and iVector levels. This paper presents SRI's submission along with a careful analysis of the approaches that provided gains for this challenging evaluation including a multiclass voice-activity detection system, the use of noisy data in system training, and the fusion of subsystems using acoustic characterization metadata.", "title": "A noise-robust system for NIST 2012 speaker recognition evaluation"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/li13f_interspeech.html", "abstract": "I4U is a joint entry of nine research Institutes and Universities across 4 continents to NIST SRE 2012. It started with a brief discussion during the Odyssey 2012 workshop in Singapore. An online discussion group was soon set up, providing a discussion platform for different issues surrounding NIST SRE\u0081f12. Noisy test segments, uneven multi-session training, variable enrollment duration, and the issue of open-set identification were actively discussed leading to various solutions integrated to the I4U submission. The joint submission and several of its 17 sub-systems were among top-performing systems. We summarize the lessons learnt from this large-scale effort.", "title": "I4u submission to NIST SRE 2012: a large-scale collaborative effort for noise-robust speaker verification"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/sun13_interspeech.html", "abstract": "The Nuisance Attribute Project (NAP) with labeled data provides an effective approach for improving the speaker recognition performance in the state-of-art speaker recognition system by removing unwanted channel and handset variation. However, the requirement for the labeled NAP training data may limit its practical application. In our previous study, a simple unsupervised clustering algorithm based on dot products between supervectors was introduced for designing NAP training dataset without a prior knowledge about channel and speaker information. Using such clustering results as the initial training dataset, in this paper, we make a further improvement of the training dataset by enhancing similarity measurement of supervectors via NAP projection and score normalization. The effectiveness of this unsupervised NAP training dataset design strategy has been verified in the experiments using the in-house development dataset of IIR submission for the 2012 NIST SRE.", "title": "Improved unsupervised NAP training dataset design for speaker recognition"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/colibro13_interspeech.html", "abstract": "This paper describes the Nuance.Politecnico di Torino (NPT) speaker recognition system submitted to the NIST SRE12 evaluation campaign. Included are the results of post-evaluation tests, focusing on the analysis of the effects of score normalization and condition-dependent calibration. The submitted system combines the results of five acoustic recognizers all based on Gaussian Mixture Models (GMMs). Each system has its own front end, with features differing by their type and dimension. We illustrate the process of development data selection and configuration of state-of-the-art technology, which contributed to obtaining good performance in all the test conditions proposed in this evaluation.", "title": "Nuance - Politecnico di torino's 2012 NIST speaker recognition evaluation system"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/chen13e_interspeech.html", "abstract": "Many glottal source models have been proposed, but none has been systematically validated perceptually. Our previous work showed that model fitting of the negative peak of the flow derivative is the most important predictor of perceptual similarity to the target voice. In this study, a new voice source model is proposed to capture perceptually-important source shape aspects. This new model, along with four other source models, was fitted to 40 voice sources (20 male and 20 female) obtained by inverse filtering and analysis-by-synthesis (AbS) of samples of natural speech. We generated synthetic copies of the voices using each modeled source pulse, with all other synthesis parameters held constant, and then conducted a visual sort-and-rate task in which listeners assessed the extent of perceived similarity between the target voice samples and each copy. Results showed that the proposed model provided a more accurate fit and a better perceptual match to the target than did the other models.", "title": "A perceptually and physiologically motivated voice source model"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/smith13b_interspeech.html", "abstract": "A real-time MRI examination of retroflex stops and rhotics in Tamil reveals that in some contexts these consonants may in fact be achieved with little or no retroflexion of the tongue tip. Rather, maneuvering and shaping of the tongue in order to achieve post-alveolar contact varies across vowel contexts. Between back vowels /a/ and /u/, post-alveolar constriction involves curling back of the tongue tip, but in the context of high front vowel /i/, the same constriction is achieved by bunching of the tongue. It appears that though there is a stable constriction target in the post-alveolar region, its achievement is not fixed but is instead a consequence of the variable state of the vocal tract in different vowel contexts. Articulatory configurations of the tongue across these vowel contexts were examined by comparing measures of Gaussian curvature at evenly spaced points along the vocal tract. The results support the notion that so-called retroflex consonants have a specified target constriction in the post-alveolar region, but that the specific articulations employed to achieve this constriction are not fixed, in keeping with the task dynamic model of speech production.", "title": "Stable articulatory tasks and their variable formation: tamil retroflex consonants"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/ramanarayanan13_interspeech.html", "abstract": "It was recently shown that vocal tract postures assumed during pauses in read speech are significantly different from those assumed at absolute rest. This paper examines whether the former category of \"articulatory settings\" are more mechanically advantageous than absolute rest postures with respect to speech articulation. Appropriate task and articulator variables are extracted from realtime Magnetic Resonance Imaging (rtMRI) data of five speakers reading aloud. Locally-weighted regression is then used to calculate Jacobian matrices representing the transformation between articulatory task velocities and postural velocities. A measure of mechanical advantage is proposed based on the obtained Jacobian. Speech-ready postures and postures during inter-speech pauses are observed to be significantly more mechanically advantageous as compared to rest postures. Furthermore, other postures, such as those that occur during the production of different vowels and consonants, are shown to have mechanical advantages that lie in between this continuum. These results could provide insights into understanding postural motor control and other linguistic phenomena, such as sonority hierarchies, in speech production.", "title": "Articulatory settings facilitate mechanically advantageous motor control of vocal tract articulators"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/rochetcapellan13_interspeech.html", "abstract": "This paper investigates the relation between the linguistic structure of the breath group and breathing kinematics in spontaneous speech. 26 female speakers of German were recorded by means of an Inductance Plethysmograph. The breath group was defined as the interval of speech produced on a single exhalation. For each group several linguistic parameters (number and type of clauses, number of syllables, hesitations) were measured and the associated inhalation was characterized. The average duration of the breath group was .3.5 s. Most of the breath groups consisted of 1.3 clauses; .53% started with a matrix clause; .24% with an embedded clause and .23% with an incomplete clause (continuation, repetition, hesitation). The inhalation depth and duration varied as a function of the first clause type and with respect to the breath group length, showing some interplay between speech-planning and breathing control. Vocalized hesitations were speaker-specific and came with deeper inhalation. These results are informative for a better understanding of the interplay of speech-planning and breathing control in spontaneous speech. The findings are also relevant for applications in speech therapies and technologies.", "title": "The interplay of linguistic structure and breathing in German spontaneous speech"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/arai13_interspeech.html", "abstract": "Certain sounds are difficult for children to produce, even if the sounds are in their native language. For example, Japanese /r/ can be difficult for Japanese children to learn. Second language learners can also have difficulty acquiring certain sounds. For example, Japanese speakers learning English often have difficulty with English /r/ and /l/. To address this problem, we have developed two new physical models of the vocal tract: one for flap sounds (Model A) and another for liquid sounds (Model B). Each of them has a flapping tongue, and for Model B, the length of the tongue is variable. When the tongue is short, we can produce alveolar/retroflex approximants, and when the tongue is long we can produce lateral approximants. We recorded several sets of sounds produced by these models, analyzed the speech data, and used them for perceptual experiments. From the acoustic analysis and the perceptual experiments, we confirmed that the sounds produced by Model A were heard as Japanese /r/, and the sounds produced by Model B were heard as English /r/ and /l/. Furthermore, the models are helpful for practicing pronunciation because learners can see the tongue, alter tongue position manually, and hear the output sounds.", "title": "Physical models of the vocal tract with a flapping tongue for flap and liquid sounds"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/laprie13_interspeech.html", "abstract": "This paper deals with articulatory copy synthesis from X-ray films. The underlying articulatory synthesizer uses an aerodynamic and an acoustic simulation using target area functions, F0 and transition patterns from one area function to the next as input data. The articulators, tongue in particular, have been delineated by hand or semi-automatically from the X-ray films. A specific attention has been paid on the determination of the centerline of the vocal tract from the image and on the coordination between glottal area and vocal tract constrictions since both aspects strongly impact on the acoustics. Experiments show that good quality speech can be resynthesized even if the interval between two images is 40 ms. The same approach could be easily applied to cine MRI data.", "title": "Articulatory copy synthesis from cine x-ray films"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/bellegarda13_interspeech.html", "abstract": "Natural language interaction has the potential to considerably enhance user experience, especially in mobile devices like smartphones and electronic tablets. Recent advances in software integration and efforts toward more personalization and context awareness have brought closer the long-standing vision of the ubiquitous intelligent personal assistant. Multiple voice-driven initiatives, such as Apple's Siri, have now reached commercial deployment. Bringing this technology into the real world raises a number of issues that ordinarily are not brought to the fore by the research practitioner. Yet paying close attention to such aspects is critical to the success of the associated product. This paper discusses some of the attendant choices made in Siri, and speculates on their likely evolution going forward.", "title": "Large-scale personal assistant technology deployment: the siri experience"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/weiss13_interspeech.html", "abstract": "An embodied conversational agent was developed implementing four strategies to adapt to the user: User tracking and recognition with a camera, remembering interests in topics, remembering preferences concerning the level of detail of information, and changes in confirmation strategy. The agent was integrated into a system providing public information on ICT related projects of research and development for visitors of the laboratories. In an interactive experiment, the adaptive version was compared to a non-adaptive version. The logging data, but not the questionnaire data, shows significant differences, indicating a benefit of the adaptive version in terms of efficiency in interaction. The logging data and results from a final interview are discussed in relation to other work on this subject, concluding on the difficulties to provide not only more efficient interaction, but also higher User Experience.", "title": "Evaluating an adaptive dialog system for the public"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/gemmeke13_interspeech.html", "abstract": "This paper gives an overview of research within the aladin project, which aims to develop an assistive vocal interface for people with a physical impairment. In contrast to existing approaches, the vocal interface is trained by the end-user himself, which means it can be used with any vocabulary and grammar, and that it is maximally adapted to the . possibly dysarthric . speech of the user. This paper describes the overall learning framework, the user-centred design and evaluation aspects, database collection and approaches taken to combat problems such as noise and erroneous input.", "title": "Self-taught assistive vocal interfaces: an overview of the ALADIN project"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/eyben13_interspeech.html", "abstract": "Automatic emotion recognition and computational paralinguistics have matured to some robustness under controlled laboratory settings, however, the accuracies are degraded in real-life conditions such as the presence of noise and reverberation. In this paper we take a look at the relevance of acoustic features for expression of valence, arousal, and interest conveyed by a speaker's voice. Experiments are conducted on the GEMEP and TUM AVIC databases. To simulate realistically degraded conditions the audio is corrupted with real room impulse responses and real-life noise recordings. Features well correlated with the target (emotion) over a wide range of acoustic conditions are analysed and an interpretation is given. Classification results in matched and mismatched settings with multi-condition training are provided to validate the benefit of the feature selection method. Our proposed way of selecting features over a range of noise types considerably boosts the generalisation ability of the classifiers.", "title": "Affect recognition in real-life acoustic conditions \u2014 a new perspective on feature selection"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/principi13_interspeech.html", "abstract": "This paper describes a system for recognizing distress calls and home automation voice commands in a smart-home. Distress calls are recognized with the purpose of assisting people in their own homes: when they are detected, a phone call is automatically established with a contact in a address book and the person can request for assistance. The voice call is established through a voice over ip stack, with hands-free communication guaranteed by an acoustic echo canceller. The acoustic environment is constantly monitored by several low-consuming devices distributed throughout the home. In each device, a voice activity detector detects speech segments, and a speech recognition engine recognizes commands and distress calls. Robustness to environmental disturbances has been increased by employing Power Normalized Cepstral Coefficients and by using an adaptive algorithm for interference cancellation. An Italian speech corpus of home automation commands and distress calls has been developed for evaluation purposes. The corpus has been recorded in a real room using multiple microphones, and each sentence has been uttered both in normal and shouted speaking styles. The system performance has been assessed in terms of commands/distress recognition accuracy in order to prove the effectiveness of the approach.", "title": "A distributed system for recognizing home automation commands and distress calls in the Italian language"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/zinovieva13_interspeech.html", "abstract": "An important component of customer call experience analysis is to distinguish different segments of a call including interactive voice response (IVR), waiting in queue, and interaction with an agent. Because segment information from telephone switches is not always available, or may be difficult to obtain, we sought a method that could perform such segmentation solely from the recorded audio. In this paper, we present a probabilistic framework for segmenting call center audio into IVR, Queue, and Agent using a suite of rich features based on both speech and non-speech content. We study different statistical classifiers such as Maximum Entropy (MaxEnt) and Conditional Random Field (CRF). We present experimental results on real-world call center data and demonstrate that the probabilistic approach achieves superior segmentation performance, and outperforms a rule-based approach, while significantly reducing the time needed to deploy the segmenter for a new call center.", "title": "Probabilistic trainable segmenter for call center audio using multiple features"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/burkhardt13b_interspeech.html", "abstract": "We describe our approach on voice search in a mobile context by a TV guide search app that integrates linked open data to identify movies from a search query. A text parser to match keywords against vocabularies and numerical value descriptors is introduced.", "title": "Voice search in mobile applications and the use of linked open data"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/vacher13_interspeech.html", "abstract": "The Sweet-Home project aims at providing audio-based interaction technology that lets the user have full control over their home environment and at detecting distress situations. This paper presents the audio analysis system PATSH developed for this project and a user experiment in a smart home that evaluates the performances of the first revision of the system regarding vocal order recognition with realistic scenarios.", "title": "Evaluation of a real-time voice order recognition system from multiple audio channels in a home"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/aman13_interspeech.html", "abstract": "In the context of technologies development aiming at helping aged people to live independently at home, the CIRDO project aims at implementing an ASR system into a social inclusion product designed for elderly people in order to detect distress situations and provide capability to call for help. In this context we present a system able to detect distress and call for help sentences on line.", "title": "In-home detection of distress calls: the case of aged users"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/liu13f_interspeech.html", "abstract": "The proliferation of mobile devices, along with advances in speech and natural language processing technologies, have given birth to a new wave of personal assistance applications that enable users to quickly and more naturally perform many tasks through voice on their smart devices. This paper focuses on a natural language understanding (NLU) solution for one such application. We adopted a data-driven approach, aiming to take advantage of large volume of deployment data for continued learning and system improvement. In this paper, we compare two different statistical models . a hidden Markov model and a maximum entropy Markov model . for the task of semantic slot extraction, and we present empirical results on real user data.", "title": "Data driven methods for utterance semantic tagging"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/gouvea13_interspeech.html", "abstract": "AT&T has recently opened its extensive portfolio of state-of-the-art Speech Technology to external end-developers as a platform called \"The AT&T Speech API\". This study discusses a series of practical challenges found in an industrial deployment of speech to text services, particularly, we examine different strategies for customizing the speech to text process by considering intrinsic factors, inherent to the audio signal, or extrinsic factors, available from other sources, in an industry-grade implementation.", "title": "The AT&t speech API: a study on practical challenges for customized speech to text service"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/dhoore13_interspeech.html", "abstract": "Speech recognition has been shown to increase driver safety in car applications, if the application is well designed. Especially destination entry by voice is not only safer, but also faster and more convenient than the traditional haptic interfaces. Building a high performing in-car destination entry system requires tackling a number of practical challenges. The vocabulary is very large, often multilingual in nature, and the application needs to run in the noisy environment of a driving car on hardware with limited resources. Moreover, the type of language drivers want to use is becoming more and more natural, with a higher demand for up-to-date services in a context of other domains such as entertainment being active as well. In this paper, we will go over some practical aspects this all brings to the problem of designing a speech recognition system for destination entry.", "title": "In-vehicle destination entry by voice: practical aspects"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/gautreau13_interspeech.html", "abstract": "Our research aims at exploring the nature of the interferences that occur during the speech-in-speech situation. French target words were inserted in 2 types of backgrounds: (i) 4-talker babble spoken in various languages such as French, Italian or Irish, containing acoustic and linguistic information, (ii) fluctuating noise derived from each 4-talker babble signal, with only acoustic information. In Experiment 1, French native participants with no knowledge of Italian or Irish performed a lexical decision task. The comparison of performances obtained with the 2 types of backgrounds for each language revealed that acoustic and linguistic information from babble spoken in a known language to the participants (French) competed with the target words; whereas for babble produced in unknown languages (Italian and Irish) only acoustic information was involved. In Experiment 2, the experimental conditions were identical as in Experiment 1, except that French native participants speaking Italian as L2 and with no knowledge of Irish were recruited. The fact that Italian became an intelligible language to the participants led to acoustic and linguistic interferences from Italian babble.", "title": "Intelligibility at a multilingual cocktail party: effect of concurrent language knowledge"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/jacewicz13_interspeech.html", "abstract": "The current understanding of listener sensitivity to regional accents comes from examination of speech processing in quiet and in noise. This study had two aims: 1) to examine the intelligibility of regional accents in a multitalker environment, and 2) to explore a methodological question of whether systematic regional features can be detected in the productions of only one representative talker or whether several talkers are necessary to provide the suitable sample. Two American English dialects, General American English and Southern American English, were systematically varied both in the target speech and in the masking babble at three sound-to-noise ratios. The results showed that regional accents did influence listeners' performance in a multitalker environment. Intelligibility was hampered when the target and the masker shared common dialect features or when listeners' heard their own dialect in the masking babble. Southern American was a more intelligible variety than General American, which can be attributable to a set of specific acoustic phonetic features. The study found that systematic regional features can be reliably detected in the production of only one representative talker.", "title": "Regional accents affect speech intelligibility in a multitalker environment"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/tokuma13_interspeech.html", "abstract": "This study investigates the effect of different types of clear speech on cross-linguistic perception. In particular, it examines whether the words produced to help L2 interlocutors are perceived more easily by them than the words produced for L1 interlocutors to counteract speech babble noise. English keywords in these conditions are extracted from the LUCID corpus, and they are presented to Japanese listeners for the identification of the words in a minimal pair, in babble noise or without noise. The results demonstrate that when presented in noise, the words produced to help L2 interlocutors are generally perceived more easily by them. This suggests that speech production varies according to listeners\u0081f needs in a specific communicative situation. The strong effect of the acoustic characteristics of Japanese /b/ on the perception by Japanese L1 listeners is also discovered. Further acoustic and perceptual research is required to investigate whether a universal or specific cue-enhancement is involved in the clear speech for L2 listeners.", "title": "Perception of English minimal pairs in noise by Japanese listeners: does clear speech for L2 listeners help?"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/sisinni13_interspeech.html", "abstract": "The present study investigates Salento Italian (SI) listeners' initial state in the perception of American English (AE) vowels. Results of categorization and discrimination tasks are discussed in terms of the Perceptual Assimilation Model and the Second Language Linguistic Perception model (L2LP). Further, the categorization results are compared to those of the Peruvian Spanish (PS) listeners in [1] to test the L2LP acoustic hypothesis, according to which the acoustic differences between the five SI and PS vowels will lead to different categorizations of AE vowels. Predictions of differential perceptual development across listener groups are provided.", "title": "Salento Italian listeners' perception of American English vowels"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/rauber13_interspeech.html", "abstract": "Many speech researchers (linguists, psycholinguists, speech therapists) who work with speech perception still struggle to prepare their data collection tests mostly due to limitations as regards programming skills and knowledge of scripting languages. Concerned with this scenario and methodological constraints, we developed an open source application software (http://www.worken.com.br/tp) that enables researchers to design not only speech perception tests, but also perceptual training tasks with immediate feedback. The objective of this paper is to show some of the main features of the software and to highlight its user-friendliness.", "title": "TP 3.1 software: a tool for designing audio, visual, and audiovisual perceptual training tasks and perception tests"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/chen13f_interspeech.html", "abstract": "The linguistic content of the masker is important for the benefit of masking release in understanding noise-corrupted speech. The present work investigated how Mandarin sentence recognition is influenced by background noise with different linguistic content via two studies, i.e., speech perception and intelligibility prediction. Mandarin sentences were corrupted by Mandarin-, Cantonese-, and English-speaking multi-talker (i.e., 2-talker and 6-talker) babbles at -5, -7.5 and -10 dB signal-to-noise ratios, and listening experiments were conducted to collect the intelligibility data of Mandarin sentence perception. Results showed that 1) the recognition of noise-corrupted Mandarin sentences was notably influenced by the linguistic content of background noise, with an increased influence from English, Cantonese to Mandarin; and 2) present intelligibility indices accounted modestly well for the variance of Mandarin sentence recognition when background noise contained different linguistic content.", "title": "Effect of linguistic masker on the intelligibility of Mandarin sentences"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/moon13_interspeech.html", "abstract": "This study investigates the process of generalizing a learned sublexical contrast across speakers of different non-native accents of English. We examine the generalization of a novel cue (voicing-cued release) that is non-contrastive in English, but contrastive in the manipulated speech of our L2 speakers of English, to a speaker with the same or different L1 as our training speakers (Exp. 1). We then examine performance when the learned contrastive cue is paired with native contrastive vowel duration (Exp. 2), and when the native contrast is present, but the learned contrast is not contrastive in the speech of the new speaker (Exp. 3). We find that learned cues are dominant enough to additively improve word recognition when paired with congruent native cues, although the performance is inhibited when native and learned cues conflict. These data illuminate the intricate balance between native contrasts and recently learned information, where learned information overrides, but is still affected by, native contrasts.", "title": "The learning and generalization of contrasts consistent or inconsistent with native biases"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/ying13_interspeech.html", "abstract": "How do L2 learners cope with L2 accent variation? We developed predictions based upon the Perceptual Assimilation Model-L2 (PAM-L2) and tested them in an eye-tracking experiment using the visual world paradigm. L2-English learners in Australia with Chinese L1 were presented with words spoken in familiar Australian-accented English (AusE), and two unfamiliar accents: Jamaican Mesolect English (JaME) and Cockney-accented English (CknE). AusE and JaME differ primarily in vowel pronunciations, while CknE differs primarily in consonant pronunciations. Words were selected to elicit two types of perceptual assimilations of JaME and CknE phonemes to AusE: Category Goodness (CG) and Category Shifting (CS) assimilations. The Perceptual Assimilation Model (PAM) predicts that, if the L2 learners have developed AusE categories, then CS differences should hinder spoken word recognition more than CG differences. Our results supported this prediction. For both unfamiliar accents, CS target words attracted more fixations to printed competitor words than did CG distracters.", "title": "L2 English learners' recognition of words spoken in familiar versus unfamiliar English accents"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/wong13b_interspeech.html", "abstract": "This study investigates and compares the effects of different training paradigms in both the perception and production of English /I/ and /i:/ by Cantonese-speaking ESL learners. A total of 39 subjects participated in this study, in which 9 were trained under the High Variability Phonetic Training (HVPT) approach (H group), 11 under explicit articulation training (P group), 9 under a combination of both (HP group) whereas 10 as the control group (C group). Only the H and HP groups, the two groups receiving the HVPT, showed significant perceptual learning and generalization to new words and new speakers. The P group which received productive training only had no significant learning in the perceptual domain. Yet, robust improvement in production was observed in all three treatment groups, with the HP group significantly outperforming the other two training groups. The HP group was also the only one group that transferred the learning at word level to sentence level during a passage reading task. The results suggest that perceptual learning can be transferred to the production domain, but mere productive training barely improves perception. Training effectiveness can also be enhanced when training in both domains are provided.", "title": "The effects of perceptual and/or productive training on the perception and production of English vowels /\u026a/ and /i\u02d0/ by Cantonese ESL learners"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/kartushina13_interspeech.html", "abstract": "Spanish (L2) speakers have difficulty in perceiving the French vowel height contrast /e/-/E/. L2 perception models attribute this perceptual difficulty to native (L1) phonology. For example, the Spanish /e/ vowel, which is perceptually similar to both French vowels, is assumed to influence their perception through an assimilation process. This pattern of assimilation depends upon the degree of the similarity between the vowels. This research aims to shed light on the relevant aspects of L1 phonology and its relation to L2 phonology that can be operationalized to predict L2 perception. Traditionally, measures of perceptual similarity have involved comparing the acoustic or gestural properties of productions by groups of \u0081ereferenced\u0081f native speakers of the two languages of interest. Here, we rather compute acoustic distances (based on F1 and F2 values) between the native productions by Spanish adolescents learning French (N=14) and a group of native French speakers. In addition to this distance measure, we also introduced a novel measure, the within-speaker variability or compactness in productions of a L1 vowel to predict L2 perception. The results revealed that individual productions of Spanish /e/ and their variability (or compactness) predicted L2 perception accuracy of the French /e/-/E/ vowels.", "title": "On the role of L1 speech production in L2 perception: evidence from Spanish learners of French"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/halle13_interspeech.html", "abstract": "French (or English) native listeners hear /kl/ when presented with the illegal consonant sequence */tl/. This robust case of perceptual repair is usually viewed as operating at a prelexical level of speech processing but the evidence against lexical feedback is somewhat weak. In this study, we report new data supporting the prelexical hypothesis, obtained with a paradigm that avoids most of the possible confounds in previous studies. In a cross-modal auditory-visual priming paradigm, lexical decisions to the same visual target \"clavier\" are facilitated by the auditory prime *tlavier, not by *dlavier. Likewise, the recognition of \"glacier\" is facilitated by *dlacier, not by *tlacier. To summarize, velar stop + /l/ words are exclusively facilitated by the dental-initial derived forms with the same voicing. Derived forms with the opposite voicing tend to induce inhibition rather than facilitation. Hence, the observed facilitation effects are not graded from */tl/ to */dl/ or vice versa. We argue that these rather surprising all-or-none priming effects exclude the possibility that the */tl/->#/kl/ and */dl/->/gl/ repairs are due, even partly, to lexical feedback.", "title": "Looking for lexical feedback effects in /tl/\u2192/kl/ repairs"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/best13_interspeech.html", "abstract": "Unfamiliar regional accents disrupt spoken word recognition by L2 and L1 learners and L1 adults, and confuse ASR and smart systems. Little is known, however, about which aspects of nonnative accents hinder word recognition, or what processes are involved. We assessed how Australian English (AusE) listeners\u0081f recognition of words in unfamiliar accents is affected by two types of cross-accent perceptual assimilation: 1) other-accent phones that constitute \u0081edeviant\u0081f versions of the matching AusE phonemes (Category Goodness assimilation: CG); 2) phones that cross a native phonological boundary, i.e., assimilate to mismatching AusE phonemes (Category Shift: CS). Eyetracking (\"visual world\") revealed the timecourse of lexical competition during online identification of words spoken in Jamaican (JaME: vowel differences from AusE) and Cockney English (CknE: consonant differences), while choosing among four printed choice words: target, onset and offset competitors, unrelated distracter. Recognition was slower, and both competitor types were considered more and longer for JaME and CknE than AusE pronunciations; these effects were stronger for CS than CG differences. We conclude that: 1) perceptual assimilation plays a key role in cross-accent word recognition; 2) lexical competition involves not only onsets but also later aspects of words; 3) vowel and consonant variations affect lexical competition similarly.", "title": "Recognizing words across regional accents: the role of perceptual assimilation in lexical competition"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/martinez13b_interspeech.html", "abstract": "Speech technologies are more important every day to assist people with speech disorders. They can help to increase their quality of life or help clinicians to make a diagnosis. In this paper a new methodology based on a total variability subspace modelled by factor analysis is proposed to assess the intelligibility of people with dysarthria. The acoustic information of each recording is efficiently compressed and a Pearson correlation of 0.91 between the vectors in this subspace (iVectors) and the intelligibility is obtained. As acoustic information only perceptual linear prediction features are used. The experiments are conducted on Universal Access Speech database. Also a new error metric to overcome the subjectivity in the intelligibility labels is proposed.", "title": "Dysarthria intelligibility assessment in a factor analysis total variability space"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/ghio13_interspeech.html", "abstract": "We present a study where we examined the influence of a regional accent in the perception of voice and/or speech disorders. These aspects are most of the time overshadowed in clinical context. This protocol, involving multiple sources of speech variations, is also interesting for perception theories. For the experiment, speakers with or without a Southern French accent and with or without speech/voice disorders were recorded on reading a text. The samples were then randomly played back to two groups of listeners (familiar vs unfamiliar with the regional accent), specialists in speech therapy. The task was the perceptual evaluation of voice quality, articulation disorders and dysprosody. We focused in this paper on the voice dimension. The main results on this part concern the weak influence of regional accent on the perception of moderate or severe dysphonia, where the speech signal is strongly disturbed by the disorder. By contrast, the effect of regional accent is important on normal voices perception: listeners unfamiliar with the regional accent judge speakers with accent without voice disorder as slightly dysphonic. This last result can be interpreted as a form of perceptual interference between different dimensions of speech variations around a central position.", "title": "Perceptual interference between regional accent and voice/speech disorders"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/balciuniene13_interspeech.html", "abstract": "The paper deals with the production of mazes in narrative speech. The analysis is based on experimental data of 24 typically developing monolingual Lithuanian children from middle-class families, attending a state kindergarten. During the experiment, the children were asked to tell a story according to the picture sequence. After transcription of audio-recorded stories, production of mazes was measured automatically by using CHILDES tools; the obtained data was compared to the main microstructural indications such as story productivity and syntactic complexity using SPSS tools. During the investigation, statistically significant positive correlation (p < 0.05) between hesitations and revisions was found. However, production of mazes cannot be related to either story productivity (namely, story length in CU) or to syntactic complexity (namely, CL/CU ratio).", "title": "Linguistic disfluency in narrative speech: evidence from story-telling in 6-year olds"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/munson13b_interspeech.html", "abstract": "Speech perception experiments conducted on the internet have numerous benefits over traditional laboratory studies. For example, they have the potential benefit of including many more participants than can be recruited in traditional laboratory-based studies, including members of special hard-to-recruit groups. In the study of phonological development, they have the potential for generating judgments of children's speech production accuracy that reflect the consensus of the child's speech community, rather than the judgments of a small group of phonetically trained individuals. Internet-based studies have the potential disadvantage of being conducted in less controlled listening environments with more variable equipment than laboratory studies. This study compared the performance of two groups of listeners: ones participating in a sound booth with high-quality headphones and ones participating over the internet with whatever equipment was available. Listeners participated in a series of tasks rating children's productions of the sounds /s/, /S/, /\u0192\u00c6/, /d/, /g/, /t/, and /k/. Using a variety of dependent measures, no group differences were found between internet and laboratory listeners. The potential utility of these judgments for studies of phonological development are discussed.", "title": "Assessing the utility of judgments of children's speech production made by untrained listeners in uncontrolled listening environments"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/antolik13_interspeech.html", "abstract": "This paper addresses the presence and type of consonant distortions in speech of 79 French speakers with dysarthria due to Parkinson's disease (PD), Amyotrophic Lateral Sclerosis (ALS) and cerebellar ataxia (CA), and 26 control speakers. A total of 4990 consonants including selected occurrences of /d/, /g/, /t/, /k/ and /s/ in CV word-initial syllables, and /t/ in CV word medial and IP initial position were examined manually. Results show that the ALS group stands out with the more distorted consonants, while the PD and CA performed similarly. The distribution of the type of distortions differs also in the three dysarthric groups. While the most frequent type of distortion in ALS is incomplete closures of stops, devoicing of voiced consonant is the most frequent in the PD and CA groups. In the ALS group, distortions are also more uniformly distributed over consonant type and positions, while voiced consonants are more prone to distortion in PD and CA, as well as consonants in word medial position for PD. Finally, consonant distortions contribute strongly to perceived intelligibility and articulatory imprecision for the ALS and PD group.", "title": "Consonant distortions in dysarthria due to parkinson's disease, amyotrophic lateral sclerosis and cerebellar ataxia"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/verdurand13_interspeech.html", "abstract": "This paper analyses coarticulation in Italian and French monolingual stutterers tested in fluent speech. Our aim is to discover whether the stutterers' fluent speech presents specificities in intra-syllabic coarticulation opposed to the fluent speakers. Two factors will be considered: the role of the syllabic complexity and that of the language. The participants repeat syllables of variable complexity included in a carrier sentence. Besides the great importance of the language, our results show a reduced degree of anticipatory coarticulation (influence of the vowel on the pre-vocalic consonant) in persons who stutter. If the complexity of the syllabic attack increases, the F2 transition of the stutterers\u0081f speech remain stable while the fluents' one does not. These results show that both French and Italian stutterers cannot adapt their articulatory gestures when the complexity increases.", "title": "Study of coarticulation and F2 transitions in French and Italian adult stutterers"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/clapham13_interspeech.html", "abstract": "The acoustics of isolated vowels, e.g. of /a/, have in many studies been linked to pathological voice types, such as tracheoesophageal (TE) voice. To study the possibilities of objective and automatic classification of pathological TE voice types, the acoustic features of /a/ were quantified and subsequently classified using a suit of machine learning technologies. Best classification was achieved by using a voiced-voiceless measurement and the harmonics-tonoise ratio. Other common acoustic features were correlated to pathological type as well, but were less distinctive in classification. We conclude that for objective and automatic classification of TE voice pathology, voicing distinction and harmonics-to-noise ratio are most relevant.", "title": "Automatic tracheoesophageal voice typing using acoustic parameters"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/mauclair13_interspeech.html", "abstract": "Classification of pathological voices is an important problem for early detection and diagnosis. The automatic analysis is a useful complementary tool to other methods based on direct observation. The pathological voices that are studied in this paper are the voices of patients who have a peripheral facial paralysis which, among other pronunciation impairments, affects their ability to pronounce bilabial sounds. The idea is then to use a burst detector to compute acoustical features and provide them to a classifier in order to automatically determine the degree of the voice impairment. The speech database that is used through the paper is unique and was recorded in an soundproof cabin at the La Pitie Salpetriere Hospital in Paris, France. Even if the database is in French, the features that are used in those experiments are independent from the language. The speech recordings used for the experiments are isolated sentences. Two kinds of artificial neural networks are studied for the classification task, a multilayer perceptron and a neural network based on learning vector quantization. Our results show a correlation between the burst-based acoustic features computed from the voices and the degree of the impairment that affects patients.", "title": "Burst-based features for the classification of pathological voices"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/helfer13_interspeech.html", "abstract": "Neurophysiological changes in the brain associated with major depression disorder can disrupt articulatory precision in speech production. Motivated by this observation, we address the hypothesis that articulatory features, as manifested through formant frequency tracks, can help in automatically classifying depression state. Specifically, we investigate the relative importance of vocal tract formant frequencies and their dynamic features from sustained vowels and conversational speech. Using a database consisting of audio from 35 subjects with clinical measures of depression severity, we explore the performance of Gaussian mixture model (GMM) and support vector machine (SVM) classifiers. With only formant frequencies and their dynamics given by velocity and acceleration, we show that depression state can be classified with an optimal sensitivity/specificity/area under the ROC curve of 0.86/0.64/0.70 and 0.77/0.77/0.73 for GMMs and SVMs, respectively. Future work will involve merging our formant-based characterization with vocal source and prosodic features.", "title": "Classification of depression state based on articulatory precision"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/fraser13_interspeech.html", "abstract": "This paper presents experiments in automatically diagnosing primary progressive aphasia (PPA) and two of its subtypes, semantic dementia (SD) and progressive nonfluent aphasia (PNFA), from the acoustics of recorded narratives and textual analysis of the resultant transcripts. In order to train each of three types of classifier (naive Bayes, support vector machine, random forest), a large set of 81 available features must be reduced in size. Two methods of feature selection are therefore compared . one based on statistical significance and the other based on minimumredundancy- maximum-relevance. After classifier optimization, PPA (or absence thereof) is correctly diagnosed across 87.4% of conditions, and the two subtypes of PPA are correctly classified 75.6% of the time.", "title": "Using text and acoustic features to diagnose progressive aphasia and its subtypes"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/alumae13b_interspeech.html", "abstract": "The paper describes a neural network language model that jointly models language in many related domains. In addition to the traditional layers of a neural network language model, the proposed model also trains a vector of factors for each domain in the training data that are used to modulate the connections from the projection layer to the hidden layer. The model is found to outperform simple neural network language models as well as domain-adapted maximum entropy language models in perplexity evaluation and speech recognition experiments.", "title": "Multi-domain neural network language model"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/long13_interspeech.html", "abstract": "This paper investigates improving lightly supervised acoustic model training for an archive of broadcast data. Standard lightly supervised training uses automatically derived decoding hypotheses using a biased language model. However, as the actual speech can deviate significantly from the original programme scripts that are supplied, the quality of standard lightly supervised hypotheses can be poor. To address this issue, word and segment level combination approaches are used between the lightly supervised transcripts and the original programme scripts which yield improved transcriptions. Experimental results show that systems trained using these improved transcriptions consistently outperform those trained using only the original lightly supervised decoding hypotheses. This is shown to be the case for both the maximum likelihood and minimum phone error trained systems.", "title": "Improving lightly supervised training for broadcast transcription"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/cerisara13_interspeech.html", "abstract": "This work proposes a new research direction to address the lack of structures in traditional n-gram models. It is based on a weakly supervised dependency parser that can model speech syntax without relying on any annotated training corpus. Labeled data is replaced by a few hand-crafted rules that encode basic syntactic knowledge. Bayesian inference then samples the rules, disambiguating and combining them to create complex tree structures that maximize a discriminative model's posterior on a target unlabeled corpus. This posterior encodes sparse selectional preferences between a head word and its dependents. The model is evaluated on English and Czech newspaper texts, and is then validated on French broadcast news transcriptions.", "title": "Weakly supervised parsing with rules"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/nussbaumthom13_interspeech.html", "abstract": "In language classification, measures like perplexity and Kullback- Leibler divergence are used to compare language models. While this bears the advantage of isolating the effect of the language model in speech and language processing problems, the measures have no clear relation to the corresponding classification error. In practice, an improvement in terms of perplexity does not necessarily correspond to an improvement in the error rate. It is well-known that Bayes decision rule is optimal if the true distribution is used for classification. Since the true distribution is unknown in practice, a model distribution is used instead, introducing suboptimality. We focus on the degradation introduced by a model distribution, and provide an upper bound on the error difference between Bayes decision and a model-based decision rule in terms of the f -Divergence between the true and model distributions. Simulations are first presented to reveal a special case of the bound, followed by an analytic proof of the generalized bound and its tightness. In addition, the conditions that result in the boundary cases will be discussed. Several instances of the bound will be verified using simulations, and the bound will be used to study the effect of the language model on the classification error.", "title": "Relative error bounds for statistical classifiers based on the f-divergence"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/premkumar13_interspeech.html", "abstract": "This paper summarizes our latest efforts in the development of a Large Vocabulary Continuous Speech Recognition (LVCSR) system for Tamil at different levels: pronunciation dictionary, language modeling (LM) and front-end. Usually in Tamil there are not many word-pronunciation pairs to train data-driven grapheme-to-phoneme (G2P) converters. Therefore, we explore the correlation between the amount of training data and the performance of the grapheme-to-phoneme (G2P) conversion. To address the morphological complexity of Tamil, we investigate different levels of morphemes for language modeling including a comparison between our Dictionary Unit Merging Algorithm (DUMA) and Morfessor, followed by various experiments on hybrid systems using word and morpheme LMs. Finally, we integrate our multilingual bottle-neck features framework with Tamil LVCSR. The final best system produced 21.34% Syllable Error Rate (SyllER) on our Tamil test set.", "title": "Experiments towards a better LVCSR system for tamil"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/thangthai13_interspeech.html", "abstract": "This paper investigates the use of a hybrid language model for open-vocabulary Thai LVCSR. Thai text is written without word boundary markers and the definition of word unit is often ambiguous due to the presence of compound words. Hence, to build open-vocabulary LVCSR, a very large lexicon is required to also handle word unit ambiguity. Pseudo-morpheme (PM), a syllable- like sub-word unit specifically designed for Thai is considered to be a more well-defined unit. To overcome the problem of out-of-vocabulary words and to also reduce the size of the lexicon, a hybrid language model which combines word and sub-word units is proposed. Words and sub-words frequently found in several domains constitute open-vocabulary for general domain Thai LVCSR. To verify our scheme, we run recognition experiments on data from various tasks including broadcast news transcription, dictation and mobile speech-to-speech translation. Open-vocabulary Thai LVCSR using the hybrid language model obviously reduces the out-of-vocabulary problem. The proposed model having a much smaller lexicon size achieves a comparable recognition error rate to a baseline system using a full-word lexicon.", "title": "A hybrid language model for open-vocabulary Thai LVCSR"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/chien13_interspeech.html", "abstract": "This paper presents a nonparametric interpretation for modern language model based on the hierarchical Pitman-Yor and Dirichlet (HPYD) process. We propose the HPYD language model (HPYD-LM) which flexibly conducts backoff smoothing and topic clustering through Bayesian nonparametric learning. The nonparametric priors of backoff n-grams and latent topics are tightly coupled in a compound process. A hybrid probability measure is drawn to build the smoothed topic-based LM. The model structure is automatically determined from training data. A new Chinese restaurant scenario is proposed to implement HPYD-LM via Gibbs sampling. This process reflects the power-law property and extracts the semantic topics from natural language. The superiority of HPYD-LM to the related LMs is demonstrated by the experiments on different corpora in terms of perplexity and word error rate.", "title": "Hierarchical pitman-yor and dirichlet process for language model"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/asami13_interspeech.html", "abstract": "This paper presents a novel unsupervised calibration framework of word confidence measures for automatic speech recognition. It makes it possible to improve the quality of confidence measures in situations where the training of parametric models is hindered by a lack of human-labeled in-domain data. The proposed method calibrates confidence scores by utilizing recognition results stored in deployed systems rather than human-labeled data. In order to stabilize correct/incorrect decision of words, the confidence score of the target word is calibrated based on the confidence scores of identical words, called \"examples,\" found in the stored recognition results. The confidence scores of examples are weighted according to the importance of each example, and the calibrated confidence score of the target word is calculated as the importance weighted average of the scores of the examples. The importance of each example is determined by context similarity between the target word and the example. Experiments confirm that the proposed calibration method can improve the correct/incorrect decision of recognized words compared to word posterior probabilities and the conventional calibration method on the unknown domain call center task.", "title": "Unsupervised confidence calibration using examples of recognized words and their contexts"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/tuske13_interspeech.html", "abstract": "Recently, a multilingual Multi Layer Perceptron (MLP) training method was introduced without having to explicitly map the phonetic units of multiple languages to a common set. This paper further investigates this method using bottleneck (BN) tandem connectionist acoustic modeling for four high-resourced languages .English, French, German, and Polish. Aiming at the improvement of already existing high performing automatic speech recognition (ASR) systems, the multilingual training of the BN-MLP is extended from short-term to hierarchical long-term (multi-resolutional RASTA) feature extraction. Furthermore, deeper structures and context-dependent target labels are also examined. We experimentally demonstrate that a single state-of-the-art BN feature set can be trained for multiple languages, which is superior to the monolingual feature set, and results in significant gains in all the four languages. Studying the scalability of the multilingual BN features, a similar gain is observed in small (50 hours) and in larger scale (300 hours) ASR experiments regardless of the distribution of the data amount between the languages. Using deeper structures, context-dependent targets, and speaker adaptation, the multilingual BN reduces the word error rates by 3.7% relative over the target language BN features and 25.30% over the conventional MFCC system.", "title": "Multilingual hierarchical MRASTA features for ASR"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/chang13c_interspeech.html", "abstract": "This paper describes a novel approach to the automatic selection of training sentences from a system-generated data feed for the development of high-precision language models (LMs) required for speech-enabled voice interface applications in the TV search domain. We develop a set of heuristic rules to select training sentences directly from the TV electronic programming guide (EPG) in their metadata form. The training corpus constructed using the selection algorithms encoded with the historical EPG data enables the adapted LMs to have a considerably lower perplexity while achieving a significant reduction in word error rate (WER). When evaluated using the user-generated spoken queries to an experimental TV search application, a 10% absolute reduction of WER is reported over the baseline LMs created without using the training sentences generated from the historical EPG data.", "title": "Heuristic selection of training sentences from historical TV guide for semi-supervised LM adaptation"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/fohr13_interspeech.html", "abstract": "This paper presents the results and conclusion of a study on the introduction of semantic information through the Random Indexing paradigm in statistical language models used in speech recognition. Random Indexing is an alternative to Latent Semantic Analysis (LSA) that addresses the scalability problem of LSA. After a brief presentation of Random Indexing (RI), this paper describes, different methods to estimate the RI matrix, then how to derive probabilities from the RI matrix and finally how to combine them with n-gram language model probabilities. Then, it analyzes the performance of these different RI methods and their combinations with a 4-gram language model by computing the perplexity of a test corpus of 290,000 words from the French evaluation campaign ETAPE. Among our results, the main conclusions are (1) regardless of the method, function words should not be taken into account in the estimation of RI matrix; (2) The two methods RI_basic and TTRI_w achieved the best perplexity, i.e. a relative gain of 3% compared to the perplexity of the 4-gram language model alone (136.2 vs. 140.4).", "title": "Combination of random indexing based language model and n-gram language model for speech recognition"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/miao13_interspeech.html", "abstract": "We investigate two strategies to improve the context-dependent deep neural network hidden Markov model (CD-DNN-HMM) in low-resource speech recognition. Although outperforming the conventional Gaussian mixture model (GMM) HMM on various tasks, CD-DNN-HMM acoustic modeling becomes challenging with limited transcribed speech, e.g., less than 10 hours. To resolve this issue, we firstly exploit dropout which prevents overfitting in DNN finetuning and improves model robustness under data sparseness. Then, the effectiveness of multilingual DNN training is evaluated when additional auxiliary languages are available. The hidden layer parameters of the target language are shared and learned over multiple languages. Experiments show that both strategies boost the recognition performance significantly. Combining them results in further reduction in word error rate, achieving 11.6% and 6.2% relative improvement on two limited data conditions.", "title": "Improving low-resource CD-DNN-HMM using dropout and multilingual DNN training"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/qin13_interspeech.html", "abstract": "Out-of-vocabulary (OOV) words can appear more than once in a conversation or over a period of time. Such multiple instances of the same OOV word provide valuable information for estimating the pronunciation or the part-of-speech (POS) tag of the word. But in a conventional OOV word detection system, each OOV word is recognized and treated individually. We therefore investigated how to identify recurrent OOV words in speech recognition. Specifically, we propose to cluster multiple instances of the same OOV word using a bottom-up approach. Phonetic, acoustic and contextual features were collected to measure the distance between OOV candidates. The experimental results show that the bottom-up clustering approach is very effective at detecting the recurrence of OOV words. We also found that the phonetic feature is better than the acoustic and contextual features, and the best performance is achieved when combining all features.", "title": "Finding recurrent out-of-vocabulary words"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/chiu13_interspeech.html", "abstract": "We describe a language independent word burst feature based on the structure of conversational speech that can be used to improve spoken term detection (STD) performance. Word burst refers to a phenomenon in conversational speech in which particular content words tend to occur in close proximity of each other as a byproduct of the topic under discussion. To take advantage of bursts, we describe a rescoring procedure that can be applied to lattice and confusion network outputs to improve STD performance. This approach is particularly effective when acoustic models are built with limited training data (and ASR performance is relatively poor). We find that word bursts appear in the four languages we examined and that STD performance can be improved for three of them; the remaining language is agglutinative.", "title": "Using conversational word bursts in spoken term detection"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/acher13_interspeech.html", "abstract": "This study aims at describing cortical and subcortical activation patterns associated with functional recovery of speech production after reconstructive mouth surgery. Our ultimate goal is the understanding of how the brain deals with altered relationships between motor commands and auditory/orosensory feedback, and establishes new inter-articulatory coordination to preserve speech communication abilities. A longitudinal sparse-sampling fMRI study involving orofacial, vowel and syllable production tasks on 9 patients and in three different sessions (one week before, one month and three months after surgery) was conducted. Healthy subjects were recorded in parallel. Results show that for patients in the pre-surgery session, activation patterns are in good agreement with the classical speech production network. Crucially, lower activity in sensorimotor control brain areas during orofacial and speech production movements is observed for patients in all sessions. One month after surgery, the superior parietal lobule is more activated for simple vowel production suggesting a strong involvement of a multimodal integration process to compensate for loss of tongue motor control. Altogether, these results indicate both altered and adaptive sensorimotor control mechanisms in these patients.", "title": "Brain activations in speech recovery process after intra-oral surgery: an fMRI study"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/mertens13_interspeech.html", "abstract": "The objective is to present a method to analyze vocal tremor in sustained speech sounds and compare the results with a perceptual scoring of the degree of tremor. The vocal cycle lengths are tracked by salience analysis and dynamic programming. The cycle length time series is then split into three components by empirical mode decomposition: jitter, tremor and trend. Tremor cues are obtained via non-uniform quantization of the spectrum of the tremor component of the cycle length time series. The results report tremor size, tremor frequency and bandwidth of a corpus of vowels sustained by Parkinson speakers as well as their correlations with the perceived degree of tremor assessed via pairwise comparison.", "title": "Acoustic and perceptual analysis of vocal tremor"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/tantibundhit13_interspeech.html", "abstract": "Lexical tone perception in Thai adults with normal hearing and those with sensorineural hearing-loss using hearing aids was investigated to evaluate the benefit of using hearing aids for tone perception. Tone perception pattern across groups was also compared. Identification tests (with white noise and clean conditions) using a two alternative forced choice task were carried out to systematically compare confusion pattern in each tonal pair. Average percent correct responses, balanced confusion matrices, and signal detection theory (SDT) bias values of c for confusable tone pairs were obtained. Overall, the results showed that there seem to be no significant gain in the tone perception of Thai hearing-impaired adults with hearing devices. Tonal confusion patterns differ across groups. Specifically, out of 10 tone pairings, average bias values of c showed that normal hearing adults favor specific members in 6 tone pairs (i.e., the first member of the following: mid-falling, mid-rising, low-falling, low-high, falling-high, and rising-high). Interestingly, for hearing-impaired adults (with and without hearing devices), reverse pattern, favoring the second member of those pairs, was found.", "title": "Lexical tone perception in Thai normal-hearing adults and those using hearing aids: a case study"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/kagomiya13_interspeech.html", "abstract": "Human listeners can perceive speech signals in a voice-modulated ultrasonic carrier from a bone-conduction stimulator, even if the listeners are patients with sensorineural hearing loss. Considering this fact, we have developed a bone-conducted ultrasonic hearing aid (BCUHA). The purpose of this study was to assess the usefulness of the BCUHA in transmission of the emotional state of the speaker. The evaluation used emotion-identification experiments. Types of emotion included Ekman's basic 6 emotions (\"anger\", \"disgust\", \"fear\", \"joy\", \"sadness\", and \"surprise\") and \"neutral\". The experiments were also conducted under air-conduction (AC) and cochlear implant simulator (CIsim) conditions. The results showed that emotion was transmitted more effectively with the BCUHA than CIsim.", "title": "Evaluation of a bone-conducted ultrasonic hearing aid in vocal emotion transmission"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/garrapa13_interspeech.html", "abstract": "Cochlear implants partially restore auditory sensation in individuals affected by severe to profound hearing loss. We investigated vowel detection, identification, and discrimination in a group of congenitally-deafened, unilaterally-implanted, Italian children and in a group of age-matched controls, by combining behavioral and neurophysiologic measures. Comparable vowel identification and discrimination performance emerged for cochlear-implant and normal-hearing children at the behavioral level. At the neurophysiologic level, on the other hand, cochlear-implant children appeared to lag behind their age-matched normal-hearing peers for vowel detection and identification, but not for vowel discrimination. Length of cochlear implant use significantly affected vowel processing at the neurophysiologic level, although not systematically.", "title": "Processing of /i/ and /u/ in Italian cochlear-implant children: a behavioral and neurophysiologic study"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/cosentino13_interspeech.html", "abstract": "A measure to predict speech intelligibility in unilateral and bilateral cochlear implant (CI) users is proposed that does not need a priori information (i.e. is non-intrusive), such as the room acoustics. Such measure, termed BiSIMCI, combines an equalization-cancellation stage together with a modulation frequency estimation stage. Simulated and actual subjective data from CI users were used to validate the proposed measure. The actual CI subjective data consisted of speech reception thresholds (SRTs) collected in anechoic rooms with a total of 28 target/interferer spatial configurations. The simulated CI subjective data were generated by running the intrusive algorithm by Culling et al. [Ear & Hearing 33 (6), 673.682 (2012)] across 109 different target/interferer conditions from two environments, one anechoic and one highly reverberant room (RT60 = 0.89s). The experimental results indicate that the proposed nonintrusive measure provides reliable predictions when compared with both actual and simulated SRT; an average correlation of 0.94 was reported for these conditions, and an average correlation of 0.97 was obtained when some intrusive assumptions were made.", "title": "Predicting the bilateral advantage in cochlear implantees using a non-intrusive speech intelligibility measure"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/huang13_interspeech.html", "abstract": "We propose a new blind segmentation approach to acoustic event detection (AED) based on i-vectors. Conventional approaches to AED often required well-segmented data with non-overlapping boundaries for competing events. Inspired by block-based automatic image annotation in image retrieval tasks, we blindly segment audio streams into equal-length pieces, label the underlying observed acoustic events with multiple categories and with no event boundary information, extract i-vector for them, and perform classification using support vector machine and maximal figureof- merit based classifiers. Experiments on various sets of audio data show promising results with an average of 8% absolute gain in F1 over the conventional hidden Markov model based approach. An enhanced robustness at different noise levels is also observed. The key to the success lies in the enhanced discrimination power offered by the i-vector representation of the acoustic data.", "title": "A blind segmentation approach to acoustic event detection based on i-vector"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/vuuren13_interspeech.html", "abstract": "Neural networks have recently been shown to be a very effective approach to the unconstrained segmentation of speech into phoneme-like units. The neural network is trained to indicate when a short local sequence of feature vectors is associated with a segment boundary, and when it is not. Although this approach delivers state-of-the-art performance, it is prone to oversegmentation at ambiguous segment boundaries. To address this, we propose the incorporation of the neural network segmenter into a dynamic programming (DP) framework. We evaluate the DP-based approach on the TIMIT corpus, and show that it leads to improved performance.", "title": "A dynamic programming framework for neural network-based automatic speech segmentation"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/prasad13_interspeech.html", "abstract": "Automatic segmentation of speech signals has been a constant engineering challenge. Even after the advances with supervised and unsupervised techniques, there still lies a challenge to equal the manually labelled segments. HMM-based segmentation techniques with modifications and corrections have been the state-of-art. These techniques are supervised in nature and thus require availability of large corpus transcribed with phone boundaries. The unsupervised techniques, on the other hand, explore gradients in various spectral and temporal properties of the speech signals. This paper presents a new and unsupervised method based on signal processing techniques to segment the speech signals. A recently developed method known as Zero Time Liftering (ZTL) is used for the analysis of speech signals, which provides fine temporal resolution of the spectral features of the segment being analyzed. It uses the Hilbert envelope of Numerator Group Delay (HNGD) of the signal to highlight its spectral activity. This representation is used to extract high SNR regions of the spectra, which in turn proves to be useful in representation of the production characteristics of the speech signal. Performance of the proposed analysis is at par with the existing baseline systems for unsupervised segmentation.", "title": "Acoustic segmentation of speech using zero time liftering (ZTL)"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/wang13e_interspeech.html", "abstract": "We consider the problem of unsupervised acoustic unit mining from unlabeled speech data. One typical method involves two steps: unsupervised segmentation and segment clustering. This paper proposes to improve segment clustering with segment-level Gaussian posteriorgram representation, which is generated by averaging the frame-level Gaussian posterior probabilities within each segment. Stacking together the segment-level Gaussian posteriorgrams of all the speech data, a Gaussian-by-segment data matrix is constructed. Given the Gaussian-by-segment matrix, we have the flexibility to cluster either the Gaussian components or the segments into different acoustic unit categories. We have investigated both normalized cut and non-negative matrix factorization approaches on the data matrix for the clustering purpose. We carried out experiments to measure the quality of the clustering results with reference to manual phoneme labels. Experimental results show that the proposed methods consistently outperform a traditional vector quantization method and a Gaussian mixture model labeling method.", "title": "Unsupervised mining of acoustic subword units with segment-level Gaussian posteriorgrams"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/kalinli13_interspeech.html", "abstract": "Segmentation of speech into phonemes is beneficial for many spoken language processing applications. Previously, a novel method which employs auditory attention features for detecting phoneme boundaries from acoustic signal was proposed in [1] outperforming [2, 3]. In this paper, we propose to use phone posterior features, which are obtained from a Deep Belief Network (DBN) based phoneme recognition system, along with attention features since they provide complementary information. When evaluated on TIMIT corpus, the proposed method is shown to successfully predict phoneme boundaries and outperform the recently published text-independent phoneme segmentation methods. Also, the combination of attention features with posterior features yield more than 30% relative improvement in F-measure over the system which used only attention features.", "title": "Combination of auditory attention features with phone posteriors for better automatic phoneme segmentation"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/yuan13b_interspeech.html", "abstract": "This study attempts to improve automatic phonetic segmentation within the HMM framework. Experiments were conducted to investigate the use of phone boundary models, the use of precise phonetic segmentation for training HMMs, and the difference between context-dependent and context-independent phone models in terms of forced alignment performance. Results show that the combination of special one-state phone boundary models and monophone HMMs can significantly improve forced alignment accuracy. HMM-based forced alignment systems can also benefit from using precise phonetic segmentation for training HMMs. Context-dependent phone models are not better than context-independent models when combined with phone boundary models. The proposed system achieves 93.92% agreement (of phone boundaries) within 20 ms compared to manual segmentation on the TIMIT corpus. This is the best reported result on TIMIT to our knowledge.", "title": "Automatic phonetic segmentation using boundary models"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/nguyen13_interspeech.html", "abstract": "This paper presents the development and evaluation of an HMMbased TTS system for the modern Hanoi dialect of Northern Vietnamese, a tonal language. A study of specific phonetic and prosodic features of Hanoi Vietnamese is discussed. Consequences on the design of an HMM-based TTS system are derived. Using this knowledge, a TTS system, called VTed, is then developed under the Mary TTS platform. The second part of the paper is devoted to perceptual evaluations of Vietnamese speech synthesis. Three kinds of evaluations are considered necessary for quality assessment of this tonal language. The general MOS assessment, utterance-level intelligibility, and tone-level intelligibility tests are conducted on the VTed system under a \"natural speech reference\" condition. The results show 1.21 points difference between natural and synthetic speech for the MOS test, a 0.2%.0.9% difference for the utterance-level intelligibility test, 23% on average and . depending on the tone type . from 0% to 37% difference for the tone-level intelligibility test. These results demonstrate the need for more specific works on tonal/prosodic level to improve automatic synthesis of Vietnamese and other tonal languages.", "title": "HMM-based TTS for hanoi vietnamese: issues in design and evaluation"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/raitio13b_interspeech.html", "abstract": "Creaky voice, also referred to as vocal fry, is a voice quality frequently produced in many languages, in both read and conversational speech. To enhance the naturalness of speech synthesis, these latter should be able to generate speech in all its expressive diversity, including creaky voice. The present study looks to exploit our recent developments, including creaky voice detection, prediction of creaky voice from context, and rendering of creaky excitation, into a fully functioning and automatic HMM-based synthesis system. HMM-based synthetic creaky voices are built and evaluated in subjective listening tests, which show that the best synthetic creaky voices are rated more natural and more creaky compared to a conventional voice. A non-creaky voice is also successfully transformed to use creak by modifying the F0 contour and excitation of the predicted creaky parts. The transformed voice is rated equal in terms of naturalness and clearly more creaky compared to the original voice.", "title": "HMM-based synthesis of creaky voice"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/wang13f_interspeech.html", "abstract": "In this paper, we present a hybrid system that combines the Joint Multi-gram Model (JMM) and the Conditional Random Field (CRF) classifiers to solve the Grapheme-to-Phone (G2P) conversion problem. JMM is a generative language model for the n-grams of the joint letter-phone units. JMM is able to model longer phonetic contextual information. However, it is difficult to incorporate complex features, such as syllabification structures, to JMM. On the other hand, CRFs can be used to perform G2P by formulating the task as a sequence-labeling problem. CRFs are discriminative classifiers that can incorporate complex feature functions. However, modeling in CRFs requires the alignment between the letters and phones. Furthermore, traditional linear chain CRFs usually only employ bigram output information for practical reasons, which is not sufficient for this task. In this work, JMM and CRFs are combined in tandem to yield the JMM-CRF hybrid system that benefits from both of the individual approaches. Results on the CMUDict and CELEX databases show that the proposed hybrid system consistently outperforms the individual JMM and CRF systems. Finally, syllabic features are incorporated into the CRFs as additional features and achieve further performance improvement with the hybrid system.", "title": "Integrating conditional random fields and joint multi-gram model with syllabic features for grapheme-to-phone conversion"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/lehnen13_interspeech.html", "abstract": "Accurate grapheme-to-phoneme (g2p) conversion is needed for several speech processing applications, such as automatic speech synthesis and recognition. For some languages, notably English, improvements of g2p systems are very slow, due to the intricacy of the associations between letter and sounds. In recent years, several improvements have been obtained either by using variable-length associations in generative models (joint-n-grams), or by recasting the problem as a conventional sequence labeling task, enabling to integrate rich dependencies in discriminative models. In this paper, we consider several ways to reconciliate these two approaches. Introducing hidden variable-length alignments through latent variables, our Hidden Conditional Random Field (HCRF) models are able to produce comparative performance compared to strong generative and discriminative models on the CELEX database.", "title": "Structure learning in hidden conditional random fields for grapheme-to-phoneme conversion"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/stan13b_interspeech.html", "abstract": "Simple4All Tundra (version 1.0) is the first release of a standardised multilingual corpus designed for text-to-speech research with imperfect or found data. The corpus consists of approximately 60 hours of speech data from audiobooks in 14 languages, as well as utterance-level alignments obtained with a lightly-supervised process. Future versions of the corpus will include finer-grained alignment and prosodic annotation, all of which will be made freely available. This paper gives a general outline of the data collected so far, as well as a detailed description of how this has been done, emphasizing the minimal language-specific knowledge and manual intervention used to compile the corpus. To demonstrate its potential use, text-to-speech systems have been built for all languages using unsupervised or lightly supervised methods, also briefly presented in the paper.", "title": "TUNDRA: a multilingual corpus of found data for TTS research created with light supervision"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/maia13_interspeech.html", "abstract": "This paper presents an approach for complex cepstrum analysis based on the minimum mean squared error criterion, and describes its application to statistical parametric speech synthesis. The proposed method alleviates some of the issues associated with conventional complex cepstrum analysis, such as choice of the window, phase unwrapping, and the need for accurate pitch marks. Given initial estimates of warped complex cepstra and respective analysis instants, the method iteratively optimizes the complex cepstrum on a warped quefrency domain by minimizing the mean squared error between the natural and the reconstructed speech waveforms. When applied to statistical parametric speech synthesis, the optimized complex cepstrum results in better performance in terms of synthesized speech quality, specially for emotional databases, when compared with the complex cepstrum calculated through conventional methods.", "title": "Minimum mean squared error based warped complex cepstrum analysis for statistical parametric speech synthesis"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/hifny13_interspeech.html", "abstract": "Augmented Conditional Random Fields (ACRFs) are undirected graphical models that maintain the Markov properties of Hidden Markov Models (HMMs), formulated using the maximum entropy (MaxEnt) principle. ACRFs incorporate acoustic context information into an augmented space in order to model the sequential phenomena of the speech signal. The augmented space is constructed using Gaussian activation functions representing the dense regions in the observation space. These activation functions are estimated using the Expectation-Maximization (EM) algorithm. Alternatively, the activation functions can be estimated using a discriminative objective function. Hence, the ACRFs are fed with discriminative features. In this paper, we show that ACRFs recognition results improve if the activation functions are estimated using the Minimum Phone Error (MPE) discriminative criterion.", "title": "Augmented conditional random fields modeling based on discriminatively trained features"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/vesely13_interspeech.html", "abstract": "Sequence-discriminative training of deep neural networks (DNNs) is investigated on a 300 hour American English conversational telephone speech task. Different sequence-discriminative criteria . maximum mutual information (MMI), minimum phone error (MPE), state-level minimum Bayes risk (sMBR), and boosted MMI . are compared. Two different heuristics are investigated to improve the performance of the DNNs trained using sequence-based criteria . lattices are regenerated after the first iteration of training; and, for MMI and BMMI, the frames where the numerator and denominator hypotheses are disjoint are removed from the gradient computation. Starting from a competitive DNN baseline trained using cross-entropy, different sequence-discriminative criteria are shown to lower word error rates by 8.9% relative, on average. Little difference is noticed between the different sequence-based criteria that are investigated. The experiments are done using the open-source Kaldi toolkit, which makes it possible for the wider community to reproduce these results.", "title": "Sequence-discriminative training of deep neural networks"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/zhang13c_interspeech.html", "abstract": "We propose a method to discriminatively train acoustic models with sparse inverse covariance (precision) matrices in order to improve the model robustness when training data is insufficient. Acoustic models with sparse inverse covariance matrices were previously proposed to address the problem of over-fitting when training data is inadequate. Since many of the entries of the inverse covariance matrices are driven to zero, the number of free parameters to be estimated is reduced. However, previously acoustic models using sparse inverse covariance matrices were trained using maximum likelihood (ML) training. It is well-known that discriminative training can further improve the recognition accuracy. Therefore, for the first time, we study the problem of training acoustic models with sparse inverse covariance matrices using the discriminative training method. An L1 regularization term is added to the traditional objective function for discriminative training to penalize complex models and to automatically sparsify the inverse covariance matrices. The new objective function is optimized by maximizing a weak-sense auxiliary function. Experimental results on the Wall Street Journal data set show that our method effectively regularizes the model complexity and allows more Gaussian components to be trained. Therefore it can better model the non-Gaussian nature of the speech feature vectors. Compared with the standard maximum mutual information (MMI) training method, our proposed method can significantly improve the recognition accuracy.", "title": "Discriminatively trained sparse inverse covariance matrices for low resource acoustic modeling"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/tachioka13_interspeech.html", "abstract": "In discriminative training methods, the objective function is designed to improve the performance of automatic speech recognition with reference to correct labels using a single system. On the other hand, system combination methods, which output refined hypotheses by a majority voting scheme, need to build multiple systems that generate complementary hypotheses. This paper aims to unify the both requirements within a discriminative training framework based on the mutual information criterion. That is, we construct complementary models by optimizing the proposed objective function, which yields to minimize the mutual information with base systems' hypotheses, while maximize that with correct labels, at the same time. We also analyze that this scheme corresponds to weight the training data of a complementary system by considering correct and error tendencies in the base systems, which has close relationship with boosting methods. In addition, the proposed method can practically construct complementary systems by simply extending a lattice-based parameter update algorithm in discriminative training, and can adjust the degree of how much the complementary system outputs are different from base system ones. The experiments on highly noisy speech recognition (\u0081eThe 2nd CHiME challenge\u0081f) show the effectiveness of the proposed method, compared with a conventional system combination approach.", "title": "Discriminative training of acoustic models for system combination"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/huang13b_interspeech.html", "abstract": "We present our study on semi-supervised Gaussian mixture model (GMM) hidden Markov model (HMM) and deep neural network (DNN) HMM acoustic model training. We analyze the impact of transcription quality and data sampling approach on the performance of the resulting model, and propose a multi-system combination and confidence re-calibration approach to improve the transcription inference and data selection. Compared to using a single system recognition result and confidence score, our proposed approach reduces the phone error rate of the inferred transcription by 23.8% relatively when top 60% of data are selected. Experiments were conducted on the mobile short message dictation (SMD) task. For the GMM-HMM model, we achieved 7.2% relative word error rate reduction (WERR) against a well-trained narrowband fMPE+bMMI system by adding 2100 hours of untranscribed data, and 28.2% relative WERR over a wide-band MLE model trained from transcribed out-of-domain voice search data after adding 10K hours of untranscribed SMD data. For the CD-DNN-HMM model, 11.7% and 15.0% relative WERRs are achieved after adding 1K hours of untranscribed data using random and importance sampling, respectively. We also found using large amount of untranscribed data for pre-training does not help.", "title": "Semi-supervised GMM and DNN acoustic model training with multi-system combination and confidence re-calibration"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/xue13_interspeech.html", "abstract": "Recently proposed deep neural network (DNN) obtains significant accuracy improvements in many large vocabulary continuous speech recognition (LVCSR) tasks. However, DNN requires much more parameters than traditional systems, which brings huge cost during online evaluation, and also limits the application of DNN in a lot of scenarios. In this paper we present our new effort on DNN aiming at reducing the model size while keeping the accuracy improvements. We apply singular value decomposition (SVD) on the weight matrices in DNN, and then restructure the model based on the inherent sparseness of the original matrices. After restructuring we can reduce the DNN model size significantly with negligible accuracy loss. We also fine-tune the restructured model using the regular back-propagation method to get the accuracy back when reducing the DNN model size heavily. The proposed method has been evaluated on two LVCSR tasks, with context-dependent DNN hidden Markov model (CD-DNN-HMM). Experimental results show that the proposed approach dramatically reduces the DNN model size by more than 80% without losing any accuracy.", "title": "Restructuring of deep neural network acoustic models with singular value decomposition"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/chen13g_interspeech.html", "abstract": "In this work, we quantify common tonal and phonetic errors made by second language learners of Mandarin Chinese. Pronunciation patterns of 300 native speakers of European languages are analyzed. Tonal errors (30.56%) are found to be more prevalent than phonetic ones (8.71%). Common errors include overemphasis of Tone 3 and inadequate aspiration of affricate consonants. Decision tree clustering was used to further characterize these error patterns with their tonal and phonetic context. Our findings are potentially useful in second language education and in computer assisted language learning.", "title": "Large-scale characterization of Mandarin pronunciation errors made by native speakers of European languages"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/delvaux13_interspeech.html", "abstract": "This paper reports on an exploratory study of the processes involved in the acquisition of new phonetic control regimes in L2 learning. We focus here on the acquisition of long VOT initial stops by French native speakers undergoing production training. Francophone speakers were asked to repeat /ta/ stimuli varying in VOT and burst intensity. The performances in production are assessed through a comparison between objective measures performed on the speech signals (VOT, burst intensity) and subjective measures from L2 listeners themselves (in terms of similarity between the model and the response) and from American English native listeners (in terms of similarity as well as L1 typicality). Results show that (i) the Francophone speakers reasonably matched in their responses the VOT and burst intensity variations of the stimuli; (ii) that the three subjective indices are highly correlated with each other, but that they only partially correlate with the acoustic parameters measured on the signals; (iii) that inter-individual variation is very large, among the speakers' productions as well as among the listeners' judgments.", "title": "Production training in second language acquisition: a comparison between objective measures and subjective judgments"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/netelenbos13_interspeech.html", "abstract": "The present study investigated the acquisition of French stop consonants in English-speaking children who attend a French immersion elementary school in Alberta, Canada. Both languages have voiced and voiceless stops, but in English the distinction is phonetically realized as short-lag vs. long-lag VOTs, while in French it is as lead vs. short-lag VOTs. Examining the development of stops in this group of children can shed light on the interaction between L1 and L2 when L2 is acquired in an immersion setting. Children in grades 1, 3, and 5 first participated in a word-repetition task, repeating back words beginning with /b/ and /p/. They were then tested on their identification of stop categories over a VOT series ranging from -70 to +70ms, with 10-ms differences among the tokens. Results from the production experiment indicated a clear separation of the two languages in /p/, but not in /b/. The perception results suggest consistent identification in both English stops, the French /p/, but not the French /b/. The results are discussed in terms of the cross-language phonetic differences, the particularity of the learning environment, and the impact of dominant language in society.", "title": "The production and perception of voice onset time in English-speaking children enrolled in a French immersion program"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/burgos13_interspeech.html", "abstract": "In this paper we report on a study on pronunciation errors by Spanish learners of Dutch, which was aimed at obtaining information to develop a dedicated Computer Assisted Pronunciation Training (CAPT) program for this fixed language pair (Spanish L1, Dutch L2). The results of our study indicate, that, first, vowel errors are more frequent and variable than consonant mispronunciations. Second, Spanish natives appear to have problems with vowel length, vowel height, and front rounded vowels. Third, they tend to fall back on the pronunciation of their L1 vowels.", "title": "Pronunciation errors by Spanish learners of Dutch: a data-driven study for ASR-based pronunciation training"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/graham13_interspeech.html", "abstract": "Several factors have been attested to affect the temporal synchronisation of tonal targets such as syllable duration, segmental structure and proximity to word or intonational boundaries. Given the apparent language-specific nature of tonal alignment, it can be expected that late bilinguals who are acquiring a second language will need to learn the alignment implementation rules of that language, in addition to other aspects. This study compared the tonal alignment patterns of Japanese late bilingual English speakers and monolingual English speakers in order to investigate to what extent learners transfer their native implementation strategies to the interlanguage, and whether alignment changes with proficiency. The results show that, although initial-accented words were aligned later than final-accented words for all groups, as expected, the Japanese bilinguals aligned the former significantly later than the monolinguals. Further analyses revealed that their off-target realisations were generally limited to a specific type of syllable structure that we speculate may be linked to peak delay in their L1. These results are taken as evidence of prosodic transfer and suggest that late bilinguals will need to learn the L2 phonetic implementation rules of alignment independently of their acquisition of the phonology.", "title": "Realisation of tonal alignment in the English of Japanese-English late bilinguals"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/benoistlucy13_interspeech.html", "abstract": "This article intends to study the influence of two parameters (language and speech task) on the manifestation of creaky voice among six American women learning French. The speakers were recorded in both reading and spontaneous conditions, in English and French. These languages were chosen for their difference at the phonatory level. The number of creaky voice occurrences, and creakiness timing were calculated. The aim was to answer the following questions: Can American native speakers learning French show a differentiated use of creaky voice depending on the language they speak? Can American native speakers show a variable use of creaky voice whether they read a text or talk spontaneously? Our study shows that the six American speakers of this research show significantly more creaky voice in English than in French and in spontaneous speech than in the reading task. This leads us to consider that phonatory aspects of a language are part of a language itself and that certain speech conditions are more prone to exhibit extreme vocal effects such as creaky voice, given their propensity to expressiveness.", "title": "The influence of language and speech task upon creaky voice use among six young American women learning French"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/bone13b_interspeech.html", "abstract": "Impaired social communication and social reciprocity are the primary phenotypic distinctions between autism spectrum disorders (ASD) and other developmental disorders. We investigate quantitative conversational cues in child-psychologist interactions using acoustic-prosodic, turn-taking, and language features. Results indicate the conversational quality degraded for children with higher ASD severity, as the child exhibited difficulties conversing and the psychologist varied her speech and language strategies to engage the child. When interacting with children with increasing ASD severity, the psychologist exhibited higher prosodic variability, increased pausing, more speech, atypical voice quality, and less use of conventional conversational cue such as assents and non-fluencies. Children with increasing ASD severity spoke less, spoke slower, responded later, had more variable prosody, and used personal pronouns, affect language, and fillers less often. We also investigated the predictive power of features from interaction subtasks with varying social demands placed on the child. We found that acoustic prosodic and turn-taking features were more predictive during higher social demand tasks, and that the most predictive features vary with context of interaction. We also observed that psychologist language features may be robust to the amount of speech in a subtask, showing significance even when the child is participating in minimal-speech, low social-demand tasks.", "title": "Acoustic-prosodic, turn-taking, and language cues in child-psychologist interactions for varying social demand"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/boril13_interspeech.html", "abstract": "This paper studies various aspects of child vocalization as captured in a newly established parallel corpus of sixteen 18.31 months old US and Shanghainese toddlers. The recordings were acquired in 16-hour sessions during an \u0081eordinary\u0081f day in the child's natural environment and manually labeled. The vocalization characteristics are studied by means of phonotactic and prosodic analysis with emphasis on automatic processing. In the phonotactic domain, a Gaussian mixture model (GMM) tokenizer, a bank of phone recognizers, and formant tracking are used to analyze the movements in the acoustic-phonetic space. In the prosodic domain, pitch patterns, duration, and rhythm are analyzed. Besides strong individual-specific characteristics of the subjects in some of the domains considered, the two language groups show differences in the occupation of the F1 . F2 formant space, choice of pitch pattern durations, and consistency in producing complex phonetic patterns.", "title": "A preliminary study of child vocalization on a parallel corpus of US and shanghainese toddlers"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/claus13_interspeech.html", "abstract": "In this paper we survey databases of children's speech. A current trend in research is the investigation of children's automatic speech recognition (ASR). Therefore, databases of children\u0081fs speech are needed for testing but also for training of ASR systems. However, unlike adult speech corpora, databases for children are rarely available, and in current literature there is no overview of existing databases to be found. Most children's speech databases contain recorded speech in English of children aged between 6 and 18 years. They are described in the first part of this paper. Subsequently databases for German and other languages are mentioned. They are even more rarely available than English databases. In particular, recordings of preschool children are very rare and therefore regarded separately. Due to the fact that preschool children are not able to read, traditional recording methods cannot be applied, which makes recording of their speech complex. Some ideas covering the difficulties of recordings for speech databases of preschool children are mentioned. Utilizing these methods a small database of German children's speech has been created. Furthermore some statistics about children's speech data are presented.", "title": "A survey about databases of children's speech"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/kouloumenta13_interspeech.html", "abstract": "In this pilot study, we investigate the differences in the electroencephalography (EEG) signal patterns of children and adults while interacting with a multimodal dialogue computer game. The gaming application is designed for preschoolers, implements five popular learning tasks and has variable levels of difficulty. In this pilot, to simplify the data collection process for young children, we use the NeuroSky MindSet device which is a single forehead dry sensor device. The raw signals and the estimated attention, meditation and arousal signals are analyzed during the interaction and compared for adult and children user populations. Results show consistent variations as a function of modality used (speech vs mouse input), difficulty level and task success. The physiological signal pattern within an interaction turn is also estimated and analyzed. Overall, children and adults demonstrated very similar physiological signal patterns during multimodal interaction.", "title": "Affective evaluation of multimodal dialogue games for preschoolers using physiological signals"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/alam13c_interspeech.html", "abstract": "The goal of speech emotion recognition (SER) is to identify the emotional or physical state of a human being from his or her voice. One of the most important things in a SER task is to extract and select relevant speech features with which most emotions could be recognized. In this paper, we present a smoothed nonlinear energy operator (SNEO)-based amplitude modulation cepstral coefficients (AMCC) feature for recognizing emotions from speech signals. SNEO estimates the energy required to produce the AM-FM signal, and then the estimated energy is separated into its amplitude and frequency components using an energy separation algorithm (ESA). AMCC features are obtained by first decomposing a speech signal using a C-channel gammatone filterbank, computing the AM power spectrum, and taking a discrete cosine transform (DCT) of the root compressed AM power spectrum. Conventional MFCC (Mel-frequency cepstral coefficients) and Mel-warped DFT (discrete Fourier transform) spectrum based cepstral coefficients (MWDCC) features are used for comparing the recognition performances of the proposed features. Emotion recognition experiments are conducted on the FAU AIBO spontaneous emotion corpus. It is observed from the experimental results that the AMCC features provide a relative improvement of approximately 3.5% over the baseline MFCC.", "title": "Amplitude modulation features for emotion recognition from speech"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/bone13c_interspeech.html", "abstract": "Rapid Automatized Naming (RAN) is a powerful tool for predicting future reading skill. A person's ability to quickly name symbols as they scan a table is related to higher-level reading proficiency in adults and is predictive of future literacy gains in children. However, noticeable differences are present in the strategies or patterns within groups having similar task completion times. Thus, a further stratification of RAN dynamics may lead to better characterization and later intervention to support reading skill acquisition. In this work, we analyze the dynamics of the eyes, voice, and the coordination between the two during performance. It is shown that fast performers are more similar to each other than to slow performers in their patterns, but not vice versa. Further insights are provided about the patterns of more proficient subjects. For instance, fast performers tended to exhibit smoother behavior contours, suggesting a more stable perception-production process.", "title": "Analyzing eye-voice coordination in rapid automatized naming"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/chaspari13_interspeech.html", "abstract": "Storytelling is a commonly used technique for rating linguistic and communicative abilities of children with Autism Spectrum Disorders (ASD). It highlights their language use beyond sentencelevel production, and their ability to cohesively link events into a plot, including incorporating social context. A key scenario of interest we consider is spoken narrative creation in interactive settings, where confederates such as parents can offer scaffolding to their children's narratives by eliciting answers with appropriate questions, shaping the structure of the resulting narrative. We analyze the structure of children's stories narrated with the help of their parents using entity-based feature-level patterns in order to see how there are influenced by the parents' narrative elicitation techniques. The frequency distribution and evolution of entities -meaning the co-referent people, objects and ideas- can capture the main axis of the story plot. Our results indicate that the type of questions the parents ask can be reflected in the entity-based features of a narrative, affecting its underlying structure and coherence.", "title": "Analyzing the structure of parent-moderated narratives from children with ASD using an entity-based approach"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/evanini13_interspeech.html", "abstract": "This study presents the results of applying automated speech scoring technology to English spoken responses provided by non-native children in the context of an English proficiency assessment for middle school students. The assessment contains three diverse task types designed to measure a student's English communication skills, and an automated scoring system was used to extract features and build scoring models for each task. The results show that the automated scores have a correlation of r = 0.70 with human scores for the Read Aloud task, which matches the human-human agreement level. For the two tasks involving spontaneous speech, the automated scores obtain correlations of r = 0.62 and r = 0.63 with human scores, which represents a drop of 0.08.0.09 from the human-human agreement level. When all 5 scores from the assessment for a given student are aggregated, the automated speaker-level scores show a correlation of r = 0.78 with human scores, compared to a human-human correlation of r = 0.90. The challenges of using automated spoken language assessment for children are discussed, and directions for future improvements are proposed.", "title": "Automated speech scoring for non-native middle school students with multiple task types"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/safavi13_interspeech.html", "abstract": "This paper presents results on gender identification (GI) for children's speech, using the OGI Kids corpus and GMM-UBM and GMM-SVM systems. Regions of the spectrum containing important gender information for children are identified by conducting GI experiments over 21 frequency sub-bands. Results show that the frequencies below 1.8 kHz and above 3.8 kHz are most useful for GI for older children, while the frequencies above 1.4 kHz are most useful for the youngest children. The effect of using age-independent and age-dependent gender modelling (including the effects of puberty on boys voices) is explored. The application of intersession variability compensation is explored but experiments showed only little improvement. Experiments on human GI were also conducted and the results show that the humans do not achieve the performance of the machine.", "title": "Identification of gender from children's speech by computers and humans"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/arai13b_interspeech.html", "abstract": "Many studies have pointed out that the /r/ sounds in Japanese tend to be difficult for native children of Japanese to acquire. To verify this, we first investigated Japanese /r/ sounds uttered by two-year-old twins as a case study. The acoustic analysis of the recordings, which included several words with various /r/ sounds, revealed that certain /r/ sounds are difficult to produce and are often produced with speech errors. We also analyzed a set of utterances of Japanese /r/ spoken in a variety of phones pronounced by an adult male speaker. Then, for comparison, we synthesized Japanese /r/ sounds using four parameters. We conducted two perceptual experiments: one for the natural speech by the male speaker of Japanese, and another for the synthesized speech sounds based on the four parameters. The results showed that variation in pronunciation in adults was widely distributed. We discussed the reasons that it takes time for children to acquire /r/ sounds, and we concluded that it is possibly due to the combination of two factors: 1) some /r/ sounds themselves are difficult to produce, and 2) there is a wide distribution of pronunciation variation in adult speakers.", "title": "On why Japanese /r/ sounds are difficult for children to acquire"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/yao13b_interspeech.html", "abstract": "Recurrent Neural Network Language Models (RNN-LMs) have recently shown exceptional performance across a variety of applications. In this paper, we modify the architecture to perform Language Understanding, and advance the state-of-the-art for the widely used ATIS dataset. The core of our approach is to take words as input as in a standard RNN-LM, and then to predict slot labels rather than words on the output side. We present several variations that differ in the amount of word context that is used on the input side, and in the use of non-lexical features. Remarkably, our simplest model produces state-of-the-art results, and we advance state-of-the-art through the use of bag-of-words, word embedding, named-entity, syntactic, and word-class features. Analysis indicates that the superior performance is attributable to the task-specific word representations learned by the RNN.", "title": "Recurrent neural networks for language understanding"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/riedhammer13_interspeech.html", "abstract": "We describe a state-of-the-art large vocabulary continuous speech recognition (LVCSR) and keyword search (KWS) system trained on roughly 70 hours of conversational telephone speech. Using the Kaldi speech recognition toolkit, we investigate several aspects: for the acoustic front-end, we analyze the use of mel-frequency cepstral coefficients (MFCC), pitch and probability-of-voicing (PoV), and deep neural network (DNN) bottleneck (BN) features, as well as their feature-level combination (\"tandem\"). For the acousticphonetic decision tree, we explore different hidden Markov model (HMM) topologies for the glottalization phoneme /?/ to model its typically short duration. For the acoustic model, we compare regular continuous HMM with a sort of multi-codebook subspace Gaussian mixture model (SGMM) that lead to an overall best word error rate (WER) of 58.7% and 56.3%, respectively. The KWS is implemented as a word lattice search, and is augmented by a syllable lattice back-up search to capture out-of-vocabulary keywords as well as misrecognized lexical surface forms due to ambiguous prefix and hyphenation rules.", "title": "A study on LVCSR and keyword search for tagalog"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/alghowinem13_interspeech.html", "abstract": "Depression is a serious psychiatric disorder that affects mood, thoughts, and the ability to function in everyday life. This paper investigates the characteristics of depressed speech for the purpose of automatic classification by analysing the effect of different speech features on the classification results. We analysed voiced, unvoiced and mixed speech in order to gain a better understanding of depressed speech and to bridge the gap between physiological and affective computing studies. This understanding may ultimately lead to an objective affective sensing system that supports clinicians in their diagnosis and monitoring of clinical depression. The characteristics of depressed speech were statistically analysed using ANOVA and linked to their classification results using GMM and SVM. Features were extracted and classified over speech utterances of 30 clinically depressed patients against 30 controls (both gender-matched) in a speaker-independent manner. Most feature classification results were consistent with their statistical characteristics, providing a link between physiological and affective computing studies. The classification results from low-level features were slightly better than the statistical functional features, which indicates a loss of information in the latter. We found that both mixed and unvoiced speech were as useful in detecting depression as voiced speech, if not better.", "title": "Characterising depressed speech for classification"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/bigot13_interspeech.html", "abstract": "Retrieving pronounced person names in spoken documents is a critical problematic in the context of audiovisual content indexing. In this paper, we present a cascading strategy for two methods dedicated to spoken name recognition in speech. The first method is an acoustic name spotting in phoneme confusion networks. It is based on a phonetic edition distance criterion based on phoneme probabilities held in confusion networks. The second method is a continuous context modelling approach applied on the 1-best transcription output. It relies on a probabilistic modelling of name-to-context dependencies. We assume that the combination of these methods, based on different types of information, may improve spoken name recognition performance. This assumption is studied through experiments done on a set of audiovisual documents from the development set of the REPERE challenge. Results report that combining acoustic and linguistic methods produces an absolute gain of 3% in terms of F-measure compared to the best system taken alone.", "title": "Combining acoustic name spotting and continuous context models to improve spoken person name recognition in speech"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/chen13h_interspeech.html", "abstract": "A hierarchical framework is proposed to address the issues of modeling different type of words in keyword spotting (KWS). Keyword models are built at various levels according to the availability of training set resources for each individual word. The proposed approach improves the performance of KWS even when no training speech is available for the keywords. It also suggests an easier way to collect training data for these resource-limited words. Experimental results show that the proposed framework improves performance in KWS in a figure-of-merit (FOM) metric regardless of the number of training instances for each keyword. For words with abundant speech data, the proposed method exploits the training data better than the conventional modeling technique and boosts the system FOM from 9.79% to 42.78%. For words with a small amount of training data, the new method increases the system FOM from 29.05% to 49.06%. Even for keywords without any training examples, the new modeling scheme improves the system FOM from 60.96% to 66.51%.", "title": "A resource-dependent approach to word modeling for keyword spotting"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/womack13_interspeech.html", "abstract": "This study explores diagnostic correctness and physicians' selfassessed feelings of confidence in spoken medical narratives. Dermatologists were shown images of dermatological cases and asked to narrate their diagnostic thought processes by providing a description of the case, a list of differential diagnoses, a final diagnosis, and, the percent confidence (from 0 to 100%) in their final diagnosis. We describe this novel corpus and present a case study from the dataset. We then report on predictive models for diagnostic accuracy and physician-reported confidence as a way of studying how these extralinguistic features affect the speech of the physicians, comparing use of narrative features, prosodic features, and disfluency features for classification.", "title": "Markers of confidence and correctness in spoken medical narratives"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/nakamura13_interspeech.html", "abstract": "This paper introduces the first online and free framework for teaching and learning Japanese prosody including word accent and phrase intonation. This framework is called OJAD (Online Japanese Accent Dictionary) and it provides three functions. 1) Visual, auditory, systematic, and comprehensive illustration of patterns of accent change (accent sandhi) of verbs and adjectives. Here only the changes caused by twelve kinds of fundamental conjugation are focused upon. 2) Visual illustration of the accent pattern of a given verbal expression, which is a combination of a verb and its postpositional auxiliary words. 3) Visual illustration of the pitch pattern of an any given sentence and the expected positions of accent nuclei in the sentence. The third function is implemented by using an accent change prediction module that we developed for Japanese text-to-speech (TTS) synthesizers. Experiments show that accent nucleus assignment to given texts by the proposed framework is much more accurate than that by native speakers. Subjective assessment and objective assessment by teachers and learners show very high pedagogical effectiveness of the framework.", "title": "Development of a web framework for teaching and learning Japanese prosody: OJAD (online Japanese accent dictionary)"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/shriberg13_interspeech.html", "abstract": "As dialog systems evolve to handle unconstrained input and for use in open environments, addressee detection (detecting speech to the system versus to other people) becomes an increasingly important challenge. We study a corpus in which speakers talk both to a system and to each other, and model two dimensions of speaking style that talkers modify when changing addressee: speech rhythm and vocal effort. For each dimension we design features that do not require speech recognition output, session normalization, speaker normalization, or dialog context. Detection experiments show that rhythm and effort features are complementary, outperform lexical models based on recognized words, and reduce error rates even if word recognition is error-free. Simulated online processing experiments show that all features need only the first couple seconds of speech. Finally, we find that temporal and spectral stylistic models can be trained on outside corpora, such as ATIS and ICSI meetings, with reasonable generalization to the target task, thus showing promise for domain-independent computer-versus-human addressee detectors.", "title": "Addressee detection for dialog systems using temporal and spectral dimensions of speaking style"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/hatano13_interspeech.html", "abstract": "In general, the end of question utterances is accompanied by a rising intonation. However, non-rising intonation is commonly observed in question utterances appearing in conversational speech. In order to clarify the factors involved in the choice of rising or non-rising intonation, in the present work, we analyzed question utterances extracted from Japanese conversational dialogue speech data of multiple speakers. Each utterance was categorized in terms of the question type, the phrase final intonation and the phrase final morpheme. Analysis results revealed that (1) about 20% of question utterances were neither accompanied by a rising intonation nor by a pitch reset; (2) among the question types, non-rising intonation appeared in more than 50% of \"request for agreement\", \"open-type questions\", \"backchannel-type questions\", and \"self-directed doubt-type questions\"; (3) regarding morphemes, non-rising intonation appeared in more than 50% of utterances ending with question-related final particles; in contrast, more than 80% of the utterances ending with morphemes other than final particles were accompanied by a rising intonation or a pitch reset.", "title": "Analysis of factors involved in the choice of rising or non-rising intonation in question utterances appearing in conversational speech"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/celikyilmaz13_interspeech.html", "abstract": "While data-driven methods for spoken language understanding (SLU) provide state of the art performances and reduce maintenance and model adaptation costs compared to handcrafted parsers, the collection and annotation of domain-specific natural language utterances for training remains a time-consuming task. A recent line of research has focused on enriching the training data with in-domain utterances by mining search engine query logs to improve the SLU tasks. However genre mismatch is a big obstacle as search queries are typically keywords. In this paper, we present an efficient discriminative binary classification method that filters large collection of online web search queries only to select the natural language like queries. The training data used to build this classifier is mined from search query click logs, represented as a bipartite graph. Starting from queries which contain natural language salient phrases, random graph walk algorithms are employed to mine corresponding keyword queries. Then an active learning method is employed for quickly improving on top of this automatically mined data. The results show that our method is robust to noise in search queries by improving over a baseline model previously used for SLU data collection. We also show the effectiveness of detected natural language like queries in extrinsic evaluations on domain detection and slot filling tasks.", "title": "IsNL? a discriminative approach to detect natural language like queries for conversational understanding"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/cheng13_interspeech.html", "abstract": "In addition to measuring job candidates spoken English proficiency, quantifying the degree of accentedness may help companies assign employees to appropriate job categories, or identify employees who could benefit from additional speech training. In this paper, we discuss methods for automatic accent quantification of Indian English speakers. Similar to techniques used in speaker recognition, we used Gaussian mixture models (GMMs) for the modeling of accent spectral characteristics in different groups of subjects. Computationally, we verified that certain consonants in Indian English have more discriminative power than others in quantifying an Indian accent. As a result, we propose the idea of using GMMs to model only certain phonemes with high predictive power. By combining features from GMMs with others, we achieved a human-machine correlation coefficient of 0.84 at the participant level. The results validate the use of new proposed methods to quantify accents automatically.", "title": "Automatic accent quantification of indian speakers of English"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/tur13_interspeech.html", "abstract": "A challenge in large vocabulary spoken language understanding (SLU) is robustness to automatic speech recognition (ASR) errors. The state of the art approaches for semantic parsing rely on using discriminative sequence classification methods, such as conditional random fields (CRFs). Most dialog systems employ a cascaded approach where the best hypotheses from the ASR system are fed into the following SLU system. In our previous work, we have proposed the use of lattices towards joint recognition and parsing. In this paper, extending this idea, we propose to exploit word confusion networks (WCNs), compiled from ASR lattices for both CRF modeling and decoding. WCNs provide a compact representation of multiple aligned ASR hypotheses, without compromising recognition accuracy. For slot filling, we show significant semantic parsing performance improvements using WCNs compared to ASR 1-best output, approximating the oracle path performance.", "title": "Semantic parsing using word confusion networks with conditional random fields"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/strombergsson13_interspeech.html", "abstract": "Questions and answers play an important role in spoken dialogue systems as well as in human-human interaction. A critical concern when responding to a question is the timing of the response. While human response times depend on a wide set of features, dialogue systems generally respond as soon as they can, that is, when the end of the question has been detected and the response is ready to be deployed. This paper presents an analysis of how different semantic and pragmatic features affect the response times to questions in two different data sets of spontaneous human-human dialogues: the Swedish Spontal Corpus and the US English Switchboard corpus. Our analysis shows that contextual features such as question type, response type, and conversation topic influence human response times. Based on these results, we propose that more sophisticated response timing can be achieved in spoken dialogue systems by using these features to automatically and deliberately target system response timing.", "title": "Timing responses to questions in dialogue"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/karafiat13_interspeech.html", "abstract": "This paper presents our work on speech recognition of Cantonese spontaneous telephone conversations. The key-points include feature extraction by 6-layer Stacked Bottle-Neck neural network and using fundamental frequency information at its input. We have also investigated into robustness of SBN training (silence, normalization) and shown an efficient combination with PLP using Region-Dependent transforms. A combination of RDT with another popular adaptation technique (SAT) was shown beneficial. The results are reported on BABEL Cantonese data.", "title": "BUT BABEL system for spontaneous Cantonese"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/norouzian13_interspeech.html", "abstract": "In this paper, the application of semi-supervised manifold learning techniques to the task of verifying hypothesized occurrences of spoken terms is investigated. These techniques are applied in a two stage spoken term detection framework where ASR lattices are first generated using a large vocabulary ASR system and hypothesized occurrences of spoken query terms in the lattices are verified in a second stage. The verification process is performed using a fixed dimensional feature representation derived from each hypothesized term occurrence. Two semi-supervised approaches namely, manifold regularized least squares (RLS) classification and spectral clustering, are investigated for distinguishing correct hypotheses from false alarms. It is shown that, exploiting unlabeled data in addition to labeled data using semi-supervised approaches, significantly improves the verification performance compared to the case where only the labeled data is used. This improvement in performance increases as the ratio of unlabeled to labeled data augments. It is also shown that, when training data is very limited, a comparable verification performance can be gained by exploiting only the acoustic similarity between the test samples using the spectral clustering approach.", "title": "Semi-supervised manifold learning approaches for spoken term verification"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/li13g_interspeech.html", "abstract": "To train a code switching language model for mixed language speech recognition, we propose to assign weights to the sentence pairs in the parallel text data. The code switching language model which is composed of the code switching boundary prediction model, code switching translation model and reconstruction model is incorporated with a language for mixed language speech recognition. The code switching translation model which is trained using selected subsets of the sentence pairs in the parallel text data allows the decoder to make the decision whether a phrase is in the matrix language or in the embedded language. Moreover, we propose a weighting procedure while training the code switching translation model. We evaluate our methods on Mandarin-English code switching lecture speech and lunch conversations. Our proposed method reduces word error rate by a statistically significant 1.74% on the lecture speech, and by 1.29% on the lunch conversation over the conventional interpolated language model.", "title": "Language modeling for mixed language speech recognition using weighted phrase extraction"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/strombergsson13b_interspeech.html", "abstract": "The severity of speech impairments can be measured in different ways; whereas some metrics focus on quantifying the specific speech deviations, other focus on the functional effects of the speech impairment, e.g. by rating intelligibility. This report describes the application of a previously untested method to the domain of deviant child speech; an audience response system-based method where listeners' responses are continuously registered during playback of speech stimuli. 20 adult listeners were given the task of clicking a button whenever they perceived something unintelligible or deviant during playback of child speech stimuli. The untrained listeners' responses were compared to clinical evaluations of the same speech samples, revealing a strong correlation between the two types of measures. Furthermore, patterns of how listeners' different experiences influence their clicking responses were explored. Qualitative analysis linking listener clicks to triggering events in the speech samples demonstrates the potential of the click method as an instrument for identification of features in children's speech that are most detrimental to intelligibility . insights that may have important implications for the selection of speech targets in clinical intervention.", "title": "Correlates to intelligibility in deviant child speech \u2014 comparing clinical evaluations to audience response system-based evaluations by untrained listeners"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/womack13b_interspeech.html", "abstract": "This study explores spoken medical narratives in which dermatologists were shown images of dermatological conditions and asked to explain their reasoning process while working toward a diagnosis. This corpus has been annotated by a domain expert for informationrich conceptual units of thought, providing opportunity for analysis of the link between diagnostic reasoning steps and speech features. We explore these annotations in regards to speech disfluencies, prosody, and type-token ratios, with the finding that speech tagged within thought units is unique from non-tagged speech in each of these aspects. Additionally, we discuss pattern differences in temporal thought unit distribution based on diagnostic correctness.", "title": "Using linguistic analysis to characterize conceptual units of thought in spoken medical narratives"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/cutugno13_interspeech.html", "abstract": "Effective human-robot communication is one of the main concerns in modern robotics. Involved systems should be very robust, allowing little chance for misunderstanding users commands. The main purpose of this work is to develop a general framework for multimodal human-robot communication, which allows users to interact with robots using speech and gestures, integrated into unique commands. The produced architecture relies on the definition of different modules separately analysing the low level inputs and presenting a further fusion module able to extract semantics from these multiple channels. In this paper, we introduce our general approach and provide a case study where gesture and speech modalities are combined.", "title": "Interacting with robots via speech and gestures, an integrated architecture"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/hatmi13_interspeech.html", "abstract": "Named Entity Recognition (NER) from speech usually involves two sequential steps: transcribing the speech using Automatic Speech Recognition (ASR) and annotating the outputs of the ASR process using NER techniques. Recognizing named entities in automatic transcripts is difficult due to the presence of transcription errors and the absence of some important NER clues, such as capitalization and punctuation. In this paper, we describe a methodology for speech NER which consists of incorporating NER into the ASR process so that the ASR system generates transcripts annotated with named entities. The combination is achieved by adapting ASR language models and pre-annotating the pronunciation dictionary. We evaluate this method on ESTER 2 corpus, and show significant improvements over traditional approaches.", "title": "Incorporating named entity recognition into the speech transcription process"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/ohno13_interspeech.html", "abstract": "The amount of Web-based multimedia data that includes speech is increasing rapidly. Spoken term detection (STD) enables rapid identification of desired-information candidates from a large quantity of speech data. Considering that these STD candidates ultimately have to be checked one at a time by the user, a long list of candidates is not desirable. However, setting an appropriate cutoff threshold for a particular STD request beforehand is not easy. In this work, we propose a novel indexing and search method for STD that requires no cutoff threshold for detection but can output detection results in increasing order of their dynamic time warping (DTW) distances for a given query term. Our experimental evaluation showed that, whereas using the strict algorithm for our method gave detection results that were exactly in increasing order of their DTW distances, its relaxed variants were able to execute much faster at the cost of only a slight loss in the exact ordering.", "title": "DTW-distance-ordered spoken term detection"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/jung13_interspeech.html", "abstract": "The ability to accurately judge the similarity between sentences is important for dialog system development in various areas such as utterance verification, context reasoning, utterance clustering. However, standard text similarity measures fail when directly applied to dialog sentences which are usually very short and have many ungrammatical omissions and inversions. This paper presents a method for sentence similarity refining method using discourse similarity of dialog sentences. First, we propose a novel discourse similarity based on the dialog act taxonomy. Given discourse similarity, we then present a novel way of rescoring original sentence score by explicitly adding discourse score to it. Experiments on test data sets demonstrate that the proposed measure significantly outperforms traditional similarity scoring measures.", "title": "Refining sentence similarity with discourse information in dialog system"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/nakatani13_interspeech.html", "abstract": "This paper presents a fully automatic word error correction on a confusion network that makes use of long contextual information. However, a problem with long contextual information is that improvement of the recognition accuracy is minimal because of the word errors surrounding words. In this paper, recognition errors are first reduced by error correction using N-gram features. After that, the long-distance context scores are applied to the correction of the residual recognition errors.", "title": "Two-step correction of speech recognition errors based on n-gram and long contextual information"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/negi13_interspeech.html", "abstract": "In recent years there has been a growing interest in inferring social relations amongst actors in a video using audiovisual features, co-appearance features or both. The discovered relations between actors have been used for identifying leading roles, detecting rival communities in a movie plot etc. In this paper we propose an unsupervised method which uses the video's transcript and closed caption information for discovering actor communities (group of actors or characters in a film that share a common perspective/viewpoint on an issue) from videos. The method proposed groups together actors using a topic model based approach, which jointly models actoractor interaction (two actors interact when they share the same scene) and the topics associated with their conversations/dialogs. This joint modeling approach shows encouraging results compared to existing methods.", "title": "Inferring actor communities from videos"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/bost13_interspeech.html", "abstract": "This paper deals with the automatic analysis of conversations between a customer and an agent in a call centre of a customer care service. The purpose of the analysis is to hypothesize themes about problems and complaints discussed in the conversation. Themes are defined by the application documentation topics. A conversation may contain mentions that are irrelevant for the application purpose and multiple themes whose mentions may be interleaved portions of a conversation that cannot be well defined. Two methods are proposed for multiple theme hypothesization. One of them is based on a cosine similarity measure using a bag of features extracted from the entire conversation. The other method introduces the concept of thematic density distributed around specific word positions in a conversation. In addition to automatically selected words, word bigrams with possible gaps between successive words are also considered and selected. Experimental results show that the results obtained with the proposed methods outperform the results obtained with support vector machines on the same data. Furthermore, using the theme skeleton of a conversation from which thematic densities are derived, it will be possible to extract components of an automatic conversation report to be used for improving the service performance.", "title": "Multiple topic identification in telephone conversations"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/chen13i_interspeech.html", "abstract": "Out-of-vocabulary named entities (OOV NEs) are always misrecognized by fixed-vocabulary automatic speech recognition (ASR) systems. This has a negative impact on downstream applications such as language understanding and machine translation (MT). Automatic detection of OOV NEs in ASR hypotheses can help mitigate this problem by triggering the use of alternative approaches to acquire and process these NEs. State-of-the-art OOV NE detection typically involves tagging ASR-hypothesized words using a sequence model, such as conditional random fields (CRF), in conjunction with a variety of contextual and ASR-derived features. In this paper, we propose a novel variable-span tagging approach for detecting OOV NEs. Instead of tagging individual words in ASR hypotheses, we directly tag longer spans of consecutive words. The proposed approach outperforms a state-of-the-art CRF tagger on two distinct held-out test sets with different OOV NE distributions. On a 5.1Kword test set rich in OOV NEs, our method achieves 56.1% detection rate at 10% false alarm rate (vs. 52.1% for the CRF detector). On a 39.4K-word test set with a natural distribution of OOV NEs, we obtain 73.0% detection rate at 10% false alarm rate (vs. 69.5% for the CRF detector). In all cases, OOV NEs are completely unobserved in our training data.", "title": "Variable-Span out-of-vocabulary named entity detection"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/kun13_interspeech.html", "abstract": "In a driving simulator study, we explore the feasibility of using pupil diameter to estimate how the cognitive load of the driver changes during a spoken dialogue with a remote conversant. We confirm that it is feasible to use pupil diameter to differentiate between parts of the dialogue that increase the cognitive load of the driver, and those that decrease it. Our long term goal is to build a spoken dialogue system that can adapt its behavior when the driver is under high cognitive load, whether from the driving task or the dialogue task.", "title": "On the feasibility of using pupil diameter to estimate cognitive load changes for in-vehicle spoken dialogues"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/mesnil13_interspeech.html", "abstract": "One of the key problems in spoken language understanding (SLU) is the task of slot filling. In light of the recent success of applying deep neural network technologies in domain detection and intent identification, we carried out an in-depth investigation on the use of recurrent neural networks for the more difficult task of slot filling involving sequence discrimination. In this work, we implemented and compared several important recurrent-neural-network architectures, including the Elman-type and Jordan-type recurrent networks and their variants. To make the results easy to reproduce and compare, we implemented these networks on the common Theano neural network toolkit, and evaluated them on the ATIS benchmark. We also compared our results to a conditional random fields (CRF) baseline. Our results show that on this task, both types of recurrent networks outperform the CRF baseline substantially, and a bi-directional Jordan-type network that takes into account both past and future dependencies among slots works best, outperforming a CRF-based baseline by 14% in relative error reduction.", "title": "Investigation of recurrent-neural-network architectures and learning methods for spoken language understanding"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/liu13g_interspeech.html", "abstract": "Natural language understanding (NLU) systems for speech applications require large quantities of annotated data. We investigate the use of a domain-independent machine-translation-based paraphrase system to improve performance without incurring the costs of obtaining additional annotated data in an NLU system. Our experimental system incorporates Support Vector Machine (SVM) domain and intent models to detect intents, and a conditional random field (CRF) model to identify semantic slots in a given query. Two approaches are compared. In the first, we retrain models using generated paraphrases to augment the original training set. In the second, we use paraphrases as supplementary features of the original queries in the SVM and CRF models. Experiments in four domains indicate that incorporating paraphrase yields useful performance gains, and that the feature-based approach provides more stable performance than synthetic augmentation of training data.", "title": "Paraphrase features to improve natural language understanding"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/hakkanitur13_interspeech.html", "abstract": "State-of-the art spoken language understanding models that automatically capture user intents in human to machine dialogs are trained with manually annotated data, which is cumbersome and time-consuming to prepare. For bootstrapping the learning algorithm that detects relations in natural language queries to a conversational system, one can rely on publicly available knowledge graphs, such as Freebase, and mine corresponding data from the web. In this paper, we present an unsupervised approach to discover new user intents using a novel Bayesian hierarchical graphical model. Our model employs search query click logs to enrich the information extracted from bootstrapped models. We use the clicked URLs as implicit supervision and extend the knowledge graph based on the relational information discovered from this model. The posteriors from the graphical model relate the newly discovered intents with the search queries. These queries are then used as additional training examples to complement the bootstrapped relation detection models. The experimental results demonstrate the effectiveness of this approach, showing extended coverage to new intents without impacting the known intents.", "title": "A weakly-supervised approach for discovering new user intents from search query logs"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/xu13_interspeech.html", "abstract": "Multi-intent natural language sentence classification aims at identifying multiple user goals in a single natural language sentence (e.g., \"find Beyonce's movie and music\" -> find_movie, find music). The main motivation of this work is to exploit the shared intents across different intent combinations rather than treating the combination as an atomic label. We propose to achieve this by (1) adding class features, and (2) adding hidden variables to identify segments belonging to each intent. Experimental results demonstrate significant gains in classification accuracy over the baseline methods across a number of training conditions (3%.8% absolute on multiintent sentences, 2%.3% absolute on single intent sentences).", "title": "Exploiting shared information for multi-intent natural language sentence classification"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/skowronek13_interspeech.html", "abstract": "The present paper addresses multiparty telephone conferences with asymmetric quality degradations. We propose a systematic method that allows to investigate how individual technical degradations can lead to the perception of quality impairments by the different interlocutors in a conference call. By conducting this analysis for a number of degradations, a detailed picture on the complexity of assessing asymmetric conditions is drawn, which in turn verifies the need for such strictly systematic assessment approaches.", "title": "Quality assessment of asymmetric multiparty telephone conferences: a systematic method from technical degradations to perceived impairments"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/imoto13_interspeech.html", "abstract": "We propose a method for estimating user activities by analyzing long-term (more than several seconds) acoustic signals represented as acoustic event temporal sequences. The proposed method is based on a probabilistic generative model of an acoustic event temporal sequence that is associated with user activities (e.g. \"cooking\") and subordinate categories of user activities (e.g. \"fry ingredients\" or \"plate food\") in which each user activity is represented as a probability distribution over unsupervised subordinate categories of user activities called activity-topics, and each activity-topic is represented as a probability distribution over acoustic events. This probabilistic generative model can express user activities that have more than one subordinate category of the user activities, which a model that takes into account only user activities cannot express adequately. User activity estimation with this model is achieved using a two-step process: frame-by-frame acoustic event estimation to output an acoustic event temporal sequence and user activity estimation with the proposed probabilistic generative model. Activity estimation experiments with real-life sounds indicated that the proposed method improved user activity estimation accuracy and stability of \"unseen\" acoustic event temporal sequences. In addition, the experiment showed that the proposed method could extract correct subordinate categories of user activities.", "title": "User activity estimation method based on probabilistic generative model of acoustic event sequence with user activity and its subordinate categories"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/kano13_interspeech.html", "abstract": "In previous work, we proposed a model for speech-to-speech translation that is sensitive to paralinguistic information such as duration and power of spoken words. This model uses linear regression to map source acoustic features to target acoustic features directly and in continuous space. However, while the model is effective, it faces scalability issues as a single model must be trained for every word, which makes it difficult to generalize to words for which we do not have parallel speech. In this work we first demonstrate that simply training a linear regression model on all words is not sufficient to express paralinguistic translation. We next describe a neural network model that has sufficient expressive power to perform paralinguistic translation with a single model. We evaluate the proposed method on a digit translation task and show that we achieve similar results with a single neural network-based model as previous work did using word-dependent models.", "title": "Generalizing continuous-space translation of paralinguistic information"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/ohgushi13_interspeech.html", "abstract": "Speech translation (ST) systems consist of three major components: automatic speech recognition (ASR), machine translation (MT), and speech synthesis (SS). In general the ASR system is tuned independently to minimize word error rate (WER), but previous research has shown that ASR and MT can be jointly optimized to improve translation quality. Independently, many techniques have recently been proposed for the optimization of MT, such as empirical comparison of joint optimization using minimum error rate training (MERT), pairwise ranking optimization (PRO) and the batch margin infused relaxed algorithm (MIRA). The first contribution of this paper is an empirical comparison of these techniques in the context of joint optimization. As the last two methods are able to use sparse features, we also introduce lexicalized features using the frequencies of recognized words. In addition, motivated by initial results, we propose a hybrid optimization method that changes the translation evaluation measure depending on the features to be optimized. Experimental results for the best combination of algorithm and features show a gain of 1.3 BLEU points at 27% of the computational cost of previous joint optimization methods.", "title": "An empirical comparison of joint optimization techniques for speech translation"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/ostendorf13_interspeech.html", "abstract": "This paper proposes a new method for automatically detecting disfluencies in spontaneous speech . specifically, self-corrections . that explicitly models repetitions vs. other disfluencies. We show that, in a corpus of Supreme Court oral arguments, repetition disfluencies can be longer and more stutter-like than the short repetitions observed in the Switchboard corpus and suggest that they can be better represented with a flat structure that covers the full sequence. Since these disfluencies are relatively easy to detect, weakly supervised training is an effective way to minimize labeling costs. By explicitly modeling these, we improve general disfluency detection within and across domains, and we provide a richer transcript.", "title": "A sequential repetition model for improved disfluency detection"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/medeiros13_interspeech.html", "abstract": "This paper focuses on the identification of disfluent sequences and their distinct structural regions, based on acoustic and prosodic features. Reported experiments are based on a corpus of university lectures in European Portuguese, with roughly 32h, and a relatively high percentage of disfluencies (7.6%). The set of features automatically extracted from the corpus proved to be discriminant of the regions contained in the production of a disfluency. Several machine learning methods have been applied, but the best results were achieved using Classification and Regression Trees (CART). The set of features which was most informative for cross-region identification encompasses word duration ratios, word confidence score, silent ratios, and pitch and energy slopes. Features such as the number of phones and syllables per word proved to be more useful for the identification of the interregnum, whereas energy slopes were most suited for identifying the interruption point.", "title": "Disfluency detection based on prosodic features for university lectures"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/meyer13_interspeech.html", "abstract": "The comparison of human speech recognition (HSR) and machine performance allows to learn from the differences between HSR and automatic speech recognition (ASR) and serves as motivation for using auditory-inspired strategies in ASR. The recognition of noisy digit strings from the Aurora 2 framework is one of the most widely used tasks in the ASR community. This paper establishes a baseline with a close-to-optimal classifier, i.e., our auditory system by comparing results from 10 normal-hearing listeners to the Aurora 2 reference system using identical speech material. The baseline ASR system reaches the human performance level only when the signal-to-noise ratio is increased by 10 or 21 dB depending on the training condition. The recognition of 1-digit recordings was found to be considerably better for HSR, indicating that onset detection is an important feature neglected in standard ASR systems. Results of recent studies are considered in the light of these findings to measure how far we have come on the way to human speech recognition performance.", "title": "What's the difference? comparing humans and machines on the Aurora 2 speech recognition task"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/gubian13_interspeech.html", "abstract": "Retrieving information from the ever-increasing amount of unannotated audio and video recordings requires techniques such as unsupervised pattern discovery or query-by-example. In this paper we focus on queries that are specified in the form of an audio snippet containing the desired word or expression excised from the target recordings. The task is to retrieve all-and-only the instances whose match score with the query meet an absolute criterion. For this purpose we introduce a distance measure between two acoustic vectors that can be calibrated in a completely unsupervised manner. The use of that measure also allows the use of a fast matching approach, which makes it possible to skip more than 97% of full-fledged DTW without affecting performance in terms of precision and recall. We demonstrate the effectiveness of the proposals with query-by-example experiments conducted on a read speech corpus for English and a spontaneous speech corpus for Dutch.", "title": "Calibration of distance measures for unsupervised query-by-example"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/castan13_interspeech.html", "abstract": "The amount of multimedia data is increasing every day and there is a growing demand for high-accuracy multimedia retrieval systems that go beyond retrieving simple events (e.g., detecting a sport video), to more specific and hard-to-detect events (e.g., a point in a tennis match). To retrieve these complex events, audio content features play an important role since they provide complementary information to image/video features. In this paper, we propose a novel approach where we employ an HMM-based acoustic concept recognition (ACR) system and convert resulting recognition lattices into acoustic concept indexes to represent multimedia audio content. Lattice indexes are created by extracting posterior-weighted N-gram counts from the ACR lattices and they are used as features in SVM-based classification for multimedia event detection (MED) task. We evaluate the proposed approach on the NIST 2011 TRECVID MED development set, which consists of user-generated videos from the internet. Proposed approach yields an Equal Error Rate (EER) of 31.6% on this acoustically challenging dataset (on a set of 5 video events) outperforming previously proposed supervised and unsupervised approaches on the same dataset (34.5% and 36.9% respectively).", "title": "Indexing multimedia documents with acoustic concept recognition lattices"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/kousidis13b_interspeech.html", "abstract": "This paper presents a collection of tools (and adaptors for existing tools) that we have recently developed, which support acquisition, annotation and analysis of multimodal corpora. For acquisition, an extensible architecture is offered that integrates various sensors, based on existing connectors (e.g. for motion capturing via VICON, or ART) and on connectors we contribute (for motion tracking via Microsoft Kinect as well as eye tracking via Seeingmachines FaceLAB 5). The architecture provides live visualisation of the multimodal data in a unified virtual reality (VR) view (using Fraunhofer Instant Reality) for control during recordings, and enables recording of synchronised streams. For annotation, we provide a connection between the annotation tool ELAN (MPI Nijmegen) and the VR visualisation. For analysis, we provide routines in the programming language Python that read in and manipulate (aggregate, transform, plot, analyse) the sensor data, as well as text annotation formats (Praat TextGrids). Use of this toolset in multimodal studies proved to be efficient and effective, as we discuss. We make the collection available as open source for use by other researchers.", "title": "MINT.tools: tools and adaptors supporting acquisition, annotation and analysis of multimodal corpora"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/favre13_interspeech.html", "abstract": "We propose an alternative evaluation metric to Word Error Rate (WER) for the decision audit task of meeting recordings, which exemplifies how to evaluate speech recognition within a legitimate application context. Using machine learning on an initial seed of human-subject experimental data, our alternative metric handily outperforms WER, which correlates very poorly with human subjects\u0081f success in finding decisions given ASR transcripts with a range of WERs.", "title": "Automatic human utility evaluation of ASR systems: does WER really predict performance?"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/sridhar13_interspeech.html", "abstract": "Real-time speech-to-speech (S2S) translation of lectures and speeches require simultaneous translation with low latency to continually engage the listeners. However, simultaneous speech-tospeech translation systems have been predominantly repurposing translation models that are typically trained for consecutive translation without a motivated attempt to model incrementality. Furthermore, the notion of translation is simplified to translation plus simultaneity. In contrast, human interpreters are able to perform simultaneous interpretation by generating target speech incrementally with very low ear-voice span by using a variety of strategies such as compression (paraphrasing), incremental comprehension, and anticipation through discourse inference and expectation of discourse redundancies. Exploiting and modeling such phenomena can potentially improve automatic real-time translation of speech. As a first step, in this work we identify and present a systematic analysis of phenomena used by human interpreters to perform simultaneous interpretation and elucidate how it can be exploited in a conventional simultaneous translation framework. We perform our study on a corpus of simultaneous interpretation of Parliamentary speeches in English and Spanish. Specifically, we present an empirical analysis of factors such as time constraint, redundancy and inference as evidenced in the simultaneous interpretation corpus.", "title": "Corpus analysis of simultaneous interpretation data for improving real time speech translation"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/cho13_interspeech.html", "abstract": "We present a real-time automatic speech translation system for university lectures that can interpret several lectures in parallel. University lectures are characterized by a multitude of diverse topics and a large amount of technical terms. This poses specific challenges, e.g., a very specific vocabulary and language model are needed. In addition, in order to be able to translate simultaneously, i.e., to interpret the lectures, the components of the systems need special modifications. The output of the system is delivered in the form of real-time subtitles via a web site that can be accessed by the students attending the lecture through mobile phones, tablet computers or laptops. We evaluated the system on our German to English lecture translation task at the Karlsruhe Institute of Technology. The system is now being installed in several lecture halls at KIT and is able to provide the translation to the students in several parallel sessions.", "title": "A real-world system for simultaneous translation of German lectures"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/wu13b_interspeech.html", "abstract": "We attack an inexplicably under-explored language genre of spoken language . lyrics in music . via completely unsupervised induction of an SMT-style stochastic transduction grammar for hip hop lyrics, yielding a fully-automatically learned challengeresponse system that produces rhyming lyrics given an input. Unlike previous efforts, we choose the domain of hip hop lyrics, which is particularly unstructured and noisy. A novel feature of our approach is that it is completely unsupervised and requires no a priori linguistic or phonetic knowledge. In spite of the level of difficulty of the challenge, the model nevertheless produces fluent output as judged by human evaluators, and performs significantly better than widely used phrase-based SMT models upon the same task.", "title": "Freestyle: a challenge-response system for hip hop lyrics via unsupervised induction of stochastic transduction grammars"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/tsiartas13b_interspeech.html", "abstract": "Speech-to-speech (S2S) translation has been of increased interest in the last few years with the research focused mainly on lexical aspects. It has however been widely acknowledged that incorporating other rich information such as expressive prosody contained in speech can enhance the cross-lingual communication experience. Motivated by recent empirical findings showing a positive relation between the transfer of emphasis and the quality of the audio translation, we propose a computational method to derive a set of acoustic cues that can be used in transferring emphasis for the English- Spanish language pair. In particular, we present an iterative algorithm that aims to discover the set of acoustic cue pairs in the two languages that maximize the accurate transfer of emphasis. We find that the relevant acoustic cues can be constructed from a diverse set of features including word/phrase level statistics of spectral, intensity and prosodic cues and can model the acoustic information related to emphasized and neutral words/phrases for the English-Spanish language pair. These features can in turn enable data-driven transformations from source to target language that preserve such rich prosodic information. We demonstrate the efficacy of this approach through experiments on a specially constructed corpus of 1800 English-Spanish words/phrases.", "title": "Toward transfer of acoustic cues of emphasis across languages"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/fujita13_interspeech.html", "abstract": "Conventional speech translation systems wait until the end of the input sentence before starting translation, causing a large delay in the translation process. Methods have been proposed to reduce this delay by dividing the input utterance on pause boundaries, but while these methods have proven useful on speech translation of language pairs with similar word order, they are insensitive to linguistic information and less effective for languages that require more word reordering. In this work, we propose a method that uses lexicalized information to perform translation unit segmentation considering the relationship between the source and target languages. In particular, we use the phrase table and reordering probabilities used in phrase-based translation systems to decide points in the sentence where we can begin translation with less delay. Through an experimental evaluation, we confirmed that the proposed method significantly reduces delay for Japanese-English and French-English translation. We also show that a parameter introduced in our model can adjust the trade-off between simultaneity and accuracy, and that in situations that require a large degree of simultaneity, our system can achieve a delay reduction of 20% compared to pause segmentation with identical accuracy.", "title": "Simple, lexicalized choice of translation timing for simultaneous speech translation"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/haidar13_interspeech.html", "abstract": "We propose a language modeling (LM) approach using interpolated distanced n-grams into a latent Dirichlet language model (LDLM) for speech recognition. The LDLM relaxes the bag-of-words assumption and document topic extraction of latent Dirichlet allocation (LDA). It uses default background n-grams where topic information is extracted from the (n-1) history words through Dirichlet distribution in calculating n-gram probabilities. The model does not capture the long-range information from outside of the n-gram events that can improve the language modeling performance. In this paper, we present an interpolated LDLM (ILDLM) by using different distanced n-grams. Here, the topic information is exploited from (n-1) history words through the Dirichlet distribution using interpolated distanced n-grams. The n-gram probabilities of the model are computed by using the distanced word probabilities for the topics and the interpolated topic information for the histories. In addition, we incorporate a cache-based LM, which models the re-occurring words, through unigram scaling to adapt the LDLM and ILDLM models that model the topical words. We have seen that our approaches give significant reductions in perplexity and word error rate (WER) over the probabilistic latent semantic analysis (PLSA) and LDLM approaches using the Wall Street Journal (WSJ) corpus.", "title": "Fitting long-range information using interpolated distanced n-grams and cache models into a latent dirichlet language model for speech recognition"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/chen13j_interspeech.html", "abstract": "Language modeling (LM), aiming to provide a statistical mechanism to associate quantitative scores to sequences of words, has long been an interesting yet challenging problem in the field of speech and language processing. Although the n-gram model remains the predominant one, a number of disparate LM methods have been developed to complement the n-gram model. Among them, relevance modeling (RM) that explores the relevance information inherent between the search history and an upcoming word has shown preliminary promise for dynamic language model adaptation. This paper continues this general line of research in two significant aspects. First, the so-called \"bag-of-words\" assumption of RM is relaxed by incorporating word proximity evidence into the RM formulation. Second, latent topic information is additionally explored in the hope to further enhance the proximity-based RM framework. A series of experiments conducted on a large vocabulary continuous speech recognition (LVCSR) task seem to demonstrate that the various language models deduced from our framework are very comparable to existing language models.", "title": "Incorporating proximity information for relevance language modeling in speech recognition"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/bayer13_interspeech.html", "abstract": "Language model (LM) adaptation is needed to improve the performance of language-based interaction systems. There are two important issues regarding LM adaptation; the selection of the target data set and the mathematical adaptation model. In the literature, usually statistics are drawn from the target data set (e.g. cache model) to augment (e.g. linearly) background statistical language models, as in the case of automatic speech recognition (ASR). Such models are relatively inexpensive to train, however they do not provide the necessary high-dimensional language context description needed for language-based interaction. Instance-based learning provides high-dimensional description of the lexical, semantic, or dialog context. In this paper, we present an instancebased approach to LM adaptation. We show that by retrieving similar instances from the training data and adapting the model with these instances, we can improve the performance of LMs. We propose two different similarity metrics for instance retrieval, edit distance and n-gram match score. We have performed instancebased adaptation on feed forward neural network LMs (NNLMs) to re-score n-best lists for ASR on the LUNA corpus, which includes conversational speech. We have achieved significant improvements in word error rate (WER) by using instance-based on-line LM adaptation on feed forward NNLMs.", "title": "Instance-based on-line language model adaptation"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/mansikkaniemi13_interspeech.html", "abstract": "Topic adaptation in automatic speech recognition (ASR) refers to the adaptation of language model and vocabulary for improved recognition of in-domain speech data. In this work we implement unsupervised topic adaptation for morph-based ASR, to improve recognition of foreign entity names. Based on first-pass ASR hypothesis similar texts are selected from a collection of articles, which are used to adapt the background language model. Latent semantic indexing is used to index the adaptation corpus and ASR output. We evaluate three different types of index terms and their usefulness in unsupervised LM adaptation: statistical morphs, words, and a combination of morphs and words. Furthermore, we implement vocabulary adaptation alongside unsupervised LM adaptation. Foreign word candidates are selected from the in-domain texts, based on how likely they are topic-related foreign entity names. Adapted pronunciation rules are generated for the selected foreign words. Morpheme adaptation is also performed by restoring over-segmented foreign words back into their base forms, to ensure more reliable pronunciation modeling.", "title": "Unsupervised topic adaptation for morph-based speech recognition"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/schlippe13_interspeech.html", "abstract": "We improve the automatic speech recognition of broadcast news using paradigms from Web 2.0 to obtain time- and topic-relevant text data for language modeling. We elaborate an unsupervised text collection and decoding strategy that includes crawling appropriate texts from RSS Feeds, complementing it with texts from Twitter, language model and vocabulary adaptation, as well as a 2-pass decoding. The word error rates of the tested French broadcast news shows from Europe 1 are reduced by almost 32% relative with an underlying language model from the GlobalPhone project and by almost 4% with an underlying language model from the Quaero project. The tools that we use for the text normalization, the collection of RSS Feeds together with the text on the related websites, a TF-IDF-based topic words extraction, as well as the opportunity for language model interpolation are available in our Rapid Language Adaptation Toolkit.", "title": "Unsupervised language model adaptation for automatic speech recognition of broadcast news using web 2.0"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/wen13_interspeech.html", "abstract": "Speech recognition has become an important feature in smartphones in recent years. Different from traditional automatic speech recognition, the speech recognition on smartphones can take advantage of personalized language models to model the linguistic patterns and wording habits of a particular smartphone owner better. Owing to the popularity of social networks in recent years, personal texts and messages are no longer inaccessible. However, data sparseness is still an unsolved problem. In this paper, we propose a three-step adaptation approach to personalize recurrent neural network language models (RNNLMs). We believe that its capability to model word histories as distributed representations of arbitrary length can help mitigate the data sparseness problem. Furthermore, we also propose additional user-oriented features to empower the RNNLMs with stronger capabilities for personalization. The experiments on a Facebook dataset showed that the proposed method not only drastically reduced the model perplexity in preliminary experiments, but also moderately reduced the word error rate in n-best rescoring tests.", "title": "Recurrent neural network based language model personalization by social network crowdsourcing"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/ayadi13_interspeech.html", "abstract": "There has been an increasing research interest in natural language call routing (NLCR) applications. One of the challenges often encountered in NLCR applications is the difficulty of performing language-dependent tasks such as morphological analysis of words and stop-word filtering. In this paper, we propose a novel NLCR system which does not depend on language-specific information and thus, it can be ported easily to many languages. The proposed system is based on a combination of character c-gram terms and discriminative training using large margin estimation principle. Compared to traditional vector-based NLCR methods, the proposed NLCR system does not need language-dependent processing and achieves around 1% increase in the classification accuracy.", "title": "Language-independent call routing using the large margin estimation principle"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/deoras13_interspeech.html", "abstract": "This paper investigates the use of deep belief networks (DBN) for semantic tagging, a sequence classification task, in spoken language understanding (SLU).We evaluate the performance of the DBN based sequence tagger on the well-studied ATIS task and compare our technique to conditional random fields (CRF), a stateof- the-art classifier for sequence classification. In conjunction with lexical and named entity features, we also use dependency parser based syntactic features and part of speech (POS) tags. Under both noisy conditions (output of automatic speech recognition system) and clean conditions (manual transcriptions), our deep belief network based sequence tagger outperforms the best CRF based system described in [1] by an absolute 2% and 1% F-measure, respectively. Upon carrying out an analysis of cases where CRF and DBN models made different predictions, we observed that when discrete features are projected onto a continuous space during neural network training, the model learns to cluster these features leading to its improved generalization capability, relative to a CRF model, especially in cases where some features are either missing or noisy.", "title": "Deep belief network based semantic taggers for spoken language understanding"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/jabaian13_interspeech.html", "abstract": "Following recent trends in the development of spoken dialogue systems, this paper proposes to improve the performance of the user's intent extraction by means of joint decoding of automatic spoken language transcription and understanding. Gains are expected not only from a better connectivity and mutual awareness of both tasks but also through the use of discriminant models and integration of an error-corrective intermediate mechanism. This latter is based on a statistical post-editing of the speech recognizer word lattice and conditional random fields instantiate the former in our system. An overall absolute reduction of 1.1% is observed by direct application of the proposed techniques on the Media task.", "title": "Error-corrective discriminative joint decoding of automatic spoken language transcription and understanding"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/lai13_interspeech.html", "abstract": "In this paper we investigate how participant involvement and turn-taking features relate to extractive summarization of meeting dialogues. In particular, we examine whether automatically derived measures of group level involvement, like participation equality and turn-taking freedom, can help detect where summarization relevant meeting segments will be. Results show that classification using turn-taking features performed better than the majority class baseline for data from both AMI and ICSI meeting corpora in identifying whether meeting segments contain extractive summary dialogue acts. The feature based approach also provided better recall than using manual ICSI involvement hot spot annotations. Turn-taking features were additionally found to be predictive of the amount of extractive summary content in a segment. In general, we find that summary content decreases with higher participation equality and overlap, while it increases with the number of very short utterances. Differences in results between the AMI and ICSI data sets suggest how group participatory structure can be used to understand what makes meetings easy or difficult to summarize.", "title": "Detecting summarization hot spots in meetings using group level involvement and turn-taking features"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/shiang13_interspeech.html", "abstract": "This paper presents a supervised approach for extractive summarization of spoken document considering utterance clusters in the documents as hidden variables. Utterances in important clusters may be jointly included in the summary, while those in less important clusters may be excluded as a whole. The summaries are therefore selected based on not only the conventional principle of including the most important utterances and minimizing the redundancy but also the hidden cluster structure in the document. The cluster structure of the documents is not known but can be inferred from the documents, and the summaries can be jointly obtained by the structured SVM learned from the training examples. Encouraging results were obtained on a lecture corpus in the preliminary experiments.", "title": "Supervised spoken document summarization based on structured support vector machine with utterance clusters as hidden variables"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/klasinas13_interspeech.html", "abstract": "The development of a speech understanding grammar for spoken dialogue systems can be greatly accelerated by using an in-domain corpus. The development of such a corpus, however, is a slow and expensive process. This paper proposes unsupervised, languageagnostic methods for finding relevant corpora in the web and mining the most informative parts. We show that by utilizing perplexity we are able to increase the in-domainess (precision) of the mined corpora, while by utilizing pragmatic constraints and search engine rank we can increase the generalizability (recall). We show that automatic grammar induction algorithms achieve superior performance on the automatically mined corpora compared to in-domain manually collected corpora for a travel application.", "title": "Web data harvesting for speech understanding grammar induction"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/toutios13_interspeech.html", "abstract": "This paper reports an experiment in synthesizing French connected speech using Maeda's digital simulation of the vocal-tract system. The dynamics of the vocal-tract shape are estimated from the dynamics of Electromagnetic Articulograph (EMA) sensors via Maeda\u0081fs geometrical articulatory model. Time-varying characteristics of the glottis and the velopharyngeal port are set using empirical rules, while the fundamental frequency pattern is copied from the concurrently recorded audio signal. A subjective experiment was performed online to assess the perceived intelligibility and naturalness of the synthesized speech. Results indicate that a properly driven simulation of the vocal tract has the potential to provide a scientifically grounded alternative to the development of text-to-speech synthesis systems.", "title": "Articulatory synthesis of French connected speech from EMA data"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/zhang13d_interspeech.html", "abstract": "We propose a new photo-realistic, voice driven only (i.e. no linguistic info of the voice input is needed) talking head. The core of the new talking head is a context-dependent, multi-layer, Deep Neural Network (DNN), which is discriminatively trained over hundreds of hours, speaker independent speech data. The trained DNN is then used to map acoustic speech input to 9,000 tied \"senone\" states probabilistically. For each photo-realistic talking head, an HMM-based lips motion synthesizer is trained over the speaker's audio/visual training data where states are statistically mapped to the corresponding lips images. In test, for given speech input, DNN predicts the likely states in their posterior probabilities and photo-realistic lips animation is then rendered through the DNN predicted state lattice. The DNN trained on English, speaker independent data has also been tested with other language input, e.g. Mandarin, Spanish, etc. to mimic the lips movements cross-lingually. Subjective experiments show that lip motions thus rendered for 15 non-English languages are highly synchronized with the audio input and photo-realistic to human eyes perceptually.", "title": "A new language independent, photo-realistic talking head driven by voice only"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/wang13g_interspeech.html", "abstract": "In order to render a high quality, versatile 3D talking head, a stable, high frame rate AV data acquisition system is constructed. It can capture 3D position, surface orientation and albedo texture of the talking head video images along with the corresponding speech signals. The system consists of a computer controlled LED lighting subsystem; high speed stereo cameras; a microphone; and a computer for synchronous recording of multi-stream AV data. The visual image data collected is processed through a binocular photometric stereo 3D reconstruction pipeline. The pipeline automatically segments out the face; computes the depth map with binocular stereo; computes the normal map with photometric stereo; generates albedo texture; and finally constructs a high-detailed 3d model with depth and normal cues as constraints. By using the data collected with the built system, we can capture high quality dynamic facial performance, synchronized with the subject's uttered speech.", "title": "Binocular photometric stereo acquisition and reconstruction for 3d talking head applications"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/hueber13b_interspeech.html", "abstract": "The article presents a method for adapting a GMM-based acousticarticulatory inversion model trained on a reference speaker to another speaker. The goal is to estimate the articulatory trajectories in the geometrical space of a reference speaker from the speech audio signal of another speaker. This method is developed in the context of a system of visual biofeedback, aimed at pronunciation training. This system provides a speaker with visual information about his/her own articulation, via a 3D orofacial clone. In previous work, we proposed to use GMM-based voice conversion for speaker adaptation. Acoustic-articulatory mapping was achieved in 2 consecutive steps: 1) converting the spectral trajectories of the target speaker (i.e. the system user) into spectral trajectories of the reference speaker (voice conversion), and 2) estimating the most likely articulatory trajectories of the reference speaker from the converted spectral features (acoustic-articulatory inversion). In this work, we propose to combine these two steps into the same statistical mapping framework, by fusing multiple regressions based on trajectory GMM and maximum likelihood criterion (MLE). The proposed technique is compared to two standard speaker adaptation techniques based respectively on MAP and MLLR.", "title": "Speaker adaptation of an acoustic-articulatory inversion model using cascaded Gaussian mixture regressions"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/benyoussef13_interspeech.html", "abstract": "This study investigates the use of articulatory features for speechdriven head motion synthesis as opposed to prosody features such as F0 and energy that have been mainly used in the literature. In the proposed approach, multi-stream HMMs are trained jointly on the synchronous streams of speech and head motion data. Articulatory features can be regarded as an intermediate parametrisation of speech that are expected to have a close link with head movement. Measured head and articulatory movements acquired by EMA were synchronously recorded with speech. Measured articulatory data was compared to those predicted from speech using an HMM-based inversion mapping system trained in a semi-supervised fashion. Canonical correlation analysis (CCA) on a data set of free speech of 12 people shows that the articulatory features are more correlated with head rotation than prosodic and/or cepstral speech features. It is also shown that the synthesised head motion using articulatory features gave higher correlations with the original head motion than when only prosodic features are used.", "title": "Articulatory features for speech-driven head motion synthesis"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/braude13_interspeech.html", "abstract": "We propose a method for synthesising head motion from speech using a combination of an Input-Output Markov model (IOMM) and Gaussian mixture models trained in a supervised manner. A key difference of this approach compared to others is to model the head motion in each angle as a series of templates of motion rather than trying to recover a frame-wise function. The templates were chosen to reflect natural patterns in the head motion, and states for the IOMM were chosen based on statistics of the templates. This reduces the search space for the trajectories and stops impossible motions such as discontinuities from being possible. For synthesis our system warps the templates to account for the acoustic features and the other angles' warping parameters. We show our system is capable of recovering the statistics of the motion that were chosen for the states. Our system was then compared to a baseline that used a frame-wise mapping that is based on previously published work. A subjective preference test that includes multiple speakers showed participants have a preference for the segment based approach. Both of these systems were trained on storytelling free speech.", "title": "Template-warping based speech driven head motion synthesis"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/larcher13_interspeech.html", "abstract": "ALIZE is an open-source platform for speaker recognition. The ALIZE library implements a low-level statistical engine based on the well-known Gaussian mixture modelling. The toolkit includes a set of high level tools dedicated to speaker recognition based on the latest developments in speaker recognition such as Joint Factor Analysis, Support Vector Machine, i-vector modelling and Probabilistic Linear Discriminant Analysis. Since 2005, the performance of ALIZE has been demonstrated in series of Speaker Recognition Evaluations (SREs) conducted by NIST and has been used by many participants in the last NIST-SRE 2012. This paper presents the latest version of the corpus and performance on the NIST-SRE 2010 extended task.", "title": "ALIZE 3.0 \u2014 open source toolkit for state-of-the-art speaker recognition"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/senoussaoui13_interspeech.html", "abstract": "This paper is a natural extension of our previous work on genderindependent speaker verification systems [1]. In a previous paper, we presented a solution to avoid using gender information in the Probabilistic Linear Discriminant Analysis (PLDA) without any loss of accuracy compared with a gender-dependent base-line implementation. In this work, we propose two solutions to make a speaker verification system based on Cosine similarity independent of speaker gender. Our choice of the Cosine similarity is motivated by the fact that it is proved itself as a second state-of-the art . in parallel with PLDA. of i-vector based speaker verification systems. As measured by Equal Error Rate and min DCF\u0081fs, performance results on the extended telephone list coreext-coreext condition of SRE2010 show no performance decrease in gender-independent Cosine similarity system compared to gender-dependent one. Tests were also successful for gender-independent propositions on a cross gender list as done in [1].", "title": "New cosine similarity scorings to implement gender-independent speaker verification"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/charlet13_interspeech.html", "abstract": "This paper is dedicated to the use of auxiliary information in order to help a classical acoustic-based speaker identification system in the specific context of TV shows. The underlying assumption is that auxiliary information could help (1) to re-rank n-best speaker hypotheses provided by the acoustic-based only speaker identification system, (2) to provide confidence score to refine a rejection process (open-set identification task), and finally, (3) to identify speakers not covered by the speaker dictionary (out-of-dictionary speakers) used by the speaker identification system (full-set verification task); the last point being one of the main issue when dealing with TV shows. In this paper, the auxiliary information is based on person names detected in overlaid text and speech. Experiments conducted in three different datasets issued from the REPERE evaluation campaign have highlighted the interest of the auxiliary information used here, and notably the use of overlaid person names to identify out-of-dictionary speakers, confirming the key assumptions made.", "title": "Improving speaker identification in TV-shows using person name detection in overlaid text and speech"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/knox13_interspeech.html", "abstract": "The focus of this work is to improve the speaker diarization error rate, and more specifically the speaker error rate. We investigate two methods of improving the speaker error rate: modifying the minimum duration constraint and incorporating novel purification techniques. First, in the final step of the speaker diarization algorithm we replace the minimum duration constraint with a simple smoothing algorithm, which averages the log-likelihoods for each of the hypothesized speakers. This method improves the speaker error rate by 12% relative for the MDM condition. Second, we utilize the difference between the largest and second largest log-likelihoods to identify frames which are believed to be correct (or \"pure\"). The difference value is shown be more effective at separating correct frames from incorrect frames than the previously used maximum log-likelihood value. Using only the \"pure\" frames, the cluster models are retrained and segmentation is performed using the above mentioned smoothing technique. The proposed purification and smoothing reduces the speaker error rate over the baseline; however, it is worse than performing the smoothing step alone.", "title": "Exploring methods of improving speaker accuracy for speaker diarization"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/price13_interspeech.html", "abstract": "This study combines a Gaussian mixture model support vector machine (GMM-SVM) system with a nonlinear feature transformation, discriminatively trained to extract speaker specific features from MFCCs. Separation of the speaker information component and non-speaker related information in the speech signal is accomplished using a regularized siamese deep network (RSDN). RSDN learns a hidden representation that well characterizes speaker information by training a subset of the hidden units using pairs of speech segments. MFCC features are input to a trained RSDN and a subset of hidden layer outputs are used as new input features in a GMM-SVM system. We demonstrate the potential of this approach for text-independent speaker verification by applying it to a subset of the NIST SRE 2006 1conv4w-1conv4w task. The hybrid RSDN GMM-SVM system achieves about 5% relative improvement over the baseline GMM-SVM system.", "title": "Combining deep speaker specific representations with GMM-SVM for speaker verification"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/schindler13_interspeech.html", "abstract": "In this study we investigated the speaker discriminating potential of nasals and fricatives. In the past, vowel formants were often used for speaker discriminating purposes, but it seems that they are not the best choice for this task. Our study suggests that the spectral moments of nasals and fricatives may discriminate speakers better than vowel formants. Furthermore, we analysed whether the context of the nasals and fricatives and the speaking style had any effect on their speaker discriminating power. The results indicate that a restriction of the context does not improve the speaker specific features, since the unrestricted context provided the best F-ratios. In the comparison of read and spontaneous speaking style, the latter shows a slightly better performance.", "title": "Using spectral moments as a speaker specific feature in nasals and fricatives"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/laurent13_interspeech.html", "abstract": "This paper presents COSMO, a Bayesian computational model, which is expressive enough to carry out syllable production, perception and imitation tasks using motor, auditory or perceptuomotor information. An imitation algorithm enables to learn the articulatory-to-acoustic mapping and the link between syllables and corresponding articulatory gestures, from acoustic inputs only: synthetic CV syllables generated with a human vocal tract model. We compare purely auditory, purely motor and perceptuo-motor syllable categorization under various noise levels.", "title": "A computational model of perceptuo-motor processing in speech perception: learning to imitate and categorize synthetic CV syllables"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/theodore13_interspeech.html", "abstract": "Research has shown that listeners accommodate talker differences in speech production by adjusting phonetic boundaries in light of a talker's unique productions. However, phonetic categories are not marked solely by boundaries, they also have a graded internal structure in that not all members of a category are considered equally good members. The current work examines whether listeners adjust the internal structure of a phonetic category for individual talkers and whether such adjustments would transfer to a novel word. Listeners participated in training and test phases. During training, two groups of listeners heard a talker produce \"cane\". Voice-onset-time (VOT) was manipulated such that one group heard the talker produce /k/ with relatively short VOTs and the other group heard relatively longer VOTs. During test, listeners were presented with a VOT continuum from \"gain\" to \"cane\" or \"goal\" to \"coal\" and asked to rate each member for goodness as /k/. The results showed that the best exemplar region at test differed between the two groups in line with exposure during training, though this effect was greater for the training word compared to the novel word. These results suggest that internal category structure is dynamically adjusted in order to accommodate talker-specific phonetic variation.", "title": "Talker-specific perceptual processing: influences on internal category structure"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/lecumberri13_interspeech.html", "abstract": "Slips of the ear are of great relevance in the study of how listeners process speech. Our interest in speech misperceptions comes from their value as diagnostic stimuli in evaluating computational models of speech perception in noise. Previous corpora of misperceptions have largely been recorded based on reports of isolated occurrences \u0081ein the wild\u0081f, and consequently are not available for further analysis or replication. The current study involves the elicitation in the laboratory of a corpus of over one thousand robust misperceptions of Spanish words induced by stationary and non-stationary maskers. Misperceptions are analysed into single-phoneme substitutions, insertions and deletions, dual vowel/consonant changes, syllable insertions/deletions, compound reformulations and eccentric cases which defy simple explanation. A novel categorisation scheme based on the interaction between the background and foreground is introduced. The new corpus will permit the evaluation of speech perception models that make detailed predictions of listeners' responses to specific speech-in-noise tokens.", "title": "Elicitation and analysis of a corpus of robust noise-induced word misperceptions in Spanish"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/cutler13_interspeech.html", "abstract": "Vocabularies contain hundreds of thousands of words built from only a handful of phonemes, so that inevitably longer words tend to contain shorter ones. In many languages (but not all) such embedded words occur more often word-initially than word-finally, and this asymmetry, if present, has far-reaching consequences for spoken-word recognition. Prior research had ascribed the asymmetry to suffixing or to effects of stress (in particular, final syllables containing the vowel schwa). Analyses of the standard French vocabulary here reveal an effect of suffixing, as predicted by this account, and further analyses of an artificial variety of French reveal that extensive final schwa has an independent and additive effect in promoting the embedding asymmetry.", "title": "Vocabulary structure and spoken-word recognition: evidence from French reveals the source of embedding asymmetry"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/bagou13_interspeech.html", "abstract": "This artificial language learning (ALL) study investigates how multiple sublexical cues contribute simultaneously to lexical segmentation in French. Previous research [9] has explored segmentation in lexical identification by presenting conflicting sub- lexical cues in the same experiment. We argue that this method of pitting conflicting cues against each other pairwise does not allow the researcher to discover interactions in cue use. We can obtain a more accurate view of speech segmentation by focusing on how French listeners exploit convergent rather than conflicting sources of segmentation information. To this end, acoustic-phonetic, phonological and prosodic cues were manipulated and combined across conditions. The data clearly showed that the combined effects of multiple sublexical cues are more complex than expected based on previous results. Moreover, phonological cues did not supersede prosodic cues as previously shown for English but rather have cumulative and synergistic effects. Two explanations can be given for these differences in the observed results. First, the processes of lexical segmentation in lexical identification [9] and lexical acquisition studied here may not be the same. Second, the hierarchy of segmentation cues is probably not universal, but rather depends upon the specific phonological and prosodic properties of the language studied.", "title": "How do multiple sublexical cues converge in lexical segmentation? an artificial language learning study"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/bosch13_interspeech.html", "abstract": "This paper describes a computational model of speech comprehension that takes the acoustic signal as input and predicts reaction times as observed in an auditory lexical decision task. By doing so, we explore a new generation of end-to-end computational models that are able to simulate the behaviour of human subjects participating in a psycholinguistic experiment. So far, nearly all computational models of speech comprehension do not start from the speech signal itself, but from abstract representations of the speech signal, while the few existing models that do start from the acoustic signal cannot directly model reaction times as obtained in comprehension experiments. The main functional components in our model are the perception stage, which is compatible with the psycholinguistic model Shortlist B and is implemented with techniques from automatic speech recognition, and the decision stage, which is based on the linear ballistic accumulation decision model. We successfully tested our model against data from 20 participants performing a large-scale auditory lexical decision experiment. Analyses show that the model is a good predictor for the average judgment and reaction time for each word.", "title": "Towards an end-to-end computational model of speech comprehension: simulating a lexical decision task"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/athanasopoulos13_interspeech.html", "abstract": "A novel approach is presented for acoustic localization based on Time Delay Estimation (TDE). The proposed method relies on the observation that the spectrum of many real-world ambient noise types sufficiently satisfies the assumption of sinusoid stationarity. By employing noise phase spectrum projection, we approximate the phase information of the equivalent clean source signals received by the microphone array. The phase-modified signals can be effectively used with different methods of conventional TDE. The proposed approach is evaluated with real noise data and for different reverberation times. Experimental results confirm the suitability of the sinusoid stationarity concept and show that the proposed method performs well under both reverberant and noisy acoustic conditions.", "title": "A phase-modified approach for TDE-based acoustic localization"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/xue13b_interspeech.html", "abstract": "Although various DOA estimation methods for human speech have been presented, most of them assume noises received by different microphones are undirected. However, strong directional interferences often also exist in practical scenarios and the performances of existing methods degrade seriously in such case. In this paper, we present a new interference robust DOA estimation method for human speech. Historical information and temporal correlation are taken advantage to deal with the problem. Firstly, utilizing the historical DOA estimates, we perform \"post-beamforming\" in the last frame to suppress the directional interferences. Then exploiting temporal correlation of speech spectra, frequency weights which highlight the effects of speech frequency bins are calculated based on the estimated a priori SNR of enhanced signal. Finally, we propose a new DOA cost function using frequency-weighted spatial correlation matrix to estimate the DOA of speech source. Experimental results show that the proposed method outperforms existing algorithms in reverberant environments with additive white Gaussian noises in the presence of different kinds of interferences.", "title": "Interference robust DOA estimation of human speech by exploiting historical information and temporal correlation"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/harte13_interspeech.html", "abstract": "The songs of birds, like human voices, are important elements of their identity. In ornithology, distinguishing the songs of different populations is as vital as identifying morphological and genetic differences. This paper takes a machine learning approach to the task. Using data gathered in Indonesia, the song from different subspecies of Black-naped Orioles and Olive-backed Sunbirds is examined. The song from different island populations is modelled with MFCCs and Gaussian Mixture Models. Analysing the performance of the classifiers on unseen test data can give an indication of song diversity. The results show that a forensic approach to birdsong analysis, inspired by speech processing, may offer invaluable insights into cryptic species diversity as well as song identification at the subspecies level.", "title": "Identifying new bird species from differences in birdsong"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/nishigaki13_interspeech.html", "abstract": "Degree of \"shout\" singing performance is effectively controlled by combining global spectral shape equalization, peak cancellation in frequency modulation spectrum of F0 trajectory, and synchronized shape-modulation of voice spectral envelope. This \"shout-reduction\" processing is based on a symmetry-based F0 extractor with fine temporal resolution, a temporally stable representation of instantaneous frequency of periodic signals, and the TANDEM-STRAIGHT, a speech analysis, modification and resynthesis framework. The proposed procedure successfully converted an expressive Japanese POP song performance with \"shout\" into a plain performance without damaging original naturalness. Possibility of adding artificial \"shout\" to plain performance is also discussed.", "title": "Controlling \u201cshout\u201d expression in a Japanese POP singing performance: analysis and suppression study"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/mehrabani13b_interspeech.html", "abstract": "In this study, we expand the question of \"what is the intrinsic dimensionality of speech?\" to \"how does the intrinsic dimensionality of speech change from speaking to singing?\". Our focus is on dimensionality of the vowel space regarding spectral features, which is important in acoustic modeling applications. Locality Preserving Projection (LPP) is applied for dimensionality reduction of the spectral feature vectors, and vowel classification performance is studied in low-dimensional subspaces. Performance analysis of singing and speaking vowel classification based on reducing the dimension shows that compared to speaking, a higher number of dimensions is required for effective representation of singing vowels. The results are also explained in terms of differences in the formant spaces of singing and speaking, and vowel classification performance is analyzed based on feature vectors consisting of formant frequencies. The formant analysis results are shown to be consistent with LPP dimensionality analysis, which verifies the inherent dimensionality increase of the vowel space from speaking to singing.", "title": "Dimensionality analysis of singing speech based on locality preserving projections"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/molla13_interspeech.html", "abstract": "This paper presents a novel audio discrimination algorithm using spatial features in time-frequency (TF) space. Three types of audio signals . speech, music without vocal and music with background vocal are taken into consideration for classification. The audio segment is transformed into TF domain yielding the spatial illustration of energy. Non-negative matrix factorization (NMF) is applied to TF space to extract a set of vectors which represents the dominant subspace of spatial energy distribution. The inverse Fourier transform is applied to individual dominant vectors to derive the features for audio discrimination. The classification is performed by using multiclass linear discriminant analysis (mcLDA). The experimental results show that the proposed algorithm is more noise robust and performs better than the recently reported methods.", "title": "Audio classification using dominant spatial patterns in time-frequency space"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/lin13b_interspeech.html", "abstract": "A spectro-temporal modulation based singing voice detection cascaded with a Viterbi based pitch tracking algorithm is proposed in this paper for singing-voice separation from monaural recordings. To detect the singing voice, the spectro-temporal modulation energy related to voice harmonics is extracted using a spectro-temporal modulation analysis framework developed for the Fourier spectrogram. Separation of singing-voice from background music is conducted using a binary mask to group estimated harmonics of singing voice. The proposed system is evaluated using MIR-1K dataset and is shown outperforming three other binary-mask based systems in the vocal/music separation task.", "title": "Spectro-temporal modulation based singing detection combined with pitch-based grouping for singing voice separation"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/ludenachoez13_interspeech.html", "abstract": "In this paper, we propose a new front-end for Acoustic Event Classification tasks (AEC) based on the combination of the temporal feature integration technique called Filter Bank Coefficients (FC) and Non-Negative Matrix Factorization (NMF). FC aims to capture the dynamic structure in the short-term features by means of the summarization of the periodogram of each short-term feature dimension in several frequency bands using a predefined filter bank. As the commonly used filter bank has been devised for other tasks (such as music genre classification), it can be suboptimal for AEC. In order to overcome this drawback, we propose an unsupervised method based on NMF for learning the filters which collect the most relevant temporal information in the short-time features for AEC. The experiments show that the features obtained with this method achieve significant improvements in the classification performance of a Support Vector Machine (SVM) based AEC system in comparison with the baseline FC features.", "title": "NMF-based temporal feature integration for acoustic event classification"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/rawat13_interspeech.html", "abstract": "In this paper we present our audio based system for detecting \"events\" within consumer videos (e.g. You Tube) and report our experiments on the TRECVID Multimedia Event Detection (MED) task and development data. Codebook or bag-of-words models have been widely used in text, visual and audio domains and form the state-of-the-art in MED tasks. The overall effectiveness of these models on such datasets depends critically on the choice of lowlevel features, clustering approach, sampling method, codebook size, weighting schemes and choice of classifier. In this work we empirically evaluate several approaches to model expressive and robust audio codebooks for the task of MED while ensuring compactness. First, we introduce the Large Scale Pooling Features (LSPF) and Stacked Cepstral Features for encoding local temporal information in audio codebooks. Second, we discuss several design decisions for generating and representing expressive audio codebooks and show how they scale to large datasets. Third, we apply text based techniques like Latent Dirichlet Allocation (LDA) to learn acoustic-topics as a means of providing compact representation while maintaining performance. By aggregating these decisions into our model, we obtained 11% relative improvement over our baseline audio systems.", "title": "Robust audio-codebooks for large-scale event detection in consumer videos"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/altaf13_interspeech.html", "abstract": "Human identification using unobtrusive methods is a challenging problem that has many applications in surveillance tasks. In this work we propose a set of biometric features extracted from a footstep audio signal that can be used to identify a person. Instead of using short-time spectral domain, Teager-Kaiser energy operator is employed to transform a time-domain signal into a representational domain where the intrinsic properties of the walking style of each individual become apparent. We show that these features are biometrically significant as they are physical correlates of human gait. The experimental results obtained using a recently recorded publicly available database show prominent results in a human identification task.", "title": "Person identification using biometric markers from footsteps sound"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/mynarski13_interspeech.html", "abstract": "Spatial localization of speech and other natural sounds with rich spectro-temporal structure is a computationally challenging task. It requires extraction of features which are informative about speaker's position and yet invariant to sound level and spectral modulation present in the signal. This paper demonstrates that this can be achieved with Independent Component Analysis (ICA) applied to binaural speech spectrograms. A small subset of learned Independent Components (ICs) captures signal structure imposed by outer ears. A Gaussian Classifier trained on those features, performs accurate localization on the azimuthal plane. The remaining majority of ICs have position invariant distributions, and can be used to reconstruct the spectrogram of the original sound source.", "title": "Learning binaural spectrogram features for azimuthal speaker localization"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/oualil13_interspeech.html", "abstract": "Multiple speaker localization algorithms generally require a binary detector, which performs the source/noise classification of the location estimates. This is mainly due to the unknown time-varying number of sources, and to the presence of noise and reverberation. In this paper, we propose an unsupervised learning approach based on a naive Bayesian classifier. The proposed approach couples two speaker location features, namely, 1) the steered response power introduced at the location estimate, and 2) the corresponding maximum likelihood error, which characterizes the variance of the estimate. The latter is experimentally shown to be highly correlated with the steered power at the location estimate. The proposed method is further extended to control the misclassification rate through the use of a loss function. This approach is general, and can be easily extended to integrate more speaker/speech features. Experiments on the AV16.3 corpus show the effectiveness of the proposed approach.", "title": "An unsupervised Bayesian classifier for multiple speaker detection and localization"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/chakraborty13_interspeech.html", "abstract": "Acoustic scene analysis usually requires several sub-systems working in parallel for carrying out the various required functionalities. Focusing to a more integrated approach, in this paper we present an attempt to jointly recognize and localize several simultaneous acoustic events that take place in a meeting room environment, by developing a computationally efficient technique that employs multiple arbitrarily-located small microphone arrays. Assuming a set of simultaneous sounds, for each array a matrix is computed whose elements are likelihoods along the set of classes and a set of discretized directions of arrival. MAP estimation is used to decide about both the recognized events and the estimated directions. Experimental results with two sources, one of which is speech, and two three-microphone linear arrays are reported. The recognition results compare favorably with the ones obtained by assuming that the positions are known.", "title": "Joint recognition and direction-of-arrival estimation of simultaneous meeting-room acoustic events"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/zhuang13_interspeech.html", "abstract": "High-level multimedia event detection aims to identify videos containing a target event. Recent approaches leveraging audio information for this task fall into two broad categories. The first corresponds to holistic bag-of-words approaches based on framelevel descriptors. These are effective for classification, but hard for humans to interpret. The second corresponds to approaches that build a limited set of mid-level concept detectors trained using large amounts of annotated data. Such approaches do not scale easily for large scale tasks with heterogeneous data. We explore using audio Self Organized Units (SOU) to capture mid-level segmental information in a completely unsupervised fashion, and devise various features based on the SOU decoding process on each video. We train BBN's speech SOU system on unannotated web audio data. A multi-pass adaptive decoder from the BBN speech recognition system is engaged to decode audio data using the HMM-based audio SOUs. We devise various vector representations from the audio SOU lattices and from the constrained maximum likelihood linear regression adaptation matrices at different stages of the decoding. High-level event detection using these representations shows promising results on the benchmark 2011 TRECVID Multimedia Event Detection dataset. Furthermore, the audio SOUs provide potential for human interpretable features.", "title": "Audio self organized units for high-level event detection"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/maddieson13b_interspeech.html", "abstract": "LAPSyD, the Lyon-Albuquerque Phonological Systems Database, is an online phonological database equipped with powerful query, mapping and visualization tools. It stems from the UPSID and WALS databases, enhanced with newly validated data not only covering segmental inventories but also syllable structures, stress and tonal systems. In its current version it covers around 700 languages and it is accessible at http://www.lapsyd.ddl.ish-lyon.cnrs.fr. This paper provides a description of the data structure in LAPSyD and the features of the interface. Brief illustrations of the types of analysis that can be done with this tool are provided, exploiting the ability to cross-reference data on segments, other phonological properties and language location.", "title": "LAPSyd: lyon-albuquerque phonological systems database"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/barbosa13b_interspeech.html", "abstract": "This work presents a reanalysis of the duration compensation issue at the VC level, for which vowel duration is lengthened when replacing a following (long) voiceless consonant with a (short) voiced consonant. Analyses of fifteen Brazilian Portuguese words and pseudo-words reveal that (1) duration compensation is not restricted to consonants differing in voicing only, (2) duration compensation seems to be restricted to narrow-focussed words, and (3) duration compensation applies to the first following consonant only in VCC sequences. These results suggest that duration compensation could be a mechanism to learn how to plan the sequence of vowel onset positions at early stages of language acquisition.", "title": "The duration compensation issue revisited"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/oh13c_interspeech.html", "abstract": "The notion of functional load (FL) quantifies the role a phonological contrast plays in keeping words distinct in a given language. Several studies have emphasized its potential impact on language evolution and acquisition, and FL has repeatedly been mentioned as a useful tool to supplement phonological descriptions for more than seventy years. It is nevertheless still rarely explored and this paper is a contribution to filling this gap. By adopting an information-theory approach and a measure of FL proposed by Hockett (1955), we performed a corpus-based comparison of three non-tonal (English, Japanese, Korean) and two tonal languages (Cantonese and Mandarin). We calculated FLs carried by segmental (vowels and consonants) contrasts and tonal contrasts (in Cantonese and Mandarin). We also evaluated the total FL associated with the vocalic system as a whole, the consonantal system as a whole, and the tonal system (when applicable). Our results suggest that i) the distributions of FLs in a phonological system are very uneven, with only a few prominent contrasts, and ii) the existence of a tonal system does not reduce the importance of vowel and consonantal contrasts, even though tone contrasts are as important as vowel contrasts in Cantonese and Mandarin.", "title": "Cross-language comparison of functional load for vowels, consonants, and tones"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/maekawa13_interspeech.html", "abstract": "In spontaneous speech, seemingly \u0081einter-speaker\u0081f variation is not thoroughly speaker-dependent. It is often the cases that, in addition to the genuine speaker-dependent factors, contextual factors like speaking rate and linguistic environment also have their influence on the variation. In this paper, the variations in the vocal-tract closure articulation of Japanese voiced obstruent consonants are reanalyzed in view of the separation between speaker-dependent and contextual factors. The consonants in the Corpus of Spontaneous Japanese were analyzed by means of logistic regression using the realization of vocal-tract closure as the binary dependent variable and the TACA (time allotted for consonant articulation), a recently proposed articulatory parameter, as the independent variable. Thresholds of TACA for the realization of vocal-tract closure were computed for all speakers. The results revealed clear inverse correlation between the threshold and the mean realization rate of closure. To explain the perturbation in the correlation pattern, influence of skewed distribution of phonemes and pauses were evaluated. The results showed the expected patterns in /z/ and /b/, but not in /d/ and /g/.", "title": "Notes on so-called inter-speaker difference in spontaneous speech: the case of Japanese voiced obstruent"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/carignan13_interspeech.html", "abstract": "Complexity in the acoustics of nasal vowels has long been acknowledged but complexity in their articulation has received less attention. A growing body of research suggests that velopharyngeal (VP) opening is complemented by other articulatory gestures which may enhance or counteract the acoustic outcomes of VP opening. In this paper we consider the role of pharyngeal aperture and lingual position in producing the phonemic distinction between oral and nasal vowels in Northern Metropolitan French. The results of a real-time MRI study of one female speaker confirm earlier findings related to tongue height and retraction. The results also suggest a role for the lower pharynx in centralizing the F1 of nasal vowels. Consideration is also given to the effect of the lowered velum on the acoustic transfer function of the oral tract of nasal vowels. We conclude that these articulations enhance some of the well-known acoustic consequences of VP coupling associated with the production of nasal vowels. This supports and extends the hypothesis that the acoustic characteristics of nasalization can be attained by a family of speech gestures that include, but are not limited to, the opening of the VP port.", "title": "The role of the pharynx and tongue in enhancement of vowel nasalization: a real-time MRI investigation of French nasal vowels"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/renwick13_interspeech.html", "abstract": "Using very large speech corpora, we can study rare but systematic pronunciation patterns in spontaneous speech. Previous studies have established that word-final alveolar consonants in English (/t/, /d/, /n/, /s/ and /z/) vary their place of articulation to match a following word-initial consonant, e.g. \"ran quickly\" -> \"ra[N] quickly\". Assimilation of bilabial or velar nasals, e.g. \"alar[N] clock\" for \"alarm clock\", is unexpected according to linguistic frameworks such as underspecification theory. The existence of systematic counterexamples would challenge that theory, but these might have been previously overlooked because they are infrequent. From the c. 8-million word Audio BNC (http://www.phon.ox.ac.uk/AudioBNC) we extracted over 14,000 tokens of relevant word pairs, to determine whether non-alveolar assimilations occur and with what distribution. Word and segment boundaries were obtained by forced alignment, and F1-F3 formant frequencies were estimated using Praat. Formant frequencies in assimilation environments were compared to non-assimilating controls (e.g. come back vs. come down). We present evidence that velar and bilabial nasals sometimes do assimilate, though possibly less frequently than alveolars.", "title": "Assimilation of word-final nasals to following word-initial place of articulation in UK English"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/chen13k_interspeech.html", "abstract": "This paper presents a new spectral modeling and conversion method for voice conversion. In contrast to the conventional Gaussian mixture model (GMM) based methods, we use restricted Boltzmann machines (RBMs) as probability density models to model the joint distributions of source and target spectral features. The Gaussian distribution in each mixture of GMM is replaced by an RBM, which can better capture the inter-dimensional and interspeaker correlations within the joint spectral features. Spectral conversion is performed by the maximum conditional output probability criterion. Our experimental results show that the similarity and naturalness of the proposed method are significantly improved comparing with the conventional GMM based method.", "title": "Joint spectral distribution modeling using restricted boltzmann machines for voice conversion"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/wu13c_interspeech.html", "abstract": "Although temporal information of speech has been shown to play an important role in perception, most of the voice conversion approaches assume the speech frames are independent of each other, thereby ignoring the temporal information. In this study, we improve conventional unit selection approach by using exemplars which span multiple frames as base units, and also take temporal information constraint into voice conversion by using overlapping frames to generate speech parameters. This approach thus provides more stable concatenation cost and avoids discontinuity problem in conventional unit selection approach. The proposed method also keeps away from the over-smoothing problem in the mainstream joint density Gaussian mixture model (JD-GMM) based conversion method by directly using target speaker's training data for synthesizing the converted speech. Both objective and subjective evaluations indicate that our proposed method outperforms JD-GMM and conventional unit selection methods.", "title": "Exemplar-based unit selection for voice conversion utilizing temporal information"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/hwang13_interspeech.html", "abstract": "In this paper, we propose a discriminative training (DT) method to alleviate the muffled sound effect caused by over smoothing in the Gaussian mixture model (GMM)-based voice conversion (VC). For the conventional GMM-based VC, we often observed a large degree of ambiguities among acoustic classes (generative classes), determined by the source feature vectors for generating the converted feature vectors, causing the \"muffled sound\" effect on the converted voice. The proposed DT method is applied to refine the parameters in the maximum likelihood (ML)-trained joint density GMM (JDGMM) in the training stage to reduce the ambiguities among acoustic classes (generative classes) to alleviate the muffled sound effect. Experimental results demonstrate that the DT method significantly enhances the discriminative power between acoustic classes (generative classes) in the objective evaluation and effectively alleviates the muffled sound effect in the subjective evaluation.", "title": "Alleviating the over-smoothing problem in GMM-based voice conversion with discriminative training"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/tanaka13_interspeech.html", "abstract": "We present a hybrid approach to improving naturalness of electrolaryngeal (EL) speech while minimizing degradation in intelligibility. An electrolarynx is a device that artificially generates excitation sounds to enable laryngectomees to produce EL speech. Although proficient laryngectomees can produce quite intelligible EL speech, it sounds very unnatural due to the mechanical excitation produced by the device. Moreover, the excitation sounds produced by the device often leak outside, adding noise to EL speech. To address these issues, previous work has proposed methods for EL speech enhancement through either noise reduction or voice conversion. The former usually causes no degradation in intelligibility but yields only small improvements in naturalness as the mechanical excitation sounds remain essentially unchanged. On the other hand, the latter method significantly improves naturalness of EL speech using spectral and excitation parameters of natural voices converted from acoustic parameters of EL speech, but it usually causes degradation in intelligibility owing to errors in conversion. We propose a hybrid method using the noise reduction method for enhancing spectral parameters and voice conversion method for predicting excitation parameters. The experimental results demonstrate the proposed method yields significant improvements in naturalness compared with EL speech while keeping intelligibility high enough.", "title": "A hybrid approach to electrolaryngeal speech enhancement based on spectral subtraction and statistical voice conversion"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/moriguchi13_interspeech.html", "abstract": "In this paper, we present a digital signal processor (DSP) implementation of real-time statistical voice conversion (VC) for silent speech enhancement and electrolaryngeal speech enhancement. As a silent speech interface, we focus on non-audible murmur (NAM), which can be used in situations where audible speech is not acceptable. Electrolaryngeal speech is one of the typical types of alaryngeal speech produced by an alternative speaking method for laryngectomees. However, the sound quality of NAM and electrolaryngeal speech suffers from lack of naturalness. VC has proven to be one of the promising approaches to address this problem, and it has been successfully implemented on devices with sufficient computational resources. An implementation on devices that are highly portable but have limited computational resources would greatly contribute to its practical use. In this paper we further implement real-time VC on a DSP. To implement the two speech enhancement systems based on real-time VC, one from NAM to a whispered voice and the other from electrolaryngeal speech to a natural voice, we propose several methods for reducing computational cost while preserving conversion accuracy. We conduct experimental evaluations and show that real-time VC is capable of running on a DSP with little degradation.", "title": "A digital signal processor implementation of silent/electrolaryngeal speech enhancement based on real-time statistical voice conversion"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/aryal13_interspeech.html", "abstract": "We present a voice morphing strategy that can be used to generate a continuum of accent transformations between a foreign speaker and a native speaker. The approach performs a cepstral decomposition of speech into spectral slope and spectral detail. Accent conversions are then generated by combining the spectral slope of the foreign speaker with a morph of the spectral detail of the native speaker. Spectral morphing is achieved by representing the spectral detail through pulse density modulation and averaging pulses in a pair-wise fashion. The technique is validated on parallel recordings from two ARCTIC speakers using both objective and subjective measures of acoustic quality, speaker identity and foreign accent.", "title": "Foreign accent conversion through voice morphing"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/audhkhasi13_interspeech.html", "abstract": "Diversity is crucial to reducing the word error rate (WER) when fusing multiple automatic speech recognition (ASR) systems. We present an empirical analysis linking diversity and fusion performance. We transcribed speech from the first 2012 US Presidential debate using multiple ASR systems trained with the Kaldi toolkit. We used the N-best ROVER algorithm to perform hypothesis fusion and measured N-best diversity by the average pairwise WER. We make three key observations. We first note that the WER of the fused hypothesis decreases significantly with increasing diversity of the N-best list. This decrease is greater than the decrease inWER of the oracle hypothesis in the list. N-best lists from systems trained on different data sets are the most diverse and give the lowest WER upon fusion. We then observe that the benefit of diversity depends on the choice of the fusion scheme. We show that confidence-weighted ROVER is able to better exploit diversity than unweighted ROVER and gives lower WERs. We finally explain the above observations by a simple linear relation linking diversity to the ROVER WER. This relation depends on the fusion scheme and also reveals the tradeoff between diversity and average WER of hypotheses in the N-best list.", "title": "Empirical link between hypothesis diversity and fusion performance in an ensemble of automatic speech recognition systems"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/bell13_interspeech.html", "abstract": "This paper presents a new system for automatic transcription of lectures. The system combines a number of novel features, including deep neural network acoustic models using multi-level adaptive networks to incorporate out-of-domain information, and factored recurrent neural network language models. We demonstrate that the system achieves large improvements on the TED lecture transcription task from the 2012 IWSLT evaluation . our results are currently the best reported on this task, showing an relative WER reduction of more than 16% compared to the closest competing system from the evaluation.", "title": "A lecture transcription system combining neural network acoustic and language models"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/soltau13_interspeech.html", "abstract": "We present a comparison of acoustic modeling techniques for the DARPA RATS program in the context of spoken term detection (STD) on speech data with severe channel distortions. Our main findings are that both Multi-Layer Perceptrons (MLPs) and Convolutional Neural Networks (CNNs) outperform Gaussian Mixture Models (GMMs) on a very difficult LVCSR task. We discuss pretraining, feature sets and training procedures, as well as weight sharing and shift invariance to increase robustness against channel distortions. We obtained about 20% error rate reduction over our state-of-the-art GMM system. Additionally, we found that CNNs work very well for spoken term detection, as a result of better lattice oracle rates compared to GMMs and MLPs.", "title": "Neural network acoustic models for the DARPA RATS program"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/ueffing13_interspeech.html", "abstract": "This paper presents improved models for the automatic prediction of punctuation marks in written or spoken text. Various kinds of textual features are combined using Conditional Random Fields. These features include language model scores, token n-grams, sentence length, and syntactic information extracted from parse trees. The resulting models are evaluated on several different tasks, ranging from formal newspaper text to informal, dictated messages and documents, and from written text to spoken text. The newly developed models outperform a hidden-event language model by up to 26% relative in F-score. Evaluation of punctuation prediction on erroneous ASR output as well as evaluation against multiple references is not straightforward. We propose modifications of existing evaluation methods to handle these cases.", "title": "Improved models for automatic punctuation prediction for spoken and written text"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/roy13_interspeech.html", "abstract": "This paper reports on a speech-to-text (STT) transcription system for Hungarian broadcast audio developed for the 2012 Quaero evaluations. For this evaluation, no manually transcribed audio data were provided for model training, however a small amount of development data were provided to assess system performance. As a consequence, the acoustic models were developed in an unsupervised manner, with the only supervision provided indirectly by the language model. The language models were trained on texts downloaded from various websites, also without any speech transcripts. This contrasts with other STT systems for Hungarian broadcast audio which use at least 10 to 50 hours of manually transcribed data for acoustic training, and typically include speech transcripts in the language models. Based on mixed results previously reported applying morph-based approaches to agglutinative languages such as Hungarian, word-based language models were used. The initial Word Error Rate (WER) of the system using context-independent seed models from other languages of 59.8% on the 3h development corpus was reduced to 25.0% after successive training iterations and system refinement. The same system obtained a WER of 23.3% on the independent Quaero 2012 evaluation corpus (a mix of broadcast news and broadcast conversation data). These results compare well with previously reported systems on similar data. Various issues affecting system performance are discussed, such as amount of training data, the acoustic features and choice of text sources for language model training.", "title": "Some issues affecting the transcription of Hungarian broadcast audio"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/golik13b_interspeech.html", "abstract": "In this paper we describe the RWTH automatic speech recognition system for Slovenian developed within the transLectures project. The project aims at supporting the transcription and translation of video lectures freely available on the web. Difficulties arise on all levels of modeling: Slovenian is a morphologically rich language with a high level of inflection (pronunciation model), and a large variety of dialects and recording conditions brings uncertainty into the audio signal (acoustic model). Moreover, the video lectures cover a wide spectrum of topics with a high share of spontaneous speech and technical terms (language model). These issues require application of robust and adaptive methods. Besides the system description, this study mainly focuses on robust acoustic modeling. Building acoustic models from various resources, we also compare the influence of speaker adaptation to different neural network based acoustic features. Systematic application of these methods allows us to reduce the word error rate on the evaluation corpus from 59.2% to 43.4%. We also give a motivation for Slovenian open vocabulary recognition and perform some first steps.", "title": "Development of the RWTH transcription system for slovenian"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/kanda13_interspeech.html", "abstract": "This paper introduces a delta cepstrum normalization (DCN) technique for speaker verification under noisy conditions. Cepstral feature normalization techniques are widely used to mitigate spectral variations caused by various types of noise; however, little attention has been paid to normalizing delta features. A DCN technique that normalizes not only base features but also delta-features was recently proposed and showed high robustness in speech recognition and language identification. We introduce here DCN for a stateof- the-art speaker verification system that uses iVectors and probabilistic linear discriminant analysis. It is not obvious whether DCN is effective against speaker verification because DCN strongly transforms cepstral features and has possibility to distort the speakerspecific properties. We compared DCN with cepstral mean normalization (CMN), mean variance normalization (MVN), and histogram equalization (HEQ) using a NIST 2008 SRE dataset with various noise settings, and found that DCN is very effective even for speaker verification. DCN was especially effective under noisy conditions and achieved a maximum 18.5% relative error reduction compared to other competing methods. Combining verification scores from various feature normalization methods further improved the accuracy, and it achieved 9.1% and 16.4% relative error reduction under clean and noisy conditions, respectively.", "title": "Noise robust speaker verification with delta cepstrum normalization"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/vandyke13_interspeech.html", "abstract": "This paper presents a new method of score post-processing which utilises previously hidden relationships among client models and test probes that are found within the scores produced by an automatic speaker recognition system. We suggest the name r-Norm (for Regression Normalisation) for the method, which can be viewed as both a score normalisation process and as a novel and improved modelling technique of inter-speaker variability. The key component of the method lies in learning a regression model between development data scores and an \u0081eideal\u0081f score matrix, which can either be derived from clean data or created synthetically. To generate scores for experimental validation of the proposed idea we perform a classic GMM-UBM experiment employing mel-cepstral features on the 1sp-female task of the NIST 2003 SRE corpus. Comparisons of the r-Norm results are made with standard score postprocessing/ normalisation methods t-Norm and z-Norm. The r - Norm method is shown to perform very strongly, improving the EER from 18.5% to 7.01%, significantly outperforming both z-Norm and t-Norm in this case. The baseline system performance was deemed acceptable for the aims of this experiment, which were focused on evaluating and comparing the performance of the proposed r-Norm idea.", "title": "R-norm: improving inter-speaker variability modelling at the score level via regression score normalisation"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/kinnunen13_interspeech.html", "abstract": "Accuracy of speaker verification is high under controlled conditions but falls off rapidly in the presence of interfering sounds. This is because spectral features, such as Mel-frequency cepstral coefficients (MFCCs), are sensitive to additive noise. MFCCs are a particular realization of warped-frequency representation with lowfrequency focus. But there are several alternative, potentially more robust, warped-frequency representations. We provide an experimental comparison of five warped-frequency features. They use exactly the same frequency warping function, the same number of coefficients and postprocessing, but differ in their internal computations. The compared variants are (1) conventional MFCCs from discrete Fourier transform (DFT), followed by Mel-scaled filterbank, (2) MFCCs via direct warping of DFT, followed by linear-scale filterbank, (3) warped linear prediction features, (4) perceptual minimum variance distortionless features and (5) recently proposed sparse Mel-scale histogram features. Experiments carried out on a subset of the SRE 10 corpus using a scaled-down i-vector system indicate that direct DFT warping outperforms conventional MFCCs in most of the cases.", "title": "Frequency warping and robust speaker verification: a comparison of alternative mel-scale representations"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/hasan13_interspeech.html", "abstract": "The Universal Background Model (UBM) is known as a speaker independent Gaussian Mixture Model (GMM) trained on a large speech corpus containing many speakers' recordings in various conditions. When noisy test data is involved, UBM trained on clean data is generally not optimal. Using noisy data for UBM training, however, creates a bias towards the specific development noise samples resulting in degraded speaker recognition performance in unseen noise types. In this study, we utilize an Acoustic Factor Analysis (AFA) based UBM that iteratively learns the dominant feature sub-spaces in each mixture component, resulting in a more robust model. We explore two variants of the model: one with an isotropic and the other with a diagonal residual noise. The Maximum-Likelihood (ML) training formulations of the models are provided. The latent variables of the model, termed acoustic factors, are used as features to train the second stage of factor analysis parameters, i.e., the traditional i-vector extractor. Experiments performed on the 2012 National Institute of Standards and Technology (NIST) Speaker Recognition Evaluation (SRE) indicate the effectiveness of the proposed strategy in both clean and noisy conditions.", "title": "Acoustic factor analysis based universal background model for robust speaker verification in noise"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/villalba13b_interspeech.html", "abstract": "In some situations the quality of the signals involved in a speaker verification trial is not as good as needed to take a reliable decision. In this work, we present a new method based on Bayesian networks and quality measures to estimate if the trial decision is reliable. We present experiments on the NIST SRE2010 dataset degraded with additive noise. A system well calibrated for clean speech, produces a large actual DCF on the degraded dataset. We use our method to discard the unreliable trials and achieve a dramatic improvement of the cost values. We also prove that our method outperforms previously published approaches.", "title": "A new Bayesian network to assess the reliability of speaker verification decisions"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/zhu13b_interspeech.html", "abstract": "IBM's submission for the Phase II speaker recognition evaluation of the DARPA sponsored Robust Automatic Transcription of Speech (RATS) program is examined. The objectives of the paper are three fold: (1) to provide a system description, (2) to identify key techniques for performance improvement, and (3) to quantify their contribution. In the system design, the fundamental idea revolves around exploiting diversity and modeling complementary information at all levels. To speed up system development a push-button system is designed whereby all system development steps could be rapidly completed. Noise robustness is improved by utilizing two speech activity detectors (SADs) and five acoustic feature extractors. Furthermore, the probabilistic linear discriminant analysis (PLDA) based back-ends were trained with two different data subsets. To better exploit the complementary information, system combination was performed in two modules. The first module trained new PLDA back-ends from concatenated compact representations while the second combined all the system scores and duration related side information in a neural network. The official results from the Phase II evaluation are also examined. The results indicate that for the 30s-30s task the performance of the overall system was better than the best single system by 46% and 40% on the internal and evaluation test sets respectively.", "title": "The IBM RATS phase II speaker recognition system: overview and analysis"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/lee13c_interspeech.html", "abstract": "This paper advocates the use of probabilistic linear discriminant analysis (PLDA) for partially open-set detection task with multiple i-vectors enrollment condition. Also referred to as speaker verification, the speaker detection task has always been considered under an open-set scenario. In this paper, a more general partially open-set speaker detection problem in considered, where the imposters might be one of the known speakers previously enrolled to the system. We show how this could be coped with by modifying the definition of the alternative hypothesis in the PLDA scoring function. We also look into the impact of the conditional-independent assumption as it was used to derive the PLDA scoring function with multiple training i-vectors. Experiments were conducted using the NIST 2012 Speaker Recognition Evaluation (SRE\u0081f12) datasets to validate various points discussed in the paper.", "title": "Multi-session PLDA scoring of i-vector for partially open-set speaker detection"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/godin13_interspeech.html", "abstract": "Many spectrum estimation methods and speech enhancement algorithms have previously been evaluated for noise-robust speaker identification (SID). However, these techniques have mostly been evaluated over artificially noised, mismatched training tasks with GMM-UBM speaker models. It is therefore unclear whether performance improvements observed with these methods translate to a broader range of noisy SID tasks. This study compares selected spectrum estimation methods from three classes: cochlear filterbanks, alternative time-domain windowing, and linear predictionbased techniques, as well as a set of frequency-domain noise reduction algorithms, across a suite of 8 evaluation tasks. The evaluation tasks are designed to expand upon the limited tasks addressed in past evaluations by exploring three research questions: performance on real noise versus artificial noise, performance on matched training tasks versus mismatched tasks, and performance when paired with an i-vector backend versus a GMM-UBM backend. We find that noise-robust spectrum estimation methods can improve the performance of SID systems over the range of noise tasks evaluated, including real noisy tasks, matched training tasks, and i-vector backends. However, performance on the typical GMM-UBM mismatched artificially noised case did not predict performance on other tasks. Finally, the matched enrollment case is a significantly different problem than the mismatched enrollment case.", "title": "Impact of noise reduction and spectrum estimation on noise robust speaker identification"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/yamada13_interspeech.html", "abstract": "In this paper we propose bottleneck features of deep neural network for distant-talking speaker identification. The accuracy of distant-talking speaker recognition is significantly degraded under reverberant environment. Feature mapping or feature transformation has been shown efficacy in channel-mismatch speaker recognition. Bottleneck feature derived from multi-layer network, which is a nonlinear feature transformation method, has been shown efficacy in automatic speech recognition (ASR) system. In this study, bottleneck features extracted from deep neural networks (DNNs) which employ an unsupervised pre-training method are used as nonlinear feature transformation for distant-talking speech. The speaker identification experiment was performed on large-scale distant-talking speech set, with reverberant environments different to the training environments. The proposed bottleneck features achieved a relative error reduction of 46.3% compared with conventional MFCC. Moreover, a combination of likelihoods of bottleneck", "title": "Improvement of distant-talking speaker identification using bottleneck features of DNN"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/brutti13_interspeech.html", "abstract": "Reverberation generated by multi-path acoustic propagation in enclosures is one of the most critical issues for distant-speech speaker verification systems. While late arrivals can be treated as additive noise, early reflections critically affects the speech spectral properties that allow differentiating among speakers. Considering a standard GMM/UBM speaker verification system based on MFCC, a geometric data contamination scheme relying on a rough modeling of the sound propagation through the image method is presented to attack the reverberation. The contribution of this paper is twofold: first, an analysis of the amount of knowledge of the impulse response needed for an effective contamination is carried out on simulated data; second, the geometric contamination is applied to real impulse responses to validate its use in real application contexts. Preliminary experimental results are provided to support the theoretical study and show the effectiveness of the approach.", "title": "Geometric contamination for GMM/UBM speaker verification in reverberant environments"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/mcclanahan13_interspeech.html", "abstract": "Speaker verification (SV) systems that employ maximum a posteriori (MAP) adaptation of a Gaussian mixture model (GMM) universal background model (UBM) incur a significant test-stage com- putational load in the calculation of a posteriori probabilities and sufficient statistics. We propose a multi-layered hash system employing a tree-structured GMM which uses Runnalls' GMM reduction technique. The proposed method is applied only to the test stage and does not require any modifications to the training stage or previously-trained speaker models. With the tree-structured hash system we are able to achieve a factor of 8\u0081~ reduction in test-stage computation with no degradation in accuracy. Furthermore, we can achieve computational reductions greater than 21\u0081~ with less than 7.5% relative degradation in accuracy.", "title": "Towards a more efficient SVM supervector speaker verification system using Gaussian reduction and a tree-structured hash"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/kanagasundaram13b_interspeech.html", "abstract": "A significant amount of speech data is required to develop a robust speaker verification system, but it is difficult to find enough development speech to match all expected conditions. In this paper we introduce a new approach to Gaussian probabilistic linear discriminant analysis (GPLDA) to estimate reliable model parameters as a linearly weighted model taking more input from the large volume of available telephone data and smaller proportional input from limited microphone data. In comparison to a traditional pooled training approach, where the GPLDA model is trained over both telephone and microphone speech, this linear-weighted GPLDA approach is shown to provide better EER and DCF performance in microphone and mixed conditions in both the NIST 2008 and NIST 2010 evaluation corpora. Based upon these results, we believe that linear-weighted GPLDA will provide a better approach than pooled GPLDA, allowing for the further improvement of GPLDA speaker verification in conditions with limited development data.", "title": "Improving the PLDA based speaker verification in limited microphone data conditions"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/villalba13c_interspeech.html", "abstract": "The I3A submission for the recent NIST 2012 speaker recognition evaluation (SRE) was based on the i-vector approach with a multichannel PLDA classifier. This PLDA is modified so that, for each i-vector, the between-class covariance depends on the type of channel where the segment was recorded (telephone, interviews, clean, noisy, etc). In this paper, we present the description of our submission and a detailed post-evaluation analysis of the results. We analyze several factors affecting performance: enrollment data selection, classifier type, scoring technique, calibration, known and unknown non-targets, target speakers included or not in development, segment duration, noise level and noise type. Some of these factor are new in this evaluation. After post-evaluation, actual costs improve by 15.43% depending on the common condition.", "title": "The I3a speaker recognition system for NIST SRE12: post-evaluation analysis"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/stafylakis13_interspeech.html", "abstract": "In this paper, we apply and enhance the i-vector-PLDA paradigm to text-dependent speaker recognition. Due to its origin in textindependent speaker recognition, this paradigm does not make use of the phonetic content of each utterance. Moreover, the uncertainty in the i-vector estimates should be taken into account in the PLDA model, due to the short duration of the utterances. To bridge this gap, a phrase-dependent PLDA model with uncertainty propagation is introduced. We examined it on the RSR-2015 dataset and we show that despite its low channel variability, improved results over the GMM-UBM model are attained.", "title": "Text-dependent speaker recognition using PLDA with uncertainty propagation"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/mallidi13_interspeech.html", "abstract": "Speaker recognition in noisy environments is challenging when there is a mis-match in the data used for enrollment and verification. In this paper, we propose a robust feature extraction scheme based on spectro-temporal modulation filtering using twodimensional (2-D) autoregressive (AR) models. The first step is the AR modeling of the sub-band temporal envelopes by the application of the linear prediction on the sub-band discrete cosine transform (DCT) components. These sub-band envelopes are stacked together and used for a second AR modeling step. The spectral envelope across the sub-bands is approximated in this AR model and cepstral features are derived which are used for speaker recognition. The use of AR models emphasizes the focus on the high energy regions which are relatively well preserved in the presence of noise. The degree of modulation filtering is controlled using AR model order parameter. Experiments are performed using noisy versions of NIST 2010 speaker recognition evaluation (SRE) data with a state-ofart speaker recognition system. In these experiments, the proposed features provide significant improvements compared to baseline features (relative improvements of 20% in terms of equal error rate (EER) and 35% in terms of miss rate at 10% false alarm).", "title": "Robust speaker recognition using spectro-temporal autoregressive models"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/rajan13b_interspeech.html", "abstract": "The i-vector representation and PLDA classifier have shown stateof- the-art performance for speaker recognition systems. The availability of more than one enrollment utterance for a speaker allows a variety of configurations which can be used to enhance robustness to noise. The well-known technique of multicondition training can be utilized at different stages of the system, including enrollment and classifier training. We also study the effect of mismatched training, averaging and length normalization. Our study indicates that multicondition training of the PLDA model, and if possible the enrollment i-vectors are the most important to achieve good performance in noisy evaluation data.", "title": "Effect of multicondition training on i-vector PLDA configurations for speaker recognition"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/mclaren13b_interspeech.html", "abstract": "The goal of this paper is to analyze the impact of codec-degraded speech on a state-of-the-art speaker recognition system and pro- pose mitigation techniques. Several acoustic features are analyzed, including the standard Mel filterbank cepstral coefficients (MFCC), as well as the noise-robust medium duration modulation cepstrum (MDMC) and power normalized cepstral coefficients (PNCC), to determine whether robustness to noise generalizes to audio compression. Using a speaker recognition system based on i-vectors and probabilistic linear discriminant analysis (PLDA), we compared four PLDA training scenarios. The first involves training PLDA on clean data, the second included additional noisy and reverberant speech, a third introduces transcoded data matched to the evaluation conditions and the fourth, using codec-degraded speech mismatched to the evaluation conditions. We found that robustness to compressed speech was marginally improved by exposing PLDA to noisy and reverberant speech, with little improvement using trancoded speech in PLDA based on codecs mismatched to the evaluation conditions. Noise-robust features offered a degree of robustness to compressed speech while more significant improvements occurred when PLDA had observed the codec matching the evaluation conditions. Finally, we tested i-vector fusion from the different features, which increased overall system performance but did not improve robustness to codec-degraded speech.", "title": "Improving robustness to compressed speech in speaker recognition"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/mitra13b_interspeech.html", "abstract": "Current state-of-the-art speaker identification (SID) systems perform exceptionally well under clean conditions, but their performance deteriorates when noise and channel degradations are introduced. Literature has mostly focused on robust modeling techniques to combat degradations due to background noise and/or channel effects, and have demonstrated significant improvement in SID performance in noise. In this paper, we present a robust acoustic feature on top of robust modeling techniques to further improve speaker-identification performance. We propose Modulation features of Medium Duration sub-band Speech Amplitudes (MMeDuSA); an acoustic feature motivated by human auditory processing, which is robust to noise corruption and captures speaker stylistic differences. We analyze the performance of MMeDuSA using SRI International's robust SID system using a channel and noise degraded multilingual corpus distributed through the Defense Advance Research Projects Agency (DARPA) Robust Automatic Transcription of Speech (RATS) program. When benchmarked against standard cepstral features (MFCC) and other noise robust acoustic features, MMeDuSA provided lower SID error rates compared to the others.", "title": "Modulation features for noise robust speaker identification"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/hautamaki13d_interspeech.html", "abstract": "Total variability modeling, based on i-vector extraction of converting a variable-length sequence of feature vectors into a fixed-length i-vector, is currently an adopted parametrization technique for state of-the-art speaker verification systems. However, when the number of the feature vectors is low, uncertainty in the i-vector representation as a point estimate of the linear-Gaussian model is understandably problematic. It is known that the zeroth and first order sufficient statistics, given the hyperparameters, completely characterize the extracted i-vectors. In this study we propose to use a minimax strategy to estimate the sufficient statistics in order to increase the robustness of the extracted i-vectors. We show by experiments that the proposed minimax technique can improve over the baseline system from 9.89% to 7.99% on the NIST SRE 2010 8conv-10sec task.", "title": "Minimax i-vector extractor for short duration speaker verification"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/fowler13_interspeech.html", "abstract": "Speech can potentially be used to identify individuals from a distance and contribute to the growing effort to develop methods for standoff, multimodal biometric identification. However, mismatched recording distances for the enrollment and verification speech samples can potentially introduce new challenges for speaker recognition systems. This paper describes a data collection, referred to as the Standoff Multi-Microphone Speech Corpus, which allows investigation of the impact of recording distance mismatch on the performance of speaker recognition systems. Additionally, a supervised method for linear subspace decomposition was evaluated in an effort to mitigate the effects of recording distance mismatch. The results of this study indicate that mismatched recording distances have a consistent negative impact on performance of a standoff speaker recognition system; however, subspace decomposition techniques may be able to reduce the penalty observed with mismatched recording distances.", "title": "Standoff speaker recognition: effects of recording distance mismatch on speaker recognition system performance"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/shaw13_interspeech.html", "abstract": "Using eye-tracking in a visual world paradigm, we sought converging evidence for the time course of Mandarin Chinese tone recognition as predicted by the availability of information in F0 and past results from a gating experiment. Our results showed that tones 1 and 2 are recognized earlier than tone 4, followed by tone 3. With the exception of tone 2, which was recognized earlier than expected, our results are consistent with those found in gating. The speed of tone 2 recognition varied significantly across vowels in our study, part of a broader pattern whereby vowels systematically influenced the time course of tone recognition. Rising tones, tone 2 and tone 3, were recognized earliest when co-produced with /a/. The falling tone, tone 4, was recognized earliest when co-produced with /u/.", "title": "Vowel identity conditions the time course of tone recognition"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/scharenborg13_interspeech.html", "abstract": "Older listeners with high-frequency hearing loss rely more on intensity for categorisation of /s/ than normal-hearing older listeners. This study addresses the question whether this increased reliance comes about immediately when the need arises, i.e., in the face of a spectrally-degraded signal. A phonetic categorisation task was carried out using intensity-modulated fricatives in a clean and a low-pass filtered condition with two younger and two older listener groups. When high-frequency information was removed from the speech signal, younger listeners started using intensity as a cue. The older adults on the other hand, when presented with the lowpass filtered speech, did not rely on intensity differences for fricative identification. These results suggest that the reliance on intensity shown by the older hearing-impaired adults may have been acquired only gradually with longer exposure to a degraded speech signal.", "title": "Changes in the role of intensity as a cue for fricative categorisation"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/yasu13_interspeech.html", "abstract": "In previous studies, we conducted several experiments, including identification tests for young and elderly listeners using /shi/-/chi/ (CV) and /ishi/-/ichi/ (VCV) continua. For the CV stimuli, confusion of /shi/ as /chi/ increased when the frication had a long rise time, and /chi/ was confused with /shi/ when the frication had a short rise time. This was true for the group with the following auditory property degradation: 1) elevation of absolute threshold, 2) presence of loudness recruitment, and 3) deficit of auditory temporal resolution. When auditory property degradation was observed, the weighting of acoustic cues shifted to frication duration rather than the gradient of the amplitude of frication. The latter was calculated by dividing frication amplitude by rise time. In the VCV stimuli, confusion of /ichi/ as /ishi/ occurred for a long silent interval between the first V and C with auditory property degradation, and the weighting of acoustic cues shifted from the silent interval to frication duration. In the present study, we unified these findings into a single framework and found that degradation of auditory properties causes listeners to prefer duration of frication as a cue for identifying fricatives and affricates.", "title": "Weighting of acoustic cues shifts to frication duration in identification of fricatives/affricates when auditory properties are degraded due to aging"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/gao13_interspeech.html", "abstract": "Previous studies have reported phonetic characteristics of the Shanghai Chinese phonological voicing contrast, which co-occurs with a tonal contrast. In stressed word-initial position, phonetic voicing is neutralized and replaced with a tonal register contrast: high \u0081eyin\u0081f tones for (phonologically) voiceless and low \u0081eyang\u0081f tones for voiced obstruents. Furthermore, breathy vs. modal voice quality, and low vs. high C/V duration ratio accompany voiced vs. voiceless obstruents. In two syllable identification experiments, we explored the impact of these characteristics on the perception of underlying phonological voicing. In Experiment 1, we manipulated tone contour (\u0081eyin\u0081f vs. \u0081eyang\u0081f) while maintaining other phonetic properties, including duration pattern. Syllable identification was mainly determined by the imposed contour, except for syllables with a voiced labial fricative onset. However, response times tended to increase when the imposed contour differed from the original one. In Experiment 2, we manipulated duration pattern and created tone contour continua from a \u0081eyin\u0081f tone to a \u0081eyang\u0081f tone. The duration pattern manipulation influenced identification in that high C/V duration ratios induced more frequent and faster \u0081eyin\u0081f identification (phonologically voiceless onset syllable). This result only held for unchecked syllables. We conclude that duration pattern contributes to the perception of phonological voicing in Shanghai Chinese.", "title": "Duration as a secondary cue for perception of voicing and tone in shanghai Chinese"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/dekerle13_interspeech.html", "abstract": "This study aimed at investigating the development of central auditory processes and their links with language skills. Seventy nine typically developing children divided up in five levels groups were recruited among primary schools. The development of central auditory processes was assessed with three main tasks. A lateralization task, a discrimination task and a central masking task were presented. These tasks were selected as each of them may correspond to important auditory processes underlying different speech abilities, and thus playing a role on its development. Verbal skills were evaluated on three levels: comprehension, vocabulary (lexical or verbal IQ), and phonological awareness. Results confirmed a developmental effect both on auditory and verbal skills. In addition, vocabulary and phonological awareness performances correlated with auditory skills, highlighting links between central auditory processing and language development.", "title": "Development of central auditory processes and their links with language skills in typically developing children"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/varnet13_interspeech.html", "abstract": "An essential step in understanding the processes underlying the general mechanism of perceptual categorization is to identify which portions of a physical stimulation modulate the responses of our perceptual system. More specifically, in the context of speech comprehension, it is still unclear what information is used to categorize a speech stimulus as one phoneme or another. Here we propose to adapt a Generalized Linear Model with smooth priors, already used in the visual domain for estimation of so-called classification images, to auditory experiments. We show how this promising approach can be applied to the identification of fine functional acoustic cues in speech perception.", "title": "Show me what you listen to! auditory classification images can reveal the processing of fine acoustic cues during speech categorization"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/brackhane13_interspeech.html", "abstract": "In mechanical speech synthesis reed pipes were mainly used for the generation of the voice. The organ stop \"vox humana\" played a central role for this concept. Historical documents report that the \"vox humana\" sounded like human vowels. In this study tones of four different \"voces humanae\" were recorded to investigate the similarity to human vowels. The acoustical and perceptual analysis revealed that some though not all tones show a high similarity to selected vowels.", "title": "The organ stop \u201cvox humana\u201d as a model for a vowel synthesiser"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/ghosh13_interspeech.html", "abstract": "We use mutual information as the criterion to rank the Mel frequency cepstral coefficients (MFCCs) and their derivatives according to the information they provide about different articulatory features in acoustic-to-articulatory (AtoA) inversion. It is found that just a small subset of the coefficients encodes maximal information about articulatory features and interestingly, this subset is articulatory feature specific. We use these subsets of MFCCs(+derivatives) in AtoA inversion using Gaussian mixture model (GMM) mapping. Inversion experiments with articulatory data support the information theoretic finding that the subsets of MFCCs(+derivatives) as selected by feature ranking method are sufficient to achieve an inversion performance similar to that obtained by a conventional full set of MFCCs(+derivatives). This drastically reduces the modeling complexity of the acoustic-articulatory map using GMM without degrading inversion performance significantly.", "title": "Information theoretic acoustic feature selection for acoustic-to-articulatory inversion"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/fejlova13_interspeech.html", "abstract": "The usefulness of dynamic formant properties for speaker discrimination was demonstrated on English, mostly exploiting long vowels or diphthongs, characterized both by actual measures along the formant tracks and coefficients from polynomial regression on these tracks. This study applies this paradigm to Czech, using less tightly controlled (more forensically realistic) material and taking into account the specific properties of the Czech vocalic system in which long vowels are much rarer than short ones. When all vowels are pooled together, the best results are achieved for unstressed vowels in asymmetrical CVC contexts. When individual vowels are considered separately, classification rates are best for long [i:] and [a:], but, most importantly, short vowels also show promising results. The performance of actual formant values and regression coefficients as predictors in discriminant analysis appears comparable.", "title": "Formant contours in Czech vowels: speaker-discriminating potential"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/liu13h_interspeech.html", "abstract": "Extracting tongue contour from high noised ultrasound image is a key issue of observing speech production procedure. Anisotropic diffusion has been widely used in reducing speckle noise of ultrasound images but it is not very effective in preserving edges and tends to blur them. Hence the blurred edges hamper the succeeding contour-based pattern analysis or modeling. In this study, we modify the standard SRAD (speckle reducing anisotropic diffusion) to improve its edge detection and suppress the intrinsic edge blurring effect of SRAD by exploiting the multidirectional separability. We experimented with both synthetic and real ultrasound images by SRAD and the proposed approach. The extracted contours in denoised images by SRAD and the proposed approach are compared in terms of the corresponding accuracy, both subjectively and objectively. The results show the proposed approach performs better than the conventional SRAD and more accurate contours can be obtained for post processing.", "title": "An anisotropic diffusion filter based on multidirectional separability"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/skarnitzl13_interspeech.html", "abstract": "Czech represents a language with a high correlation between phonological and phonetic voicing, but the perception of the voicing contrast is supported by other phonetic correlates. We employed electropalatography (EPG) to investigate the articulatory correlates of the voicing contrast in phonated and whispered speech. Eight subjects read short sentences and VCV sequences with alveolar/ postalveolar fricatives as target sounds. The results revealed significant differences in the contact patterns of fortis (voiceless) and lenis (voiced) fricatives in phonated speech but the differences diminished in whisper. Implications of these findings are discussed with respect to general phonetic theory.", "title": "The phonological voicing contrast in Czech: an EPG study of phonated and whispered fricatives"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/maeda13_interspeech.html", "abstract": "We have measured total vocal-tract (VT) length, lip-tube length and glottal height during vowels on X-ray film data of short French utterances. VT midpoints are determined by progressively fitting circles along the VT-length from the glottis to lip opening. The VT-length is obtained by summing up the distance between adjacent midpoints. Lip-tube length is defined as the distance between the incisors and the lip opening along the midline. Results show that the range of VT-length variation is 3.2cm with the average VTlength of 16.4cm. The cause of this large range appears to be the combination of the vowel dependent VT-length and prosodic position that influences on the glottal height. For example, during a vowel at sentence final position, glottis goes down with falling intonation or up with rising intonation corresponding to, respectively, VT lengthening or shortening. The lip-tube lengths are little affected by prosodic position and exhibit a clear vowel dependency. The prosodic influences manifested on the glottal height are not compensated but rather expanded in the VT-length, yet maintaining the characteristic vowel-dependency. This suggests an underlying mechanism to maintain a uniform stretching/compression of VTlength, which tends to hold the phonetic identity of a vowel under large VT-length changes.", "title": "Vowel and prosodic factor dependent variations of vocal-tract length"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/grootswagers13_interspeech.html", "abstract": "Nowadays, using state of the art multivariate machine learning approaches, researchers are able to classify brain states from brain data. One of the applications of this technique is decoding phonemes that are being produced from brain data in order to decode produced words. However, this approach has been only moderately successful. Instead, decoding articulatory features from brain data may be more feasible. As a first step towards this approach, we propose a word decoding method that is based on the detection of articulatory features (words are identified from a sequence of articulatory class labels). In essence, we investigated how the lexical ambiguity is reduced as a function of confusion between articulatory features, and a function of the confusion between phonemes after feature decoding. We created a number of models based on different combinations of articulatory features and tested word identification on an English corpus with approximately 70,000 words. The most promising model used only 11 classes and identified 71% of words correctly. The results confirmed that it is possible to decode words based on articulatory features, and this offers the opportunity for multivariate fMRI speech decoding.", "title": "Word identification using phonetic features: towards a method to support multivariate fMRI speech decoding"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/gowda13b_interspeech.html", "abstract": "Breathy phonation has a high open quotient compared to modal phonation which results in greater influence of the subglottal cavity on the estimated short-time spectrum. This is reflected as an increase in spectral density at frequencies below the first resonance of the vocal tract around the glottal formant. On the contrary, pressed phonation has lesser influence of the subglottal cavity, and hence has a relatively lesser spectral density at low frequencies. In this paper, the use of low frequency spectral density (LFSD) as a feature for analysis and classification of breathy, modal and pressed phonation types is investigated.", "title": "Analysis of breathy, modal and pressed phonation based on low frequency spectral density"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/tajima13_interspeech.html", "abstract": "Vowel length contrasts in Japanese, e.g., chizu \"map\" vs. chiizu \"cheese\", are cued primarily by vowel duration. However, since short and long vowel durations overlap considerably in ordinary speech, learning to perceive vowel length contrasts is complex. Meanwhile, infant-directed speech (IDS) is known to \"exaggerate\" certain properties of adult-directed speech (ADS). If so, then it is possible that vowel length contrasts might also be exaggerated in IDS. To investigate this, the present study analyzed vowel durations in the RIKEN Japanese Mother-Infant Conversation Corpus, which contains 11 hours of IDS by 22 mothers talking with their 18-to- 24-month-old infants, and 3 hours of ADS by the same mothers. Results indicated that vowel length contrasts were generally not exaggerated in IDS, except at the end of prosodic phrases. Furthermore, several factors that systematically affected vowel duration in IDS were identified, including phrase-final lengthening and \"nonlexical lengthening\", i.e., the lengthening of vowels for emphatic or other stylistic purposes. These results suggest that vowel duration in Japanese IDS could not only potentially facilitate learning of lexical distinctions, but also signal phrase boundaries, emphasis, or other communicative functions.", "title": "Is the vowel length contrast in Japanese exaggerated in infant-directed speech?"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/chen13l_interspeech.html", "abstract": "The glottal open quotient (OQ) is often associated with the amplitude of the first source harmonic relative to the second (H1*-H2*), which is assumed to be one cause of a change in vocal quality along a breathy-to-pressed continuum. The association between OQ and H1*-H2* was investigated in a group of 5 human subjects and also in a computational voice production simulation. The simulation incorporated a parametric voice source model into a nonlinear sourcefilter framework. H1*-H2* and OQ were measured synchronously from audio recordings and high-speed laryngeal videoendoscopy of \"glide\" phonations in which quality varied continuously from breathy to pressed. Analyses of individual speakers showed large differences in the relationship between OQ and H1*-H2*. The variability in laryngeal high-speed data was consistent with simulation results, which showed that the relationship between OQ and H1*- H2* depended on mean glottal area, a parameter associated with the degree of source-filter interaction and not directly measurable from high-speed video of the vocal folds. In addition, H1*-H2* may change with increasing glottal gap size; this change contributes to the observed variability in the relationship between H1*-H2* and OQ.", "title": "Investigating the relationship between glottal area waveform shape and harmonic magnitudes through computational modeling and laryngeal high-speed videoendoscopy"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/kim13f_interspeech.html", "abstract": "We present a novel method for estimating formant frequencies by fitting Gaussian mixtures to discrete Fourier Transform (DFT) magnitude spectra. The method first estimates the Gaussian parameters for a sequence of wideband spectra using the Expectation- Maximization (EM) algorithm. It then refines the parameters by using maximum a posteriori (MAP) adaptation. The work was evaluated using manually labeled ground truth data with 516 utterances and comparing results both with PRAAT's formant tracking algorithm in various noisy environments and one other state-of-the-art method. We obtained statistically significant improvements in the relative errors for the first three formants over all phonetic classes.", "title": "Formant frequency tracking using Gaussian mixtures with maximum a posteriori adaptation"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/yasuda13_interspeech.html", "abstract": "In Tokyo Japanese, vowel devoicing is a common process, that leads to the reduction of high, unstressed vowels ( and ) between unvoiced consonants. This article investigates to what extent native Japanese speakers (L1) learning German as foreign language (L2) show a strong tendency to produce these vowels in the foreign language as devoiced, too. Furthermore, the question is addressed whether German native speakers also devoice vowels in the same context. To this end, a production study of German words with German (L1) and Japanese (L2) native speakers was carried out. Results of this production task show that Japanese speakers devoice vowels in German words quite regularly, whereas German speakers show this pattern only rarely. For the Japanese speakers, the reduction patterns are comparable to those of the native language. Thus, interference of Japanese (L1) patterns can be observed in German (L2) indicating that this process is deeply rooted in Japanese speakers\u0081f phonetic/phonological knowledge and leads to interference when learning a foreign language, irrespective of the existence of the process in that language (L2).", "title": "Devoicing of vowels in German, a comparison of Japanese and German speakers"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/smith13c_interspeech.html", "abstract": "Liquids are unique in their ability to occupy syllable onset, nucleus, and coda positions in American English, as well as the fact that they are composed of two lingual gestures. Upon inspection of /l/ in syllable nucleus and coda positions using real-time MRI, it appears that the tongue tip constriction we might expect for /l/ is often not present, a phenomenon called /l/-vocalization. However, it is not merely the case that the consonantal gesture of /l/ is completely lost in these syllable positions, leaving behind a simple vocalic configuration. Though there is often no raising of the tongue tip in an attempt to make contact with the alveolar ridge, the /l/ exhibits complex tongue shaping involving curling in the region of the tongue blade. The result is a lowered tongue blade relative to the tongue tip and dorsum. This shaping is captured through measures of Gaussian curvature at evenly spaced points along the tongue. The results indicate that /l/-vocalization in the syllable rhyme does not involve a complete loss of the consonantal nature of the lateral, but rather a modification of its realization.", "title": "Identifying consonantal tasks via measures of tongue shaping: a real-time MRI investigation of the production of vocalized syllabic /l/ in American English"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/deng13_interspeech.html", "abstract": "In this paper, a single-channel speech enhancement method by coupling speech detection and spectral amplitude estimation is proposed. First, the optimal detector used for the spectral coefficients of the speech signal is obtained by minimizing the combined risk function which considers both detection and estimation errors. Second, according to the optimal speech detector, the optimal spectral amplitude estimator is obtained by further minimizing the combined risk function. For speech-presence and speech-absence, we propose two general weighted cost functions which are based on human auditory perception and compressive nonlinearities of the cochlea. The cost parameters are employed to balance the speech distortion and residual noise caused by missed detection and false alarm, respectively. The performance of the proposed method is evaluated under ITU-T G.160. The experimental results indicate that the proposed method evidently performs better than the reference methods.", "title": "A speech enhancement method by coupling speech detection and spectral amplitude estimation"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/zheng13_interspeech.html", "abstract": "The echo effect due to late reverberation can severely degrade speech quality and intelligibility. Prior attempts to reduce this degradation in the modulation domain used time-invariant filtering. In this paper, we show that performing minimum mean squared error spectral estimation in the modulation domain (MDMMSE) can significantly reduce the severity of audible reverberation and enhance the listening speech quality. Moreover, MDMMSE outperforms modulation domain spectral subtraction (SS) as well as performing MMSE spectral estimation or SS in the acoustic domain. In informal subjective listening tests, MDMMSE exhibits less residual echoes and artifacts than the other three methods.", "title": "Late reverberation suppression using MMSE modulation spectral estimation"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/turan13_interspeech.html", "abstract": "In this paper we investigate a new statistical excitation mapping technique to enhance throat-microphone speech using joint analysis of throat- and acoustic-microphone recordings. In a recent study we employed source-filter decomposition to enhance spectral envelope of the throat-microphone recordings. In the source-filter decomposition framework we observed that the spectral envelope difference of the excitation signals of throat- and acoustic-microphone recordings is an important source of the degradation in the throatmicrophone voice quality. In this study we model spectral envelope difference of the excitation signals as a spectral tilt vector, and we propose a new phone-dependent GMM-based spectral tilt mapping scheme to enhance throat excitation signal. Experiments are performed to evaluate the proposed excitation mapping scheme in comparison with the state-of-the-art throat-microphone speech enhancement techniques using both objective and subjective evaluations. Objective evaluations are performed with the wideband perceptual evaluation of speech quality (ITU-PESQ) metric. Subjective evaluations are performed with the A/B pair comparison listening test. Both objective and subjective evaluations yield that the proposed statistical excitation mapping consistently delivers higher improvements than the statistical mapping of the spectral envelope to enhance the throat-microphone recordings.", "title": "A new statistical excitation mapping for enhancement of throat microphone recordings"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/roman13_interspeech.html", "abstract": "Reverberation has a detrimental effect on speech perception both in terms of quality as well as intelligibility, as late reflections smear temporal and spectral cues. The ideal binary mask, which is an established computational approach to sound separation, was recently extended to remove reverberation. Experiments with both normal hearing and hearing impaired listeners have shown significant intelligibility improvements for reverberant speech processed using such a priori binary masks. The dereverberation problem can thus be formulated as a classification problem, where the desired output is the ideal binary mask. The goal in this approach is to produce a mask that selects the time-frequency regions where the direct energy dominates the energy from the late reflections. In this study, a binaural dereverberation algorithm is proposed which utilizes the binaural cues of interaural time and level differences as features. The algorithm is tested in highly reverberant environments using both simulated and recorded room impulse responses. Evaluations show significant improvements over the unprocessed condition as measured by both a speech quality measure and a speech intelligibility predictor.", "title": "Classification based binaural dereverberation"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/kim13g_interspeech.html", "abstract": "In this paper, we propose a dual-microphone target-directional speech enhancement system utilizing target-to-non-target directional signal ratio (TNR) based on dual-microphone phase differences in adverse noise environments. One of the important issues associated with multi-microphone speech enhancement methods is the effective utilization of spatial cues such as phase differences for target-speech estimation within noisy speech. To this end, a TNR estimation method is presented based on phase differences between dual-microphone signals. Then, the estimated TNR is incorporated into a Wiener filter to obtain a masking filter for speech enhancement. Consequently, it is shown from a perceptual evaluation of speech quality that the performance of the proposed speech enhancement system outperforms those of conventional single- or dual-microphone speech enhancement systems based on a Wiener filter, beamformer, and phase-error-based filter under noise conditions with a signal-to-noise ratio ranging from 0 to 20 dB.", "title": "Target-to-non-target directional ratio estimation based on dual-microphone phase differences for target-directional speech enhancement"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/lu13c_interspeech.html", "abstract": "Many speech enhancement algorithms have been proposed for speech restoration from distorted speech. However, if some components of the signal are completely missed or distorted, there is no way for those algorithms to restore the clean speech. Considering that the restricted Boltzmann machine (RBM) is a stochastic version of the Hopfield network which can be used as an associative memory, we propose to use its \"recall\" ability for speech spectrum restoration when some parts of the speech spectrum are completely missed or distorted. Traditionally, in training the RBM, speech spectral patches are randomly selected as input. There is no consideration of the temporal correlation between different input spectral patches. In this study, we further propose to model this temporal correlation by using a conditional RBM (CRBM). The inference on the CRBM is almost the same as that of on the RBM by only modifying the biases as conditional dynamic biases. We did experiments for clean speech reconstruction and distorted speech restoration based on the trained models. Our experimental results showed that both the RBM and CRBM worked well in restoration task. By incorporating temporal correlation in the CRBM, a further improvement on reconstruction and restoration accuracy was achieved.", "title": "Speech spectrum restoration based on conditional restricted boltzmann machine"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/khan13_interspeech.html", "abstract": "This work proposes a method of single-channel speaker separation that uses visual speech information to extract a target speaker\u0081fs speech from a mixture of speakers. The method requires a single audio input and visual features extracted from the mouth region of each speaker in the mixture. The visual information from speakers is used to create a visually-derived Wiener filter. The Wiener filter gains are then non-linearly adjusted by a perceptual gain transform to improve the quality and intelligibility of the target speech. Experimental results are presented that estimate the quality and intelligibility of the extracted target speaker and a comparison is made of different perceptual gain transforms. These show that significant gains are achieved by the application of the perceptual gain function.", "title": "Speaker separation using visual speech features and single-channel audio"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/chuang13_interspeech.html", "abstract": "Perceptual acoustic echo cancellers were developed by mainly considering human hearing thresholds of different acoustic frequencies in the past. In addition to the different frequency sensitivities, the human brain further analyzes sounds in terms of their spectral and temporal modulations. In this paper, we extend the perceptual normalized least mean square (P-NLMS) algorithm by adding a second pre-emphasis filter which accounts for brain's spectral modulation sensitivity. Simulation results and listening tests demonstrate our system effectively reduces the perceived residual echo during convergence due to faster convergence rates in hearing-sensitive spectral modulation bands and acoustic frequency bands.", "title": "Spectral modulation sensitivity based perceptual acoustic echo cancellation"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/abrol13_interspeech.html", "abstract": "This paper proposes an approach based on the compressed sensing for speech enhancement. It attempts to exploit the fact that it is relatively easy to find sparse representation for speech signal but the same cannot hold true for noise. Thus for a given speech signal a sparse vector is derived which extracts the speech signal from the given noisy speech signal. Approach is further modified to ensure the speech enhancement across the varying segments of speech signal, which is present due to language, emotion and speech uttered by a person. Experimental results show that the proposed approach can be an alternative to the existing approaches for speech enhancement.", "title": "Speech enhancement using compressed sensing"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/grais13b_interspeech.html", "abstract": "We propose to use minimum mean squared error (MMSE) estimates to enhance the signals that are separated by nonnegative matrix factorization (NMF). In single channel source separation (SCSS), NMF is used to train a set of basis vectors for each source from their training spectrograms. Then NMF is used to decompose the mixed signal spectrogram as a weighted linear combination of the trained basis vectors from which estimates of each corresponding source can be obtained. In this work, we deal with the spectrogram of each separated signal as a 2D distorted signal that needs to be restored. A multiplicative distortion model is assumed where the logarithm of the true signal distribution is modeled with a Gaussian mixture model (GMM) and the distortion is modeled as having a log-normal distribution. The parameters of the GMM are learned from training data whereas the distortion parameters are learned online from each separated signal. The initial source estimates are improved and replaced with their MMSE estimates under this new probabilistic framework. The experimental results show that using the proposed MMSE estimation technique as a post enhancement after NMF improves the quality of the separated signal.", "title": "Spectro-temporal post-enhancement using MMSE estimation in NMF based single-channel source separation"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/kaewtip13_interspeech.html", "abstract": "This paper presents a new pitch-based spectral enhancement algorithm on voiced frames for speech analysis and noise-robust speech processing. The proposed algorithm determines a time-warping function (TWF) and the speaker's pitch with high precision, simultaneously. This technique reduces the smearing effect in between harmonics when the fundamental frequency is not constant within the analysis window. To do so, we propose a metric called the harmonic residual which measures the difference between the actual spectrum and the resynthesized spectrum derived from the linear model of speech production with various combinations of TWF and high-precision pitch values as parameters. The TWF and pitch pair that yields the minimum harmonic residual is selected and the enhanced spectrum is obtained accordingly. We show how this new representation can be used for automatic speech recognition by proposing a robust spectral representation derived from harmonic amplitude interpolation.", "title": "A pitch-based spectral enhancement technique for robust speech processing"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/mccallum13b_interspeech.html", "abstract": "Obtaining estimates of the fundamental frequencies associated with either noise or speech in noise/speech mixtures can be important in speech enhancement. Accurate simultaneous estimation of these can result in both an improved subjective quality as well as a higher signal to noise ratio (SNR) of the resulting speech. It is crucial with such an algorithm that each periodic component be reliably identified as either noise or speech. Further, the algorithm needs to be robust to changing SNR of the noisy speech arising from a range of environmental conditions. In this paper a multipitch tracking algorithm is proposed based on a stochastic-deterministic (SD) signal model in the complex short-time Fourier transform (STFT) framework, using a factorial hidden Markov model (FHMM). Unlike previous multipitch tracking algorithms based on FHMMs, the proposed algorithm performs well even when the levels of noise and speech differ significantly from those of the training data. This robustness is attributed in part to the flexible SD model employed. With this model, a priori information of noise and speech used to identify and track non-stationary periodic components is based primarily on their spectral envelope, not their absolute amplitude.", "title": "Stochastic-deterministic signal modelling for the tracking of pitch in noise and speech mixtures using factorial HMMs"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/maymon13b_interspeech.html", "abstract": "This paper considers the problem of signal clipping and studies the performance of speech recognition in the presence of clipping. We present two novel algorithms for restoring the corrupted samples. Our first approach assumes that the original undistorted signal is band-limited and exploits this to iteratively reconstruct the original signal. We show that this approach has a monotonically nonincreasing mean square error. In the second procedure we model the signal as an auto-regressive process and develop an estimationmaximization (EM) algorithm for estimating the model parameters. As a by product of the EM procedure the signal is recovered. The effects of these methods on the accuracy of speech recognition are studied, and it is shown that over 4% relative word error rate reduction can be achieved on speech utterances with more than 0.5% clipped samples.", "title": "Restoration of clipped signals with application to speech recognition"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/uezu13_interspeech.html", "abstract": "Distributed microphone array (DMA) processing has recently started attracting a lot of attention as a promising alternative to conventional microphone arrays with co-located elements. To perform efficient blind source separation (BSS) in distributed manner, we have recently proposed a clustering based BSS approach that utilizes a distributed expectation-maximization algorithm. In this paper, we further investigate the effectiveness of this proposed method in typical DMA situations with drift error, i.e., the inevitable sampling frequency mismatch between different nodes/devices, which is a very important aspect that has yet to be examined. We compare our method experimentally with some conventional methods and their variants newly proposed here, and confirm that our method is robust against drift error and achieves stable performance in various typical DMA scenarios, while the performance of the conventional approaches changes greatly depending on the DMA setup and amount of drift error.", "title": "On the robustness of distributed EM based BSS in asynchronous distributed microphone array scenarios"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/yang13c_interspeech.html", "abstract": "Generative feature spaces provide an elegant way to apply discriminative models in speech recognition, and system performance has been improved by adapting this framework. However, the classes in the feature space may be not linearly separable. Applying a linear classifier then limits performance. Instead of a single classifier, this paper applies a mixture of experts. This model trains different classifiers as experts focusing on different regions of the feature space. However, the number of experts is not known in advance. This problem can be bypassed by employing a Bayesian non-parametric model. In this paper, a specific mixture of experts based on the Dirichlet process, namely the infinite support vector machine, is studied. Experiments conducted on the noise-corrupted continuous digit task AURORA 2 show the advantages of this Bayesian nonparametric approach.", "title": "Infinite support vector machines in speech recognition"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/giuliani13_interspeech.html", "abstract": "In this paper, a novel on-line incremental speaker adaptation technique is proposed for real time transcription applications such as automatic closed-captioning of live TV programs. Differently from previously proposed methods, our technique does not operate at utterance level but instead speaker change detection and clustering as well as speaker adaptation occur over a short chunk of the incoming audio signal. Incremental adaptation based on feature space maximum likelihood linear regression (fMLLR) is conducted w. r. t. a Gaussian mixture model (GMM) modeling the acoustic training data. Individual speakers are represented by fMLLR transforms, and these transforms are used for speaker clustering and for performing speaker adaptation. Speech recognition experiments show that the proposed incremental adaptation technique is effective, 6% relative reduction in word-error-rate (WER) w. r. t. a non-adaptive baseline system, when it is embedded in a online transcription system applied to transcribe television news broadcasts.", "title": "An on-line incremental speaker adaptation technique for audio stream transcription"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/telaar13_interspeech.html", "abstract": "Acoustic models in state-of-the-art LVCSR systems are typically trained on data from thousands of speakers and then adapted to a speaker using, e.g., various combinations of CMLLR, MLLR and MAP. This adaptation step is particularly important for speakers with accents that are not well represented in the training set. The present study explores how to improve performance on South-Asian-accented speakers (SoA-accented) with the availability of thousands of US-accented, hundreds of SoA-accented, and tens of hours of speaker-specific training data. We employ a decision tree similarity measure to analyze how varying co-articulations across accents and people manifest themselves in the decision tree. Modeling these variations in addition to adapting the GMMs of an existing baseline system to a speaker improved WER for small systems (1k GMMs), but improvement for systems with larger trees (2k, 3k GMMs) was modest. Overall, GMM adaptation/retraining yields significant performance benefits, and training a SoA-accent-specific system is particularly worthwhile when lacking speaker adaptation data.", "title": "Accent- and speaker-specific polyphone decision trees for non-native speech recognition"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/wiesler13_interspeech.html", "abstract": "Context-dependent deep neural network HMMs have been shown to achieve recognition accuracy superior to Gaussian mixture models in a number of recent works. Typically, neural networks are optimized with stochastic gradient descent. On large datasets, stochastic gradient descent improves quickly during the beginning of the optimization. But since it does not make use of second order information, its asymptotic convergence behavior is slow. In regions with pathological curvature, stochastic gradient descent may almost stagnate and thereby falsely indicate convergence. Another drawback of stochastic gradient descent is that it can only be parallelized within mini-batches. The Hessian-free algorithm is a second order batch optimization algorithm that does not suffer from these problems. In a recent work, Hessian-free optimization has been applied to a training of deep neural networks according to a sequence criterion. In that work, improvements in accuracy and training time have been reported. In this paper, we analyze the properties of the Hessian-free optimization algorithm and investigate whether it is suited for cross-entropy training of deep neural networks as well.", "title": "Investigations on hessian-free optimization for cross-entropy training of deep neural networks"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/saiko13_interspeech.html", "abstract": "We propose a method to adapt acoustic models for robust speech recognition in real environments using data from other languages. In real-world speech recognition systems, we can effectively adapt acoustic models using the speech data logged by the system. However, when developing a system for a new language, this step is impossible since we have no such speech data for it. Assuming that similar Gaussians of each language have similar transfer vectors, in our proposed method, we estimate the transfer vectors of each Gaussian of the language for acoustic model adaptation by the transfer vectors of the other language. We evaluated the performance of Indonesian acoustic models that were adapted using the transfer vectors estimated from Japanese transfer vectors. Our proposed method achieved a relative error reduction rate of 10.6% for real environmental speech data.", "title": "Cross-lingual acoustic model adaptation based on transfer vector field smoothing with MAP"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/fujimura13_interspeech.html", "abstract": "This paper proposes a novel technique to exploit discriminative models with subclasses for speech recognition. Speech recognition using discriminative models has attracted much attention in the past decade. However, most discriminative models are still based on tree clustering results of HMM states. On the contrary, our proposed method, referred to as subclass AdaBoost, jointly selects optimal data split and weak discriminators in each iteration of the training process, and forms a weak classifier as a composite of these weak discriminators. As a result, a strong discriminator robust to a variety of subclasses is constructed without explicit clustering in advance. In the experiment, the subclass AdaBoost is applied to phoneme classification, and N-best hypotheses are rescored using the subclass AdaBoost phoneme classifiers. Experimental results show that the proposed method reduces word errors by over 10% relatively in a continuous speech recognition task.", "title": "N-best rescoring by phoneme classifiers using subclass adaboost algorithm"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/ogawa13_interspeech.html", "abstract": "A moderately deep and rather wide artificial neural net is applied in phoneme recognition of noisy speech. The net is formed by first estimating posterior probabilities of phonemes in 21 bandlimited streams covering the whole speech spectrum. These 21 band-limited streams are subdivided into three seven band-limited stream subsets, by differently sub-sampling the original 21 bandlimited streams. In the second processing stage, all non-empty combinations of seven band-limited streams from each subset are formed as inputs to 127 artificial neural nets that are again trained to yield phoneme posteriors. In this way, 127 \u0081~ 3 = 381 processing streams are formed. A novel technique for finding the best combination of the resulting 381 parallel processing streams, which uses the likelihood of a single-state Gaussian mixture model of the final classifier output is applied to selecting the most efficient streams. The technique is efficient in phoneme recognition of speech that is corrupted by realistic additive noise.", "title": "Stream selection and integration in multistream ASR using GMM-based performance monitoring"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/yoma13_interspeech.html", "abstract": "This paper describes a novel feature-space VTLN method that models frequency warping as a linear interpolation of contiguous Mel filter-bank energies. The presented technique aims to reduce the distortion in the Mel filter-bank energy estimation due to the harmonic composition of voiced speech intervals and DFT sampling when the central frequency of band-pass filters is shifted. The presented interpolated filter-bank energy-based VTLN leads to relative reductions in WER as high as 11.2% and 7.6% when compared with the baseline system and standard VTLN, respectively, in a medium-vocabulary continuous speech recognition task. Also, this new scheme provides significant reductions in WER equal to 7% when compared with state-of-the-art VTLN methods based on linear transforms in the cepstral space. The warping factor estimated here shows more dependence on the speaker and more independence of the acoustic-phonetic content than the warping factor in state-of-the-art VTLN techniques.", "title": "VTLN based on the linear interpolation of contiguous mel filter-bank energies"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/triefenbach13_interspeech.html", "abstract": "Reservoir Computing (RC) has recently been introduced as an interesting alternative for acoustic modeling. For phone and continuous digit recognition, the reservoir approach obtained quite promising results. In this work, we further elaborate this concept by porting some well-known techniques used to enhance recognition rates of GMM-based models to Reservoir Computing. In particular, we introduce context-dependent (CD) triphone states to model co-articulation and pronunciation mismatches arising from an imperfect lexicon. We also propose to incorporate two speaker normalization methods in the feature space, namely mean & variance normalization and vocal tract length normalization. The impact of the investigated techniques is studied in the context of phone recognition on the TIMIT corpus. Our CD-RC-HMM hybrid yields a speaker-independent phone error rate (PER) of 22% and a speakerdependent PER of 20.5%. By combining GMM and RC-based likelihoods at the state level, these scores can be reduced further.", "title": "Context-dependent modeling and speaker normalization applied to reservoir-based phone recognition"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/fragasilva13_interspeech.html", "abstract": "Acoustic models for speech recognition are often trained on data coming from a variety of sources. The usual approach is to pool together all of the available training data, considering them all to be part of a unique training set. In this work, assuming that each source may have a different degree of relevance for a given target task, two techniques are proposed to weigh subsets of the training data. The first one is based on the interpolation of the model probability densities, while the other on data weighting. An method to automatically select the mixture coefficients is also proposed. The best technique presented here outperformed unsupervised MAP adaptation and led to improvements in word accuracy (up to 6% relative) over the pooled model.", "title": "Interpolation of acoustic models for speech recognition"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/tahir13_interspeech.html", "abstract": "The use of higher-order polynomial acoustic features can improve the performance of automatic speech recognition. However, the dimensionality of the polynomial representation can be prohibitively large, making the training of acoustic models using polynomial features for large vocabulary ASR systems infeasible. This paper presents an iterative polynomial training framework for acoustic modeling, which recursively expands the current acoustic features into their second-order polynomial feature space. In each recursion the dimensionality is reduced by a linear projection, such that increasingly higher order polynomial information is incorporated while keeping the dimensionality of the acoustic models constant. Experimental results obtained for a large-vocabulary continuous speech recognition task show that the proposed method outperforms conventional mixture models.", "title": "Training log-linear acoustic models in higher-order polynomial feature space for speech recognition"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/parinam13_interspeech.html", "abstract": "In this paper, we evaluate the front-end of Automatic Speech Recognition (ASR) systems, with respect to different types of spectral processing methods that are extensively used. Experimentally, we show that direct use of FFT spectral values is just as effective as using either Mel or Gammatone filter banks, as an intermediate processing stage, if the cosine basis vectors used for dimensionality reduction are appropriately modified. Furthermore it is shown that trajectory features computed over intervals of approximately 300ms are considerably more effective, in terms of ASR accuracy, than are delta and delta-delta terms often used for ASR. Although there is no major performance disadvantage if a filter bank is used, simplicity of analysis is a reason to eliminate this step in speech processing. The experimental results which confirm the above assertions are based on the TIMIT phonetically labeled database. The assertions hold for both clean and noisy speech.", "title": "Comparison of spectral analysis methods for automatic speech recognition"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/sanand13_interspeech.html", "abstract": "The paper proposes to train synthetic speaker models using vocal tract length normalization (VTLN). Speaker adaptation based approaches require certain amount of data from the test speaker to either update or transform the model parameters of the trained model. If there is very little or no data available from the test speaker, we propose to create a synthetic speaker model that is acoustically close to the test speaker by scaling the training data with VTLN. For this purpose, we train multiple VTLN warped speaker independent (SI) models by scaling the training data with VTLN and choosing one of the models that is acoustically close to the test speaker for performing recognition. We show that the proposed approach is advantageous in mismatched speaker conditions, especially while recognizing children speakers using models trained on adult speech.", "title": "Synthetic speaker models using VTLN to improve the performance of children in mismatched speaker conditions for ASR"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/abdelhamid13c_interspeech.html", "abstract": "Recently, convolutional neural networks (CNNs) have been shown to outperform the standard fully connected deep neural networks within the hybrid deep neural network / hidden Markov model (DNN/HMM) framework on the phone recognition task. In this paper, we extend the earlier basic form of the CNN and explore it in multiple ways. We first investigate several CNN architectures, including full and limited weight sharing, convolution along frequency and time axes, and stacking of several convolution layers. We then develop a novel weighted softmax pooling layer so that the size in the pooling layer can be automatically learned. Further, we evaluate the effect of CNN pretraining, which is achieved by using a convolutional version of the RBM. We show that all CNN architectures we have investigated outperform the earlier basic form of the DNN on both the phone recognition and large vocabulary speech recognition tasks. The architecture with limited weight sharing provides additional gains over the full weight sharing architecture. The softmax pooling layer performs as well as the best CNN with the manually tuned fixed-pooling size, and has a potential for further improvement. Finally, we show that CNN pretraining produces significantly better results on a large vocabulary speech recognition task.", "title": "Exploring convolutional neural network structures and optimization techniques for speech recognition"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/mariani13_interspeech.html", "abstract": "This paper aims at analyzing the content of the conferences contained in the ISCA Archive over the past 25 years. It follows a similar exercise that has been conducted within the Computational Linguistics community over 50 years of existence at the ACL conference in 2012, and a survey on the IEEE ICASSP conference series from 1976 to 1990, which served in the launching of the ESCA Eurospeech conference. It contains first an analysis of the evolution of the number of papers and authors over time, including their gender and nationality, and of the collaboration among authors. It then studies the references cited in the papers, including their authors and sources. It finally conducts an analysis of the evolution of the research topics within the community over time. The survey shows the present trends in the conference series and in the Spoken Language Processing scientific community. Conducting this survey also demonstrated the importance of a clear and unique identification of authors, papers and other sources to facilitate the analysis. This survey is preliminary, as many other aspects also deserve attention. But we hope it will help better understanding and forging our community in the global village.", "title": "Rediscovering 25 years of discoveries in spoken language processing: a preliminary ISCA archive analysis"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/fujisaki13_interspeech.html", "abstract": "More than a quarter century ago, I introduced the term and the concept of \u201cspoken language processing\u201d. Spoken language is our primary medium of communication in which the ultimate source is the speaker\u2019s mind and the destination is the listener\u2019s mind. Unlike written language, it is directly linked with human mechanisms and processes both in production and in comprehension. For example, speech sounds are inseparably related to the speaker\u2019s vocal mechanisms, and therefore convey information on the mental and physical states of the speaker to the listener. Studies on spoken language communication thus involve not only linguistics, phonetics, and acoustics, but also psychology, physiology, physics, and in many cases also pathology and pedagogy. It is my belief that we need to study spoken language from all these viewpoints, and share and integrate the findings in our quest for the solution of a problem, regardless of whether it is basic or applied.", "title": "An inter- and cross-disciplinary perspective of spoken language processing"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/moore13_interspeech.html", "abstract": "As many people in the speech technology R&D community are aware, I conducted surveys for the 1997, 2003 and 2009 IEEE Automatic Speech Recognition and Understanding (ASRU) workshops seeking opinions about the future prospects for our field. Respondents were offered a set of statements about possible future events, and their task was to assign a date to each. The results were published at INTERSPEECH 2005 and 2011, and the overall conclusion was that progress is perceived as slow and that the future appeared to be no nearer in 2009 than it did in 1997! Whilst this may be the view of the R&D community, it is not known what the general public feels about our technology, especially given the high visibility of products such as Apple\u2019s \u2019Siri\u2019. A fourth survey has thus been conducted in which non-specialists express their opinions. This talk will review the original surveys and relate them to what ordinary people think.", "title": "Progress and prospects for speech technology: what ordinary people think"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/shaik13_interspeech.html", "abstract": "German is a morphologically rich language having a high degree of word inflections, derivations and compounding. This leads to high out-of-vocabulary (OOV) rates and poor language model (LM) probabilities in the large vocabulary continuous speech recognition (LVCSR) systems. One of the main challenges in the German LVCSR is the recognition of the OOV words. For this purpose, datadriven morphemes are used to provide higher lexical coverage. On the other hand, the probability estimates of a sub-lexical LM could be further improved using feature-rich LMs like maximum entropy (MaxEnt) and class-based LMs. In this work, for a sub-lexical level German LVCSR task, we investigate the use of the multiple morpheme level features as classes for building class-based LMs that are estimated using the state-of-the-art MaxEnt approach. Thus, the benefits of both the MaxEnt LMs and the traditional class-based LMs are effectively combined. Furthermore, we experiment the use of Maximum a-posteriori adaptation over the MaxEnt class-based LMs. We show consistent reductions in both the OOV recognition error rate and the word error rate (WER) on a German LVCSR task from the Quaero project, compared to the traditional class-based and the N-gram morpheme based LM.", "title": "Feature-rich sub-lexical language models using a maximum entropy approach for German LVCSR"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/mousa13_interspeech.html", "abstract": "Performing large vocabulary continuous speech recognition (LVCSR) for morphologically rich languages is considered a challenging task. The morphological richness of such languages leads to high out-of-vocabulary (OOV) rates and poor language model (LM) probabilities. In this case, the use of morphemes has been shown to increase the lexical coverage and lower the LM perplexity. Another approach used to improve the LM probability estimates is to incorporate additional knowledge sources in the LM estimation process using class-based LMs (CLMs). Recently, the hierarchical Pitman-Yor LMs (HPYLMs) have shown superiority over the modified Kneser-Ney (MKN) smoothed N-gram LMs in terms of both perplexity (PPL) and word error rate (WER) on word-based LVCSR tasks. In this paper, hierarchical Pitman-Yor class-based LMs (HPYCLMs) are combined with morpheme level language modeling. This enables the application of the proposed models on top of morpheme-based systems. Experiments are conducted on Arabic and German LVCSR tasks. Consistent performance improvements are obtained for all the available corpora compared to the conventional morphemebased and class-based LMs.", "title": "Morpheme level hierarchical pitman-yor class-based language models for LVCSR of morphologically rich languages"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/lambert13_interspeech.html", "abstract": "We present a discriminatively trained dependency parser-based language model. The model operates on utterances, rather than words, and so can utilize long-distance structural features of each sentence. We train the model discriminatively on n-best lists, using the perceptron algorithm to tune the model weights. Our features include standard n-gram style features, long-distance co-occurrence features, and syntactic structural features. We evaluate this model by re-ranking n-best lists of recognized speech from the Fisher dataset of informal telephone conversations. We compare various combinations of feature types, and methods of training the model.", "title": "Discriminatively trained dependency language modeling for conversational speech recognition"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/si13_interspeech.html", "abstract": "Recurrent Neural Network Language Model (RNNLM) has recently been shown to outperform N-gram Language Models (LM) as well as many other competing advanced LM techniques. However, the training and testing of RNNLM are very time-consuming, so in realtime recognition systems, RNNLM is usually used for re-scoring a limited size of n-best list. In this paper, issues of speeding up RNNLM are explored when RNNLMs are used to re-rank a large nbest list. A new n-best list re-scoring framework, Prefix Tree based N-best list Rescoring (PTNR), is proposed to completely get rid of the redundant computations which make re-scoring ineffective. At the same time, the bunch mode technique, widely used for speeding up the training of feed-forward neural network language model, is investigated to combine with PTNR to further improve the rescoring speed. Experimental results showed that our proposed re-scoring approach for RNNLM was much faster than the standard n-best list re-scoring. Take 1000-best as an example, our approach was almost 11 times faster than the standard n-best list re-scoring.", "title": "Prefix tree based n-best list re-scoring for recurrent neural network language model used in speech recognition system"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/liu13i_interspeech.html", "abstract": "In natural languages the variability in the underlying linguistic generation rules significantly alters the observed surface word sequence they create, and thus introduces a mismatch against other data generated via alternative realizations associated with, for example, a different domain. Hence, direct modelling of out-ofdomain data can result in poor generalization to the in-domain data of interest. To handle this problem, this paper investigated using cross-domain paraphrastic language models to improve in-domain language modelling (LM) using out-of-domain data. Phrase level paraphrase models learnt from each domain were used to generate paraphrase variants for the data of other domains. These were used to both improve the context coverage of in-domain data, and reduce the domain mismatch of the out-of-domain data. Significant error rate reduction of 0.6% absolute was obtained on a stateof- the-art conversational telephone speech recognition task using a cross-domain paraphrastic multi-level LM trained on a billion words of mixed conversational and broadcast news data. Consistent improvements on the in-domain data context coverage were also obtained.", "title": "Cross-domain paraphrasing for improving language modelling using out-of-domain data"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/masumura13_interspeech.html", "abstract": "This paper introduces a new approach that directly uses latent words language models (LWLMs) in automatic speech recognition (ASR). LWLMs are effective against data sparseness because of their soft-decision clustering structure and Bayesian modeling so it can be expected that LWLMs perform robustly in multiple ASR tasks. Unfortunately, implementing a LWLM to ASR is difficult because of its computation complexity. In our previous work, we implemented an approximate LWLM for ASR by sampling words according to a stochastic process and training a word n-gram LMs. However, the previous approach cannot take into account the latent variable sequence behind the recognition hypothesis. To solve this problem, we propose a method based on Viterbi decoding that simultaneously decodes the recognition hypothesis and its latent variable sequence. In the proposed method, we use Gibbs sampling for rapid decoding. Our experiments show the effectiveness of the proposed Viterbi decoding based on n-best rescoring. Moreover, we also investigate the effects on the combination of the previous approximate LWLM and the proposed Viterbi decoding.", "title": "Viterbi decoding for latent words language models using gibbs sampling"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/backstrom13_interspeech.html", "abstract": "Speech coding with the ACELP paradigm is based on a least squares algorithm in a perceptual domain, where the perceptual domain is specified by a filter. This article shows that the computational complexity of this conventional definition of the least squares problem can be reduced by taking into account the impact of the zero impulse response into the next frame. The proposed modification introduces a Toeplitz structure to a correlation matrix appearing in the objective function, which simplifies the structure and reduces computations. It is shown that the proposed method reduces computational complexity up to 17% without reducing perceptual quality.", "title": "Computationally efficient objective function for algebraic codebook optimization in ACELP"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/moller13_interspeech.html", "abstract": "During the transition period from narrowband to wideband speech transmission services, Artificial Bandwidth Extension (ABE) algorithms are able to reduce the perceptual degradation of narrowband-transmitted speech signals by extending the audio bandwidth. In this paper, we analyze whether the resulting speech quality can be predicted reliably with instrumental models. Estimations from the new ITU standard POLQA, its predecessor WBPESQ and the diagnostic DIAL model are compared to subjective listener judgments. This comparison reveals that the instrumental measures are not fully able to cope with ABE-processed speech, particularly in predicting ABE rank orders reliably. Reasons for this finding and corresponding diagnoses are discussed.", "title": "Speech quality prediction for artificial bandwidth extension algorithms"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/xia13b_interspeech.html", "abstract": "A novel speech enhancement method with Weighted Denoising Auto-encoder (WDA) is proposed in this paper. A weighted reconstruction loss function is introduced to the conventional Denoising Auto-encoder (DA), and makes it suitable for the task of speech enhancement. First, the proposed WDA is used to model the relationship between the noisy and clean power spectrums of speech signal. Then, the estimated clean power spectrum is used in the a Posteriori SNR Controlled Recursive Averaging (PCRA) approach for the estimation of the a priori SNR. Finally, the enhanced speech is obtained by Wiener filter operating in the frequency domain. From the test results under ITU-T G.160, in comparison with the reference method, the proposed method could achieve similar amount of noise reduction in both white and colored noise, and the distortion on the level of speech signal is smaller. Also, the objective speech quality is improved in all the test conditions.", "title": "Speech enhancement with weighted denoising auto-encoder"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/cernak13_interspeech.html", "abstract": "Current HMM-based low bit rate speech coding systems work with phonetic vocoders. Pitch contour coding (on frame or phoneme level) is usually fairly orthogonal to other speech coding parameters. We make an assumption in our work that the speech signal contains supra-segmental cues. Hence, we present encoding of the pitch on the syllable level, used in the framework of a recognition/ synthesis speech coder with phonetic vocoder. The results imply that high accuracy pitch contour reconstruction with negligible speech quality degradation is possible. The proposed pitch encoding technique operates on 30.35 bits per second.", "title": "Syllable-based pitch encoding for low bit rate speech coding with recognition/synthesis architecture"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/duy13_interspeech.html", "abstract": "Artificial Bandwidth Extension (ABE) has been introduced to improve perceived speech quality and intelligibility of narrow-band telephone speech. Most of the existing algorithms divided ABE into 2 sub-problems, namely extension of the excitation signal and that of the spectral envelope. In this paper, we propose a new method for spectral envelope extension based on REgularized piecewise linear mapping with DIscriminative region weighting And Longspan features (REDIAL). REDIAL is a revised version of SPLICE, a well-known method for speech enhancement. In REDIAL, however, discriminative model is introduced for space division step of the original SPLICE. The proposed REDIAL-based method approximates non-linear transformation from narrowband features to their wideband counterpart by a summation of piecewise linear transformations. The proposed method was compared with the widely used GMM-based method, through objective and subjective evaluations in both speaker-dependent and speaker-independent conditions. Both evaluations showed that the proposed method significantly outperforms the conventional GMM-based method.", "title": "Artificial bandwidth extension based on regularized piecewise linear mapping with discriminative region weighting and long-Span features"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/lee13d_interspeech.html", "abstract": "In this paper, we propose an improved adaptive muting method using a sigmoid function for the packet loss concealment algorithm of ITU-T G.722 Recommendation. The packet loss concealment algorithm performs an adaptive muting to prevent the generation of unnecessary noise during packet loss recovery. While muting is linearly and discontinuously performed according to packet errors, our muting approach is performed by the non-linear and continuous sigmoid function. The principal parameters of the sigmoid function are obtained based on training at which minimization between the desired signal and the reconstructed signal is performed. Experimental results show that this proposed muting technique can enhance the performance of the packet loss concealment algorithm of G.722 under various packet loss environments.", "title": "Enhanced muting method in packet loss concealment of ITU-t g.722 employing optimized sigmoid function"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/nguyen13b_interspeech.html", "abstract": "A salient aspect of the tone system of Hanoi Vietnamese is its use of phonation-type characteristics. This pilot study investigates intonational variation in the realization of two tones: tone 3 (nga), a rising tone with a strong glottalization in its first half, and tone 6 (nang), which starts on a middle pitch and usually falls dramatically because of a strong glottalization in its second half. This study focuses on how speaker attitude affects the realization of glottalization on two sentence-final particles (SFPs) carrying tones 3 and 6: da [IPA: da3], conveying tense-aspect-modality information, and \"a.\" [IPA: a6], conveying politeness. Audio and electroglottographic recordings from 4 male speakers suggest that glottalization is phased earlier for surprise than for declaration. Irritation also tends to be reflected in earlier glottalization, but with an added glottal constriction at the very end. A methodological challenge is that phonetic realizations of tones 3 and 6 span a wide range of states of the glottis. A procedure is proposed for detecting the complex-repetitive patterns found in cases of lapse into creaky phonation (vocal fry). This helps quantify glottalization phenomena, with a view to arriving at a model that can be used in speech processing.", "title": "The interplay of intonation and complex lexical tones: how speaker attitudes affect the realization of glottalization on vietnamese sentence-final particles"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/nichasaide13_interspeech.html", "abstract": "This paper explores the interplay of source correlates of accentuation, examining a hypothesis (the Voice Prominence Hypothesis) that different source parameters are involved and may serve as equivalent. It predicts that where accentuation is not marked by pitch salience there will be more extensive changes in other source parameters. This follows our assumption that prosodic entities such as accentuation, focus, declination, etc. involve adjustments to the entire voice source and not simply to F0. Twelve 3-accent sentences of Connemara Irish (declaratives, WH questions and Yes/No questions) were analysed. These are typically produced and transcribed as H* H* H*L. Of particular interest were the second accents: although they are heard as accented, there are no particular pitch excursions that would account for their salience. Inverse filtering and subsequent source parameterisation was carried out to yield measures for a range of source parameters. Results support the voice prominence hypothesis: as predicted, the most striking source adjustments were found in the second accent. Even where there is substantial pitch movement (final accent), parameters other than F0 appear to be contributing to the salience of the accented syllable. The precise source changes associated with accentuation varied across sentence types and within the prosodic phrase.", "title": "The voice prominence hypothesis: the interplay of F0 and voice source features in accentuation"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/lee13e_interspeech.html", "abstract": "This study is an attempt to understand the phonetic properties of pitch accent conditions in Japanese as related to the two observed versions of H tones. We tested the hypothesis that the higher version (accented H) results from pre-low raising (PLR) rather than being inherently higher. Correlation analysis reveals an inverse relation between accent peak and the following low tone, and that the strength of such correlations is affected by both peak-toword- end distance (categorical effect) and within-mora time pressure (gradient), but the two effects work in opposite directions. We take this as evidence that the former effect is due to mora-level pre-planning while the latter is mechanical. These results suggest that in Japanese a low pitch target raises the preceding high target through anticipatory dissimilation. The findings of this study extend our previous understanding of the mechanisms of pitch production.", "title": "Mora-based pre-low raising in Japanese pitch accent"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/lvenbruck13_interspeech.html", "abstract": "Verbal irony is characterized by the use of specific acoustic modulations, especially global prosodic cues as well as vowel hyperarticulation. Little is known concerning the expression of sarcastic speech in French. Here we report on global prosodic features of sarcastic speech in a corpus of declarative French utterances. Our data show that sarcastic productions are characterized by utterance lengthening, by increased F0 modulations and a global raising of the pitch level and range. The results are discussed in the light of results on the acoustic features of ironic speech in languages other than French.", "title": "Prosodic cues of sarcastic speech in French: slower, higher, wider"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/menard13_interspeech.html", "abstract": "The role of vision in speech representation was investigated in congenitally blind speakers and sighted speakers by studying the correlates of contrastive focus, a prosodic condition in which phonemic contrasts are enhanced. It has been reported that the lips (visible articulators) are less involved in implementing the rounding feature for blind speakers. If the weight of visible gestures in speech representation is reduced in blind speakers, they should show different strategies to mark focus-induced prominence. Nine congenitally blind French speakers and nine sighted French speakers were recorded while uttering sentences in neutral and contrastive focus conditions. Internal lip area, upper lip protrusion, and acoustic values (formants, fundamental frequency, duration, and intensity) were measured. In the acoustic domain, both groups signaled focus using comparable values of fundamental frequency, intensity, and duration. Formant values in sighted speakers were more affected by the prosodic condition. In the articulatory domain, sighted speakers significantly increased lip area in the contrastive focus condition compared to the neutral condition, while blind speakers did not. These results suggest that implementation of prosodic focus is affected by congenital visual deprivation.", "title": "Correlates of contrastive focus in congenitally blind adults and sighted adults"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/georgeton13_interspeech.html", "abstract": "This paper examines whether lip protrusion is affected by initial strengthening of rounded vowels in French. Three speakers produced sentences containing the vowels /i, e, E, a, y, o, oe, u, o, O/ at the beginning of different prosodic constituents: Intonational Phrase (IP), Accentual Phrase (AP) and Word (W). Using video and Qualisys motion capture data, three parameters (lip protrusion, area/lip protrusion and lip protrusion/area) are compared to characterize the labial configuration, first between [.round] and [+round] vowels, then within [+round] vowels. These three parameters are found to discriminate between [+/.rounded], to reflect some within category distinction and to show speaker-specific labial patterns. Concerning the effect of prosodic boundary, only the combined measurement of lip protrusion/area reveals a speaker dependent effect on prosodic boundaries. Two speakers out of three show a tendency to decrease lip protrusion/area in IP initial position for back rounded vowels (/o/ and /O/) and for /y/. Prosodic boundary effects are discussed in terms of feature enhancement.", "title": "Is protrusion of French rounded vowels affected by prosodic positions?"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/cooke13_interspeech.html", "abstract": "Speech output is used extensively, including in situations where correct message reception is threatened by adverse listening conditions. Recently, there has been a growing interest in algorithmic modifications that aim to increase the intelligibility of both natural and synthetic speech when presented in noise. The Hurricane Challenge is the first large-scale open evaluation of algorithms designed to enhance speech intelligibility. Eighteen systems operating on a common data set were subjected to extensive listening tests and compared to unmodified natural and text-to-speech (TTS) baselines. The best-performing systems achieved gains over unmodified natural speech of 4.4 and 5.1 dB in competing speaker and stationary noise respectively, while TTS systems made gains of 5.6 and 5.1 dB over their baseline. Surprisingly, for most conditions the largest gains were observed for noise-independent algorithms, suggesting that performance in this task can be further improved by exploiting information in the masking signal.", "title": "Intelligibility-enhancing speech modifications: the hurricane challenge"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/erro13_interspeech.html", "abstract": "This paper describes a statistical parametric speech synthesizer that, despite having been trained on an ordinary synthesis database and without any adaptation data, is able to generate highly intelligible speech in noisy environments. By using a simple and flexible vocoder based on a harmonic model, it applies several noiseindependent modifications to durations, pitch level and range, energy contour, formant sharpness, and intensity of particular spectral bands. The system has been evaluated by means of a large subjective test, the results of which show that the suggested approach clearly outperforms the reference TTS systems and even unmodified natural speech in some conditions", "title": "Statistical synthesizer with embedded prosodic and spectral modifications to generate highly intelligible speech in noise"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/suni13_interspeech.html", "abstract": "This paper describes modification of a TTS system for improving the intelligibility of speech in various noise conditions. First, the GlottHMM vocoder is used for training a voice with modal speech data. The vocoder and voice parameters are then modified to mimic the properties of Lombard effect based on a small amount of Lombard speech from the same speaker. More specifically, the durations are increased, fundamental frequency is raised, spectral tilt is decreased, the harmonic-to-noise ratio is increased, and a pressed glottal flow pulses are used in creating excitation. The formants of the speech are also enhanced and finally the speech is compressed in order to increase noise robustness of the voice. The evaluation results of the Hurricane Challenge 2013 indicate that the modified voice is mostly less intelligible than the unmodified natural speech, as expected, but more intelligible than the reference TTS voice, especially in the low SNR conditions.", "title": "Lombard modified text-to-speech synthesis for improved intelligibility: submission for the hurricane challenge 2013"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/valentinibotinhao13_interspeech.html", "abstract": "This paper presents our entry to a speech-in-noise intelligibility enhancement evaluation: the Hurricane Challenge. The system consists of a Text-To-Speech voice manipulated through a combination of enhancement strategies, each of which is known to be individually successful: a perceptually-motivated spectral shaper based on the Glimpse Proportion measure, dynamic range compression, and adaptation to Lombard excitation and duration patterns. We achieved substantial intelligibility improvements relative to unmodified synthetic speech: 4.9 dB in competing speaker and 4.1 dB in speech-shaped noise. An analysis conducted across this and other two similar evaluations shows that the spectral shaper and the compressor (both of which are loudness boosters) contribute most under higher SNR conditions, particularly for speech-shaped noise. Duration and excitation Lombard-adapted changes are more beneficial in lower SNR conditions, and for competing speaker noise.", "title": "Combining perceptually-motivated spectral shaping with loudness and duration modification for intelligibility enhancement of HMM-based synthetic speech in noise"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/godoy13c_interspeech.html", "abstract": "In order to make speech (natural or synthetic) more intelligible for listeners in real-world noisy environments, various modifications have been proposed that exploit spectral and temporal signal features. Previously, an evaluation campaign involving several approaches illustrated that a Spectral Shaping (SS) and Dynamic Range Compression (DRC) method proved highly successful at increasing speech intelligibility. For the public follow-up campaign (i.e., the Hurricane Challenge), this work introduces additional modifications into SSDRC in an attempt to further enhance intelligibility. First aiming to slow down the articulation rate, the speech is uniformly time stretched to effectively increase signal redundancy. Second, a frequency warping mechanism to expand vowel space is incorporated into the SS. Third, scaling to enhance the transient regions of speech is applied in the time-domain along with DRC. Objective and extensive subjective (i.e., the Hurricane Challenge) evaluations show that the new approach successfully achieves intelligibility gains over natural speech for all of the noise conditions evaluated, though compared to SSDRC, there is less advantage observed at higher SNR.", "title": "Increasing speech intelligibility via spectral shaping with frequency warping and dynamic range compression plus transient enhancement"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/schepker13_interspeech.html", "abstract": "In this contribution, a new preprocessing algorithm to improve speech intelligibility in noise is proposed, which maintains the signal power before and after processing. The proposed AdaptDRC algorithm consists of two time- and frequency-dependent stages, which are both functions of the estimated SII. The first stage applies a time- and frequency-dependent amplification, while the second stage applies a time- and frequency-dependent dynamic range compression (DRC). Experiments with a competing speaker (CS) and a speech-shaped noise (SSN) show an increase in speech intelligibility for a wide range of SNRs for four different objective measures that are correlated with speech intelligibility. Listening tests conducted within the framework of the Hurricane Challenge with 175 subjects confirm these findings and show improvements of up to 20.5% in intelligibility for SSN and 12.3% for CS.", "title": "Improving speech intelligibility in noise by SII-dependent preprocessing using frequency-dependent amplification and dynamic range compression"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/taal13_interspeech.html", "abstract": "A linear time-invariant filter is designed in order to improve speech understanding when the speech is played back in a noisy environment. To accomplish this, the speech intelligibility index (SII) is maximized under the constraint that the speech energy is held constant. A nonlinear approximation is used for the SII such that a closed-form solution exists to the constrained optimization problem. The resulting filter is dependent both on the long-term average noise and speech spectrum and the global SNR and, in general, has a high-pass characteristic. In contrast to existing methods, the proposed filter sets certain frequency bands to zero when they do not contribute to intelligibility anymore. Experiments show large intelligibility improvements with the proposed method when used in stationary speech-shaped noise. However, it was also found that the method does not perform well for speech corrupted by a competing speaker. This is due to the fact that the SII is not a reliable intelligibility predictor for fluctuating noise sources. MATLAB code is provided.", "title": "SII-based speech preprocessing for intelligibility improvement in noise"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/zhang13e_interspeech.html", "abstract": "Existing algorithms for improving speech intelligibility in a noisy environment generally focus on modifying the acoustic features of live, recorded or synthesized speech while preserving the phonetic composition (the message). In this paper, we present an algorithm for text-to-speech systems that operates at a higher level of abstraction, the message-level. We use a paraphrasing system to adjust the linguistic content of the intended message such that the speech intelligibility improves under noisy conditions. To distinguish the intelligibility among paraphrases, we use the numerical integration of a normalized log-likelihood function over different signal-to-noise conditions. Objective evaluation results show that the developed measure is able to distinguish the intelligibility among paraphrases. Results from subjective evaluation confirm the effectiveness of our objective measure.", "title": "Rephrasing-based speech intelligibility enhancement"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/aubanel13_interspeech.html", "abstract": "How can speech be retimed so as to maximise its intelligibility in the face of competing speech? We present a general strategy which modifies local speech rate to minimise overlap with a known fluctuating masker. Continuous time-scale factors are derived in an optimisation procedure which seeks to minimise overall energetic masking of the speech by the masker while additionally unmasking those speech regions potentially most important for speech recognition. Intelligibility increases are evaluated with both objective and subjective measures and show significant gains over an unmodified baseline, with larger benefits at lower signal-to-noise ratios. The retiming approach does not lead to benefits for speech mixed with stationary maskers, suggesting that the gains observed for the fluctuating masker are not simply due to durational expansion.", "title": "Information-preserving temporal reallocation of speech in the presence of fluctuating maskers"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/petkov13_interspeech.html", "abstract": "We propose a method for the enhancement of intelligibility in scenarios where speech is rendered in a noisy environment. The method is based on the hypothesis that intelligibility is a monotonic function of the degree of preservation of the speech spectral dynamics. The accuracy of the speech spectral dynamics can then be traded against the power of the rendered speech signal. We can either maximize the dynamics accuracy given the signal power, or minimize the signal power given the dynamics accuracy. In our implementation, the spectral dynamics is quantified as the difference of the mel cepstra between time frames of the speech signal. We compared the speech rendered by our implementation against both natural speech and a reference method, for the scenario where signal power is minimized given a target dynamics accuracy, and observed a significantly improved intelligibility. The low system delay, and the low complexity and memory requirements make the new method particularly suitable for real-time applications.", "title": "Preservation of speech spectral dynamics enhances intelligibility"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/brouckxon13_interspeech.html", "abstract": "This paper describes the SINCoFETS entry for the Hurricane challenge, in which intelligibility enhancement algorithms for speech presentation in noise are compared. The proposed system combines noise-independent non-uniform time scaling and dynamics compression algorithms with noise-dependent frequency equalization to improve the robustness of speech intelligibility against noise. The algorithms in the system are described and a short discussion of the results is given.", "title": "An overview of the VUB entry for the 2013 hurricane challenge"}, {"url": "https://www.isca-speech.org/archive/interspeech_2013/takou13_interspeech.html", "abstract": "This report describes a method of speech modification to improve the intelligibility of speech. The spectral energy is reallocated by maintaining and emphasizing acoustical features important for speech perception. The method consists of three components: flattening of the spectral tilt, enhancement of the spectral contrast, and holding of the low-frequency region. The results of this speech intelligibility test show that this method improves the speech intelligibility for all conditions of noise and SNR employed in the test.", "title": "Improvement of speech intelligibility by reallocation of spectral energy"}, {"url": null, "abstract": null, "title": "Keynotes"}, {"url": null, "abstract": null, "title": "Systems for Search/Retrieval of Speech Documents"}, {"url": null, "abstract": null, "title": "Speech Analysis I-IV"}, {"url": null, "abstract": null, "title": "Language and Dialect Recognition"}, {"url": null, "abstract": null, "title": "ASR \u2014 Neural Networks"}, {"url": null, "abstract": null, "title": "Speech Acoustics"}, {"url": null, "abstract": null, "title": "Paralinguistic Challenge (Special Session)"}, {"url": null, "abstract": null, "title": "Perception of Prosody"}, {"url": null, "abstract": null, "title": "Prosody, Phonetics of Language Varieties"}, {"url": null, "abstract": null, "title": "Speech Synthesis I. II"}, {"url": null, "abstract": null, "title": "Perception, Dialectal Differences"}, {"url": null, "abstract": null, "title": "Speech Enhancement \u2014 Single Channel"}, {"url": null, "abstract": null, "title": "Dialog Modeling"}, {"url": null, "abstract": null, "title": "ASR \u2014 Lexical, Prosodic and Cross/Multi-Lingual"}, {"url": null, "abstract": null, "title": "Phonetic Convergence"}, {"url": null, "abstract": null, "title": "Speech Production, Acquisition and Development I, II"}, {"url": null, "abstract": null, "title": "General Topics in ASR"}, {"url": null, "abstract": null, "title": "Voice Activity Detection and Speech Segmentation"}, {"url": null, "abstract": null, "title": "Show and Tell Sessions 1-3"}, {"url": null, "abstract": null, "title": "Discourse, Intonation, Prosody"}, {"url": null, "abstract": null, "title": "Source Separation"}, {"url": null, "abstract": null, "title": "Paralinguistic Information I, II"}, {"url": null, "abstract": null, "title": "ASR \u2014 Robustness Against Noise I-III"}, {"url": null, "abstract": null, "title": "Neural Basis of Speech Perception"}, {"url": null, "abstract": null, "title": "Spoofing and Countermeasures for Automatic Speaker Verification (Special Session)"}, {"url": null, "abstract": null, "title": "Metadata, Evaluation and Resources I, II"}, {"url": null, "abstract": null, "title": "Speech Technology for Speech and Hearing Disorders I, II"}, {"url": null, "abstract": null, "title": "Discriminative Training Methods for Language Modeling"}, {"url": null, "abstract": null, "title": "ASR \u2014 Adaptive Training"}, {"url": null, "abstract": null, "title": "Speech Acquisition and Development"}, {"url": null, "abstract": null, "title": "Articulatory Data Acquisition and Processing (Special Session)"}, {"url": null, "abstract": null, "title": "Topics in Speech Perception and Emotion"}, {"url": null, "abstract": null, "title": "Discourse and Machine Learning, Paralinguistic and Nonlinguistic Cues"}, {"url": null, "abstract": null, "title": "Language Identification, Speaker Diarization"}, {"url": null, "abstract": null, "title": "Speech Synthesis \u2014 Prosody and Emotion"}, {"url": null, "abstract": null, "title": "Spoken Language Information Retrieval"}, {"url": null, "abstract": null, "title": "Speaker Recognition I, II"}, {"url": null, "abstract": null, "title": "Multimodal Speech Perception"}, {"url": null, "abstract": null, "title": "ASR \u2014 Feature Extraction"}, {"url": null, "abstract": null, "title": "ASR \u2014 Pronunciation, Prosodic and New Paradigms"}, {"url": null, "abstract": null, "title": "Dialog Systems"}, {"url": null, "abstract": null, "title": "ASR \u2014 Pronunciation Variants and Modeling"}, {"url": null, "abstract": null, "title": "Speaker Recognition Evaluation"}, {"url": null, "abstract": null, "title": "Physiology and Models of Speech Production"}, {"url": null, "abstract": null, "title": "Speech Science in End-User Applications"}, {"url": null, "abstract": null, "title": "Perception of Non Native Sounds"}, {"url": null, "abstract": null, "title": "Speech Disorders \u2014 Data and Methodology"}, {"url": null, "abstract": null, "title": "Search and Computational Issues in LVCSR"}, {"url": null, "abstract": null, "title": "Speech and Hearing Disorders"}, {"url": null, "abstract": null, "title": "Speech and Audio Segmentation"}, {"url": null, "abstract": null, "title": "Speech Synthesis \u2014 Various Topics"}, {"url": null, "abstract": null, "title": "ASR \u2014 Discriminative Training"}, {"url": null, "abstract": null, "title": "L2 Acquisition, Multilingualism"}, {"url": null, "abstract": null, "title": "Child Computer Interaction (Special Session)"}, {"url": null, "abstract": null, "title": "Dialog Systems and Applications I, II"}, {"url": null, "abstract": null, "title": "Spoken Machine Translation and Speech Natural Language Processing I, II"}, {"url": null, "abstract": null, "title": "Language Model Adaptation"}, {"url": null, "abstract": null, "title": "Spoken Language Summarization and Understanding"}, {"url": null, "abstract": null, "title": "Speech Synthesis \u2014 Multimodal and Articulatory Synthesis"}, {"url": null, "abstract": null, "title": "Speaker Diarization and Recognition"}, {"url": null, "abstract": null, "title": "Models of Speech Perception"}, {"url": null, "abstract": null, "title": "Speech and Audio Signal Processing"}, {"url": null, "abstract": null, "title": "Linguistic Systems, Phonetics-Phonology Interface"}, {"url": null, "abstract": null, "title": "Speech Synthesis \u2014 Voice Conversion"}, {"url": null, "abstract": null, "title": "Large Vocabulary Continuous Speech Recognition Systems"}, {"url": null, "abstract": null, "title": "Robust Speaker Recognition I, II"}, {"url": null, "abstract": null, "title": "Acoustic and Articulatory Cues in Speech Perception"}, {"url": null, "abstract": null, "title": "Speech Production \u2014 Data and Models"}, {"url": null, "abstract": null, "title": "Speech Enhancement"}, {"url": null, "abstract": null, "title": "ASR \u2014 Acoustic Modeling"}, {"url": null, "abstract": null, "title": "Special Event: ESCA/ISCA Anniversary"}, {"url": null, "abstract": null, "title": "Language Modeling for Conversational Speech"}, {"url": null, "abstract": null, "title": "Speech Enhancement and Coding"}, {"url": null, "abstract": null, "title": "Articulatory and Acoustic Cues of Speech Prosody"}, {"url": null, "abstract": null, "title": "Intelligibility-Enhancing Speech Modifications (Special Session)"}]