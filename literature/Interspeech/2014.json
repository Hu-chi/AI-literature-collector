[{"url": "https://www.isca-speech.org/archive/interspeech_2014/cutler14_interspeech.html", "abstract": "We human language users learn about speech all our lives. In fact if the beginning of our individual life is taken to be the moment of our birth, we learn for more than our whole lives, because even prior to birth we acquire much basic information about language sound structure. Then in early life, before we are capable of uttering any recognizable words, we have already learned a huge amount about the sounds and words of our language; furthermore, the efficiency with which we learn this is predictive of our later linguistic facility. We learn to process speech in the way best suited to the particular language (or languages) we acquire as children. This leads to a formidable efficiency and robustness in native-language processing, but it actually inhibits learning in the case of other languages we encounter only later \u2014 children are clearly very much better at learning a new language than adults are! Learning about speech nonetheless continues throughout life, in particular the everyday perceptual learning that enables us to adapt our speech processing to newly encountered talkers, and to adjust our own pronunciation in keeping with pronunciation changes in our speech community across time. This learning (at least in the native language) can draw on a wide variety of information sources, is fully in place in childhood, and is apparently unattenuated in older language users.", "title": "Learning about speech"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/liu14_interspeech.html", "abstract": "With the increasing ubiquity and power of mobile devices, as well as the prevalence of social media, more and more activities in our daily life are being recorded, tracked, and shared, creating the notion of \u201csocial media\u201d. Such abundant and still growing real life data, known as \u201cbig data\u201d, provide a tremendous research opportunity in many fields. To analyze, learn and understand such user-generated big data, machine learning has been an important tool and various machine learning algorithms have been developed. However, since the user-generated big data is the outcome of users' decisions, actions and their socio-economic interactions, which are highly dynamic, without considering users' local behaviors and interests, existing learning approaches tend to focus on optimizing a global objective function at the macroeconomic level, while totally ignore users' local decisions at the microeconomic level. As such there is a growing need in bridging machine/social learning with strategic decision making, which are two traditionally distinct research disciplines, to be able to jointly consider both global phenomenon and local effects to understand/model/analyze better the newly arising issues in the emerging social media. In this talk, we present the notion of \u201cdecision learning\u201d that can involve users' behaviors and interactions by combining learning with strategic decision making. We will discuss some examples from social media with real data to show how decision learning can be used to better analyze users' optimal decision from a user' perspective as well as design a mechanism from the system designer's perspective to achieve a desirable outcome.", "title": "Decision learning in data science: where John Nash meets social media"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/lamel14_interspeech.html", "abstract": "Speech processing encompasses a variety of technologies that automatically process speech for some downstream processing. These technologies include identifying the language or dialect spoken, the person speaking, what is said and how it is said. The downstream processing may be limited to a transcription or to a transcription enhanced with additional metadata, or may be used to carry out an action or interpreted within a spoken dialog system or more generally for analytics. With the availability of large spoken multimedia or multimodal data there is growing interest in using such technologies to provide structure and random access to particular segments. Automatic tools can also serve to annotate large corpora for exploitation in linguistic studies of spoken language, such as acoustic-phonetics, pronunciation variation and diachronic evolution, permitting the validation of hypotheses and models. In this talk I will present some of my experience with speech processing in multiple languages, drawing upon progress in the context of several research projects, most recently the Quaero program and the IARPA Babel program, both of which address the development of technologies in a variety of languages, with the aim to some highlight recent research directions and challenges.", "title": "Language diversity: speech processing in a multi-lingual context"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/wang14_interspeech.html", "abstract": "In contrast to other species, humans are unique in having developed thousands of diverse languages which are not mutually intelligible. However, any infant can learn any language with ease, because all languages are based upon common biological infrastructures of sensori-motor, memorial, and cognitive faculties. While languages may differ significantly in the sounds they use, the overall organization is largely the same. It is divided into a discrete segmental system for building words and a continuous prosodic system for expressing, phrasing, attitudes, and emotions. Within this organization, I will discuss a class of languages called `tone languages', which makes special use of F0 to build words. Although the best known of these is Chinese, tone languages are found in many parts of the world, and operate on different principles. I will also comment on relations between sound patterns in language and sound patterns in music, the two worlds of sound universal to our species.", "title": "Sound patterns in language"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/deng14_interspeech.html", "abstract": "Artificial neural networks have been around for over half a century and their applications to speech processing have been almost as long, yet it was not until year 2010 that their real impact had been made by a deep form of such networks, built upon part of the earlier work on (shallow) neural nets and (deep) graphical models developed by both speech and machine learning communities. This keynote will first reflect on the path to this transformative success, sparked by speech analysis using deep learning methods on spectrogram-like raw features and then progressing rapidly to speech recognition with increasingly larger vocabularies and scale. The role of well-timed academic-industrial collaboration will be highlighted, so will be the advances of big data, big compute, and the seamless integration between the application-domain knowledge of speech and general principles of deep learning. Then, an overview will be given on sweeping achievements of deep learning in speech recognition since its initial success in 2010 (as well as in image recognition and computer vision since 2012). Such achievements have resulted in across-the-board, industry-wide deployment of deep learning. The final part of the talk will look ahead towards stimulating new challenges of deep learning \u2014 making intelligent machines capable of not only hearing (speech) and seeing (vision), but also of thinking with a \u201cmind\u201d; i.e. reasoning and inference over complex, hierarchical relationships and knowledge sources that comprise a vast number of entities and semantic concepts in the real world based in part on multisensory data from the user. To this end, language and multimodal processing \u2014 joint exploitation and learning from text, speech/audio, and image/video \u2014 is evolving into a new frontier of deep learning, beginning to be embraced by a mixture of research communities including speech and spoken language processing, natural language processing, computer vision, machine learning, information retrieval, cognitive science, artificial intelligence, and data/knowledge management. A review of recent published studies will be provided on deep learning applied to selected language and multimodal processing tasks, with a trace back to the relevant early connectionist modeling and neural network literature and with future directions in this new exciting deep learning frontier discussed and analyzed.", "title": "Achievements and challenges of deep learning \u2014 from speech analysis and recognition to language and multimodal processing"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/zhang14_interspeech.html", "abstract": "In this paper, we explore multilingual feature-level data sharing via Deep Neural Network (DNN) stacked bottleneck features. Given a set of available source languages, we apply language identification to pick the language most similar to the target language, for more efficient use of multilingual resources. Our experiments with IARPA-Babel languages show that bottleneck features trained on the most similar source language perform better than those trained on all available source languages. Further analysis suggests that only data similar to the target language is useful for multilingual training.", "title": "Language ID-based training of multilingual stacked bottleneck features"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/do14_interspeech.html", "abstract": "Conventional acoustic models, such as Gaussian mixture models (GMM) or deep neural networks (DNN), cannot be reliably estimated when there are very little speech training data, e.g. less than 1 hour. In this paper, we investigate the use of a non-parametric kernel density estimation method to predict the emission probability of HMM states. In addition, we introduce a discriminative score calibrator to improve the speech class posteriors generated by the kernel density for speech recognition task. Experimental results on the Wall Street Journal task show that the proposed acoustic model using cross-lingual bottleneck features significantly outperforms GMM and DNN models for limited training data case.", "title": "Kernel density-based acoustic model with cross-lingual bottleneck features for resource limited LVCSR"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/vu14_interspeech.html", "abstract": "This paper presents our latest investigation of automatic speech recognition (ASR) on non-native speech. We first report on a non-native speech corpus \u2014 an extension of the GlobalPhone database \u2014 which contains English with Bulgarian, Chinese, German and Indian accent and German with Chinese accent. In this case, English is the spoken language ( L2) and Bulgarian, Chinese, German and Indian are the mother tongues ( L1) of the speakers. Afterwards, we investigate the effect of multilingual acoustic modeling on non-native speech. Our results reveal that a bilingual L1-L2 acoustic model significantly improves the ASR performance on non-native speech. For the case that L1 is unknown or L1 data is not available, a multilingual ASR system trained without L1 speech data consistently outperforms the monolingual L2 ASR system. Finally, we propose a method called crosslingual accent adaptation, which allows using English with Chinese accent to improve the German ASR on German with Chinese accent and vice versa. Without using any intra lingual adaptation data, we achieve 15.8% relative improvement in average over the baseline system.", "title": "Improving ASR performance on non-native speech using multilingual and crosslingual information"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/knill14_interspeech.html", "abstract": "Developing high-performance speech processing systems for low-resource languages is very challenging. One approach to address the lack of resources is to make use of data from multiple languages. A popular direction in recent years is to train a multi-language bottleneck DNN. Language dependent and/or multi-language (all training languages) Tandem acoustic models (AM) are then trained. This work considers a particular scenario where the target language is unseen in multi-language training and has limited language model training data, a limited lexicon, and acoustic training data without transcriptions. A zero acoustic resources case is first described where a multi-language AM is directly applied, as a language independent AM (LIAM), to an unseen language. Secondly, in an unsupervised approach a LIAM is used to obtain hypotheses for the target language acoustic data transcriptions which are then used in training a language dependent AM. 3 languages from the IARPA Babel project are used for assessment: Vietnamese, Haitian Creole and Bengali. Performance of the zero acoustic resources system is found to be poor, with keyword spotting at best 60% of language dependent performance. Unsupervised language dependent training yields performance gains. For one language (Haitian Creole) the Babel target is achieved on the in-vocabulary data.", "title": "Language independent and unsupervised acoustic models for speech recognition and keyword spotting"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/bell14_interspeech.html", "abstract": "Posterior-based or bottleneck features derived from neural networks trained on out-of-domain data may be successfully applied to improve speech recognition performance when data is scarce for the target domain or language. In this paper we combine this approach with the use of a hierarchical deep neural network (DNN) network structure \u2014 which we term a multi-level adaptive network (MLAN) \u2014 and the use of multitask learning. We have applied the technique to cross-lingual speech recognition experiments on recordings of TED talks and European Parliament sessions in English (source language) and German (target language). We demonstrate that the proposed method can lead to improvements over standard methods, even when the quantity of training data for the target language is relatively high. When the complete method is applied, we achieve relative WER reductions of around 13% compared to a monolingual hybrid DNN baseline.", "title": "Cross-lingual adaptation with multi-task adaptive networks"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/razavi14_interspeech.html", "abstract": "Despite various advances in automatic speech recognition (ASR) technology, recognition of speech uttered by non-native speakers is still a challenging problem. In this paper, we investigate the role of different factors such as type of lexical model and choice of acoustic units in recognition of speech uttered by non-native speakers. More precisely, we investigate the influence of the probabilistic lexical model in the framework of Kullback-Leibler divergence based hidden Markov model (KL-HMM) approach in handling pronunciation variabilities by comparing it against hybrid HMM/artificial neural network (ANN) approach where the lexical model is deterministic. Moreover, we study the effect of acoustic units (being context-independent or clustered context-dependent phones) on ASR performance in both KL-HMM and hybrid HMM/ANN frameworks. Our experimental studies on French part of MediaParl as a bilingual corpus indicate that the probabilistic lexical modeling approach in the KL-HMM framework can capture the pronunciation variations present in non-native speech effectively. More precisely, the experimental results show that the KL-HMM system using context-dependent acoustic units and trained solely on native speech data can lead to better ASR performance than adaptation techniques such as maximum likelihood linear regression.", "title": "On recognition of non-native speech using probabilistic lexical model"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/tanaka14_interspeech.html", "abstract": "An electrolarynx is a device that artificially generates excitation sounds to enable laryngectomees to produce electrolaryngeal (EL) speech. Although proficient laryngectomees can produce quite intelligible EL speech, it sounds very unnatural due to the mechanical excitation produced by the device. To address this issue, we have proposed several EL speech enhancement methods using statistical voice conversion and showed that statistical prediction of excitation parameters, such as F0 patterns, was essential to significantly improve naturalness of EL speech. In these methods, the original EL speech is recorded with a microphone and the enhanced EL speech is presented from a loudspeaker in real time. This framework is effective for telecommunication but it is not suitable to face-to-face conversation because both the original EL speech and the enhanced EL speech are presented to listeners. In this paper, we propose direct F0 control of the electrolarynx based on statistical excitation prediction to develop an EL speech enhancement technique also effective for face-to-face conversation. F0 patterns of excitation signals produced by the electrolarynx are predicted in real time from the EL speech produced by the laryngectomee's articulation of the excitation signals with previously predicted F0 values. A simulation experiment is conducted to evaluate the effectiveness of the proposed method. The experimental results demonstrate that the proposed method yields significant improvements in naturalness of EL speech while keeping its intelligibility high enough.", "title": "Direct F0 control of an electrolarynx based on statistical excitation feature prediction and its evaluation through simulation"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/niekerk14_interspeech.html", "abstract": "A complete intonation model based on quantitative target approximation is described for Yor\u00f9b\u00e1 text-to-speech (TTS) synthesis. This model is evaluated analytically and perceptually and compared to a fundamental frequency (F0) model using the standard HTS implementation. Analytical results suggest that the proposed approach more efficiently models F0 contours given typical data constraints in under-resourced environments and perceptual results comparing the proposed model with HTS are encouraging.", "title": "A target approximation intonation model for yor\u00f9b\u00e1 TTS"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/vadapalli14_interspeech.html", "abstract": "Phrase break prediction is the first step in modeling prosody for text-to-speech systems (TTS). Traditional methods of phrase break prediction have used discrete linguistic representations (like POS tags, induced POS tags, word-terminal syllables) for modeling these breaks. However these discrete representations suffer from a number of issues such as fixing the number of discrete classes and also such a representation does not capture the co-occurrence statistics of the words. As a result, the use of continuous valued word representation was proposed in literature. In this paper, we propose a neural network dictionary learning architecture to induce task specific continuous valued word representations, and show that these task specific features perform better at phrase break prediction as compared to continuous features derived using Latent Semantic Analysis (LSA).", "title": "Learning continuous-valued word representations for phrase break prediction"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/che14_interspeech.html", "abstract": "Previous researches indicated that the performance of automatic prosodic boundary labeling benefited from syntactic phrase information for Mandarin. However, the influence of other syntactic features such as dependency has not been studied in-depth yet, especially on large scale corpus. This paper demonstrates the usefulness of rich syntactic features for Mandarin phrase boundary prediction. Both syntactic phrase and dependency features are considered in our methods. The experimental results show that rich syntactic features improve the performance of prosodic boundary prediction effectively.", "title": "Improving Mandarin prosodic boundary prediction with rich syntactic features"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/dall14_interspeech.html", "abstract": "Filled pauses are pervasive in conversational speech and have been shown to serve several psychological and structural purposes. Despite this, they are seldom modelled overtly by state-of-the-art speech synthesis systems. This paper seeks to motivate the incorporation of filled pauses into speech synthesis systems by exploring their use in conversational speech, and by comparing the performance of several automatic systems inserting filled pauses into fluent text. Two initial experiments are described which seek to determine whether people's predicted insertion points are consistent with actual practice and/or with each other. The experiments also investigate whether there are `right' and `wrong' places to insert filled pauses. The results show good consistency between people's predictions of usage and their actual practice, as well as a perceptual preference for the `right' placement. The third experiment contrasts the performance of several automatic systems that insert filled pauses into fluent sentences. The best performance (determined by F-score) was achieved through the by-word interpolation of probabilities predicted by Recurrent Neural Network and 4gram Language Models. The results offer insights into the use and perception of filled pauses by humans, and how automatic systems can be used to predict their locations.", "title": "Investigating automatic & human filled pause insertion for speech synthesis"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/dall14b_interspeech.html", "abstract": "It has been shown that in natural speech filled pauses can be beneficial to a listener. In this paper, we attempt to discover whether listeners react in a similar way to filled pauses in synthetic and vocoded speech compared to natural speech. We present two experiments focusing on reaction time to a target word. In the first, we replicate earlier work in natural speech, namely that listeners respond faster to a target word following a filled pause than following a silent pause. This is replicated in vocoded but not in synthetic speech. Our second experiment investigates the effect of speaking rate on reaction times as this was potentially a confounding factor in the first experiment. Evidence suggests that slower speech rates lead to slower reaction times in synthetic and in natural speech. Moreover, in synthetic speech the response to a target word after a filled pause is slower than after a silent pause. This finding, combined with an overall slower reaction time, demonstrates a shortfall in current synthesis techniques. Remedying this could help make synthesis less demanding and more pleasant for the listener, and reaction time experiments could thus provide a measure of improvement in synthesis techniques.", "title": "The effect of filled pauses and speaking rate on speech comprehension in natural, vocoded and synthetic speech"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/khoury14_interspeech.html", "abstract": "Any biometric recognizer is vulnerable to direct spoofing attacks and automatic speaker verification (ASV) is no exception; replay, synthesis and conversion attacks all provoke false acceptances unless countermeasures are used. We focus on voice conversion (VC) attacks. Most existing countermeasures use full knowledge of a particular VC system to detect spoofing. We study a potentially more universal approach involving generative modeling perspective. Specifically, we adopt standard i-vector representation and probabilistic linear discriminant analysis (PLDA) back-end for joint operation of spoofing attack detector and ASV system. As a proof of concept, we study a vocoder-mismatched ASV and VC attack detection approach on the NIST 2006 speaker recognition evaluation corpus. We report stand-alone accuracy of both the ASV and countermeasure systems as well as their combination using score fusion and joint approach. The method holds promise.", "title": "Introducing i-vectors for joint anti-spoofing and speaker verification"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/leary14_interspeech.html", "abstract": "This paper describes a system for indexing acoustic feature vectors for large-scale speaker search using random projections. Given one or more target feature vectors, large-scale speaker search enables returning similar vectors (in a nearest-neighbors fashion) in sublinear time. The speaker feature space is comprised of i-vectors, derived from Gaussian Mixture Model supervectors. The index and search algorithm is derived from locality sensitive hashing with novel approaches in neighboring bin approximation for improving the miss rate at specified false alarm thresholds. The distance metric for determining the similarity between vectors is the cosine distance. This approach significantly reduced the search space by 70% with minimal increase in miss rate. When combined with further dimensionality reduction, a reduction of the search space by over 90% is also possible. All experiments are based on the NIST SRE 2010 evaluation.", "title": "Random projections for large-scale speaker search"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/fredouille14_interspeech.html", "abstract": "Inspired from the Joint Factor Analysis, the I-vector-based analysis has become the most popular and state-of-the-art framework for the speaker verification task. Mainly applied within the NIST/SRE evaluation campaigns, many studies have been proposed to improve more and more performance of speaker verification systems. Nevertheless, while the i-vector framework has been used in other speech processing fields like language recognition, a very few studies have been reported for the speaker identification task on TV shows. This work was done in the REPERE challenge context, focused on the people recognition task in multimodal conditions (audio, video, text) from TV show corpora. Moreover, the challenge participants are invited for providing systems for monomodal tasks, like speaker identification. The application of the i-vector framework is investigated through different points of views: (1) some of the i-vector based approaches are compared, (2) a specific i-vector extraction protocol is proposed in order to deal with widely varying amounts of training data among speaker population, (3) the joint use of both speaker diarization and identification is finally analyzed. Based on a 533 speaker dictionary, this joint system wins the monomodal speaker identification task of the 2014 REPERE challenge.", "title": "Analysis of i-vector framework for speaker identification in TV-shows"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/laurent14_interspeech.html", "abstract": "In this article, we tackle the problem of speaker role detection from broadcast news shows. In the literature, many proposed solutions are based on the combination of various features coming from acoustic, lexical and semantic information with a machine learning algorithm. Many previous studies mention the use of boosting over decision stumps to combine efficiently these features. In this work, we propose a modification of this state-of-the-art machine learning algorithm changing the weak learner (decision stumps) by small decision trees, denoted bonsai trees. Experiments show that using bonsai trees as weak learners for the boosting algorithm largely improves both system error rate and learning time.", "title": "Boosting bonsai trees for efficient features combination: application to speaker role identification"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/raimond14_interspeech.html", "abstract": "In this paper we describe the speaker identification feature of the BBC World Service Archive prototype, an experiment run by BBC R&D to investigate alternative ways of publishing large radio archives. This feature relies on diarization of individual programmes, supervector-based speaker models, crowdsourcing for speaker identities, and a fast distributed index based on Locality Sensitive Hashing techniques to propagate these identities. We also describe how crowdsourced data can be used to continuously evaluate and refine our mapping from speaker models to speaker identities. We believe this experiment is one of the largest of its kind.", "title": "Identifying contributors in the BBC world service archive"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/kelly14_interspeech.html", "abstract": "Assessing the impact of ageing on biometric systems is an important challenge. In this paper, an i-vector speaker verification framework is used to evaluate the impact of long-term ageing on state-of-the-art speaker verification. Using the Trinity College Dublin Speaker Ageing (TCDSA) database, it is observed that the performance of the i-vector system, in terms of both discrimination and calibration, degrades progressively as the absolute age difference between training and testing samples increases. In the case of male speakers, the equal error rate (EER) increases from 4.61% at an ageing difference of 0\u20131 years to 32.74% at an age difference of 51\u201360 years. The performance of a Gaussian Mixture Model - Universal Background Model (GMM-UBM) system is presented for comparison. It is shown that while the i-vector system outperforms the GMM-UBM system, as absolute age difference increases, the performance of both degrades at a similar rate. It is concluded that long-term ageing variability is distinct from everyday intersession variability, and therefore must be dealt with via dedicated compensation strategies.", "title": "Effect of long-term ageing on i-vector speaker verification"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/versteegh14_interspeech.html", "abstract": "Languages vary not only in terms of their sound inventory, but also in the phonological status certain sound distinctions are assigned. For example, while vowel nasality is lexically contrastive (phonemic) in Quebecois French, it is largely determined by the context (allophonic) in American English; the reverse is true for vowel tenseness. If phonetics and phonology interact, a minimal pair of sounds should span a larger acoustic divergence when it is pronounced by speakers for whom the underlying distinction is phonemic compared to allophonic. Near minimal pairs were segmented from a corpus of American English and Quebecois French using a crossed design (since nasality and tenseness have opposite phonological status in the two languages). Pairwise time-aligned divergences between contrasts were calculated on the basis of 7 mainstream spoken feature representations, and a set of linguistic phonetic measurements. Only carefully selected phonetic measurements revealed the expected cross-over, with larger divergences for English than French tokens of the tenseness contrast, and larger divergences for French than English tokens for the nasality contrast. We conclude that the phonetic effects of phonological status are subtle enough that only linguistically-informed (or supervised) measurements can pick up on them.", "title": "Acoustic correlates of phonological status"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/airaksinen14_interspeech.html", "abstract": "Parameterization of the glottal flow is a process where the glottal flow is represented in terms of a few numerical values. This study proposes a novel parameterization technique called the phase plane symmetry (PPS) parameter that utilizes the symmetrical properties of the phase plane plot. Phase plane is a way to graphically visualize the glottal source in a 2-dimensional space spanned by two amplitude-domain axes. A correctly normalized phase plane plot has also close ties to the normalized amplitude quotient (NAQ) parameter, and it is shown that the inverse NAQ value is represented as a single point in the phase plane plot. The experiments conducted in this study support that PPS is powerful in discriminating between various phonation types and within the same range of robustness as the NAQ parameter.", "title": "Parameterization of the glottal source with the phase plane plot"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/rose14_interspeech.html", "abstract": "The accuracy of the widely used and International Phonetic Association-sanctioned Chao five-point scale of tonal transcription is examined quantitatively. Perceptually transformed acoustic data are used from two Chinese dialects with complex tone systems, and a measure derived of the conformability of the data using their likelihoods. It is shown that some tones conform well to the model, but others do not, with tonal pitch targets lying equidistant between the Chao integers. It is concluded that the Chao model is probably not an accurate reflection of the distribution of tonal pitch targets.", "title": "Transcribing tone \u2014 a likelihood-based quantitative evaluation of chao's tone letters"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/hamzah14_interspeech.html", "abstract": "This paper presents original data in support of a new model of intonational phonology for Malay as spoken in Singapore. Building on the Autosegmental-Metrical approach (Beckman & Pierrehumbert, 1986), we propose that intonational variation in Malay can be explained in terms of underlying sequences of abstract tonal units (H and L), which are aligned to the edges and internal syllables of prosodic phrases organized in a hierarchy. Data was drawn from a production experiment (Hamzah, 2012) involving declarative utterances under different focus patterns in a question-answer context, as well as from story-telling interviews and TV interviews. We find evidence for at least three levels of prosodic organization: (i) an accentual phrase which comprises one or more words and bears an L and H tone at its left and right edges, respectively, (ii) an intermediate phrase, which serves as the domain of catathesis, and (iii) an intonational phrase, which may span the entire utterance and bears an additional H or L tone at its right edge. Differences in F0 peak alignment for focused words support the presence of a focus pitch accent. We outline a series of follow-up studies for extending the model further.", "title": "Intonational phonology and prosodic hierarchy in malay"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/reichel14_interspeech.html", "abstract": "We examined how well prosodic boundary strength can be captured by two declination stylization methods as well as by four different representations of pitch register. In the stylization proposed by Liebermann et al. (1985) base- and topline are fitted to peaks and valleys of the pitch contour, whereas in Reichel & M\u00e1dy (2013) these lines are fitted to medians below and above certain pitch percentiles. From each of the stylizations four feature pools were induced representing different aspects of register discontinuity at word boundaries: discontinuities related to the base-, mid-, and topline, as well as to the range between base- and topline. Concerning stylization the median-based fitting approach turned out to be more robust with respect to declination line crossing errors and yielded base-, topline and range-related discontinuity characteristics with higher correlations to perceived boundary strength. Concerning register representation, for the peak/valley fitting approach the base- and topline patterns showed weaker correspondences to boundary strength than the other feature pools. We furthermore trained generalized linear regression models for boundary strength prediction on each feature pool. It turned out that neither the stylization method nor the register representation had a significant influence on the overall good prediction performance.", "title": "Comparing parameterizations of pitch register and its discontinuities at prosodic boundaries for Hungarian"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/christodoulides14_interspeech.html", "abstract": "The automatic detection of prosodically prominent syllables is crucial for analysing speech, especially in French where prominence contributes substantially to prosodic grouping and boundary demarcation. In this paper, we compare different machine learning techniques for the automatic detection of prominent syllables, using prosodic features (including pitch, energy, duration and spectral balance) and lexical information. We explore the differences between modelling the detection of prominent syllables as a classification or as a sequence labelling problem, and combinations of the two techniques. We train and evaluate our systems on a corpus of spontaneous French speech, consisting of almost 100 different speakers; the corpus is balanced for speaker age and sex and covers 3 different regional varieties. The result of this study is a novel tool for the automatic annotation of prominent syllables in French.", "title": "An evaluation of machine learning methods for prominence detection in French"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/chen14_interspeech.html", "abstract": "The relative magnitude of the first two harmonics of the voice source (H1*-H2*) is an important measure and is assumed to be one exponent of changes in vocal quality along a breathy-to-pressed continuum. H1*-H2* is often associated with glottal open quotient (OQ) and glottal pulse skewness (as quantified by speed quotient, SQ), but may also covary with fundamental frequency (F0) and vocal intensity. We examined the relationship between H1*-H2*, F0, and vocal intensity using phonations in which vocal qualities varied continuously in F0 and intensity. Glottal area measures (OQ and SQ) and acoustic measures (F0, intensity, and H1*-H2*) were studied using simultaneously-collected laryngeal high-speed videoendoscopy and audio recordings from 9 subjects. Analyses of individual speakers showed that H1*-H2* may sometimes vary as a function of F0 alone, with OQ and SQ remaining rather constant, hypothetically when nonlinear source-filter interaction is strong. Although conventionally H1*-H2* is assumed to decrease with increasing vocal intensity due to a decrease in OQ, results showed examples where H1*-H2* increased with increasing vocal intensity, hypothetically when the effect of decreasing pulse skewness exceeds the effect of decreasing OQ. In some phonatory modes, the relationship between SQ and H1*-H2* may not be as monotonic as previously assumed.", "title": "Investigating the effect of F0 and vocal intensity on harmonic magnitudes: data from high-speed laryngeal videoendoscopy"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/delaisroussarie14_interspeech.html", "abstract": "In this paper, we present an approach that allows a TTS-system to dictate texts to primary school pupils, while being in conformity with the prosodic features of this speaking style. The approach relies on the elaboration of a preprocessing prosodic module that avoids developing a specific system for a so limited task. The proposal is based on two distinct elements: (i) the results of a preliminary evaluation that allowed getting feedback from potential users; (ii) a corpus study of 10 dictations annotated or uttered by 13 teachers or speech therapists (10 and 3 respectively).   The preliminary evaluation focused on three points: the accuracy of the segmentation procedure, the size of the automatically calculated chunks, and the intelligibility of the synthesized voice. It showed that the chunks were judged too long, and the speaking rate too fast. We thus decided to work on these two issues while analyzing the collected data, and confronting the obtained realizations with the outcome of the speech synthesis system and the chunking algorithm. The results of the analysis lead to propose a module that provides for this speaking style an enriched text that can be treated by the synthesizer to constrain the unit selection and the prosodic realization.", "title": "Adapting prosodic chunking algorithm and synthesis system to specific style: the case of dictation"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/sung14_interspeech.html", "abstract": "Palatalization in Korean is of two types \u2014 lexical palatalization governed by language-specific phonological rules, and post-lexical palatalization that appears to be purely phonetic. While lexical palatalization only occurs when a morpheme boundary intervenes between a target consonant and a palatalization trigger, post-lexical palatalization occurs irrespective of the presence of a morpheme boundary. This study investigates whether these two types of palatalization and different morphological structures of words manifest as distinct tongue gestures using ultrasound imaging of 4 native speakers of Korean. Comparison of the ultrasound tongue contours shows that the gestural distinction between lexical and post-lexical palatalization may not be the same across individual speakers. Furthermore, the effects of morpheme boundaries are not uniform across different coronal consonants and speakers in terms of tongue gestures. The findings from this study provide further empirical evidence for the role of morphological structures in coarticulation, and are in line with mounting evidence for speaker-specific variability in speech production.", "title": "The articulation of lexical and post-lexical palatalization in Korean"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/archangeli14_interspeech.html", "abstract": "Initial Consonant Mutation in Scottish Gaelic is considered to be morphological, somewhat idiosyncratic, and neutralizing, that is merging either the mutated sound and some underlying sound or merging two mutated sounds. This study explores articulation in one class of mutation, called Lenition (also Aspiration), asking the question of whether these sounds are articulated in the same fashion or not. Comparison of relevant ultrasound images collected from 3 native speakers of Scottish Gaelic shows that speakers maintain distinctions between True Lenition and False Lenition, suggesting that there is incomplete neutralization. Furthermore, when Lenition of two distinct sounds converge on the same target, subjects again keep the two articulations distinct. These results are consistent with a phonological model which distinguishes between surface forms corresponding to different sources, showing very little complete articulatory neutralisation.", "title": "Articulation and neutralization: a preliminary study of lenition in scottish gaelic"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/amino14_interspeech.html", "abstract": "The term nasality refers to the timbre of the nasal phonemes. It is also used to express the quality of sound that characterises some speakers. In this paper, we propose to classify nasality in natural speech into four types: phonemic nasality, nasality in assimilation, incidental nasality in the production of voiced plosives, and nasality associated with speaker individuality. Speech sounds recorded separately for oral and nasal outputs were analysed and the four types of nasality were observed individually. In order to investigate the relationship between the nasality in running speech and the perception of speaker similarity, we conducted an experiment. The results revealed that listeners rated speaker similarity exploiting phonemic nasality when it existed in the utterance and also used speaker-related nasality regardless of the existence of phonemic nasals.", "title": "Nasality in speech and its contribution to speaker individuality"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/brown14_interspeech.html", "abstract": "Different languages have traditionally been classified into different rhythm types. Most studies of rhythm have either implicitly or explicitly accepted that rhythm is an inherent property of a language. This study aims to determine whether rhythm is an intrinsic property of languages, or whether rhythm is an epiphenomenal byproduct of the phonotactic structures of a given stimulus. The question that this project addresses is to what extent the phonological properties of a language can be correlated with rhythmic categories; for instance, whether a language has consonant clusters, makes use of contrastive tone, has complex syllables, exhibits vowel reduction, etc. and whether these can be linked to what kind of rhythmic profile a language fits into.", "title": "Is speech rhythm an intrinsic property of language?"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/jackschina14_interspeech.html", "abstract": "The present paper investigates the conditions under which different realizations of /R/ occur in standard Austrian German. The study is based on 509 word tokens containing the phone sequence /aR/ in coda position drawn from a corpus of read speech from seven male Austrian radio speakers. Acoustic measurements of the vowel /a/ revealed that F1, F2 and F3 are significant predictors for the realization of /R/ as either trill, fricative or as absent. Moreover, /a/ tends to be longer when /R/ is absent than when it is present. Our analysis of the linguistic conditions for the different realizations of /R/ showed that /R/ is least reduced in stressed syllables and in words read in isolation. Furthermore, we observe that the segmental context significantly affects the realization of /R/. Most importantly, we find significant effects of morphology: /R/ tends to be more reduced when it is part of a grammatical morpheme than when it is part of the stem of a word. These findings inform the further development of models of pronunciation variation for human and automatic speech recognition.", "title": "Where /ar/ the /r/s in standard austrian German?"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/hu14_interspeech.html", "abstract": "This paper is an acoustic phonetic description of the vowels and diphthongs in the Yi county dialect and compares the diphthongized vowels with monophthongs and diphthongs in terms of their temporal structures and spectral characteristics. Results show that rising diphthongs have two targets while falling diphthongs only have one dynamic target. Diphthongized vowels occur as an intermediate category between monophthongs and diphthongs. Diphthongized vowels have distinctive onsets but a neutralized offset.", "title": "Diphthongized vowels in the yi county hui Chinese dialect"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/dellwo14_interspeech.html", "abstract": "The rhythmic organization of speech can vary between languages. In the present research we studied rhythmic variability between Mandarin, Cantonese and Thai using automatically retrieved prosodic temporal characteristics from read speech. We measured the variability of intervals between amplitude peaks in the amplitude envelope (", "title": "Rhythmic variability between some asian languages: results from an automatic analysis of temporal characteristics"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/braun14_interspeech.html", "abstract": "The assessment of a given speaker's age has been shown to rest on laryngeal as well as vocal tract features. This study attempts to separate these two elements by comparing listener performance based on whispered and phonated speech. A total of 45 speakers belonging to three different age groups (20\u201335; 45\u201360; 70 and above) were assessed by 20 listeners. Results show that although listener performance decreases with whispered speech, it is still well above chance level. No significant difference was found between age groups, but female speakers were assessed correctly more often than males in both conditions. There are forensic implications to this finding, since in that context, age estimation sometimes has to be carried out on whispered speech, e.g. if whispering is used as a voice disguise.", "title": "Listener estimation of speaker age based on whispered speech"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/kasisopa14_interspeech.html", "abstract": "When in a noisy environment speakers modify their speech production by increasing loudness, vowel duration, and fundamental frequency (F0), a phenomenon known as Lombard speech. Here Lombard speech in Thai was investigated in order to determine the effects of noise on the realisation of F0 in Thai lexical tones. Analysis of the acoustic characteristics of the five Bangkok Thai tones, in both continuous speech and citation form, produced in noise and in quiet showed that F0 is heightened in Lombard compared with clear speech. In addition, generally the contour of the tones also changes in Lombard speech; contours tend to be exaggerated towards the end of the tone in a direction consistent with the contour of the tone.", "title": "The Lombard effect with Thai lexical tones: an acoustic analysis of articulatory modifications in noise"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/pappu14_interspeech.html", "abstract": "To respond to a user's query, dialog agents can use a knowledge base that is either domain specific, commonsense (e.g., NELL, Freebase) or a combination of both. The drawback is that domain-specific knowledge bases will likely be limited and static; commonsense ones are dynamic but contain general information found on the web and will be sparse with respect to a domain. We address this issue through a system that solicits situational information from its users in a domain that provides information on events (seminar talks) to augment its knowledge base (covering an academic field). We find that this knowledge is consistent and useful and that it provides reliable information to users. We show that, in comparison to a base system, users find that retrievals are more relevant when the system uses its informally acquired knowledge to augment their queries.", "title": "Learning situated knowledge bases through dialog"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/misu14_interspeech.html", "abstract": "In this paper, we address issues that arise when crowdsourcing data collection of user queries to situated dialog systems in a moving car. Compared to unimodal spoken dialog systems such as systems for smartphones, collecting dialog data for situated dialog systems is more costly because a clear awareness of the physical surroundings is required for the user to make realistic queries. We consider the use of crowdsourcing to collect them. To elicit queries from crowd workers, we propose methods of prompting them using visual information. The queries collected using the crowdsourcing methods are compared to those collected using a real situated dialog system. Specifically, we evaluate them based on several performance measures of similarity in semantic content, naturalness of language expression and bias of the collected data. We demonstrate that our crowdsourcing method produced a better language resource in terms of the similarity of the text to real user utterances than those generated by a handcrafted grammar.", "title": "Crowdsourcing for situated dialog systems in a moving car"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/higashinaka14_interspeech.html", "abstract": "We propose a method for evaluating coherence between user utterances and those generated from open domain conversational systems. Our aim is to make it possible for such systems to ascertain whether utterances generated from them are appropriate to the context before generation so that possible breakdown in conversation arising from inappropriate utterances can be avoided. In our method, we train a classifier that distinguishes a pair of a user utterance and that generated from a system as coherent or incoherent by using various pieces of information related to dialogue exchange, such as dialogue acts, question types, and predicate-argument structures. Experimental results show that our method significantly outperforms the baseline, confirming its effectiveness.", "title": "Evaluating coherence in open domain conversational systems"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/bechet14_interspeech.html", "abstract": "Parsing human-human conversations consists in automatically enriching text transcription with semantic structure information. We use in this paper a FrameNet-based approach to semantics that, without needing a full semantic parse of a message, goes further than a simple flat translation of a message into basic concepts. FrameNet-based semantic parsing may follow a syntactic parsing step, however spoken conversations in customer service telephone call centers present very specific characteristics such as non-canonical language, noisy messages (disfluencies, repetitions, truncated words or automatic speech transcription errors) and the presence of superfluous information. For syntactic parsing the traditional view based on context-free grammars is not suitable for processing non-canonical text. New approaches to parsing based on dependency structures and discriminative machine learning techniques are more adapted to process spontaneous speech for two main reasons: (a) they need less training data and (b) the annotation with syntactic dependencies of conversation transcripts is simpler than with syntactic constituents. Another advantage is that partial annotation can be performed. This paper presents the adaptation of a syntactic dependency parser to process very spontaneous speech recorded in a call-centre environment. This parser is used in order to produce FrameNet candidates for characterizing conversations between an operator and a caller.", "title": "Adapting dependency parsing to spontaneous speech for open domain spoken language understanding"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/gasic14_interspeech.html", "abstract": "An important property of open domain spoken dialogue systems is their ability to deal with a set of new, previously unseen, concepts introduced in the conversation. The dialogue manager must then quickly learn how to talk about the new concepts using its knowledge of the existing concepts. It has previously been shown that a single new concept could be accommodated by mapping the kernel function of a Gaussian process to incorporate an additional concept into the domain of a statistical dialogue manager. Here we present an incremental scheme which enables the domain of a dialogue manager to be repeatedly extended by recursively specifying priors in Gaussian processes. We show that it is possible to effectively double the number of concepts understood by a system providing restaurant information using only 1000 adaptation dialogues with real users.", "title": "Incremental on-line adaptation of POMDP-based dialogue managers to extended domains"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/robichaud14_interspeech.html", "abstract": "We present a novel application of hypothesis ranking (HR) for the task of domain detection in a multi-domain, multi-turn dialog system. Alternate, domain dependent, semantic frames from a spoken language understanding (SLU) analysis are ranked using a gradient boosted decision trees (GBDT) ranker to determine the most likely domain. The ranker, trained using Lambda Rank, makes use of a range of signals derived from the SLU and previous turn context to improve domain detection. On a multi-turn corpus we show that this approach offers accuracy improvements of 3.2% absolute (25.6% relative) compared to relying solely on upfront non-contextual SLU domain models and 2.9% (24.5% relative) improvement even with contextual SLU domain models. We also show that HR can be trained to be robust to changes in the SLU.", "title": "Hypotheses ranking for robust domain classification and tracking in dialogue systems"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/ramanarayanan14_interspeech.html", "abstract": "We present a method to derive a small number of speech motor control \u201cprimitives\u201d that can produce linguistically-interpretable articulatory movements. We envision that such a dictionary of primitives can be useful for speech motor control, particularly in finding a low-dimensional subspace for such control. First, we use the iterative Linear Quadratic Gaussian with Learned Dynamics (iLQG-LD) algorithm to derive (for a set of utterances) a set of stochastically optimal control inputs to a learned dynamical systems model of the vocal tract that produces desired movement sequences. Second, we use a convolutive Nonnegative Matrix Factorization with sparseness constraints (cNMFsc) algorithm to find a small dictionary of control input primitives that can be used to reproduce the aforementioned optimal control inputs that produce the observed articulatory movements. The method performs favorably on both qualitative and quantitative evaluations conducted on synthetic data produced by an articulatory synthesizer. Such a primitives-based framework could help inform theories of speech motor control and coordination.", "title": "Motor control primitives arising from a learned dynamical systems model of speech articulation"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/yeh14_interspeech.html", "abstract": "This study demonstrates the nonword repetition format comparable to other conventional tasks (picture-naming, reading, and so on) as a plausible measure of linguistic competence for adults with language attrition. Taiwanese speakers with and without attrition symptoms, defined by frequency of use, were recruited, and so were American learners of Mandarin Chinese. The results show that (1) fluent speakers' repetition accuracy of Taiwanese tones is significantly higher than attrition speakers', and American learners' is the worst, (2) among five target tones (high-level, low-rising, low-falling, high-falling and mid-level), the repetition accuracy of high-falling tone is the highest, and that of low-level tone is the lowest in non-word-final position across the three participant groups, and (3) the least accurate mid-level tone tends to be mispronounced as low-rising. The findings suggest that the participants' frequency of use and exposure to Taiwanese is positively correlated with the repetition accuracy, and mid-level tone is the most difficult category to learn. More crucially, the percent accuracy and confusion matrix of nonword repetition enlighten how midlevel tone is more susceptible to sound change.", "title": "Nonword repetition of taiwanese disyllabic tonal sequences in adults with language attrition"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/windmann14_interspeech.html", "abstract": "We show how our optimization-based model of speech timing reproduces three effects of prosodic prominence on suprasegmental timing patterns in speech: (1), the durational interaction between lexical stress and pitch accent, (2), polysyllabic shortening in pitch-accented words and (3), differential behavior of prominent and non-prominent syllables under speaking rate variation. We review the literature and present model simulations that replicate reported phenomena. Results underline the capacity of our model to provide a unified account of the temporal organization of speech.", "title": "A unified account of prominence effects in an optimization-based model of speech timing"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/kim14_interspeech.html", "abstract": "This study develops a mathematical model that estimates the movements of (linguistically) non-crucial articulators in speech production, which provides a systematic way to study the relationship between the behaviors of crucial and non-crucial articulators; crucial articulators are those essential for realizing a speech task. The underlying assumption of our model is that non-crucial articulatory movements are governed by the physiological constraints in relation to the corresponding crucial articulators as well as by the contextual constraint from the nearest crucial time of the non-crucial articulator. These constraints have been generally assumed in the speech production literature, but they have not been incorporated directly into articulatory models. The crucial articulatory moments in an utterance are automatically determined by a novel forced-alignment algorithm for articulatory trajectories, which uses the inherent physical properties of crucial articulatory movements. Experimental results suggest that the proposed algorithm is capable of estimating non-crucial articulatory positions well in both neutral and emotional speech, significantly better than the simple interpolation of crucial points.", "title": "Estimation of the movement trajectories of non-crucial articulators based on the detection of crucial moments and physiological constraints"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/sudhakar14_interspeech.html", "abstract": "Speech recognition using articulatory features estimated using Acoustic-to-Articulatory Inversion (AAI) is considered. A recently proposed sparse smoothing approach is used to postprocess the estimates from Gaussian Mixture Model (GMM) based AAI using Minimum Mean Squared Error (MMSE) criterion. It is well known that low-pass smoothing as post-processing improves the AAI performance. Sparse smoothing, on the other hand, not only improves the AAI performance but also preserves the MMSE optimality for as many estimates as possible. In this work we investigate the benefit of preserving MMSE optimality during postprocessing by using the smoothed articulatory estimates in a broad class phonetic recognition task. Experimental results show that the low-pass filter based smoothing results in a significant drop in the recognition accuracy compared to that using articulatory estimates without any smoothing. However, the recognition accuracy obtained by articulatory features from sparse smoothing is similar to that using articulatory features directly from GMM based AAI without any postprocessing. Thus, sparse smoothing provides benefit both in terms of the inversion performance as well as recognition accuracy, while that is not the case with low-pass smoothing.", "title": "Sparse smoothing of articulatory features from Gaussian mixture model based acoustic-to-articulatory inversion: benefit to speech recognition"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/wang14b_interspeech.html", "abstract": "Speaking requires the coordinated movements of individual articulators. Understanding each articulator's contribution to speech is fundamental not only for understanding how speech is produced, but also for optimizing speech assessment and treatment. Our recent work has studied the individual contributions of tongue tip, tongue blade, tongue body front, tongue body back, upper lip, and lower lip movement to speech sound production by tracking the motion of sensors attached on the midline of tongue and lips. An optimal set of articulators (tongue tip, tongue body back, upper lip, and lower lip) has been found. However, the tongue lateral (side)'s contribution to speech is still poorly understood. We therefore investigated the contribution of the tongue lateral region to consonant production by analyzing the motion of a sensor attached to the side of tongue. Repeated productions of 12 consonants (including the lateral approximant /l/) were collected from six native English speakers. Consonant classification accuracy based on articulatory movement data obtained using a support vector machine was used as an indication of contribution level. The results suggest that sagittal movement of the tongue lateral sensor did not significantly benefit consonant classification, over and above the optimal set. Implications of these findings are discussed.", "title": "Contribution of tongue lateral to consonant production"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/liu14b_interspeech.html", "abstract": "This paper investigated the potential acoustic correlates of word stress within a disyllabic tonal sequence, a rising tone followed by a rising tone (Tone2 + Tone2) in Mandarin, based on a large corpus with adequate information of stress patterns and prosodic boundary levels. The results showed that a) For Tone2+Tone2 words, features based on tone nucleus were more effective than that of the whole F0 contour for stress identification. Particularly, three new acoustic correlates of stress were proposed, namely, the F0 change difference, the duration difference and the F0 slope difference of tone nucleus segment between the two syllables. b) These three parameters could serve properly as acoustic cues to differentiate initial-stressed and final-stressed words, as well as initial-stressed and equal-stressed words irrespective of prosodic boundary levels. c) With the increase of prosodic boundary levels, the duration difference of the tone nucleus segment got smaller for the initial-stressed words while getting larger for the final-stressed words, due to stronger enlargement of pre-boundary lengthening at higher prosodic boundary levels. d) At higher prosodic boundary levels, there tend to be a compensation effect between the F0 change difference of the tone nucleus segment and the duration difference within each stress pattern.", "title": "A preliminary study on acoustic correlates of tone2+tone2 disyllabic word stress in Mandarin"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/abuoudeh14_interspeech.html", "abstract": "Investigations of changes in speech rate and speaking style have reported that locus equation parameters are sensible to temporal variation though they usually do not evidence any major impact on place of articulation classification. The aim of the present study was to investigate the role of vowel length contrasts on locus equation parameters in Jordanian Arabic (JA) speech production in order to study the influence of intrinsic temporal changes (vowel length contrasts) on locus equation parameters. Statistical analyses of slopes and intercepts show that the locus equation parameters of a consonant produced with long vowels are significantly different from those of the same consonant produced with short vowels. Though this observation confirms the impact of time variations on locus equations, further analyses will need to address issues related to the potential impact of duration-related spectral modifications and the role these changes may play on the perceptual categorization of consonantal place of articulation.", "title": "Vowel length impact on locus equation parameters: an investigation on jordanian Arabic"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/roberts14_interspeech.html", "abstract": "Acoustic cues to the distinction between sibilant fricatives are claimed to be invariant across languages. In [1], Evers et al. present a method for distinguishing automatically between [s] and [\u0283], using the slope of regression lines over separate frequency ranges within a DFT spectrum. They report accuracy rates in excess of 90% for fricatives extracted from recordings of minimal pairs in English, Dutch and Bengali. These findings are broadly replicated by [2], using VCV tokens recorded in the lab.   We tested the algorithm from [1] against tokens of fricatives extracted from the TIMIT corpus of American English read speech, and the Kiel corpora of German. We were able to achieve similar accuracy rates to those reported in [1] and [2], with the following caveats: (1) the measure relies on being able to perform a DFT for frequencies from 0 to 8 kHz, so that a minimum sampling rate of 16 kHz is necessary for it to be effective, and (2) although the measure draws a similarly clear distinction between [s] and [\u0283] to those found in previous studies, the absolute value of the threshold between the two sounds is sensitive to the dynamic range of the input signal.Acoustic cues to the distinction between sibilant fricatives are claimed to be invariant across languages. In [1], Evers et al. present a method for distinguishing automatically between [s] and [\u0283], using the slope of regression lines over separate frequency ranges within a DFT spectrum. They report accuracy rates in excess of 90% for fricatives extracted from recordings of minimal pairs in English, Dutch and Bengali. These findings are broadly replicated by [2], using VCV tokens recorded in the lab.   We tested the algorithm from [1] against tokens of fricatives extracted from the TIMIT corpus of American English read speech, and the Kiel corpora of German. We were able to achieve similar accuracy rates to those reported in [1] and [2], with the following caveats: (1) the measure relies on being able to perform a DFT for frequencies from 0 to 8 kHz, so that a minimum sampling rate of 16 kHz is necessary for it to be effective, and (2) although the measure draws a similarly clear distinction between [s] and [\u0283] to those found in previous studies, the absolute value of the threshold between the two sounds is sensitive to the dynamic range of the input signal.", "title": "Corpus-testing a fricative discriminator; or, just how invariant is this invariant?"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/bush14_interspeech.html", "abstract": "Modeling coarticulation in speech has been largely limited to short sequences and/or limited phonetic context. We introduce a methodology for modeling both formant frequency and bandwidth in continuous speech, allowing examination of sentence-level coarticulation. The model represents continuous trajectories as a combination of overlapping local trajectories, which are represented by a weighted-addition of acoustic event targets by sigmoidal coarticulation functions characterized by slope and position. Estimation is achieved using a combination of hill-climbing and grid-search, with global target, joint slope for identical contexts, and local position parameters. We evaluate model performance for two speakers using an intelligibility test that compares vocoded model output to a purely vocoded and a natural condition.", "title": "Modeling coarticulation in continuous speech"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/daoudi14_interspeech.html", "abstract": "A large amount of research in pathological voice classification consider the task of feature extraction for discrimination between normal and dysphonic sustained vowels. The most widely used dataset for this purpose is the Massachusetts Eye & Ear Infirmary (MEEI) Voice Disorders Database commercialized by KayPENTAX Corp. During the last two decades, dozens of methods have been proposed to extract discriminative features from these signals in order to design accurate classifiers between the two classes of this database. The main contribution of this paper is to show that the normal and dysphonic sustained vowels of the KayPENTAX database are actually perfectly separable. This implies that this dataset is not suited for the normal-vs-dysphonic classification task, as long as the only concern is to achieve high classification accuracy. Indeed, we show that a single scalar parameter extracted from a matching pursuit decomposition of these signals (with a Gabor dictionary) yields a prefect classification accuracy (100% with a large margin). We then discuss the implication of this finding on the precaution that should be taken with this database and on research in pathological voice detection in general.", "title": "On classification between normal and pathological voices using the MEEI-kayPENTAX database: issues and consequences"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/bukmaier14_interspeech.html", "abstract": "The aim of the present study was to relate articulatory properties of the Polish sibilants /s ? ?/ to a potential neutralization of /?/ as either /s/ or /?/, the former having occurred in a number of Polish dialects. For this purpose tongue tip (TT) movement data was obtained together with acoustic data using electromagnetic articulography. The sibilants, that were always followed by either /a e o/, were produced by four L1-Polish speakers at fast and slow speech rates. While /s ?/ had almost identical transitions, they differed greatly in the spectral characteristics with /?/ being closer to /?/. In order to capture differences in tongue position as well as shape both TT position and TT orientation data were analyzed. The vertical TT orientation showed similarities in /?/ and /s/ production, but the two sibilants were clearly separated in TT position, with /?/ being produced far more back than /s/ and /?/, and the latter two being very similar. The tendentially greater effect of speech rate on /?/ together with the varying acoustic and articulatory similarities between the sibilants are taken as an indicator for greater instability of /?/. This synchronic instability is discussed in terms of potential diachronic mergers.", "title": "Synchronic variation in the articulation and the acoustics of the Polish three-way place distinction in sibilants and its implications for diachronic change"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/gupta14_interspeech.html", "abstract": "Motivational interviewing (MI) is a goal oriented psychotherapy that facilitates intrinsic motivation within a client in order to change behavior in a dialog setting. The Motivational Interviewing Skills Code (MISC) is a manual observational coding method used to quantify and evaluate the quality of MI sessions using their audio-visual recordings. However, this coding method is both labor intensive and expensive. We present an approach towards automating MISC assignments in MI involving addiction cure. Specifically, we focus on predicting valence for \u201cClient Change Talk\u201d ( ChangeTalk) utterances, which indicate a client's attitude towards a \u201cTarget Behavior Change\u201d ( Target). We further study the effect of incorporating counselor behavior in the model. We observe that our best model achieves an unweighted accuracy of 50.8% in a 3-way classification of positive vs negative valence ChangeTalk vs no ChangeTalk. Furthermore, we study the effect of including non-verbal behavior, specifically laughters, in our model. Information regarding location of laughters improves the unweighted accuracy of our model to 51.4% and our experimental results suggest prosodic differences in laughters belonging to ChangeTalk utterances with different valences.", "title": "Predicting client's inclination towards target behavior change in motivational interviewing and investigating the role of laughter"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/xiao14_interspeech.html", "abstract": "Empathy measures the capacity of the therapist to experience the same cognitive and emotional dispositions as the patient, and is a key quality factor in counseling. In this work we build computational models to infer the empathy of therapist using prosodic cues. We extract pitch, energy, jitter, shimmer and utterance duration from the speech signal, and normalize and quantize these features in order to estimate the distribution of certain prosodic patterns during each interaction. We find significant correlation between empathy and the distribution of prosodic patterns, and achieve 75% accuracy in classifying therapist empathy levels using this distribution. Experiment results suggest high pitch and energy of the therapist are negatively correlated with empathy. These observations agree with domain literature and human intuition.", "title": "Modeling therapist empathy through prosody in drug addiction counseling"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/bone14_interspeech.html", "abstract": "Researchers from various disciplines are concerned with the study of affective phenomena, especially arousal. Expressed affective modulations, which reflect both an individual's internal state and external factors, are central to the communicative process. Bone et al. developed a robust, unsupervised (rule-based) method which provides a scale-continuous, bounded arousal rating from the vocal signal. In this study, we investigate the joint-dynamics of child and psychologist vocal arousal in autism spectrum disorder (ASD) diagnostic interactions. Arousal synchrony is assessed with multiple methods. Results indicate that children with higher ASD severity tend to lead the arousal dynamics more, seemingly because the children aren't as responsive to the psychologist's affective modulations. A vocal arousal model is also proposed which incorporates social and conversational constructs. The model captures conversational signal relations, and is able to distinguish between high and low ASD severity at accuracies well-above chance.", "title": "An investigation of vocal arousal dynamics in child-psychologist interactions using synchrony measures and a conversation-based model"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/han14_interspeech.html", "abstract": "Speech emotion recognition is a challenging problem partly because it is unclear what features are effective for the task. In this paper we propose to utilize deep neural networks (DNNs) to extract high level features from raw data and show that they are effective for speech emotion recognition. We first produce an emotion state probability distribution for each speech segment using DNNs. We then construct utterance-level features from segment-level probability distributions. These utterance-level features are then fed into an extreme learning machine (ELM), a special simple and efficient single-hidden-layer neural network, to identify utterance-level emotions. The experimental results demonstrate that the proposed approach effectively learns emotional information from low-level features and leads to 20% relative accuracy improvement compared to the state-of-the-art approaches.", "title": "Speech emotion recognition using deep neural network and extreme learning machine"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/truong14_interspeech.html", "abstract": "Sighs are non-verbal vocalisations that can carry important information about a speaker's emotional (and psychological) state. Although sighs are commonly associated with negative emotions (e.g. giving up on something, `a sigh of despair', sadness), sighs can also be associated with positive emotions such as relief. In order to gain a better understanding of sighing as a social and affective signal in dialogue, and to advance towards an automatic classification and interpretation of the emotional content of sighs, it is necessary to learn more about the various phonetic characteristics of sighs. To that end, we developed an annotation scheme for sighs that takes the variation in phonetic form into account. Using this scheme, an oral history corpus containing emotionally-coloured dialogues was annotated for sighs. Results show that sighs can be annotated with a sufficient level of reliability (Cohen's Kappa of 0.713), and that indeed, various types of sighs can be identified as well (Cohen's Kappa between 0.637 and 0.805). Through a preliminary analysis of emotional content words, indications were found that certain types of sighs can be associated with specific emotional contexts.", "title": "An annotation scheme for sighs in spontaneous dialogue"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/he14_interspeech.html", "abstract": "This study explored speaker idiosyncrasy by measuring the syllabic intensity variability in the speech signal. Sixteen speakers of the TEVOID corpus, each producing 256 read sentences, were analyzed. Characteristics of intensity variability (average or peak) between syllables were measured either holistically (standard deviation of intensity changes between syllables) or locally (pairwise variability indices of intensity changes between syllables). The results indicated significant effects of the speakers in all the metrics, suggesting a potential application of the methods for speaker recognition, and in particular for forensic speaker comparison.", "title": "Speaker idiosyncratic variability of intensity across syllables"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/mariooryad14_interspeech.html", "abstract": "A key element in affective computing is to have large corpora of genuine emotional samples collected during natural conversations. Recording natural interactions through telephone is an appealing approach to build emotional databases. However, collecting real conversational data with expressive reactions is a challenging task, especially if the recordings are to be shared with the community (e.g., privacy concerns). This study explores a novel approach consisting in retrieving emotional reactions from existing spontaneous speech databases collected for general speech processing problems. Although most of the recordings in these databases are expected to have non-emotional expressions, given the naturalness of the interactions, the flow of the conversation can lead to emotional responses from conversation partners which we aim to retrieve. We use the IEMOCAP and SEMAINE databases to build emotion detector systems. We use these classifiers to identify emotional behaviors from the FISHER database, which is a large conversational speech corpus recorded over the phone. Subjective evaluations over the retrieved samples demonstrate the potential of the proposed scheme to build naturalistic emotional speech database.", "title": "Building a naturalistic emotional speech corpus by retrieving expressive behaviors from existing speech corpora"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/safavi14_interspeech.html", "abstract": "This paper presents results on age-group identification (Age-ID) for children's speech, using the OGI Kids corpus and GMM-UBM, GMM-SVM and i-vector systems. Regions of the spectrum containing important age information for children are identified by conducting Age-ID experiments over 21 frequency sub-bands. Results show that the frequencies above 5.5 kHz are least useful for Age-ID. The effect of using gender-independent and gender-dependent age-group modelling is explored. The GMM-UBM and i-vector systems considerably outperform the GMM-SVM system. The best Age-ID performance of 85.77% is obtained by the i-vector system applied to band-limited speech to 5.5 kHz. Experiments on human Age-ID were also conducted and the results show that the humans do not achieve the performance of the machine.", "title": "Identification of age-group from children's speech by computers and humans"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/morchid14_interspeech.html", "abstract": "This paper describes a research on topic identification in a real-world customer service telephone conversations between an agent and a customer. Separate hidden spaces are considered for agents, customers and the combination of them. The purpose is to separate semantic constituents from the speaker types and their possible relations. Probabilities of hidden topic features are then used by separate Gaussian classifiers to compute theme probabilities for each speaker type. A simple strategy, that does not require any additional parameter estimation, is introduced to classify themes with confidence indicators for each theme hypothesis. Experimental results on a real-life application show that the use of features from speaker type specific hidden spaces capture useful semantic contents with significantly superior performance with respect to independent word-based features or a single set of features. Experimental results also show that the proposed strategy makes it possible to perform surveys on collections of conversations by automatically selecting processed samples with high theme identification accuracy.", "title": "Theme identification in human-human conversations with features from specific speaker type hidden spaces"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/marin14_interspeech.html", "abstract": "This paper explores a novel method for learning phrase pattern features for text classification, employing a mapping of selected words into a knowledge graph and self-training over unlabeled data. Using Support Vector Machine classification, we obtain improvements over lexical and fully-supervised phrase pattern features in domain and intent detection for language understanding, particularly in conjunction with the use of unlabeled data. Our best results are obtained using unlabeled data filtered for both model training and feature learning based on the confidence of the baseline classifiers.", "title": "Learning phrase patterns for text classification using a knowledge graph and unlabeled data"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/xu14_interspeech.html", "abstract": "In slot filling with conditional random field (CRF), the strong current word and dictionary features tend to swamp the effect of contextual features, a phenomenon also known as feature undertraining. This is a dangerous tradeoff especially when training data is small and dictionaries are limited in their coverage of the entities observed during testing. In this paper, we propose a simple and effective solution that extends the feature dropout algorithm, directly aiming at boosting the contribution from entity context. We show with extensive experiments that the proposed technique can significantly improve the robustness against unseen entities, without degrading performance on entities that are either seen or exist in the dictionary.", "title": "Targeted feature dropout for robust slot filling in natural language understanding"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/shiang14_interspeech.html", "abstract": "In this paper, we consider a spoken question answering (QA) task, in which the questions are in form of speech, while the knowledge source for answers are the webpages (in text) over the Internet to be accessed by an information retrieval engine, and we mainly focus on query formulation and re-ranking part. Because the recognition results for the spoken questions are less reliable, we use N-best lists in order to have higher probabilities to induce more correct keywords for the questions, but more noisy words are inevitably included as well. We therefore propose a hierarchical labeling method using tree-structured conditional random fields (CRF) to leverage the parse tree information or the syntactic structure obtained from the N-best-lists of the spoken questions, such that the queries for information retrieval can be better formulated. In addition, because queries formulated from the N-best results naturally generate more noisy information, we further propose to use two-layer random walk for re-ranking the retrieved webpages to produce better documents containing answers. Initial experiments performed on a set of question answering pairs in Mandarin Chinese verified that improved performance was achievable with the proposed approaches.", "title": "Spoken question answering using tree-structured conditional random fields and two-layer random walk"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/sarikaya14_interspeech.html", "abstract": "In this paper we propose a set of class-based features that are generated in an unsupservised fashion to improve slot tagging with Conditional Random Fields (CRFs). The feature generation is based on the idea behind shrinkage based language models, where shrinking the sum of parameter magnitudes in an exponential model tends to improve performance. We use these features with CRFs and show that they consistently improve the slot tagging performance against baselines on several natural language understanding tasks. Since the proposed features are generated in an unsupervised manner without significant computational overhead, the improvements in performance comes for free and we expect that the same features may result in gains in other tagging tasks.", "title": "Shrinkage based features for slot tagging with conditional random fields"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/shi14_interspeech.html", "abstract": "Abbreviations in Chinese are widely observed in Chinese spoken language. Automatic generation of Chinese abbreviations helps to improve Chinese natural language understanding systems and Chinese search engine. The abbreviation generation is treated as a character-based tagging problem. Due to limited training data, Chinese abbreviation generation suffers from data sparseness. Two types of strategies are proposed to reduce the impact from data sparseness. First of all, in addition to using a traditional sequence labelling method \u2014 Conditional Random Fields (CRF), we propose to apply Recurrent Neural Network with Maximum Entropy Extension (RNNME), which actually shows similar performance as using crf in our experiment. Secondly, we propose to use training data clustering and latent topic modeling in abbreviation generation. Using training data clustering or topic modeling not only addresses the data sparseness, but also takes advantage of the fact that full-names from the same cluster or the same latent topic have similar abbreviation patterns. Our experimental results show that using manual clustering, the accuracy of abbreviation generation achieves relatively 8% improvement. Using Latent topics that are obtained from Latent Dirichlet Allocation (LDA), the accuracy achieves relative 10% improvement.", "title": "Cluster based Chinese abbreviation modeling"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/zhang14b_interspeech.html", "abstract": "Named entity recognition (NER) plays an important role in many natural language processing applications. This paper presents a novel approach to Chinese NER. It differentiates from most of the previous approaches mainly in three respects. First of all, while previous work is good at modeling features between observation elements, our model incorporates syntactic structure as higher level information. It is crucial for recognizing long named entities, which are one of the main difficulties of NER. Secondly, NER and syntactic analysis have been modeled separately in natural language processing until now. We integrate them in a unified framework. It allows the information from each type of annotation to improve performance on the other, and produces the consistent output. Finally, few studies have been reported on the recognition of nested named entities in Chinese. This paper presents a structured prediction model for Chinese nested named entity recognition. Our approach have been implemented through a joint representation of syntactic and named entity structures. We have provided empirical evidence that parsing model can utilize syntactic constraints for recognizing named entities, and exploit the composition patterns of named entities. Experiment results demonstrate the mutual benefits for each task and output syntactic structure of named entities.", "title": "Parsing named entity as syntactic structure"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/tur14_interspeech.html", "abstract": "Using different sources of information for grammar induction results in grammars that vary in coverage and precision. Fusing such grammars with a strategy that exploits their strengths while minimizing their weaknesses is expected to produce grammars with superior performance. We focus on the fusion of grammars produced using a knowledge-based approach using lexicalized ontologies and a data-driven approach using semantic similarity clustering. We propose various algorithms for finding the mapping between the (non-terminal) rules generated by each grammar induction algorithm, followed by rule fusion. Three fusion approaches are investigated: early, mid and late fusion. Results show that late fusion provides the best relative F-measure performance improvement by 20%.", "title": "Detecting out-of-domain utterances addressed to a virtual personal assistant"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/georgiladakis14_interspeech.html", "abstract": "Using different sources of information for grammar induction results in grammars that vary in coverage and precision. Fusing such grammars with a strategy that exploits their strengths while minimizing their weaknesses is expected to produce grammars with superior performance. We focus on the fusion of grammars produced using a knowledge-based approach using lexicalized ontologies and a data-driven approach using semantic similarity clustering. We propose various algorithms for finding the mapping between the (non-terminal) rules generated by each grammar induction algorithm, followed by rule fusion. Three fusion approaches are investigated: early, mid and late fusion. Results show that late fusion provides the best relative F-measure performance improvement by 20%.", "title": "Fusion of knowledge-based and data-driven approaches to grammar induction"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/katerenchuk14_interspeech.html", "abstract": "In natural language processing (NLP) the problem of named entity (NE) recognition in speech is well known, yet remains a challenge where performance is dependent on automatic speech recognition (ASR) system error rates. NEs are often foreign or out-of-vocabulary (OOV) words, leaving conventional ASR systems unable to recognize them. In our research, we improve a CRF-based NE recognition system by incorporating two styles of prosodic features, hypothesized ToBI labels and unsupervised clusters of acoustic features. ToBI-based features improve NE recognition by 6% absolute (F1:0.39 v.s. F1: 0.45) on automatically recognized spontaneous speech from ACE'05.", "title": "Improving named entity recognition with prosodic features"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/ravuri14_interspeech.html", "abstract": "Addressee detection for dialog systems aims to detect which utterances are directed at the system, as opposed to someone else. An important means for classification is the lexical content of the utterance, and N-gram models have been shown to be effective for this task. In this paper we investigate whether neural networks can enhance lexical addressee detection, using data from a human-human-computer dialog system. Even though we find no improvement from simply replacing the standard N-gram LM with a neural-network LM as class likelihood estimators, improved classification accuracy can be obtained from a modified neural net model that learns distributed word representations in a first training phase, and is trained on the utterance classification task in a second phase. We obtain additional gains by combining the class likelihood estimation and classification training criteria in the second phase, and by combining multiple model architectures at the score level. Overall, we achieve over 2% absolute reduction in equal error rate over the N-gram model baseline of 27%.", "title": "Neural network models for lexical addressee detection"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/freeman14_interspeech.html", "abstract": "The ATAROS project aims to identify acoustic signals of stance-taking in order to inform the development of automatic stance recognition in natural speech. Due to the typically low frequency of stance-taking in existing corpora that have been used to investigate related phenomena such as subjectivity, we are creating an audio corpus of unscripted conversations between dyads as they complete collaborative tasks designed to elicit a high density of stance-taking at increasing levels of involvement. To validate our experimental design and provide a preliminary assessment of the corpus, we examine a fully transcribed and time-aligned portion to compare the speaking styles in two tasks, one expected to elicit low involvement and weak stances, the other high involvement and strong stances. We find that although overall measures such as task duration and total word count do not indicate consistent differences across tasks, speakers do display significant differences in speaking style. Factors such as increases in speaking rate, turn length, and disfluencies from weak- to strong-stance tasks are consistent with increased involvement by the participants and provide evidence in support of the experimental design.", "title": "Manipulating stance and involvement using collaborative tasks: an exploratory comparison"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/ghigi14_interspeech.html", "abstract": "Incremental Dialog Processing (IDP) enables Spoken Dialog Systems to gradually process minimal units of user speech in order to give the user an early system response. In this paper, we present an application of IDP that shows its effectiveness in a task-oriented dialog system. We have implemented an IDP strategy and deployed it for one month on a real-user system. We compared the resulting dialogs with dialogs produced over the previous month without IDP. Results show that the incremental strategy significantly improved system performance by eliminating long and often off-task utterances that generally produce poor speech recognition results. User behavior is also affected; the user tends to shorten utterances after being interrupted by the system.", "title": "Incremental dialog processing in a task-oriented dialog"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/hotta14_interspeech.html", "abstract": "Appropriate turn-taking is important in spoken dialogue systems as well as generating correct responses. We have developed a method that performs a posteriori restoration of incorrectly segmented utterances caused by erroneous voice activity detection (VAD), which result in automatic speech recognition (ASR) errors and inappropriate turn-taking. A crucial part of the method is to classify whether the restoration is required or not. We cast it as a binary classification problem detecting originally single utterances from pairs of utterance fragments. Various features are used representing timing, prosody, and ASR result information to improve its accuracy. Furthermore, two kinds of feature selection are performed to obtain effective and domain-independent features. The experimental results showed that the proposed method outperformed a baseline with manually-selected features by 4.8% and 3.9% in cross-domain evaluations with two domains. More detailed analysis revealed that the dominant and domain-independent features were utterance intervals and results from the Gaussian mixture model (GMM).", "title": "Detecting incorrectly-segmented utterances for posteriori restoration of turn-taking and ASR results"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/hassan14_interspeech.html", "abstract": "In this paper we focus on the effect of on-line speech segmentation and disfluency removal methods on conversational speech translation. In a real-time conversational speech to speech translation system, on-line segmentation of speech is required to avoid latency beyond few seconds. While sentential unit segmentation and disfluency removal have been heavily studied mainly for off-line speech processing, to the best of our knowledge, the combined effect of these tasks on conversational speech translation has not been investigated. Furthermore, optimization of performance given maximum allowable system latency to enable a conversation is a newer problem for these tasks. We show that the conventional assumption of doing segmentation followed by disfluency removal is not the best practice. We propose a new approach to do simple-disfluency removal followed by segmentation and then by complex-disfluency removal. The proposed approach shows a significant gain on translation performance of up to 3 Bleu points with only 6 second latency to look ahead, using state-of-the-art machine translation and speech recognition systems.", "title": "Segmentation and disfluency removal for conversational speech translation"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/watanabe14_interspeech.html", "abstract": "Statistical dialog managers can potentially make more robust decisions than their rule-based counterparts, because they can account for uncertainties due to errors in speech recognition and natural language understanding. In practice, however, statistical dialog managers can be difficult to use, as they may require a large number of parameters to be inferred from limited data. Consequently, hand-crafted rule based systems are still effective for practical use. This paper proposes a method to integrate an existing rule-based dialog manager with a statistical dialog manager based on Bayes decision theory, by incorporating the rule-based dialog manager into the cost function of the statistical dialog manager. The cost function has two parts: an efficiency cost that penalizes inefficient actions, as in conventional statistical dialog approaches, and a regularization cost that slightly penalizes system actions that differ from those that would be chosen by the rule-based system. Our experiments, which use a destination-setting task in an automobile dialog scenario, demonstrate that the integrated system produces system actions that are similar to those of an existing rule-based dialog manager but enable task completion using fewer turns than the rule-based system.", "title": "Cost-level integration of statistical and rule-based dialog managers"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/kim14b_interspeech.html", "abstract": "Existing spoken dialogue systems are typically not designed to provide natural interaction since they impose a strict turn-taking regime in which a dialogue consists of interleaved system and user turns. To allow more responsive and natural interaction, this paper describes a system in which turn-taking decisions are taken at a more fine-grained micro-turn level. A decision-theoretic approach is then applied to optimise turn-taking control. Inverse reinforcement learning is used to capture the complex but natural behaviours from human-human dialogues and optimise interaction without specifying a reward function manually. Using a corpus of human-human interaction, experiments show that IRL is able to learn an effective reward function which outperforms a comparable handcrafted policy.", "title": "Inverse reinforcement learning for micro-turn management"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/kane14_interspeech.html", "abstract": "For many applications in human-computer interaction, it is desirable to predict between-(gaps) and within-(pauses) speaker silences independently of automatic speech recognition (ASR). In this study, we focus a dataset of 6 dyadic task-based interactions and aim at automatic discrimination of gaps and pauses based on F0, energy and glottal parameters derived from the speech just preceding the silence. Initial manual annotation reveals strong discriminative power of intonation tune types. In a subsequent automatic analysis using descriptive statistics of parameter contours, as well as a modelling of such contours using principal component analysis, we are able to speaker-independently predict pauses and gaps at an accuracy of 70% compared to a 56% baseline", "title": "Analysing the prosodic characteristics of speech-chunks preceding silences in task-based interactions"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/sak14_interspeech.html", "abstract": "Long Short-Term Memory (LSTM) is a specific recurrent neural network (RNN) architecture that was designed to model temporal sequences and their long-range dependencies more accurately than conventional RNNs. In this paper, we explore LSTM RNN architectures for large scale acoustic modeling in speech recognition. We recently showed that LSTM RNNs are more effective than DNNs and conventional RNNs for acoustic modeling, considering moderately-sized models trained on a single machine. Here, we introduce the first distributed training of LSTM RNNs using asynchronous stochastic gradient descent optimization on a large cluster of machines. We show that a two-layer deep LSTM RNN where each LSTM layer has a linear recurrent projection layer can exceed state-of-the-art speech recognition performance. This architecture makes more effective use of model parameters than the others considered, converges quickly, and outperforms a deep feed forward neural network having an order of magnitude more parameters.", "title": "Long short-term memory recurrent neural network architectures for large scale acoustic modeling"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/saon14_interspeech.html", "abstract": "We introduce recurrent neural networks (RNNs) for acoustic modeling which are unfolded in time for a fixed number of time steps. The proposed models are feedforward networks with the property that the unfolded layers which correspond to the recurrent layer have time-shifted inputs and tied weight matrices. Besides the temporal depth due to unfolding, hierarchical processing depth is added by means of several non-recurrent hidden layers inserted between the unfolded layers and the output layer. The training of these models: (a) has a complexity that is comparable to deep neural networks (DNNs) with the same number of layers; (b) can be done on frame-randomized minibatches; (c) can be implemented efficiently through matrix-matrix operations on GPU architectures which makes it scalable for large tasks. Experimental results on the Switchboard 300 hours English conversational telephony task show a 5% relative improvement in word error rate over state-of-the-art DNNs trained on FMLLR features with i-vector speaker adaptation and hessian-free sequence discriminative training.", "title": "Unfolded recurrent neural networks for speech recognition"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/tomar14_interspeech.html", "abstract": "Deep neural networks (DNNs) have been successfully applied to a variety of automatic speech recognition (ASR) tasks, both in discriminative feature extraction and hybrid acoustic modeling scenarios. The development of improved loss functions and regularization approaches have resulted in consistent reductions in ASR word error rates (WERs). This paper presents a manifold learning based regularization framework for DNN training. The associated techniques attempt to preserve the underlying low dimensional manifold based relationships amongst speech feature vectors as part of the optimization procedure for estimating network parameters. This is achieved by imposing manifold based locality preserving constraints on the outputs of the network. The techniques are presented in the context of a bottleneck DNN architecture for feature extraction in a tandem configuration. The ASR WER obtained using these networks is evaluated on a speech-in-noise task and compared to that obtained using DNN-bottleneck networks trained without manifold constraints.", "title": "Manifold regularized deep neural networks"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/li14_interspeech.html", "abstract": "Deep Neural Networks (DNNs) have been shown to outperform traditional Gaussian Mixture Models in many Automatic Speech Recognition tasks. In this work, we investigate the potential of modeling long temporal acoustic contexts using DNNs. The complete temporal context is split into several sub-contexts. Multiple sub-context DNNs initialized with the same set of Restricted Boltzmann Machines are fine-tuned independently and their last hidden layer activations are combined to jointly predict the desired state posteriors through a single softmax output layer. From preliminary experiments on the Aurora2 multi-style training task, our proposed system models a 65-frame temporal window of speech signals and yields a 4.4% WER, outperforming the best single DNN by 12.0% relatively. With the local independence assumption, both training and testing of the sub-context DNNs can be done in parallel. Moreover, our system has a relative 48.2% parameter reduction compared to a single DNN with the same amount of hidden units.", "title": "Modeling long temporal contexts for robust DNN-based speech recognition"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/li14b_interspeech.html", "abstract": "A long deep and wide artificial neural net (LDWNN) with multiple ensemble neural nets for individual frequency subbands is proposed for robust speech recognition in unknown noise. It is assumed that the effect of arbitrary additive noise on speech recognition can be approximated by white noise (or speech-shaped noise) of similar level across multiple frequency subbands. The ensemble neural nets are trained in clean and speech-shaped noise at 20, 10, and 5 dB SNR to accommodate noise of different levels, followed by a neural net trained to select the most suitable neural net for optimum information extraction within a frequency subband. The posteriors from multiple frequency subbands are fused by another neural net to give a more reliable estimation. Experimental results show that the subband ensemble net adapts well to unknown noise.", "title": "A long, deep and wide artificial neural net for robust speech recognition in unknown noise"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/seps14_interspeech.html", "abstract": "This paper studies the use of hybrid context-dependent Deep Neural Network Hidden Markov Model (DNN-HMM) architecture for robust recognition of speech affected by real-world nonlinear distortions. We consider two types of distortions; a) signals distorted through overgained microphone preamplifier in the analog domain and b) recordings exhibiting unnatural spectral sparseness, caused by excessive denoising or low-bit-rate compression. We compare the performance of DNN-HMM architecture with that of the conventional system, based on context-dependent Gaussian Mixture Model (GMM)-HMMs, which applies channel/speaker adaptation and/or feature compensation in the front-end via Histogram Equalization (HEQ). We show that DNN-HMM architecture achieves a significantly lower Word Error Rate (WER) on the considered distorted datasets and that the obtained relative WER reduction is higher than 60%. We also investigate the usefulness of the feature compensation via HEQ for a DNN-HMM system and show that it can be helpful in the case of shallower networks.", "title": "Investigation of deep neural networks for robust recognition of nonlinearly distorted speech"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/banse14_interspeech.html", "abstract": "During late-2013 through early-2014 NIST coordinated a special i-vector challenge based on data used in previous NIST Speaker Recognition Evaluations (SREs). Unlike evaluations in the SRE series, the i-vector challenge was run entirely online and used fixed-length feature vectors projected into a low-dimensional space (i-vectors) rather than audio recordings. These changes made the challenge more readily accessible, especially to participants from outside the audio processing field. Compared to the 2012 SRE, the i-vector challenge saw an increase in the number of participants by nearly a factor of two, and a two orders of magnitude increase in the number of systems submitted for evaluation. Initial results indicate the leading system achieved an approximate 37% improvement relative to the baseline system.", "title": "Summary and initial results of the 2013-2014 speaker recognition i-vector machine learning challenge"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/leeuwen14_interspeech.html", "abstract": "In this paper we study speaker linking (a.k.a. partitioning) given constraints of the distribution of speaker identities over speech recordings. Specifically, we show that the intractable partitioning problem becomes tractable when the constraints pre-partition the data in smaller cliques with non-overlapping speakers. The surprisingly common case where speakers in telephone conversations are known, but the assignment of channels to identities is unspecified, is treated in a Bayesian way. We show that for the Dutch CGN database, where this channel assignment task is at hand, a lightweight speaker recognition system can quite effectively solve the channel assignment problem, with 93% of the cliques solved. We further show that the posterior distribution over channel assignment configurations is well calibrated.", "title": "Constrained speaker linking"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/novoselov14_interspeech.html", "abstract": "This paper presents the Speech Technology Center (STC) system submitted to NIST i-vector challenge. The system includes different subsystems based on TV-PLDA, TV-SVM, and RBM-PLDA. In this paper we focus on examining the third RBM-PLDA subsystem. Within this subsystem, we present our RBM extractor of the pseudo i-vector. Experiments performed on the test dataset of NIST-2014 demonstrate that although the RBM-PLDA subsystem is inferior to the former two subsystems in terms of absolute minDCF, during the final fusion it provides a substantial input into the efficiency of the resulting STC system reaching 0.241 at the minDCF point.", "title": "RBM-PLDA subsystem for the NIST i-vector challenge"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/shum14_interspeech.html", "abstract": "In this paper, we attempt to quantify the amount of labeled data necessary to build a state-of-the-art speaker recognition system. We begin by using i-vectors and the cosine similarity metric to represent an unlabeled set of utterances, then obtain labels from a noiseless oracle in the form of pairwise queries. Finally, we use the resulting speaker clusters to train a PLDA scoring function, which is assessed on the 2010 NIST Speaker Recognition Evaluation. After presenting the initial results of an algorithm that sorts queries based on nearest-neighbor pairs, we develop techniques that further minimize the number of queries needed to obtain state-of-the-art performance. We show the generalizability of our methods in anecdotal fashion by applying our methods to two different distributions of utterances-per-speaker and, ultimately, find that the actual number of pairwise labels needed to obtain state-of-the-art results may be a mere fraction of the queries required to fully label the entire set of utterances.", "title": "Limited labels for unlimited data: active learning for speaker recognition"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/brummer14_interspeech.html", "abstract": "We introduce a Bayesian solution for the problem in forensic speaker recognition, where there may be very little background material for estimating score calibration parameters. We work within the Bayesian paradigm of evidence reporting and develop a principled probabilistic treatment of the problem, which results in a Bayesian likelihood-ratio as the vehicle for reporting weight of evidence. We show in contrast, that reporting a likelihood-ratio distribution does not solve this problem. Our solution is experimentally exercised on a simulated forensic scenario, using NIST SRE'12 scores, which demonstrates a clear advantage for the proposed method compared to the traditional plugin calibration recipe.", "title": "Bayesian calibration for forensic evidence reporting"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/ishihara14_interspeech.html", "abstract": "In this paper, we report on a study which demonstrates that the mismatch in within-speaker replicate numbers (the number of tokens used to model each sample) between test/background and development databases has a large impact on the performance of a forensic voice comparison (FVC) system. We describe how and to what extent the different degrees of the mismatch influence the performance of the FVC system. The performance of an FVC system based on temporal MFCC features and the Multivariate Kernel Density Likelihood Ratio procedure is tested in terms of its validity and reliability under the mismatched conditions. The Monte Carlo technique is employed to repeatedly carry out FVC tests. We report that the databases matched with respect to replicate numbers result in optimal performance in terms of validity, but not in terms of reliability.", "title": "Replicate mismatch between test/background and development databases: the impact on the performance of likelihood ratio-based forensic voice comparison"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/airaksinen14b_interspeech.html", "abstract": "In the analysis of speech production, glottal inverse filtering has proved to be an effective yet non-invasive method for obtaining information about the voice source. One of the main challenges of the existing methods is blind estimation of the contribution of the lip radiation, which must often be manually determined. To obtain a fully automatic system, we propose an automatic method for determining the lip radiation parameter. Our method is based on a physically-motivated quality criteria for the glottal flow, which can be approximated by minimization of the norm-1. Experiments show that the parameters obtained by the automatic method are mostly within the 95% confidence intervals of the mean values obtained by manual tuning by experts.", "title": "Automatic estimation of the lip radiation effect in glottal inverse filtering"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/rosa14_interspeech.html", "abstract": "Here 3D larynx models whose viscoelastic properties of the superficial tissues of the true vocal folds were altered in specific regions at the glottis \u2014 producing different left-right fold asymmetries \u2014 are analyzed. The idea is to initiate studies about the dynamic of larynges under possible pathological conditions. All simulations used a strategy based on the finite element method to solve a fluid-structure interaction (airflow in the larynx) with a contact-impact problem (glottal closure). The results show that glottal closure is directly influenced by the presence and location of a stiffer mass in the surface of one of the vocal folds. Changes in glottal signal F0 (up to +13.7Hz), open quotient (up to 0.5812), and relative jitter (up to 15.2381%) were observed and are discussed here.", "title": "Simulation of 3d larynges with asymmetric distribution of viscoelastic properties in their vocal folds"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/takemoto14_interspeech.html", "abstract": "Acoustic characteristics of the vocal tract have been investigated extensively in the literature using a one-dimensional (1D) acoustic simulation method. Because the 1D method assumes plane wave propagation only, it is recognized to be valid only in the low frequency region (below about 4 or 5 kHz). Recently, a three-dimensional (3D) acoustic simulation method was developed, to obtain more precise acoustic characteristics of the vocal tract. In the present study, from a male's vocal tract shapes, transfer functions were calculated using the 1D and 3D methods and compared with each other to evaluate the valid frequency range of the 1D method. As a result, when acoustic effects of the piriform fossae were considered in the 1D method, the transfer functions agreed with each other up to 7 kHz (ignoring small dips). The 3D method showed that a deep dip was generated at around 8 kHz by the transverse resonance mode in the pharynx. Above this dip frequency, the transfer functions disagreed with each other. Thus, the 1D method is valid up to 7 kHz for this subject. Because this subject has a relatively large vocal tract, in general the upper limit of the valid frequency range could exceed 8 kHz.", "title": "Comparison of vocal tract transfer functions calculated using one-dimensional and three-dimensional acoustic simulation methods"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/kim14c_interspeech.html", "abstract": "Invariant properties of vocal organ controls at an abstract level are crucial for better understanding and modeling of the speech production mechanism. Despite the large variability of articulatory movements at the execution level, the Converter/ Distributor (C/D) model provides a systematic and comprehensive framework for the prosodic organization of speech production, based on the invariant properties of articulatory movements with the concept of \u201ciceberg\u201d region. The goal of this paper is two-fold: (i) to examine the invariant properties in the C/D model in emotional speech, and (ii) to understand emotion-dependent variation patterns of important parameters in the C/D model framework. Experimental results support the validity of strong linear relationship between the speed and excursion of critical articulators at the iceberg points for emotional speech. Also, emotion-dependent variation patterns of the C/D model parameters, (e.g., relatively smaller \u201cshadow\u201d angle and greater syllable magnitude for happiness) are reported. Finally, the emotion-dependent relationships between the abstract-level C/D model parameters and the surface-level parameters of the invariant articulatory behaviors are reported.", "title": "A study of invariant properties and variation patterns in the converter/distributor model for emotional speech"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/hewer14_interspeech.html", "abstract": "Vocal tract magnetic resonance imaging (MRI) has become one of the preferred imaging modalities for the analysis of human speech production. However, the raw image data must be segmented before further analysis can take place. This paper describes a hybrid approach to extract a 3D tongue model from 3D or 2D MRI scans of the vocal tract during speech, which combines unsupervised image segmentation with a mesh deformation technique. An efficient, minimally supervised segmentation algorithm can also be used as an alternative to provide a robust fallback in certain isolated cases. Both image segmentation algorithms produce a point cloud, which is completed and registered by deforming a template mesh to the data. Since the mesh deformation can be applied even with a sparse point cloud, it is possible to extract realistic 3D tongue shapes even from the 2D video frames of real-time MRI. Our approach is applied to several sets of available MRI data and yields promising results.", "title": "A hybrid approach to 3d tongue modeling from vocal tract MRI using unsupervised image segmentation and mesh deformation"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/kaburagi14_interspeech.html", "abstract": "Precise control of articulatory parameters is difficult and prevents a physical model from generating natural sounding speech signals. To determine vocal-tract shape from speech, this paper presents an inversion method for simultaneously estimating the cross-sectional area and length of the vocal tract. In addition, we performed speech resynthesis from a time-series of estimated vocal-tract shapes. The vocal-tract shape is determined through an iterative procedure that gradually optimizes the parameter values to produce the target speech spectrum. The vocal-tract shape is updated using a sensitivity function that represents the change in formant frequency caused by a small perturbation of the vocal-tract shape. When combined with a perturbation relationship of speech spectrum parameters (i.e., cepstrum parameters) and formants, our method effectively optimizes the vocal-tract shape. We quantitatively examined the accuracy using area function data for 10 isolated vowels. The results showed that the average area error was 0.43 cm2 and the average length error was 0.23 cm. This indicates that the vocal-tract shape was determined with satisfactory accuracy. We also performed an estimation experiment for continuous speech and synthesized speech from the estimated vocal-tract shape.", "title": "Estimation of vocal-tract shape from speech spectrum and speech resynthesis based on a generative model"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/benitez14_interspeech.html", "abstract": "Previous work has shown that languages differ in their articulatory setting, the postural configuration that the vocal tract articulators tend to adopt when they are not engaged in any active speech gesture, and that this posture might be specified as part of the phonological knowledge speakers have of the language. This study tests whether the articulatory setting of a language can be acquired by non-native speakers. Three native speakers of German who had learned English as a second language were imaged using real-time MRI of the vocal tract while reading passages in German and English, and features that capture vocal tract posture were extracted from the inter-speech pauses in their native and non-native languages. Results show that the speakers exhibit distinct inter-speech postures in each language, with a lower and more retracted tongue in English, consistent with classic descriptions of the differences between the German and the English articulatory settings. This supports the view that non-native speakers may acquire relevant features of the articulatory setting of a second language, and also lends further support to the idea that articulatory setting is part of a speaker's phonological competence in a language.", "title": "A real-time MRI study of articulatory setting in second language speech"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/arai14_interspeech.html", "abstract": "It is known that American English /r/ can be produced as a retroflex or bunched /r/, but it can be challenging to teach students how to articulate both. We already developed a physical model for retroflex /r/ and demonstrated that the model produces the /r/ sound. However, almost no studies have reported a physical model for bunched /r/. We developed a new physical model using sliding blocks for the lips and tongue to help teach students how to produce bunched /r/. We recorded several sets of sounds produced by the models, analyzed the output signals, and used them for perceptual experiments. Acoustic analysis and perceptual experiments confirmed that the retroflex and bunched /r/ models produced clear American /r/ sounds, and that the narrow constriction placed between 5\u20137 cm from the lips seems to be the key in producing these sounds. Furthermore, bunched /r/ with lip rounding produced the most clear /r/ sound. Both models are helpful for practicing pronunciation because learners can readily see there are two ways to produce /r/, they can see and alter the tongue position manually, and they can hear the output sounds.", "title": "Retroflex and bunched English /r/ with physical models of the human vocal tract"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/rong14_interspeech.html", "abstract": "A combination of parallel factor analysis (PARAFAC) and principal component analysis (PCA) was used to parameterize the articulatory pattern of tongue, jaw and lip movements in 8 English vowels produced by 7 subjects with amyotrophic lateral sclerosis (ALS). A two-factor PARAFAC model derived an overall articulatory pattern represented by two basic modes dominated by tongue raising and advancement, respectively. The relation between the two articulatory modes and the acoustic formants (F1, F2) followed a simple one-to-one linear mapping. The PCA on the residuals of the PARAFAC model showed various individualized articulatory features superimposed on the overall pattern. These articulatory features contributed in a systematic way to the acoustic deviation across different subjects. The parameterization approach (1) provided a simple and generalizable way to explore the underlying articulatory mechanism of speech decline in ALS and (2) accounted for the articulatory features across affected individuals. With further development of the approach and a comparison with the articulatory pattern for healthy subjects, it is possible to derive a set of quantitative articulatory indicators of speech impairment in ALS.", "title": "Parameterization of articulatory pattern in speakers with ALS"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/p14_interspeech.html", "abstract": "Electromagnetic articulography (EMA) data provides the movement of sensors attached to different articulators of a subject when the subject is speaking. EMA data often contains missing segments due to sensor failure. In this work, we propose an equality constrained Kalman smoother to estimate the missing samples in the EMA data. We incorporate the dynamics of the articulatory movement for missing samples estimation by considering the EMA data vector as the observations from a linear dynamical system. The proposed approach gives 41% reduction on the root mean square error of the estimates compared to the minimum mean square error estimator which does not utilize the dynamics of the articulatory movement. When compared to the maximum a-posteriori estimation with continuity constraints (MAPC) which incorporates smoothness of the articulatory trajectory during estimation, the proposed approach gives an average performance improvement of 4.8%.", "title": "Missing samples estimation in electromagnetic articulography data using equality constrained kalman smoother"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/ji14_interspeech.html", "abstract": "The selection of effective articulatory features is an important component of tasks such as acoustic-to-articulator inversion and articulatory synthesis. Although it is common to use direct articulatory sensor measurements as feature variables, this approach fails to incorporate important physiological information such as palate height and shape and thus is not as representative of vocal tract cross section as desired. We introduce a set of articulator feature variables that are palate referenced and normalized with respect to the articulatory working space in order to improve the quality of the vocal tract representation. These features include normalized horizontal positions plus the normalized palatal height of two midsagittal and one lateral tongue sensor, as well as normalized lip separation and lip protrusion. The quality of the feature representation is evaluated subjectively by comparing the variances and vowel separation in the working space and quantitatively through measurement of acoustic-to-articulator inversion error. Results indicate that the palate-referenced features have reduced variance and increased separation between vowels spaces and substantially lower inversion error than direct sensor measures.", "title": "Palate-referenced articulatory features for acoustic-to-articulator inversion"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/uchida14_interspeech.html", "abstract": "The alignment of the transmitter coils for the three-dimensional electromagnetic articulography (3D-EMA), an instrument used to measure articulatory movements, was studied. The receiver coils of the 3D-EMA are used as position markers and are placed in an alternating magnetic field produced by multiple transmitter coils. The estimation of state (the position and orientation) of each receiver coil is based on the minimization of signal error between the measured and predicted receiver signals using a model of the magnetic field. Previous studies report a noticeable increase in the position estimation error at a specific portion of the measurement region irrespective of small signal error values. The existence of non-uniqueness in the position estimation problem is hypothesized to be the cause of this problem. To resolve the problem, we optimized the alignment of the transmitter coils by maximizing the difference between the receiver signals at any two states in the measurement region and evaluated the alignment using a computer simulation and an experiment. As a result, a measurement accuracy of approximately 0.4 mm was obtained.", "title": "A study on the improvement of measurement accuracy of the three-dimensional electromagnetic articulography"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/schuller14_interspeech.html", "abstract": "The INTERSPEECH 2014 Computational Paralinguistics Challenge provides for the first time a unified test-bed for the automatic recognition of speakers' cognitive and physical load in speech. In this paper, we describe these two Sub-Challenges, their conditions, baseline results and experimental procedures, as well as the COMPARE baseline features generated with the openSMILE toolkit and provided to the participants in the Challenge.", "title": "The INTERSPEECH 2014 computational paralinguistics challenge: cognitive & physical load"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/pohjalainen14_interspeech.html", "abstract": "This paper investigates approaches to modeling the time evolution of short-time spectral features in paralinguistic speech type classification, where we focus on detection of speech influenced by physical exertion. The time series model consists of autoregressive processes of multiple time scales and orders and is trained to describe the long-term dynamics of a given target speech class. Themodel is applied in two ways in improving long-term modeling in the detection task: 1) to perform predictive filtering of the features and 2) to automatically select instantaneous classification subspaces. The spectrum analysis method underlying the short-time features is also varied between the standard discrete Fourier transform and a time-weighted linear predictive method which yields smooth all-pole spectrum envelope models. Configurations of the proposed methods are evaluated in the Physical Load task of the Interspeech 2014 Computational Paralinguistics Challenge and show improvement over the baseline timbral classifier and the challenge baseline. Also the interrelationships among the methods are discussed.", "title": "Filtering and subspace selection for spectral features in detecting speech under physical stress"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/li14c_interspeech.html", "abstract": "This paper presents an automatic speaker physical load recognition approach using posterior probability based features from acoustic and phonetic tokens. In this method, the tokens for calculating the posterior probability or zero-order statistics are extended from the conventional MFCC trained Gaussian Mixture Models (GMM) components to parallel phonetic phonemes and tandem feature trained GMM components. Phoneme recognizers from five different languages are employed to extract the phoneme posterior probabilities. We show that these histogram style features at both the acoustic and phonetic levels are effective and complementary for capturing the speaker physical load information from short utterances. Support vector machine is adopted as the supervised classifier. By combining the proposed methods with the OpenSMILE baseline which covers the acoustic and prosodic information further improves the final performance. The proposed fusion system achieves 70.18% and 72.81% unweighted accuracy on the validation and test set of the Munich Bio-voice Corpus for the binary physical load level recognition task in the INTERSPEECH 2014 Computational Paralinguistics Challenge.", "title": "Automatic recognition of speaker physical load using posterior probability based features from acoustic and phonetic tokens"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/kaya14_interspeech.html", "abstract": "In this study we present our system for INTERSPEECH 2014 Computational Paralinguistics Challenge (ComParE 2014), Physical Load Sub-challenge (PLS). Our contribution is twofold. First, we propose using Low Level Descriptor (LLD) information as hints, so as to partition the feature space into meaningful subsets called views. We also show the virtue of commonly employed feature projections, such as Canonical Correlation Analysis (CCA) and Local Fisher Discriminant Analysis (LFDA) as ranking feature selectors. Results indicate the superiority of multi-view feature reduction approach to its single-view counterpart. Moreover, the discriminative projection matrices are observed to provide valuable information for feature selection, which generalize better than the projection itself. In our preliminary experiments we reached 75.35% Unweighted Average Recall (UAR) on PLS test set, using CCA based multi-view feature selection.", "title": "Canonical correlation analysis and local fisher discriminant analysis based multi-view acoustic feature reduction for physical load prediction"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/jing14_interspeech.html", "abstract": "We present our methods and results on participating in the Interspeech 2014 Computational Paralinguistics ChallengE (ComParE) of which the goal is to detect certain type of load of a speaker using acoustic features. There are in total seven classification models contributing to our final prediction, namely, neural network with rectified linear unit and dropout (ReLUNet), conditional restricted Boltzmann machine (CRBM), logistic regression (LR), support vector machine (SVM), Gaussian discriminant analysis (GDA), k-nearest neighbors (KNN), and random forest (RF). When linearly blending the predictions of these models, we are able to get significant improvements over the challenge baseline.", "title": "Ensemble of machine learning algorithms for cognitive and physical speaker load detection"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/gosztolya14_interspeech.html", "abstract": "The Interspeech ComParE 2014 Challenge consists of two machine learning tasks, which have quite a small number of examples. Due to our good results in ComParE 2013, we considered AdaBoost a suitable machine learning meta-algorithm for these tasks, besides we also experimented with Deep Rectifier Neural Networks. These differ from traditional neural networks in that the former have several hidden layers, and use rectifier neurons as hidden units. With AdaBoost we achieved competitive results, whereas with the neural networks we were able to outperform baseline SVM scores in both Sub-Challenges.", "title": "Detecting the intensity of cognitive and physical load using AdaBoost and deep rectifier neural networks"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/montacie14_interspeech.html", "abstract": "The Cognitive Load (CL) refers to the load imposed on an individual's cognitive system when performing a given task, and is usually associated with the limitations of the human working memory. Stress, fatigue, lower ability to make decisions and perceptual narrowing are induced by cognitive overload which occurs when too much information has to be processed. As many physiological measures and for a nonintrusive measurement, speech features have been investigated in order to find reliable indicators of CL levels. In this paper, we have investigated high-level speech events automatically detected using the CMU-Sphinx toolkit for speech recognition. Temporal events (speech onset latency, event starting time-codes, pause and phone segments) were extracted from the speech transcriptions (phoneme, word, silent pause, filled pause, breathing). Seven audio feature sets related to the speech events were designed and assessed. Three-class SVM classifiers (Low, Medium and High level) were developed and assessed on the CSLE (Cognitive-Load with Speech and EGG) databases provided for the Interspeech'2014 Cognitive Load Sub-Challenge. These experiments have shown an improvement of 1.5% on the Test set compared to the official baseline Unweighted Average Recall (UAR).", "title": "High-level speech event analysis for cognitive load classification"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/nwe14_interspeech.html", "abstract": "This paper presents a method for detecting cognitive load levels from speech. When speech is modulated by different levels of cognitive load, acoustic characteristics of speech change. In this paper, we measure acoustic distance of a stressed utterance from the baseline stress free speech using GMM-SVM kernel with Bhattacharyya based GMM distance. In addition, it is believed that airflow structure of speech production is nonlinear. This motivates us to investigate better techniques to capture nonlinear characteristic of stress information in acoustic features. Inspired by the recent success of neural networks for representation learning, we employ a single hidden layer feed forward network with non-linear activation to extract the feature vectors. Furthermore, people have different reactions to a particular task load. This inter-speaker difference in stress responses presents a major challenge for stress level detection. We use a bootstrapped training process to learn the stress response of a particular speaker. We perform experiments using data sets from Cognitive Load with Speech and EGG (CLSE) provided for the Cognitive Load Sub-Challenge of the INTERSPEECH 2014 Computational Paralinguistics Challenge. The results show that the system with our proposed strategies performs well on validation and test sets.", "title": "On the use of Bhattacharyya based GMM distance and neural net features for identification of cognitive load levels"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/huckvale14_interspeech.html", "abstract": "This paper describes the UCL system for the cognitive load task of the Interspeech 2014 Computational Paralinguistics Challenge. The UCL system evaluates whether additional voice features computed by the VOQAL voice analysis toolbox improves performance over the baseline feature set. 144 different system configurations are evaluated on the development test set, with some systems achieving 100% classification accuracy of cognitive load in the two Stroop subtasks. The difficulty of the reading span sub-task is shown to be caused in part by the duration of the audio material. Performance of the best systems on the test set confirm the importance of building speaker dependent systems. While the VOQAL augmented features gave the best performance on the development test set, no benefit was found for the test set.", "title": "Prediction of cognitive load from speech with the VOQAL voice quality toolbox for the interspeech 2014 computational paralinguistics challenge"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/kua14_interspeech.html", "abstract": "Speech based cognitive load estimation is a new field of research. Due to this relative `lack of maturity', a single best approach to building cognitive load estimation systems has not been established yet. The primary aim of this submission is to report the performance of various basic utterance level classification frameworks developed using important elements of state-of-the-art speaker recognition systems. This may lead to a suitable basis for future cognitive load estimation systems. As a consequence of being a part of a challenge, it is expected that these frameworks will be compared to a much larger number of alternative approaches than what would otherwise be possible. In keeping with this focused aim, the GMM supervector approaches along with some variants are utilised. The systems outlined in this paper include a frame-level MFCC-GMM system along with utterance level GMM-supervector-SVM, GMM-ivector-SVM and GMM-JFA-SVM systems. The best combined system has an accuracy (UAR) of 66.6% as evaluated on the challenge development set and 63.7% as evaluated on the test set.", "title": "The UNSW submission to INTERSPEECH 2014 compare cognitive load challenge"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/segbroeck14_interspeech.html", "abstract": "The goal in this work is to automatically classify speakers' level of cognitive load (low, medium, high) from a standard battery of reading tasks requiring varying levels of working memory. This is a challenging machine learning problem because of the inherent difficulty in defining/measuring cognitive load and due to intra-/inter-speaker differences in how their effects are manifested in behavioral cues. We experimented with a number of static and dynamic features extracted directly from the audio signal (prosodic, spectral, voice quality) and from automatic speech recognition hypotheses (lexical information, speaking rate). Our approach to classification addressed the wide variability and heterogeneity through speaker normalization and by adopting an i-vector framework that affords a systematic way to factorize the multiple sources of variability.", "title": "Classification of cognitive load from speech using an i-vector framework"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/iyer14_interspeech.html", "abstract": "Cherry (1953) reported that when listeners were presented with dichotic signal over headphones, they could reliably report words presented to the attended ear, while only being aware of the gross properties of the talker in the unattended ear. More recently, Gallun et al. (2007) showed that there were large differences in performance on dichotic tasks depending on ear of presentation, with significantly larger errors occurring when the target was presented to the left, rather than right ear (i.e., a right-ear advantage). In the current experiment, we explored two factors, type of signal in the non-target ear and uncertainty about the target ear, and their effects on right-ear advantage. The results indicated that the right-ear advantage was modulated by two factors: 1) nature of the speech stimuli presented in the unattended ear, and 2) target ear uncertainty. Substantial differences were observed between listeners in the tasks, leading to varying amounts of right-ear advantage across listeners for the listening conditions tested. These results and their implications for the design of multichannel speech communication displays are discussed, and the use of these methods is recommended as a useful screening tool for selection of personnel who listen to and use multichannel speech displays.", "title": "Revisiting the right-ear advantage for speech: implications for speech displays"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/bosch14_interspeech.html", "abstract": "This paper addresses the question how to compare reaction times computed by a computational model of speech comprehension with observed reaction times by participants. The question is based on the observation that reaction time sequences substantially differ per participant, which raises the issue of how exactly the model is to be assessed. Part of the variation in reaction time sequences is caused by the so-called local speed: the current reaction time correlates to some extent with a number of previous reaction times, due to slowly varying variations in attention, fatigue etc. This paper proposes a method, based on time series analysis, to filter the observed reaction times in order to separate the local speed effects. Results show that after such filtering the between-participant correlations increase as well as the average correlation between participant and model increases. The presented technique provides insights into relevant aspects that are to be taken into account when comparing reaction time sequences.", "title": "Comparing reaction time sequences from human participants and computational models"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/andrei14_interspeech.html", "abstract": "This study describes an experiment designed to establish the maximum number of competing speakers that can be detected accurately by a human listener and compares the results with the ones produced by using a distance based estimator working in frequency domain. We mixed a set of high quality audio samples with continuous speech, produced by publicly known people (actors, journalists and politicians) and also unknown persons and then we played the tracks to each listener within a target group. The volunteers were asked how many cumulated speakers they counted and how they obtained the response. We observed that while human subjects showed a correct detection ratio of 31%, we were able to establish a set of equally spaced thresholds for the estimator in order to achieve 66% accuracy. The paper also summarizes the methods that were reported by the listeners to help in the detection.", "title": "Detecting the number of competing speakers \u2014 human selective hearing versus spectrogram distance based estimator"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/li14d_interspeech.html", "abstract": "Studies on talker normalization have reported that contexts would affect how a vowel/consonant/tone is perceived, which suggests that listeners use cues in the context as a reference for speech perception. However, it is unclear how the cues of the context are encoded in memory, and to what extent the context effects are influenced by interruption in sensory memory and reduction in attentional resources. To fill in this gap, this study examined the effects of noise interruption and a secondary visual task on the identification of words carrying level tones in context by native Cantonese speakers. Experiment 1 compared the tone identification performance between a block where a 300ms noise was presented immediately after the context to interrupt the normalization effects of the context on the following target word and a block without noise interruption. Experiment 2 compared the tone identification performance between a block where participants had to perform a secondary visual task (picture same/different discrimination) and a block without a secondary visual task. Results suggest that context cues are likely encoded both in sensory memory and in short-term categorical memory and that reduction in attentional resources has marginal influence on tone normalization.", "title": "The influence of sensory memory and attention on the context effect in talker normalization"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/lin14_interspeech.html", "abstract": "The aim of this study is to devise a computational method to predict cochlear implant (CI) speech recognition. Here, we describe a high-throughput screening system for optimizing CI speech processing strategies using hidden Markov model (HMM)-based automatic speech recognition (ASR). Word accuracy was computed on vocoded CI speech synthesized from primarily multi-channel temporal envelope information. The ASR performance increased with the number of channels in a similar manner displayed in human recognition scores. Results showed the computational method of HMM-based ASR offers better process control for comparing signal carrier type. Training-test mismatch reduction provided a novel platform for reevaluating the relative contributions of spectral and temporal cues to human speech recognition.", "title": "Automatic speech recognition with primarily temporal envelope information"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/lai14_interspeech.html", "abstract": "Hearing-impaired patients have limited hearing dynamic range for speech perception, which partially accounts for their poor speech understanding abilities, particularly in noise. Wide dynamic range compression aims to compress speech signal into the usable hearing dynamic range of hearing-impaired listeners; however, it normally uses a static compression based strategy. This work proposed a strategy to continuously adjust the envelope compression ratio for speech processing in cochlear implants, named adaptive envelope compression (AEC) strategy. This AEC strategy aims to keep the compression processing as close to linear as possible, while still confine the compressed amplitude envelope within the pre-set dynamic range. Vocoded simulation experiments showed that, when narrowed down to a small dynamic range, the intelligibility of AEC-processed sentences was significantly better than those processed by static envelope compression. This makes the proposed AEC strategy a promising way to improve speech recognition performance for implanted patients in the future.", "title": "An adaptive envelope compression strategy for speech processing in cochlear implants"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/helfer14_interspeech.html", "abstract": "Speech analysis has shown potential for identifying neurological impairment. With brain trauma, changes in brain structure or connectivity may result in changes in source, prosodic, or articulatory aspects of voice. In this work, we examine the articulatory components of speech reflected in formant tracks, and how changes in track dynamics and coordination map to cognitive decline. We address a population of athletes regularly receiving impacts to the head and showing signs of preclinical mild traumatic brain injury (mTBI), a state indicated by impaired cognitive performance occurring prior to concussion. We hypothesize that this preclinical damage results in 1) changes in average vocal tract dynamics measured by formant frequencies, their velocities, and acceleration, and 2) changes in articulatory coordination measured by a novel formant-frequency cross-correlation characterization. These features allow machine learning algorithms to detect preclinical mTBI identified by a battery of cognitive tests. A comparison is performed of the effectiveness of vocal tract dynamics features versus articulatory coordination features. This evaluation is done using receiver operating characteristic (ROC) curves along with confidence bounds. The articulatory dynamics features achieve area under the ROC curve (AUC) values between 0.72 and 0.98, whereas the articulatory coordination features achieve AUC values between 0.94 and 0.97.", "title": "Articulatory dynamics and coordination in classifying cognitive change with preclinical mTBI"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/jinbo14_interspeech.html", "abstract": "Hearing impairment simulation is an effective technique to educate normal-hearing people about auditory perception of the hearing-impaired. Because auditory characteristics of the hearing impaired vary greatly from person-to-person, personalization of the hearing impairment simulation systems is essential to accurately simulate these individual differences. However, measurement of auditory characteristics of individuals is time-consuming work. In this paper, we propose a hearing impairment simulation method that is easily applied to individual hearing-impaired persons. Auditory filter characteristics and gain characteristics are estimated from easily measurable audiograms of each individual. We also implement a method for manually adjusting the hearing impairment level to improve accuracy of the proposed hearing impairment simulation. An experimental evaluation is conducted to compare intelligibility between hearing-impaired and normal-hearing persons with the proposed hearing impairment simulation. The experimental results show that the proposed method effectively makes the word correct rate and phoneme confusion tendency of the normal hearing persons similar to those of the hearing impaired persons.", "title": "A hearing impairment simulation method using audiogram-based approximation of auditory charatecteristics"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/wang14c_interspeech.html", "abstract": "In this paper, we investigate the relative perceptual importance of the temporal envelop (TE) and temporal fine structure (TFS) between tonal language and non-tonal language perception. The \u201cauditory chimera\u201d experiment is conducted on both American English and Mandarin Chinese with the same conditions. Our experimental results show that there is no significant perceptual difference of TE and TFS between Mandarin Chinese and American English when the interference noise is speech-shaped noise. However, there is a distinct relative perceptual importance difference when the interference noise is a competing speech. The results also show that speech perception in Mandarin Chinese language is more sensitive to TFS distortion than is American English. In addition, with fewer auditory channels (one or two bands), speech perception in American English is more sensitive to TE distortion compared to Mandarin Chinese. Finally, the related discussions are made according to the perceptual difference between tonal language and non-tonal language regarding the speech signal processing strategies.", "title": "Investigation of the relative perceptual importance of temporal envelope and temporal fine structure between tonal and non-tonal languages"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/fogerty14_interspeech.html", "abstract": "The current study investigated the spectral distribution of vowel cues to Mandarin and English sentence intelligibility. Sentences were segmentally interrupted to preserve various amounts of vowel information. Interruption parameters ensured that similar durations of speech were presented across the two languages. The remaining vowel cues were then either high-pass or low-pass filtered. Results demonstrated significant contributions of information present during vowels to sentence intelligibility. Performance was much higher in most of the conditions for Mandarin speech. In addition, Mandarin vowels contained a higher distribution of cues in the low-frequencies, below 1000 Hz. Results suggest that Mandarin vowels carry additional cues in the low-frequencies, most likely related to lexical tone. This is further supported by the highly similar performance between Mandarin and English sentence intelligibility when spectral cues are confined to frequencies above 2000 Hz.", "title": "Vowel spectral contributions to English and Mandarin sentence intelligibility"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/mittal14_interspeech.html", "abstract": "In this paper, we study the significance of aperiodicity in the pitch-perception of expressive voices such as Noh voice and laughter signals. The excitation source characteristics in the production of these signals is represented in terms of a sequence of impulses. The impulse sequence is derived from the acoustic signal using a modified zero-frequency filtering method. The time intervals between successive impulses and relative amplitudes of impulses are related to the presence of subharmonics and pitch-perception in expressive voices. The role of aperiodicity and subharmonics in the perception of distinct voice quality of expressive voices is examined. The significance of aperiodicity is also analysed by synthesis, using two synthetic AM/FM sequences for excitation. Saliency is used as a measure of pitch perception. The F0 extraction using this pitch perception information for expressive voices is also demonstrated.", "title": "Significance of aperiodicity in the pitch perception of expressive voices"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/wester14_interspeech.html", "abstract": "This paper describes a corpus of conversations recorded using an extension of the DiapixUK task: the Diapix Foreign Language corpus (DIAPIX-FL) . English and Spanish native talkers were recorded speaking both English and Spanish. The bidirectionality of the corpus makes it possible to separate language (English or Spanish) from speaking in a first language (L1) or second language (L2). An acoustic analysis was carried out to analyse changes in F0, voicing, intensity, spectral tilt and formants that might result from speaking in an L2. The effect of L1 and nativeness on turn types was also studied. Factors that were investigated were pausing, elongations, and incomplete words. Speakers displayed certain patterns that suggest an on-going process of L2 phonological acquisition, such as the overall percentage of voicing in their speech. Results also show an increase in hesitation phenomena (pauses, elongations, incomplete turns), a decrease in produced speech and speech rate, a reduction of F0 range, raising of minimum F0 when speaking in the non-native language which are consistent with more tentative speech and may be used as indicators of non-nativeness.", "title": "DIAPIX-FL: a symmetric corpus of problem-solving dialogues in first and second languages"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/coupe14_interspeech.html", "abstract": "Recent research on speech rate (Pellegrino et al., 2011) has shown that languages differ in terms of syllable rate, and that these differences are compensated by the average amount of information carried by syllables. The more syllables a language needs to express a given amount of information, the higher its syllable rate tends to be.   These results were obtained with subjects reading texts on a computer screen. The question arose whether silent reading rates would correlate with oral reading rates across languages. Although silent and oral reading fluency has been studied with respect to reading comprehension and how it develops in children, little literature focuses on comparing them in different languages.   We present here data for 8 languages (Cantonese, Finnish, French, Japanese, Korean, Mandarin, Serbian, and Thai). For each language, silent and oral syllables rates as well as reading durations were measured for several subjects and 15 different texts. Various comparisons were performed, and mixed-effects regression models were used to further evaluate the weight of the different variables (gender, language, speaker and text).   Most significantly, oral and silent reading rates appear to be well correlated, suggesting that language-specific syllabic complexity impacts silent reading in a similar way to oral reading.", "title": "Cross-linguistic investigations of oral and silent reading"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/coumans14_interspeech.html", "abstract": "When listening in noisy conditions, word recognition seems to be much harder in a non-native language than in one's native language. Native listeners use both word-initial and word-final information for word recognition in clean listening conditions, where word-initial information is the most important. When listening in noise, however, word-final information becomes relatively more important. This study investigates whether non-native listeners are able to use word-initial and word-final information when recognizing words in noise, and whether these information sources are equally important when listening conditions become increasingly harder. Forty-seven Dutch students participated in an English word recognition experiment, where either a word's onset or offset was masked by speech-shaped noise with different signal-to-noise ratios. The results showed that non-native listeners are able to use both word-initial and word-final information for word recognition, but fewer words were recognized with increasing difficulty of the listening conditions when the onset of words was masked. Thus, word-initial information is more important than word-final information for word recognition when listening conditions become harder. This increasing effect occurred independently from the proficiency level in the non-native language of the participants, although proficiency level was correlated to test performance in general.", "title": "Non-native word recognition in noise: the role of word-initial and word-final information"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/wong14_interspeech.html", "abstract": "This study investigated the effects of two perceptually-based training paradigms in both the perception and production of English /e/ and /\u00e6/ by Cantonese ESL learners with high and low listening and oral proficiency levels. Sixty-four subjects participated in the study, in which 22 (9 with high proficiency, H-HV; 13 with low proficiency, L-HV) were trained under High Variability Phonetic Training (HVPT) approach, 19 (8 with high proficiency, H-LV; 11 with low proficiency, L-LV) were trained under Low Variability Phonetic Training (LVPT) approach whereas 23 (10 with high proficiency, H-CO; 13 with low proficiency, L-CO) were the control subjects. Both training approaches were effective in improving the subjects' perception of the two vowels, with HVPT groups showing more robust improvement than LVPT groups. Perceptual learning could also be generalized to new words and new speakers and be transferred to the production domain, with HVPT groups outperforming LVPT groups. However, subjects with different proficiency levels learned to similar degrees in all tests. The results demonstrated that both approaches offered a type of learning that allows attention to focus on phonetic information, which is different from what is learned in an L2 classroom; whereas stimulus variability also plays a role in the learning.", "title": "The effects of high and low variability phonetic training on the perception and production of English vowels /e/-/\u00e6/ by Cantonese ESL learners with high and low L2 proficiency levels"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/burgos14_interspeech.html", "abstract": "In this paper we present a study on Dutch vowel production by Spanish learners that was carried out within the framework of our research on Computer Assisted Pronunciation Training (CAPT). The aim of this study was to obtain detailed information on production of Dutch vowels by Spanish learners, which can be employed to develop effective CAPT programs for this specific target group. We collected speech from learners with varying proficiency levels (A1 - B2 of the CEFR), which was transcribed, segmented and acoustically analyzed. We present data on the frequency of pronunciation errors and on detailed analyses of duration and acoustic properties of the vocalic realizations. The results indicate that Spanish learners of Dutch have difficulties in realizing several Dutch vowel contrasts and that they differ from native speakers in the way they employ duration and spectral properties to realize these contrasts. We discuss these results in relation to those of previous studies on Dutch vowel perception by Spanish listeners and relate them to current theories on speech learning.", "title": "Dutch vowel production by Spanish learners: duration and spectral features"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/lengeris14_interspeech.html", "abstract": "This study investigated English consonant identification by Greek listeners and the role of phonological short-term memory (PSTM) in listeners' identification ability. Twenty Greek university students who had received formal instruction in English identified 24 English consonants (embedded in VCV syllables) presented in quiet and in two noise types, a competing talker at a signal-to-noise ratio (SNR) of -6dB and an 8-speaker babble at an SNR of -2dB. Participants' PSTM was assessed via a serial non-word recognition task in Greek. The results showed that identification scores in quiet were significantly higher than in noise. There was no difference in scores between the two noise conditions. PSTM correlated with English consonant identification in quiet and in the two types of noise; listeners with greater PSTM capacity were also better in identifying English consonants in quiet and noise, a finding that extends previous research in quiet to L2 perception in adverse listening conditions. English consonant confusion patterns are interpreted as caused by a combination of first-language interference (at both the phonetic and phonological levels) and spectral/articulatory factors.", "title": "English consonant confusions by Greek listeners in quiet and noise and the role of phonological short-term memory"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/detey14_interspeech.html", "abstract": "Processing large amounts of non-native (L2) phonological data for acquisition-related research remains a challenging task, especially when acoustic analyses are not straightforward as is the case with nasal vowels. Within the InterPhonologie du Fran\u00e7ais Contemporain project (IPFC), we have developed a perceptual coding procedure and a piece of dedicated software aimed at providing an intermediate stage between fine-grained acoustic analyses and coarse-grained phonological categorization, such as `substitution' or `deletion', of non-native productions. Our code allows us to examine the left and right phonological contexts of the segment under scrutiny and assess the nasality, quality and potential consonantal excrescences of the non-native nasal vowels. We have applied this procedure to Japanese data collected in a longitudinal study of French interphonology, focusing on the vowels /A~/, /O~/, /E~/ produced by 22 beginner university students in a wordlist repetition and reading task. Our study reveals an overall good production rate in terms of nasality for such beginner learners but also a lower rate of quality accuracy for the three vowels, as well as better performances in the repetition task. We discuss our results in light of current L2 learning theories and the phonetic-phonological characteristics of Japanese.", "title": "Corpus-based L2 phonological data and semi-automatic perceptual analysis: the case of nasal vowels produced by beginner Japanese learners of French"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/pinter14_interspeech.html", "abstract": "This study provides a direct comparison of boundary and prominence perception strategies between Japanese EFL learners and native speakers of English using the Rapid Prosody Transcription (RPT) method. Although RPT experiments are available for both native English speakers and Japanese EFL learners, a direct comparison of the available data is problematic as the stimuli sets used in the experiments are not identical. The present research addresses this issue by using identical stimuli sets across L1 and L2 listeners. The data for native English speakers was taken from RPT experiments carried out by Jennifer Cole with Yoonsook Mo and colleagues [1\u20133, 5\u20138]. The non-native data was collected by re-running a subset of RPT tasks reported in the work by Cole and Mo with 108 Japanese undergraduate students. Although native speakers perceived more boundaries and prominent words than L2 speakers, the results outlined surprisingly similar perceptual strategies. A strong correlation was found between the responses of native speakers and Japanese learners of English in both boundary and prominence perception tasks. In boundary perception both groups relied heavily on silent pauses and vocal fillers. In prominence detection the responses correlated with vowel duration, maximum amplitude, and maximum pitch in this specific order for both language groups.", "title": "Perception of prosodic prominence and boundaries by L1 and L2 speakers of English"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/kalathottukaren14_interspeech.html", "abstract": "Twenty-five school aged children with normal hearing were tested on their perception of prosody using the receptive prosody subtests of the Profiling Elements of Prosody in Speech-Communication (PEPS-C) and Child Paralanguage subtest of Diagnostic Analysis of Non Verbal Accuracy 2 (DANVA 2). Performance of four children with hearing loss on the two prosody measures was compared with performance of normal hearing children. Children were also tested on their reading accuracy, comprehension of nonliteral language, and music and tonal pitch discrimination. Overall results showed that younger children aged 7;1 to 9;11 years had significantly poorer scores than 10;1 to 12;11 year olds on the Contrastive Stress Reception subtest of PEPS-C and the DANVA 2 Child Paralanguage subtest, indicating a developmental effect on speech prosody perception. Children with hearing loss had poorer scores and greater variability on PEPS-C and DANVA 2 assessments compared to normal hearing controls. Statistically significant correlations were observed between prosody perception scores and musical pitch perception and reading measures for the normal hearing group. This is consistent with previous studies showing links between reading and prosody perception. Significant correlation between prosody perception and musical pitch discrimination indicates that pitch is an important cue for prosody perception.", "title": "Prosody perception, reading accuracy, nonliteral language comprehension, and music and tonal pitch discrimination in school aged children"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/drozdova14_interspeech.html", "abstract": "Previous studies have demonstrated that native listeners modify their interpretation of a speech sound when a talker produces an ambiguous sound in order to quickly tune into a speaker, but there is hardly any evidence that non-native listeners employ a similar mechanism when encountering ambiguous pronunciations. So far, one study demonstrated this lexically-guided perceptual learning effect for nonnatives, using phoneme categories similar in the native language of the listeners and the non-native language of the stimulus materials. The present study investigates the question whether phoneme category retuning is possible in a nonnative language for a contrast, /l/-/r/, which is phonetically differently embedded in the native (Dutch) and nonnative (English) languages involved. Listening experiments indeed showed a lexically-guided perceptual learning effect. Assuming that Dutch listeners have different phoneme categories for the native Dutch and non-native English /r/, as marked differences between the languages exist for /r/, these results, for the first time, seem to suggest that listeners are not only able to retune their native phoneme categories but also their non-native phoneme categories to include ambiguous pronunciations.", "title": "Phoneme category retuning in a non-native language"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/chiou14_interspeech.html", "abstract": "In this paper, we investigate cross-lingual automatic speech emotion recognition. The basic idea is that since the emotion recognition system is based on the acoustic features only, it is possible to combine data in different languages to improve the recognition accuracy. We begin with the construction of a Mandarin database of emotional speech, which is similar to the well-known Berlin Database of Emotional Speech (EMO-DB) in the composition and size. In order to reduce the variability due to different languages and different speakers, we propose to apply histogram equalization as a data normalization method. Recognition systems based on support vector machines have been evaluated on EMO-DB. Compared to the baseline system without multi-lingual databases and data normalization, the proposed system has achieved a relative improvement of 39.9% in the emotion recognition accuracy, from 86.2% to 91.7%. The accuracy is among the best known results reported on EMO-DB, if not the best.", "title": "Speech emotion recognition with cross-lingual databases"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/inoue14_interspeech.html", "abstract": "We present a novel speaker diarization method by using eye-gaze information in multi-party conversations. In real environments, speaker diarization or speech activity detection of each participant of the conversation is challenging because of distant talking and ambient noise. In contrast, eye-gaze information is robust against acoustic degradation, and it is presumed that eye-gaze behavior plays an important role in turn-taking and thus in predicting utterances. The proposed method stochastically integrates eye-gaze information with acoustic information for speaker diarization. Specifically, three models are investigated for multi-modal integration in this paper. Experimental evaluations in real poster sessions demonstrate that the proposed method improves accuracy of speaker diarization from the baseline acoustic method.", "title": "Speaker diarization using eye-gaze information in multi-party conversations"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/huang14_interspeech.html", "abstract": "We address the problem of speaker clustering for robust unsupervised speaker diarization. We model each speaker-homogeneous segment as one single full multivariate Gaussian probability density function (pdf) and take into consideration the Riemannian property of Gaussian pdfs. By assuming that segments from different speakers lie on different (possibly intersected) sub-manifolds of the manifold of Gaussian pdfs, we formulate the original problem as a Riemannian manifold clustering problem. To apply the computationally simple Riemannian locally linear embedding (LLE) algorithm, we impose a constraint on the length of each segment so as to ensure the fitness of single-Gaussian modeling and to increase the chance that all k-nearest neighbors of a pdf are from the same sub-manifold (speaker). Experiments on the microphone-recorded conversational interviews from NIST 2010 speaker recognition evaluation set demonstrate promising results of less than 1% DER.", "title": "Unsupervised speaker diarization using riemannian manifold clustering"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/delgado14_interspeech.html", "abstract": "Speaker diarization is the task of partitioning an audio stream into homogeneous segments according to speaker identity. Today state-of-the-art speaker diarization systems have achieved very competitive performance. However, any small improvement in Diarization Error Rate (DER) is usually subject to very large processing times (real time factor above one), which makes systems not suitable for some time-critical, real-life applications. Recently, a novel fast speaker diarization technique based on speaker modeling using binary keys was presented. The proposed technique speeds up the process up to ten times faster than real-time with little increase of DER. Although the approach shows great potential, the presented results are still preliminary. The goal of this paper is to further investigate this technique, in order to move towards a complete binary key based system for the speaker diarization task. Preliminary experiments in Speech Activity Detection (SAD) based on binary keys show the feasibility of the binary key modeling approach for this task. Furthermore, the system has been tested on two different kinds of test data: meeting audio recordings and TV shows. The experiments carried out on NIST RT05 and REPERE databases show promising results and indicate that there is still room for further improvement.", "title": "Towards a complete binary key system for the speaker diarization task"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/ghaemmaghami14_interspeech.html", "abstract": "In this paper we present a novel scheme for improving speaker diarization by making use of repeating speakers across multiple recordings within a large corpus. We call this technique speaker re-diarization and demonstrate that it is possible to reuse the initial speaker-linked diarization outputs to boost diarization accuracy within individual recordings. We first propose and evaluate two novel re-diarization techniques. We demonstrate their complementary characteristics and fuse the two techniques to successfully conduct speaker re-diarization across the SAIVT-BNEWS corpus of Australian broadcast data. This corpus contains recurring speakers in various independent recordings that need to be linked across the dataset. We show that our speaker re-diarization approach can provide a relative improvement of 23% in diarization error rate (DER), over the original diarization results, as well as improve the estimated number of speakers and the cluster purity and coverage metrics.", "title": "An iterative speaker re-diarization scheme for improving speaker-based entity extraction in multimedia archives"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/gebre14_interspeech.html", "abstract": "We demonstrate how the problem of speaker diarization can be solved using both gesture and speaker parametric models. The novelty of our solution is that we approach the speaker diarization problem as a speaker recognition problem after learning speaker models from speech samples corresponding to gestures (the occurrence of gestures indicates the presence of speech and the location of gestures indicates the identity of the speaker). This new approach offers many advantages: comparable state-of-the-art performance, faster computation and more flexibility. In our implementation, parametric models are used to model speakers' voice and their gestures: more specifically, Gaussian mixture models are used to model the voice characteristics of each person and all persons, and gamma distributions are used to model gestural activity based on features extracted from Motion History Images. Tests on 4.24 hours of the AMI meeting data show that our solution makes DER score improvements of 19% on speech-only segments and 4% on all segments including silence (the comparison is with the AMI system).", "title": "Speaker diarization using gesture and speech"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/dupuy14_interspeech.html", "abstract": "Current cross-show diarization systems are mainly based on an overall clustering process which handles all the shows within a collection simultaneously. This approach has already been studied in various situations and seems to be the best way so far to achieve low error rates. However, this process has limits in realistic applicative contexts where large and dynamically increasing collections have to be processed. In this paper we investigate the use of an incremental clustering cross-show speaker diarization architecture to iteratively process new shows within an existing collection. The new shows to be inserted are processed one after another, according to the chronological order of their broadcasting dates. Experiments were conducted on the data distributed for the ETAPE and the REPERE French evaluation campaigns. It consist of 142 hours of data collected from 310 shows, from a period from Sept. 2010 to Oct. 2012.", "title": "Is incremental cross-show speaker diarization efficient for processing large volumes of data?"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/dighe14_interspeech.html", "abstract": "Successfully modeling overlapping speech is a crucial step towards improving the performance of current speaker diarization systems. In this direction, we present ongoing work on a novel Multi-Class Vector Taylor Series (MC-VTS) approach that models overlapping speech from knowledge of the individual speaker models and the feature extraction process. We explore several variants of the MC-VTS technique that aim at modeling overlapping speech more precisely. Bootstrapping the algorithm with both oracle and diarization output segmentations, we show the potential of this approach in terms of overlapping speech detection and speaker labeling performances through a set of experiments on far-field microphone meeting data.", "title": "Detecting and labeling speakers on overlapping speech using vector taylor series"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/yella14_interspeech.html", "abstract": "Acoustic variability of speakers arises due to differences in their vocal tract characteristics. These individual speaker characteristics are reflected in a speech signal when speakers pronounce a given phoneme. The current work hypothesizes that clusters within a phoneme spoken by multiple speakers roughly correspond to different speakers. Based on this hypothesis, a Gaussian mixture model (GMM) based phoneme background model (PBM) is estimated. The components of such a PBM are used as a set of relevance variables in information bottleneck based speaker diarization system. Experiments are done using phone transcripts obtained from ground-truth and automatic speech recognition (ASR) system to estimate the PBM. The diarization experiments done on meeting recordings from AMI and NISTRT corpora show that the proposed method achieves significant improvements over the system using a background model which ignores phoneme information.", "title": "Phoneme background model for information bottleneck based speaker diarization"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/ferras14_interspeech.html", "abstract": "Speaker diarization of a collection of recordings with uniquely identified speakers is a challenging task. A system addressing such task must account for the inter-session variability present from recording to recording and it is asked to scale well to massive amounts of data. In this paper we use a two-stage approach to corpus-wide speaker diarization involving speaker diarization and speaker linking stages. The speaker linking system agglomeratively clusters speaker factor posterior distributions obtained via Joint Factor Analysis using the Ward method and the Hotteling t-square statistic as distance measure. We extend this framework to link speakers based on both speech and visual modalities to improve the robustness of the system. The system is evaluated using the data collected for the Augmented Multiparty Interaction (AMI) project, involving over one hundred meetings. We provide results in terms of within-recording and across-recording diarization error rates (DER) to support the effectiveness of multi-modal speaker linking to enable large scale speaker diarization.", "title": "Diarizing large corpora using multi-modal speaker linking"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/bechet14b_interspeech.html", "abstract": "This paper describes a multi-modal person recognition system for video broadcast developed for participating in the Defi-Repere challenge. The main track of this challenge targets the identification of all persons occurring in a video either in the audio modality (speakers) or the image modality (faces). This system is developed by the PERCOL team involving 4 research labs in France and was ranked first at the 2014 Defi-Repere challenge. The main scientific issue addressed by this challenge is the combination of audio and video information extraction processes for improving the extraction performance in both modalities. In this paper, we present the strategy followed by the PERCOL team for speaker identification based on enriching the speaker diarization with features related to the \u201cunderstanding\u201d of the video scenes: text overlay transcription and analysis, automatic situation identification (TV set, report), the amount of people visible, TV set disposition and even the camera when available. Experiments on the REPERE corpus show interesting results on the speaker identification system enriched by the scene understanding features and the usefulness of the speaker to identify faces.", "title": "Multimodal understanding for person recognition in video broadcasts"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/gibson14_interspeech.html", "abstract": "We compare the performance of Directional Derivatives features for automatic speech recognition when extracted from different time-frequency representations. Specifically, we use the short-time Fourier transform, Mel-frequency, and Gammatone spectrograms as a base from which we extract spectro-temporal modulations. We then assess the noise robustness of each representation with varied number of frequency bins and dynamic range compression schemes for both word and phone recognition. We find that the choice of dynamic range compression approach has the most significant impact on recognition performance. Whereas, the performance differences between perceptually motivated filter-banks are minimal in the proposed framework. Furthermore, this work presents significant gains in speech recognition accuracy for low SNRs over MFCCs, GFCCs, and Directional Derivatives extracted from the log-Mel spectrogram.", "title": "Comparing time-frequency representations for directional derivative features"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/du14_interspeech.html", "abstract": "We propose a signal pre-processing front-end to enhance speech based on deep neural networks (DNNs) and use the enhanced speech features directly to train hidden Markov models (HMMs) for robust speech recognition. As a comprehensive study, we examine its effectiveness for different acoustic features, acoustic models, and training-testing combinations. Tested on the Aurora4 task the experimental results indicate that our proposed framework consistently outperform the state-of-the-art speech recognition systems in all evaluation conditions. To our best knowledge, this is the first showcase on the Aurora4 task yielding performance gains by using only an enhancement pre-processor without any adaptation or compensation post-processing on top of the best DNN-HMM system. The word error rate reduction from the baseline system is up to 50% for clean-condition training and 15% for multi-condition training. We believe the system performance could be improved further by incorporating post-processing techniques to work coherently with the proposed enhancement pre-processing scheme.", "title": "Robust speech recognition with speech enhanced deep neural networks"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/vincent14_interspeech.html", "abstract": "Noise-robust automatic speech recognition (ASR) systems rely on feature and/or model compensation. Existing compensation techniques typically operate on the features or on the parameters of the acoustic models themselves. By contrast, a number of normalization techniques have been defined in the field of speaker verification that operate on the resulting log-likelihood scores. In this paper, we provide a theoretical motivation for likelihood normalization due to the so-called \u201chubness\u201d phenomenon and we evaluate the benefit of several normalization techniques on ASR accuracy for the 2nd CHiME Challenge task. We show that symmetric normalization (S-norm) reduces the relative error rate by 43% alone and by 10% after feature and model compensation.", "title": "An investigation of likelihood normalization for robust ASR"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/spille14_interspeech.html", "abstract": "Previous comparisons of human speech recognition (HSR) and automatic speech recognition (ASR) focused on monaural signals in additive noise, and showed that HSR is far more robust against intrinsic and extrinsic sources of variation than conventional ASR. The aim of this study is to analyze the man-machine gap (and its causes) in more complex acoustic scenarios, particularly in scenes with two moving speakers, reverberation and diffuse noise. Responses of nine normal-hearing listeners are compared to errors of an ASR system that employs a binaural model for direction-of-arrival estimation and beamforming for signal enhancement. The overall man-machine gap is measured in terms for the speech recognition threshold (SRT), i.e., the signal-to-noise ratio at which a 50% recognition rate is obtained. The comparison shows that the gap amounts to 16.7 dB SRT difference which exceeds the difference of 10 dB found in monaural situations. Based on cross comparisons that use oracle knowledge (e.g., the speakers' true position), incorrect responses are attributed to localization errors (7 dB) or missing spectral information to distinguish between speakers with different gender (3 dB). The comparison hence identifies specific ASR components that can profit from learning from binaural auditory signal processing.", "title": "Identifying the human-machine differences in complex binaural scenes: what can be learned from our auditory system"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/geiger14_interspeech.html", "abstract": "One method to achieve robust speech recognition in adverse conditions including noise and reverberation is to employ acoustic modelling techniques involving neural networks. Using long short-term memory (LSTM) recurrent neural networks proved to be efficient for this task in a setup for phoneme prediction in a multi-stream GMM-HMM framework. These networks exploit a self-learnt amount of temporal context, which makes them especially suited for a noisy speech recognition task. One shortcoming of this approach is the necessity of a GMM acoustic model in the multi-stream framework. Furthermore, potential modelling power of the network is lost when predicting phonemes, compared to the classical hybrid setup where the network predicts HMM states. In this work, we propose to use LSTM networks in a hybrid HMM setup, in order to overcome these drawbacks. Experiments are performed using the medium-vocabulary recognition track of the 2nd CHiME challenge, containing speech utterances in a reverberant and noisy environment. A comparison of different network topologies for phoneme or state prediction used either in the hybrid or double-stream setup shows that state prediction networks perform better than networks predicting phonemes, leading to state-of-the-art results for this database.", "title": "Robust speech recognition using long short-term memory recurrent neural networks for hybrid acoustic modelling"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/liu14c_interspeech.html", "abstract": "Context-dependent Deep Neural Network has obtained consistent and significant improvements over the Gaussian Mixture Model (GMM) based systems for various speech recognition tasks. However, since DNN is discriminatively trained, it is more sensitive to label errors and is not reliable for unsupervised adaptation. Moreover, DNN parameters do not have a clear and meaningful interpretation, therefore, it has been difficult to develop effective adaptation techniques for the DNNs. Nevertheless, unadapted multi-style trained DNNs have already shown superior performance to the GMM system with joint noise/speaker adaptation and adaptive training. Recently, Temporally Varying Weight Regression (TVWR) has been successfully applied to combine DNN and GMM for robust unsupervised speaker adaptation. In this paper, joint speaker/noise adaptation and adaptive training of TVWR using DNN posteriors are investigated for robust speech recognition. Experimental results on the Aurora 4 corpus showed that after joint adaptation and adaptive training, TVWR achieved 21.3% and 11.6% relative improvements over the DNN baseline system and the best system in currently reported literatures, respectively.", "title": "Joint adaptation and adaptive training of TVWR for robust automatic speech recognition"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/park14_interspeech.html", "abstract": "The precedence effect describes the ability of the auditory system to suppress the later-arriving components of sound in a reverberant environment, maintaining the perceived arrival azimuth of a sound in the direction of the actual source, even though later reverberant components may arrive from other directions. It is also widely believed that precedence-like processing can also improve speech intelligibility, as well as the accuracy of speech recognition systems, in reverberant environments. While the mechanisms underlying the precedence effect have traditionally been assumed to be binaural in nature, it is also possible that the suppression of later components may take place monaurally, and that the suppression of the later-arriving components of the spatial image may be a consequence of this more peripheral processing. This paper compares the potential contributions of onset enhancement (and consequent steady-state suppression) of the envelopes of subband components of speech at both the monaural and binaural levels. Experimental results indicate that substantial improvement in recognition accuracy can be obtained in reverberant environments if the feature extraction includes both onset enhancement and binaural interaction. Recognition accuracy appears to be relatively unaffected by the stage in the suppression processing at which the binaural interaction takes place.", "title": "Robust speech recognition in reverberant environments using subband-based steady-state monaural and binaural suppression"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/zhao14_interspeech.html", "abstract": "In this paper, we propose variable-component DNN (VCDNN) to improve the robustness of context-dependent deep neural network hidden Markov model (CD-DNN-HMM). This method is inspired by the idea from variable-parameter HMM (VPHMM) in which the variation of model parameters are modeled as a set of polynomial functions of environmental signal-to-noise ratio (SNR), and during the testing, the model parameters are recomputed according to the estimated testing SNR. In VCDNN, we refine two types of DNN components: (1) weighting matrix and bias (2) the output of each layer. Experimental results on Aurora4 task show VCDNN achieved 6.53% and 5.92% relative word error rate reduction (WERR) over the standard DNN for the two methods, respectively. Under unseen SNR conditions, VCDNN gave even better result (8.46% relative WERR for the DNN varying matrix and bias, 7.08% relative WERR for the DNN varying layer output). Moreover, VCDNN with 1024 units per hidden layer beats the standard DNN with 2048 units per hidden layer with 3.22% WERR and a half computational/memory cost reduction, showing superior ability to produce sharper and more compact models.", "title": "Variable-component deep neural network for robust speech recognition"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/kao14_interspeech.html", "abstract": "Modulation spectrum processing of acoustic features has received considerable attention in the area of robust speech recognition because of its relative simplicity and good empirical performance. An emerging school of thought is to conduct nonnegative matrix factorization (NMF) on the modulation spectrum domain so as to distill intrinsic and noise-invariant temporal structure characteristics of acoustic features for better robustness. This paper presents a continuation of this general line of research and its main contribution is two-fold. One is to explore the notion of sparsity for NMF so as to ensure the derived basis vectors have sparser and more localized representations of the modulation spectra. The other is to investigate a novel cluster-based NMF processing, in which speech utterances belonging to different clusters will have their own set of cluster-specific basis vectors. As such, the speech utterances can retain more discriminative information in the NMF processed modulation spectra. All experiments were conducted on the Aurora-2 corpus and task. Empirical evidence reveals that our methods can offer substantial improvements over the baseline NMF method and achieve performance competitive to or better than several widely-used robustness methods.", "title": "Effective modulation spectrum factorization for robust speech recognition"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/ravuri14b_interspeech.html", "abstract": "Tandem systems based on multi-layer perceptrons (MLPs) have improved the performance of automatic speech recognition systems on both large vocabulary and noisy tasks. One potential problem of the standard Tandem approach, however, is that the MLPs generally used do not model temporal dynamics inherent in speech. In this work, we propose a hybrid MLP/Structured-SVM model, in which the parameters between the hidden layer and output layer and temporal transitions between output layers are modeled by a Structured-SVM. A Structured-SVM can be thought of as an extension to the classical binary support vector machine which can naturally classify \u201cstructures\u201d such as sequences. Using this approach, we can identify sequences of phones in an utterance.   We try this model on two different corpora \u2014 Aurora2 and the large-vocabulary section of the ICSI meeting corpus \u2014 to investigate the model's performance in noisy conditions and on a large-vocabulary task. Compared to a difficult Tandem baseline in which the MLP is trained using 2nd-order optimization methods, the MLP/Structured-SVM system decreases WER in noisy conditions by 7.9% relative. On the large vocabulary corpus, the proposed system decreasesWER by 1.1% absolute compared to the 2nd-order Tandem system.", "title": "Hybrid MLP/structured-SVM tandem systems for large vocabulary and robust ASR"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/kim14d_interspeech.html", "abstract": "In this paper, we present a new dereverberation algorithm called Temporal Masking and Thresholding (TMT) to enhance the temporal spectra of spectral features for robust speech recognition in reverberant environments. This algorithm is motivated by the precedence effect and temporal masking of human auditory perception. This work is an improvement of our previous dereverberation work called Suppression of Slowly-varying components and the falling edge of the power envelope (SSF). The TMT algorithm uses a different mathematical model to characterize temporal masking and thresholding compared to the model that had been used to characterize the SSF algorithm. Specifically, the nonlinear highpass filtering used in the SSF algorithm has been replaced by a masking mechanism based on a combination of peak detection and dynamic thresholding. Speech recognition results show that the TMT algorithm provides superior recognition accuracy compared to other algorithms such as LTLSS, VTS, or SSF in reverberant environments.", "title": "Robust speech recognition using temporal masking and thresholding algorithm"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/xie14_interspeech.html", "abstract": "Recently deep neural networks (DNNs) have become increasingly popular for acoustic modelling in automatic speech recognition (ASR) systems. As the bottleneck features they produce are inherently discriminative and contain rich hidden factors that influence the surface acoustic realization, the standard approach is to augment the conventional acoustic features with the bottleneck features in a tandem framework. In this paper, an alternative approach to incorporate bottleneck features is investigated. The complex relationship between acoustic features and DNN bottleneck features is modelled using generalized variable parameter HMMs (GVP-HMMs). The optimal GVP-HMM structural configuration and model parameters are automatically learnt. Significant error rate reductions of 48% and 8% relative were obtained over the baseline multi-style HMM and tandem HMM systems respectively on Aurora 2.", "title": "Deep neural network bottleneck features for generalized variable parameter HMMs"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/bu14_interspeech.html", "abstract": "Model compensation approach has been successfully applied to various noise robust speech recognition tasks. In this paper, based on Continuous Time (CT) approximation, the dynamic mismatch function is derived without further approximation. With such mismatch function, a novel approach to deriving the formula for calculating the dynamic statistics is presented. Besides, we also provide an insight on the processing of the pseudo inverse of non-square discrete cosine transform (DCT) matrix during model compensation. Experiments on Aurora 4 showed that the proposed approach obtained 23.2% relative WER reduction over traditional first-order Vector Taylor Series (VTS) approach.", "title": "A novel dynamic parameters calculation approach for model compensation"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/hashimoto14_interspeech.html", "abstract": "We considered a speech recognition method for mixed sound, which is composed of both speech and music, that only removes music based on non-negative matrix factorization (NMF). We used Itakura-Saito divergence instead of Kullback-Leibler divergence to compare the cost function, and the dynamics and sparseness constraints of a weight matrix to improve speech recognition. For isolated word recognition using the matched condition model, we reduced the word error rate of 52.1% relative from the case that didn't remove music (on average, from 69.3% to 85.3%).", "title": "Speech recognition based on Itakura-Saito divergence and dynamics/sparseness constraints from mixed sound of speech and music by non-negative matrix factorization"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/chung14_interspeech.html", "abstract": "In conventional VTS-based noisy speech recognition methods, the parameters of the clean speech HMM are adapted to test noisy speech, or the original clean speech is estimated from the test noisy speech. However, in noisy speech recognition, improved performance is generally expected by employing noisy acoustic models produced by methods such as Multi-condition TRaining (MTR) and Multi-Model based Speech Recognition (MMSR) framework compared with using clean HMMs. Motivated by this idea, a method has been developed that can make use of the noisy acoustic models in the VTS algorithm where additive noise was adapted for the speech feature compensation. In this paper, we modified the previous method to adapt channel noise as well as additive noise. The proposed method was applied to noise-adapted HMMs trained by the MTR and MMSR and could reduce the relative word error rate by 6.5% and 7.2%, respectively, in the noisy speech recognition experiments on the Aurora 2 database.", "title": "Noise robust speech recognition based on noise-adapted HMMs using speech feature compensation"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/alam14_interspeech.html", "abstract": "This work presents a noise spectrum estimator based on the Gaussian mixture model (GMM)-based speech presence probability (SPP) for robust speech recognition. Estimated noise spectrum is then used to compute a subband a posteriori signal-to-noise ratio (SNR). A sigmoid shape weighting rule is formed based on this subband a posteriori SNR to enhance the speech spectrum in the auditory domain, which is used in the Mel-frequency cepstral coefficient (MFCC) framework for robust feature, denoted here as Robust MFCC (RMFCC) extraction. The performance of the GMM-SPP noise spectrum estimator-based RMFCC feature extractor is evaluated in the context of speech recognition on the AURORA-4 continuous speech recognition task. For comparison we incorporate six existing noise estimation methods into this auditory domain spectrum enhancement framework. The ETSI advanced front-end (ETSI-AFE), power normalized cepstral coefficients (PNCC), and robust compressive gammachirp cepstral coefficients (RCGCC) are also considered for comparison purposes. Experimental speech recognition results show that, in terms of word accuracy, RMFCC provides an average relative improvements of 8.1%, 6.9% and 6.6% over RCGCC, ETSI-AFE, and PNCC, respectively. With GMM-SPP-based noise estimation method an average relative improvement of 3.6% is obtained over other six noise estimation methods in terms of word recognition accuracy.", "title": "Noise spectrum estimation using Gaussian mixture model-based speech presence probability for robust speech recognition"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/chen14b_interspeech.html", "abstract": "Recurrent neural network language models (RNNLMs) are becoming increasingly popular for a range of applications including speech recognition. However, an important issue that limits the quantity of data, and hence their possible application areas, is the computational cost in training. A standard approach to handle this problem is to use class-based outputs, allowing systems to be trained on CPUs. This paper describes an alternative approach that allows RNNLMs to be efficiently trained on GPUs. This enables larger quantities of data to be used, and networks with an unclustered, full output layer to be trained. To improve efficiency on GPUs, multiple sentences are \u201cspliced\u201d together for each mini-batch or \u201cbunch\u201d in training. On a large vocabulary conversational telephone speech recognition task, the training time was reduced by a factor of 27 over the standard CPU-based RNNLM toolkit. The use of an unclustered, full output layer also improves perplexity and recognition performance over class-based RNNLMs.", "title": "Efficient GPU-based training of recurrent neural network language models using spliced sentence bunch"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/nolden14_interspeech.html", "abstract": "The search effort in LVCSR depends on the order of the language model ( LM); search hypotheses are only recombined once the LM allows for it. In this work we show how the LM dependence can be partially eliminated by exploiting the well-known word pair approximation. We enforce preemptive unigram- or bigram-like LM recombination at word boundaries. We capture the recombination in a lattice, and later expand the lattice using LM rescoring. LM rescoring unfolds the same search space which would have been encountered without the preemptive recombination, but the overall efficiency is improved, because the amount of redundant HMM expansion in different LM contexts is reduced. Additionally, we show how to expand the recombined hypotheses on-the-fly, omitting the intermediate lattice form. Our new approach allows using the full n-gram LM for decoding, but based on a compact unigram- or bigram search space. We show that our approach works better than common lattice rescoring pipelines, where a pruned lower-order LM is used to generate lattices; such pipelines suffer from the weak lower-order LM, which guides the pruning sub-optimally. Our new decoding approach improves the runtime efficiency by up to 40% at equal precision when using a large vocabulary and high-order LM.", "title": "Word pair approximation for more efficient decoding with high-order language models"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/adel14_interspeech.html", "abstract": "In this paper, we investigate and compare three different possibilities to convert recurrent neural network language models (RNNLMs) into backoff language models (BNLM). While RNNLMs often outperform traditional n-gram approaches in the task of language modeling, their computational demands make them unsuitable for an efficient usage during decoding in an LVCSR system. It is, therefore, of interest to convert them into BNLMs in order to integrate their information into the decoding process. This paper compares three different approaches: a text based conversion, a probability based conversion and an iterative conversion. The resulting language models are evaluated in terms of perplexity and mixed error rate in the context of the Code-Switching data corpus SEAME. Although the best results are obtained by combining the results of all three approaches, the text based conversion approach alone leads to significant improvements on the SEAME corpus as well while offering the highest computational efficiency. In total, the perplexity can be reduced by 11.4% relative on the evaluation set and the mixed error rate by 3.0% relative on the same data set.", "title": "Comparing approaches to convert recurrent neural networks into backoff language models for efficient decoding"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/nolden14b_interspeech.html", "abstract": "In this work we present a novel word lattice filtering algorithm which removes redundancy from lattices. We use the filtering algorithm to analyze lattices obtained from dynamic network and transducer-based LVCSR decoders from several sites regarding size, coverage, and redundancy. We show that our filtering algorithm reduces the size of lattices by 30 to 90% without degrading the oracle word error rate.", "title": "Removing redundancy from lattices"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/sundermeyer14_interspeech.html", "abstract": "With long-span neural network language models, considerable improvements have been obtained in speech recognition. However, it is difficult to apply these models if the underlying search space is large.   In this paper, we combine previous work on lattice decoding with long short-term memory (LSTM) neural network language models. By adding refined pruning techniques, we are able to reduce the search effort by a factor of three.   Furthermore, we introduce two novel approximations for full lattice rescoring, which opens the potential of lattice-based speech recognition techniques. Compared to 1000-best lists, we find that we can increase the word error rate improvements obtained with LSTMs from 8.2% to 10.7% relative over a state-of-the-art baseline, while the resulting lattices are even considerably smaller. In addition, we investigate the use of LSTMs for Babel Assamese keyword search, obtaining significant improvements of 2.5% relative.", "title": "Lattice decoding and rescoring with long-Span neural network language models"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/levit14_interspeech.html", "abstract": "We present a modification of the traditional n-gram language modeling approach that departs from the word-level data representation and seeks to re-express the training text in terms of tokens that could be either words, common phrases or instances of one or several classes. Our iterative optimization algorithm considers alternative parses of the corpus in terms of these tokens, re-estimates token n-gram probabilities and also updates within-class distributions. In this paper, we focus on the cold start approach that only assumes availability of the word-level training corpus, as well as a number of generic class definitions. Applied to the calendar scenario in the personal assistant domain, our approach reduces word error rates by more than 13% relative to the word-only n-gram language models. Only a small fraction of these improvements can be ascribed to a larger vocabulary.", "title": "Word-phrase-entity language models: getting more mileage out of n-grams"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/sarkar14_interspeech.html", "abstract": "This paper explores the significance of an ensemble of boosted Support Vector Machine (SVM) classifiers in the i-vector framework for speaker verification (SV) in noisy environments. Prior work in this field have established the significance of supervector-based approaches and more specifically the i-vector extraction paradigm for robust SV. However, in highly degraded environments, SVMs trained using i-vectors are susceptible to misclassifications. For enhanced classification accuracy, we explore the impact of multiple SVM classifiers trained by adaptive boosting. To mitigate the effect of statistical mismatches due to difference in utterance lengths and data imbalance caused by a disproportionate ratio of target speaker and impostor utterances, we propose a novel combination scheme of the adaptive boosting algorithm with a data generation technique using partitioned utterances. All experiments are conducted on the NIST-SRE-2003 database under mismatched conditions with training utterances degraded by 4 types of additive noises (car, factory, pink and white) collected from the NOISEX-92 database, at 0 dB and 5 dB SNRs. Results indicate that the proposed method significantly outperforms the baseline i-vector SVM based SV systems across all noisy environments.", "title": "A novel boosting algorithm for improved i-vector based speaker verification in noisy environments"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/campbell14_interspeech.html", "abstract": "Deep belief networks (DBNs) have become a successful approach for acoustic modeling in speech recognition. DBNs exhibit strong approximation properties, improved performance, and are parameter efficient. In this work, we propose methods for applying DBNs to speaker recognition. In contrast to prior work, our approach to DBNs for speaker recognition starts at the acoustic modeling layer. We use sparse-output DBNs trained with both unsupervised and supervised methods to generate statistics for use in standard vector-based speaker recognition methods. We show that a DBN can replace a GMM UBM in this processing. Methods, qualitative analysis, and results are given on a NIST SRE 2012 task. Overall, our results show that DBNs show competitive performance to modern approaches in an initial implementation of our framework.", "title": "Using deep belief networks for vector-based speaker recognition"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/lei14_interspeech.html", "abstract": "We recently proposed the use of deep neural networks (DNN) in place of Gaussian Mixture models (GMM) in the i-vector extraction process for speaker recognition. We have shown significant accuracy improvements on the 2012 NIST speaker recognition evaluation (SRE) telephone conditions. This paper explores how this framework can be effectively used on the microphone speech conditions of the 2012 NIST SRE. In this new framework, the verification performance greatly depends on the data used for training the DNN. We show that training the DNN using both telephone and microphone speech data can yield significant improvements. An in-depth analysis of the influence of telephone speech data on the microphone conditions is also shown for both the DNN and GMM systems. We conclude by showing that the the GMM system is always outperformed by the DNN system on the telephone-only and microphone-only conditions, and that the new DNN / i-vector framework can be successfully used providing a good match in the training data.", "title": "A deep neural network speaker verification system targeting microphone speech"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/mclaren14_interspeech.html", "abstract": "This paper applies a convolutional neural network (CNN) trained for automatic speech recognition (ASR) to the task of speaker identification (SID). In the CNN/i-vector front end, the sufficient statistics are collected based on the outputs of the CNN as opposed to the traditional universal background model (UBM). Evaluated on heavily degraded speech data, the CNN/i-vector front end provides performance comparable to the UBM/i-vector baseline. The combination of these approaches, however, is shown to provide improvements of 26% in miss rate to considerably outperform the fusion of two different features in the traditional UBM/i-vectors approach. An analysis of the language- and channel-dependency of the CNN/i-vector approach is also provided to highlight future research directions.", "title": "Application of convolutional neural networks to speaker recognition in noisy conditions"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/pelecanos14_interspeech.html", "abstract": "In this paper we extend a variation of the trial-based SVM speaker verification work proposed by Cumani et al to exploit multiple enrollment sessions. Specifically, Cumani proposed the use of a 2nd order SVM kernel for the binary classification of basic trials. In this new work, trials with multiple enrollment sessions are modelled by stacking the i-vectors of the test and enrollment sessions. We further exploit the fact that the score should be independent of the enrollment recording order and present a simplified 2nd order polynomial kernel scoring function accordingly.   In the second part of this work we examine the utility of enrollment pruning for multi-session enrollments. Past work demonstrates that pruning can be beneficial for PLDA based systems. We examine the effects of enrollment pruning in the context of the proposed SVM model.   The results demonstrate that the multi-session enrollment SVM kernel is generally better than the model trained using single sessions. The model is also comparable in performance to the PLDA based approach. Further gains are observed through combination of the PLDA and SVM scores.", "title": "SVM based speaker recognition: harnessing trials with multiple enrollment sessions"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/gallardo14_interspeech.html", "abstract": "Past studies have shown evidence of important speaker-specific content in the higher frequencies of the spectrum, which are filtered out by narrowband channels. Besides, wideband transmissions, which are gaining ground over narrowband communications, offer an extended range of frequencies which account not only for better speech quality and intelligibility, but also for an improved speaker recognition performance. In this work, different phoneme classes (fricatives, nasals, and vowels) were removed from speech of different bandwidths, and a series of i-vector based speaker verification experiments were conducted. Our results show that the performance enhancement with clean wideband speech with respect to clean narrowband speech is principally due to the presence of unvoiced fricative consonants. The effects of codec schemes of different bandwidths on the aforementioned speech are discussed.", "title": "I-vector speaker verification based on phonetic information under transmission channel effects"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/zang14_interspeech.html", "abstract": "This paper addresses the problem of automatically labeling focus word pairs in spontaneous spoken English, where a focus word pair refers to salient part of text or speech and the word motivating it. The prediction of focus word pairs is important for speech applications such as expressive text-to-speech (TTS) synthesis and speech recognition. It can also help in better textual and intention understanding for spoken dialog systems. Traditional approaches such as support vector machines (SVMs) prediction neglect the dependency between words and meet the obstacle of the imbalanced distribution of positive and negative samples of dataset. This paper introduces conditional random fields (CRFs) to the task of automatically predicting focus word pair from lexical, syntactic and semantic features. Furthermore, several new features related to syntactic and semantic information are proposed to achieve better performance. Experiments on the publicly available Switchboard corpus demonstrate that CRF model outperforms the baseline and SVM model for focus word pair prediction, and newly proposed features can further improve performance for CRF based predictor. Specifically, compared to the low recall rate of 11.31% achieved by the SVM model, the proposed CRF based predictor can yield a high recall rate of 70.88% with little impact on precision.", "title": "Using conditional random fields to predict focus word pair in spontaneous spoken English"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/sproat14_interspeech.html", "abstract": "We report on two applications of Maximum Entropy-based ranking models to problems of relevance to automatic speech recognition and text-to-speech synthesis. The first is stress prediction in Russian, a language with notoriously complex morphology and stress rules. The second is the classification of alphabetic non-standard words, which may be read as words ( NATO), as letter sequences USA, or as a mixed ( mymsn). For this second task we report results on English, and five other European languages.", "title": "Applications of maximum entropy rankers to problems in spoken language processing"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/gonzalvo14_interspeech.html", "abstract": "Modern Text-To-Speech (TTS) systems need to increasingly deal with multilingual input. Navigation, social and news are all domains with a large proportion of foreign words. However, when typical monolingual TTS voices are used, the synthesis quality on such input is markedly lower. This is because traditional TTS derives pronunciations from a lexicon or a Grapheme-To-Phoneme (G2P) model which was built using a pre-defined sound inventory and a phonotactic grammar for one language only. G2P models perform poorly on foreign words, while manual lexicon development is labour-intensive, expensive and requires extra storage. Furthermore, large phoneme inventories and phonotactic grammars contribute to data sparsity in unit selection systems. We present an automatic system for deriving pronunciations for foreign words that utilises the monolingual voice design and can rapidly scale to many languages. The proposed system, based on a neural network cross-lingual G2P model, does not increase the size of the voice database, doesn't require large data annotation efforts, is designed not to increase data sparsity in the voice, and can be sized to suit embedded applications.", "title": "Text-to-speech with cross-lingual neural network-based grapheme-to-phoneme models"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/nagahama14_interspeech.html", "abstract": "This paper proposes a novel transform mapping technique based on shared decision tree context clustering (STC) for HMM-based cross-lingual speech synthesis. In the conventional cross-lingual speaker adaptation based on state mapping, the adaptation performance is not always satisfactory when there are mismatches of languages and speakers between the average voice models of input and output languages. In the proposed technique, we alleviate the effect of the mismatches on the transform mapping by introducing a language-independent decision tree constructed by STC, and represent the average voice models using language-independent and dependent tree structures. We also use a bilingual speech corpus for keeping speaker characteristics between the average voice models of different languages. The experimental results show that the proposed technique decreases both spectral and prosodic distortions between original and generated parameter trajectories and significantly improves the naturalness of synthetic speech while keeping the speaker similarity compared to the state mapping.", "title": "Transform mapping using shared decision tree context clustering for HMM-based cross-lingual speech synthesis"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/ramani14_interspeech.html", "abstract": "A polyglot speech synthesizer, synthesizes speech for any given monolingual or multilingual text, in a single speaker's voice. In this regard, a polyglot speech corpus is required. It is difficult to find a speaker proficient in multiple languages. Therefore, in the current work, by exploiting the acoustic similarity of phonemes across Indian languages, a polyglot speech corpus is obtained for four Indian languages and Indian English, using GMM-based cross-lingual voice conversion. The optimum target speaker and GMM topology is chosen based on the performance of a speaker identification system. It is observed that, the language that shares the most number of phonemes with the other languages, serves as the best target. A polyglot speech corpus derived in this target speaker's voice, is further used to develop an HMM-based polyglot speech synthesizer. The performance of this synthesizer is evaluated in terms of speaker identity using ABX listening test, quality using mean opinion score (MOS) and speaker switching using subjective listening test.", "title": "Cross-lingual voice conversion-based polyglot speech synthesizer for indian languages"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/hu14b_interspeech.html", "abstract": "This paper applies a dynamic sinusoidal synthesis model to statistical parametric speech synthesis (HTS). For this, we utilise regularised cepstral coefficients to represent both the static amplitude and dynamic slope of selected sinusoids for statistical modelling. During synthesis, a dynamic sinusoidal model is used to reconstruct speech. A preference test is conducted to compare the selection of different sinusoids for cepstral representation. Our results show that when integrated with HTS, a relatively small number of sinusoids selected according to a perceptual criterion can produce quality comparable to using all harmonics. A Mean Opinion Score (MOS) test shows that our proposed statistical system is preferred to one using mel-cepstra from pitch synchronous spectral analysis.", "title": "An investigation of the application of dynamic sinusoidal models to statistical parametric speech synthesis"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/patil14_interspeech.html", "abstract": "Linear Prediction (LP) analysis has proven to be very powerful and widely used method in speech analysis and synthesis. Synthesis by LP-based approach is carried by exciting an all-pole model (whose parameters are derived by LP analysis). Synthesis is carried by using mixed excitation source consisting of a sequence of impulses for voiced regions and white-noise source for unvoiced regions. In this paper, we present novel chaotic excitation source using chaotic titration method. The voiced and unvoiced regions in speech are modeled by chaos which is quantified by adding noise of known standard deviation (determined using chaotic titration method). It is observed that on an average for synthesized voices (both male and female), MOS increases from 2 to 2.4, DMOS from 2.1 to 2.4 and preference is increased from 39% to 61% via A/B test. PESQ score increases from 1 to 1.5 and MCD score decreases from 4.06 to 4.03, relatively for voices synthesized by proposed chaotic mixed excitation source. The relatively better performance of proposed approach is may be due to the novel chaotic mixed source of excitation.", "title": "Chaotic mixed excitation source for speech synthesis"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/sorin14_interspeech.html", "abstract": "In multi-form speech synthesis, speech output is constructed by splicing waveform segments and parametric speech segments which are generated from statistical models. The decision whether to use the waveform or the statistical parametric form is made per segment. This approach faces certain challenges in the context of inter-segment joining. In this work, we present a novel method whereby all non-contiguous joints are represented by statistically generated speech frames without compromising on naturalness. Speech frames surrounding non-contiguous joints between the waveform segments are re-generated from the models and optimized for concatenation. In addition, a novel pitch smoothing algorithm that preserves the original intonation trajectory while maintaining smoothness is applied. We implemented the spectrum and the pitch smoothing algorithms within a multi-form speech synthesis framework that employs a uniform parametric representation for the natural and statistically modeled speech segments. This framework facilitates pitch modification in natural segments. Subjective evaluation results reveal that the proposed smoothing methods significantly improve the perceived speech quality.", "title": "Refined inter-segment joining in multi-form speech synthesis"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/zhang14c_interspeech.html", "abstract": "The hybrid speech synthesis system, which combines the hidden Markov model and unit selection method, has become an additional main stream in state-of-the-art TTS systems. However, traditional Viterbi algorithm is based on global minimization of a cost function and the procedure can end up selecting some poor-quality units with larger local errors, which can hardly be tolerated by the listeners. In Mandarin and many other languages, the naturalness of the region of consecutive voiced speech segments (CVS) is more essential to the overall quality of the synthetic speech. Consequently, in this paper, we proposed to use a hierarchical Viterbi algorithm which involves two rounds of Viterbi search: one is for the sub-paths in the CVS regions; the other is for the utterance path connecting all the sub-paths. In the proposed technique, we defined CVS Region as a region which is formed by two or more voiced phones, and whose observation of pitch has a continuous value. Subjective evaluations suggest that the use of hierarchical Viterbi algorithm in the Mandarin hybrid speech synthesis system outperforms the use of traditional algorithm in both the naturalness and speech quality of synthetic speech.", "title": "A hierarchical viterbi algorithm for Mandarin hybrid speech synthesis system"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/fabre14_interspeech.html", "abstract": "This paper presents a method for automatically animating the articulatory tongue model of a reference speaker from ultrasound images of the tongue of another speaker. This work is developed in the context of speech therapy based on visual biofeedback, where a speaker is provided with visual information about his/her own articulation. In our approach, the feedback is delivered via an articulatory talking head, which displays the tongue during speech production using augmented reality (e.g. transparent skin). The user's tongue movements are captured using ultrasound imaging and parameterized using the PCA-based EigenTongue technique. Extracted features are then converted into control parameters of the articulatory tongue model using Gaussian Mixture Regression. This procedure was evaluated by decoding the converted tongue movements at the phonetic level using an HMM-based decoder trained on the reference speaker's articulatory data. Decoding errors were then manually reassessed in order to take into account possible phonetic idiosyncrasies (i.e. speaker / phoneme specific articulatory strategies). With a system trained on a limited set of 88 VCV sequences, the recognition accuracy at the phonetic level was found to be approximately 70%.", "title": "Automatic animation of an articulatory tongue model from ultrasound images using Gaussian mixture regression"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/tobing14_interspeech.html", "abstract": "This paper presents a novel speech modification method capable of controlling unobservable articulatory parameters based on a statistical feature mapping technique with Gaussian Mixture Models (GMMs). In previous work [1], the GMM-based statistical feature mapping was successfully applied to acoustic-to-articulatory inversion mapping and articulatory-to-acoustic production mapping separately. In this paper, these two mapping frameworks are integrated to a unified framework to develop a novel speech modification system. The proposed system sequentially performs the inversion and the production mapping, making it possible to modify phonemic sounds of an input speech signal by intuitively manipulating articulatory parameters estimated from the input speech signal. We also propose a manipulation method to automatically compensate for unmodified articulatory movements considering inter-dimensional correlation of the articulatory parameters. The proposed system is implemented for a single English speaker and its effectiveness is evaluated experimentally. The experimental results demonstrate that the proposed system is capable of modifying phonemic sounds by manipulating the estimated articulatory movements and higher speech quality is achieved by considering the inter-dimensional correlation in the manipulation.", "title": "Articulatory controllable speech modification based on statistical feature mapping with Gaussian mixture models"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/ding14_interspeech.html", "abstract": "This paper presents a neural network approach for speech-driven head motion synthesis, which can automatically predict a speaker's head movement from his/her speech. Specifically, we realize speech-to-head-motion mapping by learning a multi-layer perceptron from audio-visual broadcast news data. First, we show that a generatively pre-trained neural network significantly outperforms a randomly initialized network and the hidden Markov model (HMM) approach. Second, we demonstrate that the feature combination of log Mel-scale filter-bank (FBank), energy and fundamental frequency (F0) performs best in head motion prediction. Third, we discover that using long context acoustic information can further improve the performance. Finally, extra unlabeled training data used in the pre-training stage can achieve more performance gain. The proposed speech-driven head motion synthesis approach increases the CCA from 0.299 (the HMM approach) to 0.565 and it can be effectively used in expressive talking avatar animation.", "title": "Speech-driven head motion synthesis using neural networks"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/song14_interspeech.html", "abstract": "In this paper, we propose a novel voice conversion method called speaker model alignment (SMA), which does not require parallel training speech. Firstly, the source and target speaker models, described by Gaussian mixture model (GMM), are trained, respectively. Then, the transformation function of spectral features is learned by aligning the components of source and target speaker models iteratively. Additionally, the transformation function is further combined with GMM, enabling the multiple local mappings, and a local consistent GMM (LCGMM) is also considered for model training to improve the conversion accuracy. Finally, we carry out experiments to evaluate the performance of the proposed method. Objective and subjective experimental results demonstrate that compared with the well-known INCA approach, the proposed method achieves lower spectral distortions and higher correlations, and obtains a significant improvement in perceptual quality and similarity.", "title": "Text-independent voice conversion using speaker model alignment method from non-parallel speech"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/chen14c_interspeech.html", "abstract": "This paper presents a deep neural network (DNN) based spectral envelope conversion method. A global DNN is employed to model the complex non-linear mapping relationship between the spectral envelopes of source and target speakers. The proposed DNN is generatively trained layer-by-layer by cascade of two restricted Boltzmann machines (RBMs) and a bidirectional associative memory (BAM), which are considered as generative models estimated using the contrastive divergence algorithm. Further, multiple spectral envelopes are adopted instead of dynamic features for better modeling using the DNN. The superiority of the proposed method is validated by the subjective experimental results.", "title": "Voice conversion using generative trained deep neural networks with multiple frame spectral envelopes"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/sanchez14_interspeech.html", "abstract": "Voice conversion systems deal with the conversion of a speech signal to sound as if it was uttered by another speaker. The conversion of the spectral features has attracted a lot of research attention but the conversion of pitch, modeling the speaker-dependent prosody, is often achieved by just controlling the F0 level and range. However, the detailed prosody, including different linguistic units at several distinct temporal scales, can carry a significant amount of speaker identity related information. This paper introduces a new method for the conversion of the prosody, using wavelets to decompose the pitch contour into ten temporal scales ranging from microprosody to the utterance level, which allows modeling the different timings of the prosody phenomena. The prosody conversion is carried out in the wavelet domain, using regression techniques originally developed for the spectral conversion of speech. The performance of the proposed prosody conversion method is evaluated within a real voice conversion system. The results for cross-gender conversion indicate a significant improvement in naturalness when compared to the traditional approach of shifting and scaling the F0 to match the target speaker's mean and variance.", "title": "Hierarchical modeling of F0 contours for voice conversion"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/kadowaki14_interspeech.html", "abstract": "This paper deals with the problem of generating the fundamental frequency (F0) contour of speech from a text input for text-to-speech synthesis. We have previously introduced a statistical model describing the generating process of speech F0 contours, based on the discrete-time version of the Fujisaki model. One remarkable feature of this model is that it has allowed us to derive an efficient algorithm based on powerful statistical methods for estimating the Fujisaki-model parameters from raw F0 contours. To associate a sequence of the Fujisaki-model parameters with a text input based on statistical learning, this paper proposes extending this model to a context-dependent one. We further propose a parameter training algorithm for the present model based on a decision tree-based context clustering.", "title": "Speech prosody generation for text-to-speech synthesis based on generative model of F0 contours"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/chen14d_interspeech.html", "abstract": "EDHMM with decision trees is a popular model for parametric speech synthesis. Traditional training procedure constructs the decision trees after observation probability densities have been optimized with the EM algorithm, assuming the state assignment probability does not change much during tree construction. This paper proposes an iterative algorithm that removes the assumption. In the new algorithm, the decision tree construction is incorporated into the EM iteration, with a safeguard procedure that ensures convergence. Evaluation on The Boston University Radio Speech corpus shows that the proposed algorithm can achieve a significantly better optimum in the training set than the original one, and that the advantage is well generalizable to the test set.", "title": "An iterative approach to decision tree training for context dependent speech synthesis"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/nguyen14_interspeech.html", "abstract": "This research aims at modeling prosodic phrasing for improving the naturalness of Vietnamese (a tonal language) speech synthesis. The proposed phrasing model includes hypotheses on: (i) prosodic structure based on syntactic rules (ii) final lengthening linked to syllabic structures and tone types. Audio files in the analysis corpus are manually transcribed at the syllable level and perceived pauses. Text files are parsed and represented with annotated-syntax trees. Statistical treatment brings out a correlation between syntactic element boundaries and pause duration. Major breaks may appear at the end of a clause or between predicates or head elements. Other rules between grammatical phrases/words or shorter clauses may trigger minor breaks. Break levels (including ones predicted by syntactic rules) and relative positions of syllables are used to train VTed, an HMM-based Text-To-Speech (TTS) system for Vietnamese. In the synthesis phase, break levels are explicitly inserted while lengthening is applied for last syllables of prosodic phrases. Perceptive testing shows an increase of 0.34 on a 5 point MOS scale, for the new prosodic informed system (3.95/5) compared to the previous TTS system (3.61/5). In the pair-wise comparison test, about 70% of the synthetic voice with the proposed model is preferred to the previous version.", "title": "Prosodic phrasing modeling for vietnamese TTS using syntactic information"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/koriyama14_interspeech.html", "abstract": "This paper proposes an automatic prosodic labeling technique for constructing speech database used for speech synthesis. In the corpus-based Japanese speech synthesis, it is essential to use annotated speech data with prosodic information such as phrase boundaries and accent types. However, manual annotation is generally time-consuming and expensive. To overcome this problem, we propose an estimation technique of accent types and phrase boundaries from speech waveform and its transcribed text using both language and acoustic models. We use conditional random field (CRF) for the language model, and HMM for the acoustic model which has shown to be effective in prosody modeling in speech synthesis. By introducing HMM, continuously changing features of F0 contours are modeled well and this results in higher estimation accuracy than conventional techniques that use simple polygonal line approximation of F0 contours.", "title": "Accent type and phrase boundary estimation using acoustic and language models for automatic prosodic labeling"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/fang14_interspeech.html", "abstract": "Kinematic articulatory data are important for researches of speech production, articulatory speech synthesis, robust speech recognition, and speech inversion. Electromagnetic Articulograph (EMA) is a widely used instrument for collecting kinematic articulatory data. However, in EMA experiment, one or more coils attached to articulators are possible to be mistracked due to various reasons. To make full use of the EMA data, we attempt to reconstruct the location of mistracked coils with Gaussian Mixture Model (GMM) regression method. In this paper, we explore how additional information (spectrum, articulatory velocity, etc.) affects the performance of the proposed method. The result indicates that acoustic feature (MFCC) is the most effective additional features that improve the reconstruction performance.", "title": "Reconstruction of mistracked articulatory trajectories"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/chen14e_interspeech.html", "abstract": "A continuous expression space assumes that each utterance contains individual expressions. Thus, it can be used to model detailed expression information in speech data. However, since an infinite number of different expressions can be contained in the continuous expression space, it is very difficult to manually label them. That means, these expressions are very hard to identify and to extract for synthesising expressive speech. A mechanism to control the continuous expression space is missing. In the discrete expression space though, only a few emotions are defined, thus users can easily choose from these emotions, but the range of expressivity is limited. This work proposes a method to automatically annotate expressions in the continuous expression space based on the cluster adaptive training (CAT) method. Using the proposed method, complex emotion information can be associated to the individual expressions in the continuous space. These emotion labels can be used as indexes of the expressions in the continuous space to enable users to select desired expressions at synthesis time, i.e. enable the controllability for the continuous expression space. Meanwhile, the rich expressive information in the continuous space is kept so that more expressive speech can be generated compared to the discrete space.", "title": "Enabling controllability for continuous expression space"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/nose14_interspeech.html", "abstract": "This paper analyzes the problem of the spectral enhancement technique using global variance (GV) in HMM-based speech synthesis. In the conventional GV-based parameter generation, spectral enhancement with variance compensation is achieved by considering a GV pdf with fixed parameters for every output utterances through the generation process. Although the spectral peaks of the generated trajectory are clearly emphasized and subjective clarity is improved, the use of the fixed GV parameters results in a much smaller variation of GVs among the synthesized utterances than that of the natural speech, which sometimes causes undesirable effect. In this paper, we examine the above problem in terms of multiple objective measures such as variance characteristics, spectral and GV distortions, and GV correlations and discuss the result. We propose a simple alternative technique based on an affine transformation that provides a closer GV distribution to the original speech and improves the correlation of GVs of generated parameter sequences. The experimental results show that the proposed spectral enhancement outperforms the conventional GV-based parameter generation in the objective measures.", "title": "Analysis of spectral enhancement using global variance in HMM-based speech synthesis"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/valentinibotinhao14_interspeech.html", "abstract": "In this paper we analyse the effect of speech corpus and compression method on the intelligibility of synthesized speech at fast rates. We recorded English and German language voice talents at a normal and a fast speaking rate and trained an HSMM-based synthesis system based on the normal and the fast data of each speaker. We compared three compression methods: scaling the variance of the state duration model, interpolating the duration models of the fast and the normal voices, and applying a linear compression method to generated speech. Word recognition results for the English voices show that generating speech at normal speaking rate and then applying linear compression resulted in the most intelligible speech at all tested rates. A similar result was found when evaluating the intelligibility of the natural speech corpus. For the German voices, interpolation was found to be better at moderate speaking rates but the linear method was again more successful at very high rates, for both blind and sighted participants. These results indicate that using fast speech data does not necessarily create more intelligible voices and that linear compression can more reliably provide higher intelligibility, particularly at higher rates.", "title": "Intelligibility analysis of fast synthesized speech"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/lopezpelaez14_interspeech.html", "abstract": "This paper addresses the issue of generating synthetic speech in changing noise conditions. We will investigate the potential improvements that can be introduced by using a speech synthesiser that is able to modulate between a normal speech style and a speech style produced in a noisy environment according to a changing level of noise. We demonstrate that an adaptive system where the speech style is changed to suit the noise conditions maintains intelligibility and improves naturalness compared to traditional systems.", "title": "Speech synthesis reactive to dynamic noise environmental conditions"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/baumann14_interspeech.html", "abstract": "When humans speak, they do not plan their full utterance in all detail before beginning to speak, nor do they speak piece-by-piece and ignoring their full message \u2014 instead humans use partial representations in which they fill in the missing parts as the utterance unfolds. Incremental speech synthesizers, in contrast, have not yet made use of partial representations and the information contained there-in.   We analyze the quality of prosodic parameter assignments (pitch and duration) generated from partial utterance specifications (substituting defaults for missing features) in order to determine the requirements that symbolic incremental prosody modelling should meet. We find that broader, higher-level information helps to improve prosody even if lower-level information about the near future is yet unavailable. Furthermore, we find that symbolic phrase-level or utterance-level information is most helpful towards the end of the phrase or utterance, respectively, that is, when this information is becoming available even in the incremental case. Thus, the negative impact of incremental processing can be minimized by using partial representations that are filled in incrementally.", "title": "Partial representations improve the prosody of incremental speech synthesis"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/tsiakoulis14_interspeech.html", "abstract": "This paper extends our recent work on rich context utilization for expressive speech synthesis in spoken dialogue systems in which significant improvements to the appropriateness of HMM-based synthetic voices were achieved by introducing dialogue context into the decision tree state clustering stage. Continuing in this direction, this paper investigates the performance of dialogue context-sensitive voices in different domains. The Context Adaptive Training with Factorized Decision trees (FD-CAT) approach was used to train a dialogue context-sensitive synthetic voice which was then compared to a baseline system using the standard decision tree approach. Preference-based listening tests were conducted for two different domains. The first domain concerned restaurant information and had significant coverage in the training data, while the second dealing with appointment bookings had minimal coverage in the training data. No significant preference was found for any of the voices when tested in the restaurant domain whereas in the appointment booking domain, listeners showed a statistically significant preference for the adaptively trained voice.", "title": "Dialogue context sensitive speech synthesis using factorized decision trees"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/wang14d_interspeech.html", "abstract": "In conventional concept-to-speech (CTS) methods, a common step is predicting abstract prosodic descriptions, such as the locations of accents and phrase boundaries, from the linguistic information provided by the text generation module. But the prediction results always contain errors, and unacceptable prosodic prediction may ruin the synthesized speech. In addition, linguistic information, which can be given conveniently and accurately by text generation, has not been directly utilized in the acoustic modelling and speech generation of CTS. This paper displays a CTS method utilizing HMM-based speech synthesis (HTS) and a text generation module called Komet-Penman multilingual (KPML). In this method, syntagmatic features derived from the linguistic information given by KPML is directly added to the context features for context-dependent HMM modelling. Further, prosodic features are discarded during acoustic modelling to avoid costly prosodic annotation on the training waveforms and inaccurate prosodic prediction at synthesis time. Experiments show that the proposed method performs no worse than the conventional method with automatic prosodic prediction. When manual prosodic annotation on the training corpus is unavailable, the proposed method performs better.", "title": "Concept-to-speech generation by integrating syntagmatic features into HMM-based speech synthesis"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/gowda14_interspeech.html", "abstract": "In this paper, we study the role of a recently proposed feature enhancement technique in building HMM-based synthetic voices using reverberant speech data. The feature enhancement technique studied combines the advantages of missing data imputation and non-negative matrix factorization (NMF) based methods in cleaning up the reverberant features. Speaker adaptation of a clean average voice using noisy data is generally better than building a speaker dependent voice using the noisy data. In this paper, we show that the proposed feature enhancement technique can further improve the spectral match between the enhanced feature adapted voice and a clean speaker dependent voice.", "title": "On the role of missing data imputation and NMF feature enhancement in building synthetic voices using reverberant speech"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/do14b_interspeech.html", "abstract": "In this paper, we propose a new objective evaluation method for hidden Markov model (HMM)-based speech synthesis using Kullback-Leibler divergence (KLD). The KLD is used to measure the difference between the probability density functions (PDFs) of the acoustic feature vectors extracted from natural training and synthetic speech data. For the evaluation, Gaussian mixture model (GMM) is used to model the distribution of acoustic feature vectors, including the fundamental frequency (F0). Continuous F0, obtained with linear interpolation, is used in the evaluation. In essence, the KLD is the expectation of the logarithmic difference between the likelihoods calculated on training and synthetic speech. This likelihood difference is appropriate to characterize the quality of a HMM-based speech synthesis system in generating synthetic speech using a maximum likelihood criterion. The objective evaluation is tested with 3 different HMM-based speech synthesis systems which use multi-space distribution (MSD) to model discontinuous F0. These systems are trained on a common speech corpus in French. We propose an index to evaluate HMM-based speech synthesis system which takes into account the relative variation of the KLDs on test sets of synthetic and natural speech. This index correlates inversely with the result of the MOS (mean opinion score) perceptual test.", "title": "Objective evaluation of HMM-based speech synthesis system using kullback-leibler divergence"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/latorre14_interspeech.html", "abstract": "The standard evaluation of intonation models is by means of non-referenced subjective tests (pair or MOS) in which subjects rate the quality or compare different samples without any explicit reference. These tests are usually conducted on an isolated sentence basis. However, for a single sentence, with no contextual information, there are multiple valid intonations. A subject's preference over this range of intonation patterns may be highly personal. This paper investigates the degree to which this ambiguity in the appropriate intonation pattern impacts the assessments of prosody for speech synthesis systems. To examine this problem, the variance of the F0 pattern of several vocoded sentences was modified and subjects asked to compare multiple versions with different levels of modification in terms of preference/quality. Then, they were presented with the reference which defines the original intonation and asked about the similarity to that reference. The results show that subjects can identify the samples with no F0 variance modification when given a reference but they don't always prefer them. Thus, non-referenced tests with no context, though may help to analyse user acceptability, may not be appropriate to measure the performance of intonation models.", "title": "Speech intonation for TTS: study on evaluation methodology"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/miao14_interspeech.html", "abstract": "When deployed in automated speech recognition (ASR), deep neural networks (DNNs) can be treated as a complex feature extractor plus a simple linear classifier. Previous work has investigated the utility of multilingual DNNs acting as language-universal feature extractors (LUFEs). In this paper, we explore different strategies to further improve LUFEs. First, we replace the standard sigmoid nonlinearity with the recently proposed maxout units. The resulting maxout LUFEs have the nice property of generating sparse feature representations. Second, the convolutional neural network (CNN) architecture is applied to obtain more invariant feature space. We evaluate the performance of LUFEs on a cross-language ASR task. Each of the proposed techniques results in word error rate reduction compared with the existing DNN-based LUFEs. Combining the two methods together brings additional improvement on the target language.", "title": "Improving language-universal feature extraction with deep maxout and convolutional neural networks"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/fernandez14_interspeech.html", "abstract": "A traditional framework in speech production describes the output speech as an interaction between a source excitation and a vocal-tract configured by the speaker to impart segmental characteristics. In general, this simplification has led to approaches where systems that focus on phonetic segment tasks (e.g. speech recognition) make use of a front-end that extracts features that aim to distinguish between different vocal-tract configurations. The excitation signal, on the other hand, has received more attention for speaker-characterization tasks. In this work we look at augmenting the front-end in a recognition system with vocal-source features, motivated by our work with languages that are low in resources and whose phonology and phonetics suggest the need for complementary approaches to classical ASR features. We demonstrate that the additional use of such features provides improvements over a state-of-the-art system for low-resource languages from the BABEL Program.", "title": "Exploiting vocal-source features to improve ASR accuracy for low-resource languages"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/ragni14_interspeech.html", "abstract": "Recently there has been interest in the approaches for training speech recognition systems for languages with limited resources. Under the IARPA Babel program such resources have been provided for a range of languages to support this research area. This paper examines a particular form of approach, data augmentation, that can be applied to these situations. Data augmentation schemes aim to increase the quantity of data available to train the system, for example semi-supervised training, multilingual processing, acoustic data perturbation and speech synthesis. To date the majority of work has considered individual data augmentation schemes, with few consistent performance contrasts or examination of whether the schemes are complementary. In this work two data augmentation schemes, semi-supervised training and vocal tract length perturbation, are examined and combined on the Babel limited language pack configuration. Here only about 10 hours of transcribed acoustic data are available. Two languages are examined, Assamese and Zulu, which were found to be the most challenging of the Babel languages released for the 2014 Evaluation. For both languages consistent speech recognition performance gains can be obtained using these augmentation schemes. Furthermore the impact of these performance gains on a down-stream keyword spotting task are also described.", "title": "Data augmentation for low resource languages"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/jouvet14_interspeech.html", "abstract": "This paper introduces the combination of speech decoders for selecting automatically transcribed speech data for unsupervised training or adaptation of acoustic models. Here, the combination relies on the use of a forward-based and a backward-based decoder. Best performance is achieved when selecting automatically transcribed data (speech segments) that have the same word hypotheses when processed by the Sphinx forward-based and the Julius backward-based transcription systems, and this selection process outperforms confidence measure based selection. Results are reported and discussed for adaptation and for full training from scratch, using data resulting from various selection processes, whether alone or in addition to the baseline manually transcribed data. Overall, selecting automatically transcribed speech segments that have the same word hypotheses when processed by the Sphinx forward-based and Julius backward-based recognizers, and adding this automatically transcribed and selected data to the manually transcribed data leads to significant word error rate reductions on the ESTER2 data when compared to the baseline system trained only on manually transcribed speech data.", "title": "About combining forward and backward-based decoders for selecting data for unsupervised training of acoustic models"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/grezl14_interspeech.html", "abstract": "Multilingual training of neural networks for ASR is widely studied these days. It has been shown that languages with little training data can benefit largely from the multilingual resources for training. The use of unlabeled data for the neural network training in semi-supervised manner has also improved the ASR system performance. Here, the combination of both methods is presented. First, multilingual training is performed to obtain an ASR system to automatically transcribe the unlabeled data. Then, the automatically transcribed data are added. Two neural networks are trained \u2014 one from random initialization and one adapted from multilingual network \u2014 to evaluate the effect of multilingual training under presence of larger amount of training data. Further, the CMLLR transform is applied in the middle of the stacked Bottle-Neck neural network structure. As the CMLLR rotates the features to better fit given model, we evaluated whether it is better to adapt the existing NN on the CMLLR features or if it is better to train it from random initialization. The last step in our training procedure is the fine-tuning on the original data.", "title": "Combination of multilingual and semi-supervised training for under-resourced languages"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/vu14b_interspeech.html", "abstract": "Deep neural networks (DNNs) have become state-of-the-art techniques of automatic speech recognition in the last few years. They can be used at the preprocessing level (Tandem or Bottle-Neck features) or at the acoustic model level (hybrid Hidden Markov Model/DNN). Moreover, they allow exploiting multilingual data to improve monolingual systems. This paper presents our investigation of the learning effect of neural networks in the context of multilingual Bottle-Neck features. For this, we perform a visual analysis of the output of the Bottle-Neck layer of a neural network using t-Distributed Stochastic Neighbor Embedding. Our results show that multilingual Bottle-Neck features seem to learn phoneme characteristics, such as the F1 and F2 formants which characterize different vowels, and other articulatory features, such as fricatives and nasals which characterize consonants. Furthermore, they seem to normalize language dependent variations and transfer the learned representation to unseen languages.", "title": "Investigating the learning effect of multilingual bottle-neck features for ASR"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/miao14b_interspeech.html", "abstract": "Multilingual deep neural networks (DNNs) can act as deep feature extractors and have been applied successfully to cross-language acoustic modeling. Learning these feature extractors becomes an expensive task, because of the enlarged multilingual training data and the sequential nature of stochastic gradient descent (SGD). This paper investigates strategies to accelerate the learning process over multiple GPU cards. We propose the DistModel and DistLang frameworks which distribute feature extractor learning by models and languages respectively. The time-synchronous DistModel has the nice property of tolerating infrequent model averaging. With 3 GPUs, DistModel achieves 2.6 \u00d7 speed-up and causes no loss on word error rates. When using DistLang, we observe better acceleration but worse recognition performance. Further evaluations are conducted to scale DistModel to more languages and GPU cards.", "title": "Distributed learning of multilingual DNN feature extractors using GPUs"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/rath14_interspeech.html", "abstract": "In recent years there has been significant interest in Automatic Speech Recognition (ASR) and KeyWord Spotting (KWS) systems for low resource languages. One of the driving forces for this research direction is the IARPA Babel project. This paper examines the performance gains that can be obtained by combining two forms of deep neural network ASR systems, Tandem and Hybrid, for both ASR and KWS using data released under the Babel project. Baseline systems are described for the five option period 1 languages: Assamese; Bengali; Haitian Creole; Lao; and Zulu. All the ASR systems share common attributes, for example deep neural network configurations, and decision trees based on rich phonetic questions and state-position root nodes. The baseline ASR and KWS performance of Hybrid and Tandem systems are compared for both the \u201cfull\u201d, approximately 80 hours of training data, and limited, approximately 10 hours of training data, language packs. By combining the two systems together consistent performance gains can be obtained for KWS in all configurations.", "title": "Combining tandem and hybrid systems for improved speech recognition and keyword spotting on low resource languages"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/cui14_interspeech.html", "abstract": "In this paper we focus on several techniques that improve deep neural network (DNN) acoustic modeling for low-resource languages. We explore the use of different features such as, fundamental-frequency variation (FFV), tonal features, and normalization of these features for deep neural network training. Specifically we study the impact of these features in conjunction with a tonal lexicon and several neural network architectures including hybrid and bottleneck feature-based configurations. We also explore the use of un-transcribed data and ways to balance it with transcribed data, to enhance the performance of the best performing LVCSR system. Results are presented in the context of the IARPA Babel program on development languages from Babel option period as well as on the surprise language from the base period of the program. We show that these improved methods can provide up to 15% relative reduction in WER and improvements in keyword search, in the languages explored under the BABEL program.", "title": "Recent improvements in neural network acoustic modeling for LVCSR in low resource languages"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/huang14b_interspeech.html", "abstract": "Modeling heterogeneous data sources remains a fundamental challenge of acoustic modeling in speech recognition. We call this the multi-condition problem because the speech data come from many different conditions. In this paper, we introduce the fundamental confusability problem in multi-condition learning, then discuss the problem formalization, the taxonomy, and the architectures for multi-condition learning. While the ideas presented are applicable to all classifiers, we focus our attention in this work on acoustic models based on deep neural networks (DNN).We propose four different strategies for multi-condition learning of a DNN that we refer to as a mixed-condition model, a condition-dependent model, a condition-normalizing model, and a condition-aware model. Based on the experimental results on the voice search and short message dictation task and the Aurora 4 task, we show that the confusability introduced when modeling heterogeneous data depends on the source of acoustic distortion itself, the front-end feature extractor, and the classifier. We also demonstrate the best approach for dealing with heterogeneous data may not be to let the model sort it out blindly, even with a classifier as sophisticated as a DNN.", "title": "Towards better performance with heterogeneous training data in acoustic modeling using deep neural networks"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/higuchi14_interspeech.html", "abstract": "This paper proposes to introduce a new model called \u201cthe multi-channel factorial hidden Markov Model (MFHMM)\u201d for under-determined blind signal separation (BSS). For monaural source separation, one successful approach involves applying non-negative matrix factorization (NMF) to the magnitude spectrogram of a mixture signal, interpreted as a non-negative matrix. Up to now, multichannel extensions of NMF, which allow for the use of spatial information as an additional clue for source separation, have been proposed by several authors and proven to be an effective approach for underdetermined BSS. This approach is based on the assumption that an observed signal is a mixture of a limited number of source signals each of which has a static power spectral density scaled by a time-varying amplitude. However, many source signals in real world are non-stationary in nature and the variations of the spectral densities are much richer in time. Moreover, many sources including speech tend to stay inactive for some while until they switch to an active mode, implying that the total power of a source may depend on its underlying state. To reasonably characterize such a non-stationary nature of source signals, this paper proposes to extend the multichannel NMF model by modeling the transition of the set consisting of the spectral densities and the total power of each source using a hidden Markov model (HMM). By letting each HMM contain states corresponding to active and inactive modes, we will show that voice activity detection and source separation can be solved simultaneously through parameter inference of the present model. The experiment showed that the proposed algorithm provided a 7.65 dB improvement compared with the conventional multichannel NMF in terms of the signal-to-distortion ratio.", "title": "A unified approach for underdetermined blind signal separation and source activity detection by multichannel factorial hidden Markov models"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/vaz14_interspeech.html", "abstract": "We propose a spectro-temporal regularization approach for NMF that accounts for a source's spectral variability over time. The regularization terms allow NMF to adapt the spectral basis matrices optimally to reduce mismatch between the spectral characteristics of sources observed during training and encountered during separation. We first tested our algorithm on a simulated source separation task. Preliminary results show significant improvement of SAR, SDR, and SIR values over some current NMF methods. We also tested our algorithm on a speech enhancement task and were able to show a modest improvement of the PESQ scores of the recovered speech.", "title": "Enhancing audio source separability using spectro-temporal regularization with NMF"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/mirzaei14_interspeech.html", "abstract": "In this paper, the tasks of speech source localization, source counting and source separation are addressed for an unknown number of sources in a stereo recording scenario. In the first stage, the angles of arrival of individual source signals are estimated through a peak finding scheme applied to the angular spectrum which has been derived using non-linear GCC-PHAT. Then, based on the known channel mixture coefficients, we propose an approach for separating the sources based on Maximum Likelihood (ML) estimation. The predominant source in each time-frequency bin is identified through ML assuming a diffuse noise model. The separation performance is improved over a binary time-frequency masking method. The performance is measured by obtaining the existing metrics for blind source separation evaluation. The experiments are performed on synthetic speech mixtures in both anechoic and reverberant environments.", "title": "Blind speech source localization, counting and separation for 2-channel convolutive mixtures in a reverberant environment"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/weninger14_interspeech.html", "abstract": "The objective of single-channel source separation is to accurately recover source signals from mixtures. Non-negative matrix factorization (NMF) is a popular approach for this task, yet previous NMF approaches have not optimized directly this objective, despite some efforts in this direction. Our paper introduces discriminative training of the NMF basis functions such that, given the coefficients obtained on a mixture, a desired source is optimally recovered. We approach this optimization by generalizing the model to have separate analysis and reconstruction basis functions. This generalization frees us to optimize reconstruction objectives that incorporate the filtering step and SNR performance criteria. A novel multiplicative update algorithm is presented for the optimization of the reconstruction basis functions according to the proposed discriminative objective functions. Results on the 2nd CHiME Speech Separation and Recognition Challenge task indicate significant gains in source-to-distortion ratio with respect to sparse NMF, exemplar-based NMF, as well as a previously proposed discriminative NMF criterion.", "title": "Discriminative NMF and its application to single-channel source separation"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/kawahara14_interspeech.html", "abstract": "A highly-reproducible estimation method of vocal tract length (VTL) and text independent VTL estimation method are proposed based on a Japanese vowel database spoken by 385 male and female speakers ranging from age 6 to 56 and other vowel database with MRI-based vocal tract shape information. Proposed methods are based on interference-free power spectral representation and systematic suppression of biasing factors. MRI data is used to calibrate VTL estimation result to be represented in terms of physically meaningful unit. These databases are normalized based on the estimated VTL information to provide a reference template, which is used to implement a text independent VTL estimation method. A prototype system for text independent estimation of VTL is implemented using Matlab and runs faster than realtime on a PC.", "title": "Vocal tract length estimation based on vowels using a database consisting of 385 speakers and a database with MRI-based vocal tract shape information"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/wang14e_interspeech.html", "abstract": "This paper describes a new approach to unsupervised acoustic modeling, that is to build acoustic models for phoneme-like sub-word units from untranscribed speech data. The proposed approach is based on Gaussian component clustering. Initially a large set of Gaussian components are estimated from the untranscribed data. Then clustering is performed to group these Gaussian components into different clusters. Each cluster of Gaussian components forms an acoustic model for an induced sub-word unit. We have defined several similarity measures among the Gaussian components, and investigated several different graph-based clustering algorithms. Experiments on the TIMIT corpus demonstrate the effectiveness of our approach.", "title": "A graph-based Gaussian component clustering approach to unsupervised acoustic modeling"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/ziaei14_interspeech.html", "abstract": "The ability to count the number of words spoken by an individual over long durations is important to researchers investigating language development, healthcare, education, etc. In this study, we attempt to build a speech system that can compute daily word counts using data from the Prof-Life-Log corpus. The task is challenging as typical audio files from Prof-Life-Log tend to be 8-to-16 hours long, where audio is collected continuously using the LENA device. This device is worn by the primary speaker and all his daily interactions are collected in fine detail. The recordings contain a wide variety of noise types with varying SNR (signal-to-noise ratio) including large crowd, babble, and competing secondary speakers. In this study, we develop a word-count estimation (WCE) system based on syllable detection and we use the method proposed by Wang and Narayanan as the baseline system [1]. We propose many modifications to the original algorithm to improve its effectiveness in noise. Particularly, we incorporate speech activity detection and enhancement techniques to remove non-speech from analysis and improve signal quality for superior syllable detection, respectively. We also investigate features derived from syllable detection for better word count estimation. The proposed method show significant improvement over the baseline.", "title": "A speech system for estimating daily word counts"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/lu14_interspeech.html", "abstract": "Denoising autoencoder (DAE) is effective in restoring clean speech from noisy observations. In addition, it is easy to be stacked to a deep denoising autoencoder (DDAE) architecture to further improve the performance. In most studies, it is supposed that the DAE or DDAE can learn any complex transform functions to approximate the transform relation between noisy and clean speech. However, for large variations of speech patterns and noisy environments, the learned model is lack of focus on local transformations. In this study, we propose an ensemble modeling of DAE to learn both the global and local transform functions. In the ensemble modeling, local transform functions are learned by several DAEs using data sets obtained from unsupervised data clustering and partition. The final transform function used for speech restoration is a combination of all the learned local transform functions. Speech denoising experiments were carried out to examine the performance of the proposed method. Experimental results showed that the proposed ensemble DAE model provided superior restoration accuracy than traditional DAE models.", "title": "Ensemble modeling of denoising autoencoder for speech spectrum restoration"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/tuske14_interspeech.html", "abstract": "In this paper we investigate how much feature extraction is required by a deep neural network (DNN) based acoustic model for automatic speech recognition (ASR). We decompose the feature extraction pipeline of a state-of-the-art ASR system step by step and evaluate acoustic models trained on standard MFCC features, critical band energies (CRBE), FFT magnitude spectrum and even on the raw time signal. The focus is put on raw time signal as input features, i.e. as much as zero feature extraction prior to DNN training. Noteworthy, the gap in recognition accuracy between MFCC and raw time signal decreases strongly once we switch from sigmoid activation function to rectified linear units, offering a real alternative to standard signal processing. The analysis of the first layer weights reveals that the DNN can discover multiple band pass filters in time domain. Therefore we try to improve the raw time signal based system by initializing the first hidden layer weights with impulse responses of an audiologically motivated filter bank. Inspired by the multi-resolutional analysis layer learned automatically from raw time signal input, we train the DNN on a combination of multiple short-term features. This illustrates how the DNN can learn from the little differences between MFCC, PLP and Gammatone features, suggesting that it is useful to present the DNN with different views on the underlying audio.", "title": "Acoustic modeling with deep neural networks using raw time signal for LVCSR"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/mitra14_interspeech.html", "abstract": "Deep Neural Network (DNN) based acoustic models have shown significant improvement over their Gaussian Mixture Model (GMM) counterparts in the last few years. While several studies exist that evaluate the performance of GMM systems under noisy and channel degraded conditions, noise robustness studies on DNN systems have been far fewer. In this work we present a study exploring both conventional DNNs and deep Convolutional Neural Networks (CNN) for noise- and channel-degraded speech recognition tasks using the Aurora4 dataset. We compare the baseline mel-filterbank energies with noise-robust features that we have proposed earlier and show that the use of robust features helps to improve the performance of DNNs or CNNs compared to mel-filterbank energies. We also show that vocal tract length normalization has a positive role in improving the performance of the robust acoustic features. Finally, we show that by combining multiple systems together we can achieve even further improvement in recognition accuracy.", "title": "Evaluating robust features on deep neural networks for speech recognition in noisy and channel mismatched conditions"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/sainath14_interspeech.html", "abstract": "Log-mel filterbank features, which are commonly used features for CNNs, can remove higher-resolution information from the speech signal. A novel technique, known as Deep Scattering Spectrum (DSS), addresses this issue and looks to preserve this information. DSS features have shown promise on TIMIT, both for classification and recognition. In this paper, we extend the use of DSS features for LVCSR tasks. First, we explore the optimal multi-resolution time and frequency scattering operations for LVCSR tasks. Next, we explore techniques to reduce the dimension of the DSS features. We also incorporate speaker adaptation techniques into the DSS features. Results on a 50 and 430 hour English Broadcast News task show that the DSS features provide between a 4\u20137% relative improvement in WER over log-mel features, within a state-of-the-art CNN framework which incorporates speaker-adaptation and sequence training. Finally, we show that DSS features are similar to multi-resolution log-mel + MFCCs, and similar improvements can be obtained with this representation.", "title": "Deep scattering spectra with deep neural networks for LVCSR tasks"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/chang14_interspeech.html", "abstract": "As has been extensively shown, acoustic features for speech recognition can be learned from neural networks with multiple hidden layers. However, the learned transformations may not sufficiently generalize to test sets that have a significant mismatch to the training data. Gabor features, on the other hand, are generated from spectro-temporal filters designed to model human auditory processing. In previous work, these features are used as inputs to neural networks, which improved word accuracy for speech recognition in the presence of noise. Here we propose a neural network architecture called a Gabor Convolutional Neural Network (GCNN) that incorporates Gabor functions into convolutional filter kernels. In this architecture, a variety of Gabor features served as the multiple feature maps of the convolutional layer. The filter coefficients are further tuned by back-propagation training. Experiments used two noisy versions of the WSJ corpus: Aurora 4, and RATS re-noised WSJ. In both cases, the proposed architecture performs better than other noise-robust features that we have tried, namely, ETSI-AFE, PNCC, Gabor features without the CNN-based approach, and our best neural network features that don't incorporate Gabor functions.", "title": "Robust CNN-based speech recognition with Gabor filter kernels"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/lu14b_interspeech.html", "abstract": "We have recently proposed a new acoustic model based on probabilistic linear discriminant analysis (PLDA) which enjoys the flexibility of using higher dimensional acoustic features, and is more capable to capture the intra-frame feature correlations. In this paper, we investigate the use of bottleneck features obtained from a deep neural network (DNN) for the PLDA-based acoustic model. Experiments were performed on the Switchboard dataset \u2014 a large vocabulary conversational telephone speech corpus. We observe significant word error reduction by using the bottleneck features. In addition, we have also compared the PLDA-based acoustic model to three others using Gaussian mixture models (GMMs), subspace GMMs and hybrid deep neural networks (DNNs), and PLDA can achieve comparable or slightly higher recognition accuracy from our experiments.", "title": "Probabilistic linear discriminant analysis with bottleneck features for speech recognition"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/schatz14_interspeech.html", "abstract": "The Minimal-Pair ABX (MP-ABX) paradigm has been proposed as a method for evaluating speech features for zero-resource/unsupervised speech technologies. We apply it in a phoneme discrimination task on the Articulation Index corpus to evaluate the resistance to noise of various speech features. In Experiment 1, we evaluate the robustness to additive noise at different signal-to-noise ratios, using car and babble noise from the Aurora-4 database and white noise. In Experiment 2, we examine the robustness to different kinds of convolutional noise. In both experiments we consider two classes of techniques to induce noise resistance: smoothing of the time-frequency representation and short-term adaptation in the time-domain. We consider smoothing along the spectral axis (as in PLP) and along the time axis (as in FDLP). For short-term adaptation in the time-domain, we compare the use of a static compressive non-linearity followed by RASTA filtering to an adaptive compression scheme.", "title": "Evaluating speech features with the minimal-pair ABX task (II): resistance to noise"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/geiger14b_interspeech.html", "abstract": "In the light of the improvements that were made in the last years with neural network-based acoustic models, it is an interesting question whether these models are also suited for noise-robust recognition. This has not yet been fully explored, although first experiments confirm this question. Furthermore, preprocessing techniques that improve the robustness should be re-evaluated with these new models. In this work, we present experimental results to address these questions. Acoustic models based on Gaussian mixture models (GMMs), deep neural networks (DNNs), and long short-term memory (LSTM) recurrent neural networks (which have an improved ability to exploit context) are evaluated for their robustness after clean or multi-condition training. In addition, the influence of non-negative matrix factorization (NMF) for speech enhancement is investigated. Experiments are performed with the Aurora-4 database and the results show that DNNs perform slightly better than LSTMs and, as expected, both beat GMMs. Furthermore, speech enhancement is capable of improving the DNN result.", "title": "Investigating NMF speech enhancement for neural network based acoustic models"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/lilley14_interspeech.html", "abstract": "Following pediatric cochlear implantation, it is important to monitor the recipient's auditory perception skills to (a) determine appropriate therapeutic strategies, and (b) guide adjustment (mapping) of how the CI stimulation is distributed over its channels. One evaluation tool used in clinical and research settings is the OlimSpac [1], which assesses perception at the phonetic feature level by asking children to repeat stimuli that are presented aurally. Errors in the child's repetition of an item are taken to reflect errors in their perception of the stimulus features. While normally administered by a trained clinician who must score the child's productions, we present here results from a study involving automatic scoring of productions from 17 pediatric CI patients aged 3;0 to 6;10 years. Taken independently, the automated system was in agreement with human feature classifications over 93% of the time overall (range was 99% for vowel height to 89% for consonant voicing) for stimuli on which human judges were in unanimous agreement. Automatic scoring of utterances from young CI recipients could allow the OlimSpac and similar testing procedures to be used more frequently in a clinical setting, and even administered remotely under appropriate conditions.", "title": "Automatic speech feature classification for children with cochlear implants"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/tachioka14_interspeech.html", "abstract": "Linear discriminant analysis (LDA) is a simple and effective feature transformation technique that aims to improve discriminability by maximizing the ratio of the between-class variance to the within-class variance. However, LDA does not explicitly consider the sequential discriminative criterion which consists in directly reducing the errors of a speech recognizer. This paper proposes a simple extension of LDA that is called sequential LDA (sLDA) based on a sequential discriminative criterion computed from the Gaussian statistics, which are obtained from sequential maximum mutual information (MMI) training. Although the objective function of the proposed LDA can be regarded as a special case of various discriminative feature transformation techniques (for example, f-MPE or the bottom layer of a neural network), the transformation matrix can be obtained as the closed-form solution to a generalized eigenvalue problem, in contrast to the gradient-descent-based optimization methods usually used in these techniques. Experiments on LVCSR (Corpus of Spontaneous Japanese) and noisy speech recognition task (2nd CHiME challenge) show consistent improvements from standard LDA due to the sequential discriminative training. In addition, the proposed method, despite its simple and fast computation, improved the performance in combination with discriminative feature transformation (f-bMMI), perhaps by providing a good initialization to f-bMMI.", "title": "Sequential maximum mutual information linear discriminant analysis for speech recognition"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/ghaffarzadegan14_interspeech.html", "abstract": "This study proposes model and feature based strategies for automatic whispered speech recognition. Our goal is to compensate for the mismatch between neutral-trained recognizer models and parameters of whispered speech. We propose a pseudo-whisper generation from neutral speech samples for efficient acoustic model adaptation. The scheme is based on the popular Vector Taylor Series (VTS) algorithm. In the first step, a `background' model capturing a rough estimate of the target whispered speech characteristics from a small amount of whispered data is trained. Second, the target background model is utilized in the VTS strategy to establish broad phone classes (consonants and vowels) transformations for individual neutral utterances and transform them towards whisper. Finally, these pseudo-whisper samples are used to adapt neutral recognizer models towards whisper. This approach is evaluated together with Vocal Tract Length Normalization (VTLN) and Shift frequency transforms and show to greatly benefit recognition performance compared to a traditional whisper-adaptation approach. The absolute WER on the closed speakers whisper scenario has been reduced from 17.3% to 8.4% and the open speakers scenario from 27.7% to 17.5 %.", "title": "Model and feature based compensation for whispered speech recognition"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/moghimi14_interspeech.html", "abstract": "In the context of array processing for speech and audio applications, linear beamforming has long been the approach of choice, for reasons including good performance, robustness and analytical simplicity. Nevertheless, various nonlinear techniques, typically based on the study of auditory scene analysis, have also been of interest. The class of techniques known as time-frequency (T-F) masking, in particular, shows promise; T-F masking is based on accepting or rejecting individual time-frequency cells based on some estimate of local signal quality. While these approaches have been shown to outperform linear beamforming in two-sensor arrays, extensions to larger arrays have been few and unsuccessful. This paper seeks to gain a deeper understanding of the limitations of T-F masking in larger arrays and to develop an approach to overcome them. It is shown that combining beamforming and masking can bring the benefits of masking to larger arrays. As a result, a hybrid beamforming-masking approach, called post-masking, is developed that improves upon the performance of MMSE beamforming (and can be used with any beamforming technique). Post-masking extends the benefits of masking up to arrays of six elements or more, with the potential for even greater improvement in the future.", "title": "Post-masking: a hybrid approach to array processing for speech recognition"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/delacallesilos14_interspeech.html", "abstract": "In this paper we present advances in the modeling of the masking behavior of the Human Auditory System to enhance the robustness of the feature extraction stage in Automatic Speech Recognition. The solution adopted is based on a non-linear filtering of a spectro-temporal representation applied simultaneously on both the frequency and time domains, by processing it using mathematical morphology operations as if it were an image. A particularly important component of this architecture is the so called structuring element: biologically-based considerations are addressed in the present contribution to design an element that closely resembles the masking phenomena taking place in the cochlea. The second feature of this contribution is the choice of underlying spectro-temporal representation. The best results were achieved by the representation introduced as part of the Power Normalized Cepstral Coefficients together with a spectral subtraction step. On the Aurora 2 noisy continuous digits task, we report relative error reductions of 18.7% compared to PNCC and 39.5% compared to MFCC.", "title": "ASR feature extraction with morphologically-filtered power-normalized cochleograms"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/martinez14_interspeech.html", "abstract": "Features inspired by the auditory system have previously demonstrated improvement in automatic speech recognition (ASR). Similarly, the use of Deep Neural Networks (DNN) was found to outperform classic approaches to ASR in many conditions. Since DNNs have the potential to learn the task-relevant features from a conventional filter bank output, we investigate if the combination of auditory features and deep learning should be preferred over self-learned patterns. Specifically, noise-robust Gabor features and Amplitude Modulation Filter-Bank (AMFB) features, highly invariant against reverberation, are used as input to a state-of-the-art ASR system incorporating DNN processing. On the Aurora-4 task, both mel-frequency cepstral coefficients (MFCC) and filter bank (FBank) features are outperformed in many acoustic conditions through auditory processing, yielding average relative improvements of up to 69% over MFCC and 21% over the commonly used DNN-FBank setup. This highlights the mutual benefit of auditory signal processing and recent advances in machine learning.", "title": "Should deep neural nets have ears? the role of auditory features in deep learning approaches"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/fox14_interspeech.html", "abstract": "Limabeam is an approach to multi-microphone array processing for ASR which makes minimal assumptions about system geometry, instead searching for filters to maximise output likelihoods under a speech model. The first results of Limabeam on the AMI meeting corpus are given, then two extensions of the algorithm for this corpus. First, it is shown that the original local gradient following sticks in local minima, and a coarser gradient is used. Second, a new discriminative objective function is provided to handle mismatched silence models. The extensions are based on examination of 2D receptive fields and 2D likelihood maps which are novel near-field analogs of radial beamformer response patterns, but do not show radial symmetry and have many local minima. The extended Limabeam improves WER on TDOA baselines on the AMI corpus, by 1% rel. when both are adapted with decodes and by 19% rel. when both adapted with ground truth.", "title": "Extending Limabeam with discrimination and coarse gradients"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/mukherjee14_interspeech.html", "abstract": "In Text to Speech synthesis system F0 contour plays an important role in conveying prosodic information but the process of synthesizing F0 contour from the underlying linguistic information using deep architecture has not been investigated in case of Bengali languages. This paper describes a method for synthesizing F0 contours of Bengali readout speech from the textual features of input text using Deep Boltzmann Machine (DBM) and Twin Gaussian Process (TGP) hybrid model. DBM will capture the high-level linguistic structure of input text and improve the prediction accuracy when plugged into the TGP model. Unlike Gaussian Process (GP) models which only focus on the prediction of a single output (F0), TGP can generalize across multiple outputs (F0, delta F0, delta-delta F0) by encoding relations between both inputs and outputs with GP priors. The performance of the proposed method is evaluated and compared with other available methods using objective and perceptual listening tests and the results are found to be satisfactory.", "title": "Generation of F0 contour using deep boltzmann machine and twin Gaussian process hybrid model for bengali language"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/moralescordovilla14_interspeech.html", "abstract": "The problem of room localization is to determine where, in a multi-room environment, a person is producing a speech utterance. In our work, we are exploiting the information gained from a network of microphones installed all over a house, where the lack of calibration of the microphone energies creates an additional challenge. This paper compares room localizers based on different features (such as energy and cross-correlation between microphones) and classifiers (such as neural networks and discriminative analysis). In order to evaluate the different room localizers in terms of word accuracy this paper also presents a complete distant speech recognition system which tries to take advantage of synergy between the different components without using any oracle information. Finally, the system is analyzed in terms of computational and time resources.", "title": "Room localization for distant speech recognition"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/bahaadini14_interspeech.html", "abstract": "Posterior features have been shown to yield very good performance in multiple contexts including speech recognition, spoken term detection, and template matching. These days, posterior features are usually estimated at the output of a neural network. More recently, sparse representation has also been shown to potentially provide additional advantages to improve discrimination and robustness. One possible instance of this, is referred to as exemplar-based sparse representation.   The present work investigates how to exploit sparse modelling together with posterior space properties to further improve speech recognition features. In that context, we leverage exemplar-based sparse representation, and propose a novel approach to project phone posterior features into a new, high-dimensional, sparse feature space. In fact, exploiting the properties of posterior spaces, we generate, new, high-dimensional, linguistically inspired (sub-phone and words), posterior distributions. Validation experiments are performed on the Phonebook (isolated words) and HIWIRE (continuous speech) databases, which support the effectiveness of the proposed approach for speech recognition tasks.", "title": "Posterior-based sparse representation for automatic speech recognition"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/tabain14_interspeech.html", "abstract": "This study examines dental, alveolar, retroflex and palatal lateral formants. Data are taken from three languages of Central Australia: Arrernte, Pitjantjatjara and Warlpiri. Results show that in relation to the alveolar lateral, the dental has a lower F1 and a higher F4; the retroflex has lower F3 and F4 and slightly higher F2; and the palatal has lower F1 and higher F2, F3 and F4. These results are discussed in light of various acoustic models of lateral production.", "title": "Lateral formants in three central australian languages"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/khasanova14_interspeech.html", "abstract": "Examining articulatory compensation has been important in understanding how the speech production system is organized, and how it relates to the acoustic and ultimately phonological levels. This paper offers a method that detects articulatory compensation in the acoustic signal, which is based on linear regression modeling of co-variation patterns between acoustic cues. We demonstrate the method on selected acoustic cues for spontaneously produced American English stop consonants. Compensatory patterns of cue variation were observed for voiced stops in some cue pairs, while uniform patterns of cue variation were found for stops as a function of place of articulation or position in the word. Overall, the results suggest that this method can be useful for observing articulatory strategies indirectly from acoustic data and testing hypotheses about the conditions under which articulatory compensation is most likely.", "title": "Detecting articulatory compensation in acoustic data through linear regression modeling"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/guo14_interspeech.html", "abstract": "The relationship between vowel formants and the second subglottal resonance (Sg2) has previously been explored in English, German, Hungarian and Korean. Results from these studies indicate that vowel space is categorically divided by Sg2 and that Sg2 correlates well with standing height. One of the goals of this work is to verify if the above findings hold true in Mandarin as well. The correlation between Sg2 and sitting height (trunk length) is also studied. Further, since Mandarin is a tonal language (with more pitch variations compared to English), we study the relationship between Sg2 and fundamental frequency (F0). A new corpus of simultaneous recordings of speech and subglottal acoustics was collected from 20 native Mandarin speakers. Results on this corpus indicate that Sg2 divides vowel space in Mandarin as well, and that it is more correlated with sitting height than standing height. Paired t-tests are conducted on the Sg2 measurements from different vowel parts, which represent different F0 regions. Preliminary results show that there is no statistically-significant variation of Sg2 with F0 within a tone.", "title": "The relationship between the second subglottal resonance and vowel class, standing height, trunk length, and F0 variation for Mandarin speakers"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/meenakshi14_interspeech.html", "abstract": "In the recordings using electromagnetic articulograph AG 501, sensors are glued to subject's articulators such as jaw, lips and tongue and both speech and articulatory movements are simultaneously recorded. In this work, we study the effect of the presence of the sensors on the quality of speech spoken by the subject. This is done by recording when a subject speaks a set of 19 VCV stimuli while sensors are attached to subject's articulators. For comparison we also record the same set of stimuli spoken by the same subject but with no sensors attached to subject's articulators. Both subjective and objective comparisons are made on the recorded stimuli in these two settings. Subjective evaluation is carried out using 16 evaluators. Listening experiments with recordings from five subjects show that the recordings with sensors attached are significantly different from those without sensors attached in terms of human recognition score as well as on a perceptual difference measure. This is also supported in the objective comparison which computes dissimilarity measure using the spectral shape information.", "title": "Comparison of speech quality with and without sensors in electromagnetic articulograph AG 501 recording"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/albuquerque14_interspeech.html", "abstract": "The elderly population is quickly increasing in the developed countries. However, in European Portuguese (EP) no studies have examined the impact of age-related structural changes in speech acoustics. The purpose of this paper is to analyse the effect of age ([60\u201370], [71\u201380] and [81\u201390]), gender and type of vowel in the acoustic characteristics (fundamental frequency (F0), first formant (F1), second formant (F2) and duration) of the EP vowels. A sample of 78 speakers was selected from the database of elderly speech collected by Microsoft Language Development Center (MLDC) within the Living Usability Lab (LUL) project. It was observed that duration is the only parameter that significantly changes with ageing, being the highest value found in the [81\u201390] group. Moreover, F0 decreases in females and increases in males with ageing. In general, F1 and F2 decreases with ageing, mainly in females. Comparing the data obtained with the results of previous studies with adult speakers, a trend towards the centralization of vowels with ageing is observed. This investigation is the starting point for a broader study which will allow to analyse the changes in vowels acoustics from childhood to old age in EP.", "title": "Impact of age in the production of European Portuguese vowels"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/yu14_interspeech.html", "abstract": "Speech and language processing technology has the potential of playing an important role in future deep space missions. To be able to replicate the success of speech technologies from ground to space, it is important to understand how astronaut's speech production mechanism changes when they are in space. In this study, we investigate the variations of astronaut's voice characteristic during NASA Apollo 11 mission. While the focus is constrained to analysis of the three astronauts voices who participated in the Apollo 11 mission, it is the first step towards our long term objective of automating large components of space missions with speech and language technology. The result of this study is also significant from an historical point of view as it provides a new perspective of understanding the key moment of human history \u2014 landing a man on the moon, as well as employed for future advancement in speech and language technology in \u201cnon-neutral\u201d conditions.", "title": "`houston, we have a solution': a case study of the analysis of astronaut speech during NASA apollo 11 for long-term speaker modeling"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/luan14_interspeech.html", "abstract": "Differences in pronunciation have been shown to underlie significant talker-dependent intelligibility differences. There are several dimensions of variability that are correlated with talker intelligibility including pitch range, vowel-space expansion, and rhythmic patterns. Prior work has shown that some of the better predictors of individual intelligibility are based on the talker's F1 by F2 vowel space, but findings are based on hand-corrected measurements on carefully balanced sets of vowels, making large scale analysis impractical. This paper proposes a novel method for automatic estimation of a talker's vowel space using sparse expanded vowel space representations, including an approximate convex hull sampling, which are projected to a low dimensional space for intelligibility scoring. Both supervised and unsupervised mappings are used to generate an intelligibility score. Automatic intelligibility rankings are assessed in terms of correlation with an intelligibility score based on human transcription accuracy. We find that including a larger sample of vowels (beyond point vowels) leads to improved performance, obtaining correlations of roughly 0.6 for this feature alone, which is a strong result given that there are other factors that may also contribute to a talker's intelligibility in addition to a talker's vowel space area.", "title": "Relating automatic vowel space estimates to talker intelligibility"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/kawahara14b_interspeech.html", "abstract": "A group delay-based excitation source analysis and design method is introduced for extension of TANDEM-STRAIGHT, a speech analysis, modification and synthesis system. This introduction makes all components of the system be based on interference-free representations. They are power spectrum, instantaneous frequency and group delay representations. This unification has potential to solve the major weak point of VOCODER architecture for high-quality speech manipulation applications.", "title": "Excitation source analysis for high-quality speech manipulation systems based on an interference-free representation of group delay with minimum phase response compensation"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/pedersen14_interspeech.html", "abstract": "Efficient speech signal representations are prerequisite for efficient speech processing algorithms. The Vandermonde transform is a recently introduced time-frequency transform which provides a sparse and uncorrelated speech signal representation. In contrast, the Fourier transform only decorrelates the signal approximately. To achieve complete decorrelation, the Vandermonde transform is signal adaptive like the Karhunen-Lo\u00e8ve transform. Unlike the Karhunen-Lo\u00e8ve, however, the Vandermonde transform is a time-frequency transform where the transform domain components correspond to frequency components of the analysis window.   In this paper we analyze the performance of sparse speech signal representation by the Vandermonde transform. This is done by applying matching pursuit and comparing with sparse representations based on dictionaries with Fourier, Cosine, Gabor and Karhunen-Lo\u00e8ve atoms. Our results show that Karhunen-Lo\u00e8ve yields the best sparse signal recovery; however, this is not strictly a time-frequency transform. Of the true time-frequency transforms, Vandermonde is the most efficient for sparse speech signal representation.", "title": "Sparse time-frequency representation of speech by the vandermonde transform"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/nandwana14_interspeech.html", "abstract": "In this paper, we present analysis of the characteristics of scream to identify its discriminating features from neutral speech. The impact of screaming on the performance of text independent speaker recognition systems has also been reported. We have observed that speaker recognition systems are not reliable when tested with scream. Also perceptual listeners test reveal that the speaker content in scream is very less for human to distinguish and classify it. This analysis will be useful for development of robust speaker recognition systems and their implementation in real-time situations.", "title": "Analysis and identification of human scream: implications for speaker recognition"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/wang14f_interspeech.html", "abstract": "In this study, we propose a frequency domain F0 estimation approach based on long term Harmonic Feature Analysis combined with artificial neural network ( ANN) classification. Long term spectrum analysis is proposed to gain better harmonic resolution, which reduces the spectrum interference between speech and noise. Next pitch candidates are extracted for each frame from the long term spectrum. Five specific features related to harmonic structure are computed for each candidate and combined together as a feature vector to indicate the status of each candidate. An ANN is trained to model the relation between the harmonic features and the true pitch values. In the test phase, target pitch is selected from the candidates according to the maximum output score from the ANN. Finally, post-processing is applied based on average segmental output to eliminate inconsistent or fluctuating decision errors. Experimental results show that the proposed algorithm outperforms several state-of-the-art methods for F0 estimation under adverse conditions, including white noise and multi-speaker babble noise.", "title": "F0 estimation in noisy speech based on long-term harmonic feature analysis combined with neural network classification"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/slaney14_interspeech.html", "abstract": "Most features used for speech recognition are derived from the output of a filterbank inspired by the auditory system. The two most commonly used filter shapes are the triangular filters used in MFCC (mel-frequency cepstral coefficients) and the gammatone filters that model psychoacoustic critical bands. However, for both of these filterbanks there are free parameters that must be chosen by the system designer. In this paper, we explore the effect that different parameter settings have on the discriminability of speech sound classes. Specifically, we focus our attention on two primary parameters: the filter shape (triangular or gammatone) and the filter bandwidth. We use variations in the noise level and the pitch to explore the behavior of different filterbanks. We use the Fisher linear discriminant to give us insight about why some filterbanks perform better than others. We observe three things: 1) there are significant differences even among different implementations of the same filterbank, 2) wider filters help remove the non-informative pitch information, and 3) the Fisher criteria helps us understand why. We validate the Fisher measure with speech recognition experiments on the Aurora-4 speech corpus.", "title": "The influence of pitch and noise on the discriminability of filterbank features"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/harwath14_interspeech.html", "abstract": "Speech recognition is an increasingly important input modality, especially for mobile computing. Because errors are unavoidable in real applications, efficient correction methods can greatly enhance the user experience. In this paper we study a reranking and classification strategy for choosing word alternates to display to the user in the framework of a tap-to-correct interface. By employing a logistic regression model to estimate the probability that an alternate will offer a useful correction to the user, we can significantly reduce the average length of the alternates lists generated with no reduction in the number of words they are able to correct.", "title": "Choosing useful word alternates for automatic speech recognition correction interfaces"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/chen14f_interspeech.html", "abstract": "Meeting transcription is a very useful and challenging task. The majority of research to date has focused on individual meeting, or only a small group of meetings. In many practical deployments, multiple related meetings will take place over a long period of time. This paper describes an initial investigation of how this long-term data can be used to improve meeting transcription. A corpus of technical meetings, using a single microphone array, was collected over a two year period, yielding a total of 179 hours of meeting data. Baseline systems using deep neural network acoustic models, in both Tandem and Hybrid configurations, and neural network-based language models are described. The impact of supervised and unsupervised adaptation of the acoustic models is then evaluated, as well as the impact of improved language models.", "title": "An initial investigation of long-term adaptation for meeting transcription"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/ng14_interspeech.html", "abstract": "This paper presents a set of techniques that we used to improve our keyword search system for the third phase of the DARPA RATS (Robust Automatic Transcription of Speech) program, which seeks to advance state of the art detection capabilities on audio from highly degraded radio communication channels. The results for both Levantine and Farsi, which are the two target languages for the keyword search (KWS) task, are reported. About 13% absolute reduction in word error rate (from 70.2% to 57.6%) is achieved by using acoustic features derived from stacked Multi-Layer Perceptrons (MLP) and Deep Neural Network (DNN) acoustic models. In addition to score normalization and score/system combination for keyword search, we showed that the false alarm rate at the target false reject rate (15%) was reduced by about 1% (from 5.39% to 4.45%) by reducing the deletion errors of the speech-to-text system.", "title": "Progress in the BBN keyword search system for the DARPA RATS program"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/nouza14_interspeech.html", "abstract": "In this paper, we present the outcome of a 4-year project whose ultimate goal is to develop a complex platform that can transcribe, index and make searchable the historical archive of Czech and Czechoslovak Radio. The archive covers 90 years of public broadcasting and contains hundreds of thousands audio documents. The developed modular platform employs our LVCSR system that has to cope with 2 related languages: Czech and Slovak. Furthermore, it must deal with audio files of varying quality (e.g. recordings originally stored on matrices or tapes, data passed through analog and digital telephone lines, speech recorded during parliament or court sessions, etc.) The system includes speaker and language identification modules, a narrow-band signal detector, a music/song detector, and several other components to enhance transcription accuracy and provide support for multi-optional search. We evaluate the performance on broadcast news test sets grouped according to decades. We show that after acoustic and language model adaptation WER values are in range 8\u201314% and do not differ much since 1960s to present. We report also results achieved on other types of documents (e.g. talk shows, political debates, public speeches, etc), where the WER is higher but still acceptable for most search tasks.", "title": "Speech-to-text technology to transcribe and disclose 100,000+ hours of bilingual documents from historical Czech and Czechoslovak radio archive"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/ylmaz14_interspeech.html", "abstract": "Reading skills of children can be improved with the help of automatic reading tutors (ART), i.e. interactive software with an appealing interface which supports and challenges the child in the reading task, provides instantaneous feedback and automatically assesses its reading skills. For this purpose, ARTs benefit from automatic speech recognition technology for tracking the child's responses and detecting reading miscues (errors). In previous work, a novel speech recognition architecture has been proposed which adopts a two-layered structure: first a phone recognizer uses task-independent acoustic and language models to generate a phone lattice which is then decoded using a lexicon of expected words and task-dependent finite state grammars. This approach has shown significant improvements in reading miscue detection. In this paper, we extend this technique by employing a more flexible decoding scheme that allows substitution, deletion and insertion of phones. Specifically, the phone lattice generated in the first layer is extended based on a phone confusion matrix that models the typical phone confusions in a language. The proposed system has provided improved miscue detection on the CHOREC database compared to a baseline system without a phone confusion model.", "title": "Automatic assessment of children's reading with the FLaVoR decoding using a phone confusion model"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/shaik14_interspeech.html", "abstract": "In this paper, German, Polish, Spanish, and Portuguese large vocabulary continuous speech recognition (LVCSR) systems developed by the RWTH Aachen University are presented. All the above mentioned systems for the aforementioned languages are used for the Quaero and EU-Bridge project evaluations. The LVCSR systems developed for these competitive evaluations focus on various domains like broadcast news, podcasts and lecture domain. Transcription of the speech for these tasks is challenging due to huge variability in the acoustic conditions and a significant portion of audio data includes spontaneous speech. Good improvements are obtained using state-of-the-art multilingual bottleneck features, minimum phone error trained acoustic models, language model (LM) adaptation and confusion-network based system combination. In addition, an open vocabulary approach using morphemic units is investigated along with the LM adaptation for the German LVCSR.", "title": "RWTH LVCSR systems for quaero and EU-bridge: German, Polish, Spanish and Portuguese"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/zohrer14_interspeech.html", "abstract": "Single channel source separation (SCSS) is ill-posed and thus challenging. In this paper, we apply general stochastic networks (GSNs) \u2014 a deep neural network architecture \u2014 to SCSS. We extend GSNs to be capable of predicting a time-frequency representation, i.e. softmask by introducing a hybrid generative-discriminative training objective to the network. We evaluate GSNs on data of the 2nd CHiME speech separation challenge. In particular, we provide results for a speaker dependent, a speaker independent, a matched noise condition and an unmatched noise condition task. Empirically, we compare to other deep architectures, namely a deep belief network (DBN) and a multi-layer perceptron (MLP). In general, deep architectures perform well on SCSS tasks.", "title": "Single channel source separation with general stochastic networks"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/yeung14_interspeech.html", "abstract": "Conditional random field (CRF) formulations for single-microphone speech separation are improved by large-margin parameter estimation. Speech sources are represented by acoustic state sequences from speaker-dependent acoustic models. The large-margin technique improves the classification accuracy of acoustic states by reducing generalization error in the training phase. Non-linear mappings inspired from the mixture-maximization (MIXMAX) model are applied to speech mixture observations. Compared with a factorial hidden Markov model baseline, the improved CRF formulations achieve better separation performance with significantly fewer training data. The separation performance is evaluated in terms of objective speech quality measures and speech recognition accuracy on the reconstructed sources. Compared with the CRF formulations without large-margin parameter estimation, the improved formulations achieve better performance without modifying the statistical inference procedures, especially when the sources are modeled with increased number of acoustic states.", "title": "Large-margin conditional random fields for single-microphone speech separation"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/jafari14_interspeech.html", "abstract": "In this paper, we investigate the application of a generative clustering technique for the estimation of time-frequency source separation masks. Recent advances in time-frequency clustering-based approaches to blind source separation have touched upon the Watson mixture model (WMM) as a tool for source separation. However, most methods have been frequency bin-wise and have thus required the additional permutation alignment stage, and previous full-band methods which employ the WMM have yet to be applied to the under-determined setting. We propose to evaluate the clustering ability of the WMM within the clustering-based source separation framework. Evaluations confirm the superiority of the WMM against other previously used clustering techniques such as the fuzzy c-means.", "title": "On the use of the Watson mixture model for clustering-based under-determined blind source separation"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/hsu14_interspeech.html", "abstract": "In this paper, a binary mask estimation algorithm is proposed based on modulations of speech. A multi-resolution spectro-temporal analytical auditory model is utilized to extract modulation features to estimate the binary mask, which is often used in speech segregation applications. The proposed method estimates noise from the beginning of each test sentence, a common approach seen in many conventional speech enhancement algorithms, to further enhance the modulation features. Experimental results demonstrate that the proposed method outperforms the AMS-GMM system in terms of the HIT-FA rate when estimating the binary mask.", "title": "Binary mask estimation based on frequency modulations"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/yang14_interspeech.html", "abstract": "This paper proposes a new Bayesian nonnegative matrix factorization (NMF) for speech and music separation. We introduce the Poisson likelihood for NMF approximation and the exponential prior distributions for the factorized basis matrix and weight matrix. A variational Bayesian (VB) EM algorithm is developed to implement an efficient solution to variational parameters and model parameters for Bayesian NMF. Importantly, the exponential prior parameter is used to control the sparseness in basis representation. The variational lower bound in VB-EM procedure is derived as an objective to conduct adaptive basis selection for different mixed signals. The experiments on single-channel speech/music separation show that the adaptive basis representation in Bayesian NMF via model selection performs better than the NMF with the fixed number of bases in terms of signal-to-distortion ratio.", "title": "Bayesian factorization and selection for speech and music separation"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/wohlmayr14_interspeech.html", "abstract": "Single-channel source separation (SCSS) usually uses pre-trained source-specific models to separate the sources. These models capture the characteristics of each source and they perform well when matching the test conditions.   In this paper, we extend the applicability of SCSS. We develop an EM-like iterative adaption algorithm which is capable to adapt the pre-trained models to the changed characteristics of the specific situation, such as a different acoustic channel introduced by variation in the room acoustics or changed speaker position. The adaption framework requires signal mixtures only, i.e. specific single source signals are not necessary. We consider speech/noise mixtures and we restrict the adaption to the speech model only. Model adaption is empirically evaluated using mixture utterances from the CHiME 2 challenge. We perform experiments using speaker dependent (SD) and speaker independent (SI) models trained on clean or reverberated single speaker utterances. We successfully adapt SI source models trained on clean utterances and achieve almost the same performance level as SD models trained on reverberated utterances.", "title": "Self-adaption in single-channel source separation"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/vacher14_interspeech.html", "abstract": "Voice command system in multi-room smart homes for assisting people in loss of autonomy in their daily activities faces several challenges, one of which being the distant condition which impacts the ASR system performance. This paper presents an approach to improve voice command recognition at the decoding level by using multiple sources and model adaptation. The method has been tested on data recorded with 11 elderly and visually impaired participants in a real smart home. The results show an error rate of 3.2% in off-line condition and of 13.2% in on-line condition.", "title": "Multichannel automatic recognition of voice command in a multi-room smart home: an experiment involving seniors and users with visual impairment"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/walter14_interspeech.html", "abstract": "In this paper, we investigate unsupervised acoustic model training approaches for dysarthric-speech recognition. These models are first, frame-based Gaussian posteriorgrams, obtained from Vector Quantization (VQ), second, so-called Acoustic Unit Descriptors (AUDs), which are hidden Markov models of phone-like units, that are trained in an unsupervised fashion, and, third, posteriorgrams computed on the AUDs. Experiments were carried out on a database collected from a home automation task and containing nine speakers, of which seven are considered to utter dysarthric speech. All unsupervised modeling approaches delivered significantly better recognition rates than a speaker-independent phoneme recognition baseline, showing the suitability of unsupervised acoustic model training for dysarthric speech. While the AUD models led to the most compact representation of an utterance for the subsequent semantic inference stage, posteriorgram-based representations resulted in higher recognition rates, with the Gaussian posteriorgram achieving the highest slot filling F-score of 97.02%.", "title": "An evaluation of unsupervised acoustic model training for a dysarthric speech interface"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/gonzalez14_interspeech.html", "abstract": "This paper investigates the potential of a silent speech interface (SSI) based on Permanent Magnetic Articulography (PMA) to be used in applications involving unconstrained, phonetically rich speech. In previous work the SSI was evaluated on isolated-word and connected-digits recognition tasks with promising results. Furthermore, it was shown that PMA data is enough to distinguish between minimal pairs of consonants with the same manner and place of articulation but different voicing. The study presented in this paper extends previous work by investigating to what extent information about the speech production process can be recovered from PMA data. In particular, the three main aspects of speech production are investigated here: voicing, place of articulation and manner of articulation. The results show that PMA achieves comparable accuracy to using the audio signal for discriminating the place of articulation of consonants, but it provides little information regarding the voicing and manner of articulation.", "title": "Analysis of phonetic similarity in a silent speech interface based on permanent magnetic articulography"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/karpov14_interspeech.html", "abstract": "In this paper, we present some novel methods and applications for audio and video signal processing for a multimodal environment of an assisted living smart space. This intelligent environment was developed during the 7th Summer Workshop on Multimodal Interfaces eNTERFACE. It integrates automatic systems for audio and video-based monitoring and user tracking in the smart space. In the assisted living environment, users are tracked by some omnidirectional video cameras, as well as speech and non-speech audio events are recognized by an array of microphones. The multiple objects tracking precision (MOTP) of the developed video monitoring system was 0.78 and 0.73 and the multiple objects tracking accuracy (MOTA) was 62.81% and 72.31% for single person and three people scenarios, respectively. The recognition accuracy of the proposed multilingual speech and audio events recognition system was 96.5% and 93.8% for user's speech commands and non-speech acoustic events, correspondingly. The design of the assisted living environment, the certain test scenarios and the process of audio-visual database collection are described in the paper.", "title": "Audio-visual signal processing in a multimodal assisted living environment"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/ravanelli14_interspeech.html", "abstract": "Distant-speech recognition represents a technology of fundamental importance for future development of assistive applications characterized by flexible and unobtrusive interaction in home environments. State-of-the-art speech recognition still exhibits lack of robustness, and an unacceptable performance variability, due to environmental noise, reverberation effects, and speaker position. In the past, multi-condition training and contamination methods were explored to reduce the mismatch between training and test conditions. However, the performance evaluation can be biased by factors as limited number of positions of speaker and microphones, adopted set of impulse responses, vocabulary and grammars defining the recognition task. The purpose of this paper is to investigate in more detail some critical aspects that characterize such experimental context. To this purpose, our work addressed a microphone network distributed over different rooms of an apartment and a related set of speaker-microphone pairs leading to a very large set of impulse responses. Besides simulations, the experiments also tackled real speech interactions. The performance evaluation was based on a phone-loop task, in order to minimize the influence of linguistic constraints. The experimental results show how less critical is an accurate selection of impulse responses, if compared to other factors as the signal-to-noise ratio introduced by additive background noise.", "title": "On the selection of the impulse responses for distant-speech recognition based on contaminated speech training"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/casanueva14_interspeech.html", "abstract": "Spoken control interfaces are very attractive to people with severe physical disabilities who often also have a type of speech disorder known as dysarthria. This condition is known to decrease the accuracy of automatic speech recognisers (ASRs) especially for users with moderate to severe dysathria. In this paper we investigate how applying probabilistic dialogue management (DM) techniques can improve interaction performance of an environmental control system for such users. The effect of having access to different amounts of adaptation data, as well as using different vocabulary size for speakers of different intelligibilities is investigated. We explore the effect of adapting the DM models as the ASR performance increases, such as is the case in systems where more adaptation data is collected through system use. Improvements compared to a non-probabilistic DM baseline are seen both in terms of dialogue length and success rate, 9% and 25% mean relative improvement respectively. Looking at just the more severe dysarthric speakers these numbers rise 25% and 75% mean relative improvement. These improvements are higher when the ASR data adaptation amount is small. Further results show that a DM trained on data from multiple speakers outperform a DM trained on data from a single speaker.", "title": "Adaptive speech recognition and dialogue management for users with speech disorders"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/yu14b_interspeech.html", "abstract": "Neurophysiological changes in the brain associated with early dementia can disrupt articulatory timing and precision in speech production. Motivated by this observation, we address the hypothesis that speaking rate and articulatory coordination, as manifested through formant frequency tracks, can predict performance on an animal fluency task administerd to the elderly. Specifically, using phoneme-based measures of speaking rate and articulatory coordination derived from formant cross-correlation measures, we investigate the capability of speech features, estimated from paragraph-recall and naturalistic free speech, to predict animal fluency assessment scores. Using a database consisting of audio from elderly subjects over a 4 year period, we develop least-squares regression models of our cognitive performance measures. The best performing model combined speaking rate and formant features, resulting in a correlation (R) of 0.61 and a root mean squared error (RMSE) of 5.07 with respect to a 9\u201334 score range. Vocal features thus provide a reduction by about 30% in MSE from a baseline (mean score) in predicting cognitive performance derived from the animal fluency assessment.", "title": "Prediction of cognitive performance in an animal fluency task based on rate and articulatory markers"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/ishi14_interspeech.html", "abstract": "The extraction of sound events in environments where a large number of people are present is a challenging problem. In order to tackle that problem, we have been developing a sound environment intelligence system which is able to get information about who is talking, where and when, based on integration of multiple microphone arrays and human tracking technologies. We installed the developed system in a science room of an elementary school, and collected data of real science classes during a period of one month. In the present paper, among the sound activities appearing in the science classes, we focused on the analysis of laughter events, considering that laughter conveys important social functions in communication. Laughter events were extracted by making use of visual displays of spatial-temporal information provided by the developed system. Subjective evaluation of the laughter events revealed relationship between the laughter type (including production, style, and vowel-quality aspects), the functions in communication, and the appropriateness in the classroom context.", "title": "Analysis of laughter events in real science classes by using multiple environment sensor data"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/sainath14b_interspeech.html", "abstract": "While Deep Neural Networks (DNNs) have achieved tremendous success for LVCSR tasks, training these networks is slow. To date, the most common approach to train DNNs is via stochastic gradient descent (SGD), serially on a single GPU machine. Serial training, coupled with the large number of training parameters and speech data set sizes, makes DNN training very slow for LVCSR tasks. While 2nd order, data-parallel methods have also been explored, these methods are not always faster on CPU clusters due to the large communication cost between processors. In this work, we explore using a specialized hardware/software approach, utilizing a Blue Gene/Q (BG/Q) system, which has thousands of processors and excellent inter-processor communication. We explore using the 2nd order Hessian-free (HF) algorithm for DNN training with BG/Q, for both cross-entropy and sequence training of DNNs. Results on three LVCSR tasks indicate that using HF with BG/Q offers up to an 11x speedup, as well as an improved word error rate (WER), compared to SGD on a GPU.", "title": "Parallel deep neural network training for LVCSR tasks using blue gene/Q"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/bengio14_interspeech.html", "abstract": "Speech recognition systems have used the concept of states as a way to decompose words into sub-word units for decades. As the number of such states now reaches the number of words used to train acoustic models, it is interesting to consider approaches that relax the assumption that words are made of states. We present here an alternative construction, where words are projected into a continuous embedding space where words that sound alike are nearby in the Euclidean sense. We show how embeddings can still allow to score words that were not in the training dictionary. Initial experiments using a lattice rescoring approach and model combination on a large realistic dataset show improvements in word error rate.", "title": "Word embeddings for speech recognition"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/seide14_interspeech.html", "abstract": "We show empirically that in SGD training of deep neural networks, one can, at no or nearly no loss of accuracy, quantize the gradients aggressively \u2014 to but one bit per value \u2014 if the quantization error is carried forward across minibatches (error feedback). This size reduction makes it feasible to parallelize SGD through data-parallelism with fast processors like recent GPUs.   We implement data-parallel deterministically distributed SGD by combining this finding with AdaGrad, automatic minibatch-size selection, double buffering, and model parallelism. Unexpectedly, quantization benefits AdaGrad, giving a small accuracy gain.   For a typical Switchboard DNN with 46M parameters, we reach computation speeds of 27k frames per second (kfps) when using 2880 samples per minibatch, and 51kfps with 16k, on a server with 8 K20X GPUs. This corresponds to speed-ups over a single GPU of 3.6 and 6.3, respectively. 7 training passes over 309h of data complete in under 7h. A 160M-parameter model training processes 3300h of data in under 16h on 20 dual-GPU servers \u2014 a 10 times speed-up \u2014 albeit at a small accuracy loss.", "title": "1-bit stochastic gradient descent and its application to data-parallel distributed training of speech DNNs"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/takeda14_interspeech.html", "abstract": "The boundary contraction training for acoustic models based on deep neural networks in a discrete system (discrete DNNs) is presented in this paper. Representing the parameters of DNNs with small bits (such as 4 bits) can reduce not only memory usage but also computational complexity by utilizing a CPU cache and look-up tables efficiently. However, simply quantizing parameters of the normal continuous DNNs degrades the recognition accuracy seriously. We tackle this problem by developing a specialized training algorithm for discrete DNNs. In our algorithm, continuous DNNs with boundary constraint are first trained, and the trained parameters are then quantized to meet the representation of discrete DNNs. When training continuous DNNs, we introduce the boundary contraction mapping to shrink the distribution of parameters for reducing the quantization error. In our experiments with 4-bit discrete DNNs, while simply quantizing normally trained DNNs degrades the word accuracy by more than 50 points, our method can maintain the high word accuracy of DNNs with only two points degradation.", "title": "Boundary contraction training for acoustic models based on discrete deep neural networks"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/kubo14_interspeech.html", "abstract": "This paper attempts to optimize a topology of hidden Markov models (HMMs) for automatic speech recognition. Current state-of-the-art acoustic models for ASR involve HMMs with deep neural network (DNN)-based emission density functions. Even though DNN parameters are typically trained by optimizing a discriminative criterion, topology optimization of HMMs is usually performed by optimizing a generative criterion. Several approaches have been studied to achieve a discriminative state clustering, these approaches typically assume underlying Gaussian distributions of the acoustic features, and do not compatible with DNN-based emission density functions. In this paper, we attempt to derive a discriminative restructuring method of an HMM topology by introducing discriminative optimization with discrete constraints on the parameters, which force the parameters to be tied with the parameters of the other states. By applying this constrained optimization to the clustering of parameters of DNN-based acoustic models, we derived a discriminative HMM restructuring method that maintains discriminative performance of the original HMMs with the large number of states.", "title": "Restructuring output layers of deep neural networks using minimum risk parameter clustering"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/chan14_interspeech.html", "abstract": "Recently, deep Convolutional Neural Networks have been shown to outperform Deep Neural Networks for acoustic modelling, producing state-of-the-art accuracy in speech recognition tasks. Convolutional models provide increased model robustness through the usage of pooling invariance and weight sharing across spectrum and time. However, training convolutional models is a very computationally expensive optimization procedure, especially when combined with large training corpora. In this paper, we present a novel algorithm for scalable training of deep Convolutional Neural Networks across multiple GPUs. Our distributed asynchronous stochastic gradient descent algorithm incorporates sparse gradients, momentum and gradient decay to accelerate the training of these networks. Our approach is stable, neither requiring warm-starting or excessively large minibatches. Our proposed approach enables convolutional models to be efficiently trained across multiple GPUs, enabling a model to be scaled asynchronously across 5 GPU workers with \u02dc68% efficiency.", "title": "Distributed asynchronous optimization of convolutional neural networks"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/toth14_interspeech.html", "abstract": "Convolutional neural networks have recently been shown to outperform fully connected deep neural networks on several speech recognition tasks. Their superior performance is due to their convolutional structure that processes several, slightly shifted versions of the input window using the same weights, and then pools the resulting neural activations. This pooling operation makes the network less sensitive to translations. The convolutional network results published up till now used sigmoid or rectified linear neurons. However, quite recently a new type of activation function called the maxout activation has been proposed. Its operation is closely related to convolutional networks, as it applies a similar pooling step, but over different neurons evaluated on the same input. Here, we combine the two technologies, and experiment with deep convolutional neural networks built from maxout neurons. Phone recognition tests on the TIMIT database show that switching to maxout units from rectifier units decreases the phone error rate for each network configuration studied, and yields relative error rate reductions of between 2% and 6%.", "title": "Convolutional deep maxout networks for phone recognition"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/chen14g_interspeech.html", "abstract": "Multi-task learning (MTL) can be an effective way to improve the generalization performance of singly learning tasks if the tasks are related, especially when the amount of training data is small. Our previous work applied MTL to the joint training of triphone and trigrapheme acoustic models using deep neural networks (DNNs) for low-resource speech recognition. Significant recognition improvement over the performance of their DNNs trained by single-task learning (STL) was obtained. In that work, both STL-DNNs and MTL-DNNs were trained by minimizing the total frame-wise cross entropies. Since phoneme and grapheme recognition are inherently sequence classification tasks, here we study the effect of sequence-discriminative training on their joint estimation using MTL-DNNs. Experimental evaluation on TIMIT phoneme recognition shows that joint sequence training outperforms frame-wise training of phone and grapheme MTL-DNNs significantly.", "title": "Joint sequence training of phone and grapheme acoustic model based on multi-task learning deep neural networks"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/hsiao14_interspeech.html", "abstract": "In this work, we investigate how to improve semi-supervised DNN for low resource languages where the initial systems may have high error rate. We propose using semi-supervised MLP features for DNN training, and we also explore using confidence to improve semi-supervised cross entropy and sequence training. The work conducted in this paper was evaluated under the IARPA Babel program for the keyword spotting tasks. We focus on the limited condition where there are around 10 hours of supervised data for training.", "title": "Improving semi-supervised deep neural network for keyword search in low resource languages"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/liu14d_interspeech.html", "abstract": "A main advantage of the deep neural network (DNN) model lies on the fact that no artificial assumptions are placed on the data distribution and model structure, which offers the possibility to learn very flexible models. This flexibility, however, may lead to highly redundant parameters, hence demanding computation and risk of over-fitting. Network pruning cuts off unimportant connections, and therefore can be used to produce parsimonious and well generalizable models.   This paper proposes to utilize optimal brain damage (OBD) to conduct DNN pruning. OBD computes connection salience based on Hessians, and thus is sound in theory and reliable in practice. We present our implementation of OBD for DNNs, and demonstrate that the OBD pruning can produce very sparse DNNs while retaining the discriminative power of the original network to a large extent. By comparing with a simple magnitude-based pruning, we find that for weak pruned networks, pruning methods are unimportant since retraining can largely recover the function loss caused by pruning; while for highly pruned networks, sophisticated pruning methods (such as OBD) are clearly superior.", "title": "Pruning deep neural networks by optimal brain damage"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/avila14_interspeech.html", "abstract": "While considerable work has been done to characterize the detrimental effects of channel variability on automatic speaker verification (ASV) performance, little attention has been paid to the effects of room reverberation. This paper investigates the effects of room acoustics on the performance of two far-field ASV systems: GMM-UBM (Gaussian mixture model - universal background model) and i-vector. We show that ASV performance is severely affected by reverberation, particularly for i-vector based systems. Three multi-condition training methods are then investigated to mitigate such detrimental effects. The first uses matched train/test speaker models based on estimated reverberation time (RT) values. The second utilizes two-condition training where clean and reverberant models are used. Lastly, a four-condition training setup is proposed where models for clean, mild, moderate, and severe reverberation levels are used. Experimental results show the first and third multi-condition training methods providing significant gains in performance relative to the baseline, with the latter being more suitable for practical resource-constrained far-field applications.", "title": "Improving the performance of far-field speaker verification using multi-condition training: the case of GMM-UBM and i-vector systems"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/lee14_interspeech.html", "abstract": "In this paper, we first reformulate the derivation of the conventional i-vector scheme, which is the state-of-the-art utterance representation for speaker verification, as a modeling of universal background model (UBM)-based mixtures of factor analyzers (UMFA), and then propose a clustering-based UMFA method called CMFA. In UMFA, each analyzer is characterized by a subspace, and the same projection coordinate of an utterance into individual subspaces is called the i-vector. We relax this assumption by grouping the mixture components of the UBM into clusters according to their acoustic traits. Therefore, in CMFA, each utterance is represented by multiple i-vectors, each of which generated by similar subspaces associated with a same cluster. We also investigate two strategies for merging these i-vectors into a single one to be applied in the classifier of the conventional i-vector framework. The results of experiments conducted on the male portion of the core task in the NIST 2005 Speaker Recognition Evaluation (SRE) in terms of normalized decision cost function (minDCF) and equal error rate (EER) demonstrate the merits of the new i-vector method over the conventional i-vector method.", "title": "Clustering-based i-vector formulation for speaker recognition"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/arsikere14_interspeech.html", "abstract": "Motivated by the speaker-specificity and stationarity of subglottal acoustics, this paper investigates the utility of subglottal cepstral coefficients (SGCCs) for speaker identification (SID) and verification (SV). SGCCs can be computed using accelerometer recordings of subglottal acoustics, but such an approach is infeasible in real-world scenarios. To estimate SGCCs from speech signals, we adopt the Bayesian minimum mean squared error (MMSE) estimator proposed in the speech-to-articulatory inversion literature. The joint distribution of SGCCs and speech MFCCs is modeled using theWashU-UCLA corpus (containing simultaneous recordings of speech and subglottal acoustics), and the resulting model is used to obtain an MMSE estimate of SGCCs from unseen (test) MFCCs. Cross-validation experiments on the WashU-UCLA corpus show that the estimation efficacy, on average, is speaker dependent. A score-level fusion of MFCC and SGCC systems outperforms the MFCC-only baseline in both SID and SV tasks. On the TIMIT database (SID), the relative reduction in identification error is 16, 40 and 51% for G.712-filtered (300\u20133400 Hz), narrowband (0\u20134000 Hz) and wideband (0\u20138000 Hz) speech, respectively. On the NIST 2008 database (SV), the relative reduction in equal error rate is 4 and 11% for 10 and 5 second utterances, respectively.", "title": "Speaker recognition via fusion of subglottal features and MFCCs"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/sun14_interspeech.html", "abstract": "This paper presents an improved speaker recognition system for the summed channel evaluation tasks in the 2008 NIST SRE (SRE08) with multiple summed-channel excerpts for speaker training and one summed-channel excerpt for testing. The system includes three main modules in which a hybrid speaker purification and clustering algorithm is adopted to segregate the summed-channel speech, a common speaker identification is proposed by mapping multiple summed-channel excerpts for a common speaker cluster, and the GMM-SVM-NAP algorithm is used for the speaker recognition system. The system achieves an overall EER of 7.82% for all the trials and 4.19% for English trials in the SRE08 3summed-summed task.", "title": "The NIST SRE summed channel speaker recognition system"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/gallardo14b_interspeech.html", "abstract": "Wideband communications permit the transmission of an extended frequency range compared to the traditional narrowband. While benefits for automatic speaker recognition can be expected, the extent of the contribution of the additional bandwidth in wideband is still unclear. This work compares the i-vector speaker verification performances employing speech signals of 0\u20134 kHz, 4\u20138 kHz, and 0\u20138 kHz and different sets of cepstral features extracted using linearly- and a mel-spaced filterbanks. Analyses of clean speech and of speech transmitted through commonly employed codecs are conducted separately for male and for female speech. Our evaluation on two different datasets shows the improved speaker verification performance with the extended bandwidth, and also that the linear scale can lead to better results for narrowband signals. The advantages of linear- over mel-scaled features for wideband depend on the speakers' gender and on the channel distortion.", "title": "Advantages of wideband over narrowband channels for speaker verification employing MFCCs and LFCCs"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/li14e_interspeech.html", "abstract": "This paper presents a generalized i-vector framework with phonetic tokenizations and tandem features for speaker verification as well as language identification. First, the tokens for calculating the zero-order statistics is extended from the MFCC trained Gaussian Mixture Models (GMM) components to phonetic phonemes, 3-grams and tandem feature trained GMM components using phoneme posterior probabilities. Second, given the calculated zero-order statistics (posterior probabilities on tokens), the feature used to calculate the first-order statistics is also extended from MFCC to tandem features and is not necessarily the same feature employed by the tokenizer. Third, the zero-order and first-order statistics vectors are then concatenated and represented by the simplified supervised i-vector approach followed by the standard back end modeling methods. We study different system setups with different tokens and features. Finally, selected effective systems are fused at the score level to further improve the performance. Experimental results are reported on the NIST SRE 2010 common condition 5 female part task and the NIST LRE 2007 closed set 30 seconds task for speaker verification and language identification, respectively. The proposed generalized i-vector framework outperforms the i-vector baseline by relatively 45% in terms of equal error rate (EER) and norm minDCF values.", "title": "Speaker verification and spoken language identification using a generalized i-vector framework with phonetic tokenizations and tandem features"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/asha14_interspeech.html", "abstract": "Feature fusion is a paradigm that has found success in a number of speech related tasks. The primary objective in applying fusion is to leverage the complementary information present in the features. Conventionally, either early or late fusion is employed. Early fusion leads to large dimensional feature vectors. Further, the range of feature values for different streams require appropriate normalisation. Late fusion is carried out at score level, where the contribution from each type of feature is determined from the set of weights used. Feature switching is yet another paradigm that attempts to capture the diversity in the feature types used. Feature switching gains significance particularly in the context of speaker verification, where the feature type that best discriminates a speaker is used to verify the claims corresponding to that speaker. Earlier, feature switching was attempted in the conventional UBM-GMM framework. In this paper, the idea is extended to the Total Variability Space (TVS) framework. Two different feature types namely Modified Group Delay (MGD) and Mel-Frequency Cepstral Coefficients (MFCC) are explored in the proposed framework. Results are presented on NIST 2010 male database for the speaker verification task.", "title": "Feature Switching in the i-vector framework for speaker verification"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/zhong14_interspeech.html", "abstract": "We have previously developed a Fishervoice framework that maps the JFA-mean supervectors into a compressed discriminant subspace using nonparametric Fishers discriminant analysis. It was shown that performing cosine distance scoring (CDS) on these Fishervoice projected vectors (denoted as f-vectors) can outperform the classical joint factor analysis. Unlike the i-vector approach in which the channel variability is suppressed in the classification stage, in the Fishervoice framework, channel variability is suppressed when the f-vectors are constructed. In this paper, we investigate whether channel variability can be further suppressed by performing Gaussian probabilistic discriminant analysis (PLDA) in the classification stage. We also use random subspace sampling to enrich the speaker discriminative information in the f-vectors. Experiments on NIST SRE10 show that PLDA can boost the performance of Fishervoice in speaker verification significantly by a relative decrease of 14.4% in minDCF (from 0.526 to 0.450).", "title": "PLDA modeling in the fishervoice subspace for speaker verification"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/martin14_interspeech.html", "abstract": "The 2012 NIST Speaker Recognition Evaluation, held in the autumn of 2012, was designed to examine a variety of factors affecting the performance of automatic systems for speaker recognition. Here we examine, for leading systems included in this evaluation, the observed effects on performance of five such factors: the inclusion in test segment speech of environmental noise or of added synthetic noise of one of three types and one of two intensity levels, the duration of test segment speech, the number and the channel type of target speaker training sessions, the type of the microphone channel used in test segment speech, and the sex of the target speaker. This evaluation is notable for being the first in the series to include examination of the effects on performance of synthetic added noise. The greater impact of crowd noise compared to HVAC noise, and of single speaker noise compared to crowd noise is observed. Future evaluation plans are also discussed.", "title": "Performance factor analysis for the 2012 NIST speaker recognition evaluation"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/fujimura14_interspeech.html", "abstract": "This paper proposes a novel technique for simultaneously executing gender classification and voice activity detection (VAD) using Deep Neural Networks (DNNs). Speaker information such as gender is important in some speech recognition applications such as recommendation systems and trend analysis. Usually, gender classification is applied after speech segments are detected by VAD. In previous studies, gender classification and VAD are separately considered. For the past few years, DNN has been applied to both of them as a powerful classifier. However huge calculation cost is needed if two separate DNNs are used for them. In our method, a single DNN classifies each frame into male, female, and silence classes. The frame-based classification results are used for both gender classification and VAD. For VAD, the sum of male and female posterior probabilities from the DNN is used as voice posterior probability. Gender classification is also carried out based on the results of the DNN classifier. Experimental results show that the proposed method achieves high accuracy for both gender classification and VAD.", "title": "Simultaneous gender classification and voice activity detection using deep neural networks"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/abdelaziz14_interspeech.html", "abstract": "Jointly using audio and video features can increase the robustness of automatic speech recognition systems in noisy environments. A systematic and reliable performance gain, however, is only achieved if the contributions of the audio and video stream to the decoding decision are dynamically optimized, for example via so-called stream weights. In this paper, we address the problem of dynamic stream weight estimation for coupled-HMM-based audio-visual speech recognition. We investigate the multilayer perceptron (MLP) for mapping reliability measure features to stream weights. As an input for the multilayer perceptron, we use a feature vector containing different model-based and signal-based reliability measures. Training of the multilayer perceptron has been achieved using dynamic oracle stream weights as target outputs, which are found using a recently proposed expectation maximization algorithm. This new approach of MLP-based stream-weight estimation has been evaluated using the Grid audio-visual corpus and has outperformed the best baseline performance, yielding a 23.72% average relative error rate reduction.", "title": "Dynamic stream weight estimation in coupled-HMM-based audio-visual speech recognition using multilayer perceptrons"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/noda14_interspeech.html", "abstract": "In recent automatic speech recognition studies, deep learning architecture applications for acoustic modeling have eclipsed conventional sound features such as Mel-frequency cepstral coefficients. However, for visual speech recognition (VSR) studies, handcrafted visual feature extraction mechanisms are still widely utilized. In this paper, we propose to apply a convolutional neural network (CNN) as a visual feature extraction mechanism for VSR. By training a CNN with images of a speaker's mouth area in combination with phoneme labels, the CNN acquires multiple convolutional filters, used to extract visual features essential for recognizing phonemes. Further, by modeling the temporal dependencies of the generated phoneme label sequences, a hidden Markov model in our proposed system recognizes multiple isolated words. Our proposed system is evaluated on an audio-visual speech dataset comprising 300 Japanese words with six different speakers. The evaluation results of our isolated word recognition experiment demonstrate that the visual features acquired by the CNN significantly outperform those acquired by conventional dimensionality compression approaches, including principal component analysis.", "title": "Lipreading using convolutional neural network"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/tao14_interspeech.html", "abstract": "Whisper is a speech production mode normally used to protect confidential information. Given the differences in the acoustic domain, the performance of automatic speech recognition (ASR) systems decreases with whisper speech. An appealing approach to improve the performance is the use of lipreading. This study explores the use of visual features characterizing the lips' geometry and appearance to recognize digits under normal and whisper speech conditions using hidden Markov models (HMMs). We evaluate the proposed features on the digit part of the audiovisual whisper (AVW) corpus. While the proposed system achieves high accuracy in speaker dependent conditions (80.8%), the performance decreases when we evaluate speaker independent models (52.9%). We propose supervised adaptation schemes to reduce the mismatch between speakers. Across all conditions, the performance of the classifiers remain competitive even in the presence of whisper speech, highlighting the benefits of using visual features.", "title": "Lipreading approach for isolated digits recognition under whisper and neutral speech"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/masaka14_interspeech.html", "abstract": "This paper presents a multimodal voice conversion (VC) method for noisy environments. In our previous exemplar-based VC method, source exemplars and target exemplars are extracted from parallel training data, in which the same texts are uttered by the source and target speakers. The input source signal is then decomposed into source exemplars, noise exemplars obtained from the input signal, and their weights. Then, the converted speech is constructed from the target exemplars and the weights related to the source exemplars. In this paper, we propose a multimodal VC method that improves the noise robustness of our previous exemplar-based VC method. As visual features, we use not only conventional DCT but also the features extracted from Active Appearance Model (AAM) applied to the lip area of a face image. Furthermore, we introduce the combination weight between audio and visual features and formulate a new cost function in order to estimate the audio-visual exemplars. By using the joint audio-visual features as source features, the VC performance is improved compared to a previous audio-input exemplar-based VC method. The effectiveness of this method was confirmed by comparing its effectiveness with that of a conventional Gaussian Mixture Model (GMM)-based method.", "title": "Multimodal exemplar-based voice conversion using lip features in noisy environments"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/deng14b_interspeech.html", "abstract": "Our recent efforts towards developing a practical surface electromyography (sEMG) based silent speech recognition interface have resulted in significant advances in the hardware, software and algorithmic components of the system. In this paper, we report our algorithmic progress, specifically: sEMG feature extraction parameter optimization, advances in sEMG acoustic modeling, and sEMG sensor set reduction. The key findings are: 1) the gold-standard parameters for acoustic speech feature extraction are far from optimum for sEMG parameterization, 2) advances in state-of-the-art speech modelling can be leveraged to significantly enhance the continuous sEMG silent speech recognition accuracy, and 3) the number of sEMG sensors can be reduced by half with little impact on the final recognition accuracy, and the optimum sensor subset can be selected efficiently based on basic mono-phone HMM modeling.", "title": "Towards a practical silent speech recognition system"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/freitas14_interspeech.html", "abstract": "In research on Silent Speech Interfaces (SSI), different sources of information (modalities) have been combined, aiming at obtaining better performance than the individual modalities. However, when combining these modalities, the dimensionality of the feature space rapidly increases, yielding the well-known \u201ccurse of dimensionality\u201d. As a consequence, in order to extract useful information from this data, one has to resort to feature selection (FS) techniques to lower the dimensionality of the learning space. In this paper, we assess the impact of FS techniques for silent speech data, in a dataset with 4 non-invasive and promising modalities, namely: video, depth, ultrasonic Doppler sensing, and surface electromyography. We consider two supervised (mutual information and Fisher's ratio) and two unsupervised (mean-median and arithmetic mean geometric mean) FS filters. The evaluation was made by assessing the classification accuracy (word recognition error) of three well-known classifiers (k-nearest neighbors, support vector machines, and dynamic time warping). The key results of this study show that both unsupervised and supervised FS techniques improve on the classification accuracy on both individual and combined modalities. For instance, on the video component, we attain relative performance gains of 36.2% in error rates. FS is also useful as pre-processing for feature fusion.", "title": "Enhancing multimodal silent speech interfaces with feature selection"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/katz14_interspeech.html", "abstract": "We describe an interactive 3D system to provide talkers with real-time information concerning their tongue and jaw movements during speech. Speech movement is tracked by a magnetometer system (Wave; NDI, Waterloo, Ontario, Canada). A customized interface allows users to view their current tongue position (represented as an avatar consisting of flesh-point markers and a modeled surface) placed in a synchronously moving, transparent head. Subjects receive augmented visual feedback when tongue sensors achieve the correct place of articulation. Preliminary data obtained for a group of adult talkers suggest this system can be used to reliably provide real-time feedback for American English consonant place of articulation targets. Future studies, including tests with communication disordered subjects, are described.", "title": "Opti-speech: a real-time, 3d visual feedback system for speech training"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/wang14g_interspeech.html", "abstract": "Silent speech interfaces (SSIs), which recognize speech from articulatory information (i.e., without using audio information), have the potential to enable persons with laryngectomy or a neurological disease to produce synthesized speech with a natural sounding voice using their tongue and lips. Current approaches to SSIs have largely relied on speaker-dependent recognition models to minimize the negative effects of talker variation on recognition accuracy. Speaker-independent approaches are needed to reduce the large amount of training data required from each user; only limited articulatory samples are often available for persons with moderate to severe speech impairments, due to the logistic difficulty of data collection. This paper reported an across-speaker articulatory normalization approach based on Procrustes matching, a bidimensional regression technique for removing translational, scaling, and rotational effects of spatial data. A dataset of short functional sentences was collected from seven English talkers. A support vector machine was then trained to classify sentences based on normalized tongue and lip movements. Speaker-independent classification accuracy (tested using leave-one-subject-out cross validation) improved significantly, from 68.63% to 95.90%, following normalization. These results support the feasibility of a speaker-independent SSI using Procrustes matching as the basis for articulatory normalization across speakers.", "title": "Across-speaker articulatory normalization for speaker-independent silent speech recognition"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/zahner14_interspeech.html", "abstract": "This paper reports on our recent research on surface electromyographic (EMG) speech synthesis: a direct conversion of the EMG signals of the articulatory muscle movements to the acoustic speech signal. In this work we introduce a unit selection approach which compares segments of the input EMG signal to a database of simultaneously recorded EMG/audio unit pairs and selects the best matching audio unit based on target and concatenation cost, which will be concatenated to synthesize an acoustic speech output. We show that this approach is feasible to generate a proper speech output from the input EMG signal. We evaluate different properties of the units and investigate what amount of data is necessary for an initial transformation. Prior work on EMG-to-speech conversion used a frame-based approach from the voice conversion domain, which struggles with the generation of a natural F0 contour. This problem may also be tackled by our unit selection approach.", "title": "Conversion from facial myoelectric signals to speech: a unit selection approach"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/wand14_interspeech.html", "abstract": "This paper deals with a Silent Speech Interface based on Surface Electromyography (EMG), where electrodes capture the electric activity generated by the articulatory muscles from a user's face in order to decode the underlying speech, allowing speech to be recognized even when no sound is heard or created. So far, most EMG-based speech recognizers described in literature do not allow electrode reattachment between system training and usage, which we consider unsuitable for practical applications. In this study we report on our research on unsupervised session adaptation: A system is pre-trained with data from multiple recording sessions and then adapted towards the current recording session using data accruable during normal use, without requiring a time-consuming specific enrollment phase. We show that considerable accuracy improvements can be achieved with this method, paving the way towards real-life applications of the technology.", "title": "Towards real-life application of EMG-based speech recognition by using unsupervised adaptation"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/liang14_interspeech.html", "abstract": "Conventional error correction interfaces for speech recognition require a user to first mark an error region and choose the correct word from a candidate list. Taking the user's effort and the limited user interface available in a smartphone use into account, this operation should be simpler. In this paper, we propose an interface where users mark the error region once, and then the word will be replaced by another candidate. Assuming that the words preceding/succeeding the error region are validated by the user, we search the Web n-grams for long word sequences matched to such a context. The acoustic features of the error region are also utilized to rerank the candidate words. The experimental result proved the effectiveness of our method. 30.2% of the error words were corrected by a single operation.", "title": "Simple gesture-based error correction interface for smartphone speech recognition"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/kumar14_interspeech.html", "abstract": "Speech recognition confidence classifier (CC) score quantitatively represents the correctness of decoded utterances in a [0,1] range. We associate an operating threshold with the classifier and accept recognitions with scores greater than the threshold. Speech developers may set their own threshold but often an acoustic model (AM) or CC update alters the correct-accept (CA) vs. false-accept (FA) profile, necessitating a threshold reselection. This is specifically a problem when, (a) threshold is hardcoded with a shipped hardware or software, (b) developers may not have expertise for threshold tuning, (c) tuning isn't cost-effective and may need to be done often. To our knowledge, our work is the first to present this practical and interesting problem of avoiding threshold reselection and proposes novel confidence-mapping-based techniques to improve or retain both CA and FA at previously set thresholds. We propose and evaluate, (a) histogram-based mapping, (b) polynomial-fitting, (c) tanh-fitting, based methods to map confidences associated with false-recognitions and discuss their issues and benefits. In our tests, all of the above mapping methods fix the mean regression in CA of 21% to a gain to 1\u20132%, with tanh-mapping providing the best CA and FA tradeoff in our tests.", "title": "Normalization of ASR confidence classifier scores via confidence mapping"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/alumae14_interspeech.html", "abstract": "In this paper, we describe a novel phone duration model that is used to improve the accuracy of a large vocabulary speech recognition system based on state-of-the-art speaker-adapted DNN acoustic models. The duration model calculates the probability density function of phone duration from phone's contextual features using a neural network which is then applied for word lattice rescoring. Experimental results are given for Estonian, English and Finnish transcription tasks. An absolute word error rate reduction of 0.8\u20131.4% is observed across all evaluation sets.", "title": "Neural network phone duration model for speech recognition"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/sak14b_interspeech.html", "abstract": "We recently showed that Long Short-Term Memory (LSTM) recurrent neural networks (RNNs) outperform state-of-the-art deep neural networks (DNNs) for large scale acoustic modeling where the models were trained with the cross-entropy (CE) criterion. It has also been shown that sequence discriminative training of DNNs initially trained with the CE criterion gives significant improvements. In this paper, we investigate sequence discriminative training of LSTM RNNs in a large scale acoustic modeling task. We train the models in a distributed manner using asynchronous stochastic gradient descent optimization technique. We compare two sequence discriminative criteria \u2014 maximum mutual information and state-level minimum Bayes risk, and we investigate a number of variations of the basic training strategy to better understand issues raised by both the sequential model, and the objective function. We obtain significant gains over the CE trained LSTM RNN model using sequence discriminative training techniques.", "title": "Sequence discriminative distributed training of long short-term memory recurrent neural networks"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/huang14c_interspeech.html", "abstract": "We propose two approaches for improving the objective function for the deep neural network (DNN) frame-level training in large vocabulary continuous speech recognition (LVCSR). The DNNs used in LVCSR are often constructed with an output layer with softmax activation and the cross-entropy objective function is always employed in the frame-leveling training of DNNs. The pairing of softmax activation and cross-entropy objective function contributes much in the success of DNN. The first approach developed in this paper improves the cross-entropy objective function by boosting the importance of the frames for which the DNN model has low target predictions (low target posterior probabilities) and the second one considers jointly minimizing the cross-entropy and maximizing the log posterior ratio between the target senone (tied-triphone states) and the most competing one. Experiments on Switchboard task demonstrate that the two proposed methods can provide 3.1% and 1.5% relative word error rate (WER) reduction , respectively, against the already very strong conventional cross-entropy trained DNN system.", "title": "Beyond cross-entropy: towards better frame-level objective functions for deep neural network training in automatic speech recognition"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/tang14_interspeech.html", "abstract": "Segmental models such as segmental conditional random fields have had some recent success in lattice rescoring for speech recognition. They provide a flexible framework for incorporating a wide range of features across different levels of units, such as phones and words. However, such models have mainly been trained by maximizing conditional likelihood, which may not be the best proxy for the task loss of speech recognition. In addition, there has been little work on designing cost functions as surrogates for the word error rate. In this paper, we investigate various losses and introduce a new cost function for training segmental models. We compare lattice rescoring results for multiple tasks and also study the impact of several choices required when optimizing these losses.", "title": "A comparison of training approaches for discriminative segmental models"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/mcdermott14_interspeech.html", "abstract": "Previous work presented a proof of concept for sequence training of deep neural networks (DNNs) using asynchronous stochastic optimization, mainly focusing on a small-scale task. The approach offers the potential to leverage both the efficiency of stochastic gradient descent and the scalability of parallel computation. This study presents results for four different voice search tasks to confirm the effectiveness and efficiency of the proposed framework across different conditions: amount of data (from 60 hours to 20,000 hours), type of speech (read speech vs. spontaneous speech), quality of data (supervised vs. unsupervised data), and language. Significant gains over baselines (DNNs trained at the frame level) are found to hold across these conditions. The experimental results are analyzed, and additional practical details for the approach are provided. Furthermore, different sequence training criteria are compared.", "title": "Asynchronous stochastic optimization for sequence training of deep neural networks: towards big data"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/rao14_interspeech.html", "abstract": "Paralinguistic cues in children's speech convey the child's affective state and can serve as important markers for the early detection of autism spectrum disorder (ASD). In this paper, we detect paralinguistic events, such as laughter and fussing/crying, along with toddlers' speech from the Multi-modal Dyadic Behavior Dataset (MMDB). We use both spectral and prosodic acoustic features selected using a combination of filter and wrapper-based methods. The classification accuracy using a support vector machine with a linear kernel for detecting laughter in children's speech was 77.87% and that for fussing/crying was 79.37%. A tertiary classification scheme for detecting laughter, fussing/crying, and speech yielded an accuracy of 69.73%. To test for the generalization of the approach for detecting fussing/ crying, we used recordings from the Strange Situation protocol, which is used to observe attachment behavior between an infant and a parent. Using a cross-corpus testing set for detecting fussing/crying, we obtained a detection accuracy of 71.6%. These results indicate that the selected acoustic features are capable of discriminating children's laughter, fussing/crying, and speech and the algorithms generalize well to a dataset consisting of paralinguistic cues of a different age group, infants (12\u201318 months of age), gathered in a different context.", "title": "Detection of children's paralinguistic events in interaction with caregivers"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/pettorino14_interspeech.html", "abstract": "Age-related variations in the speech production mechanism affecting acoustic cues such as pitch, speech rate, formant frequencies have been extensively investigated. Changes in speech rhythm and in utterance composition in terms of vowel and consonant portions are, instead, scarcely examined. Given the relevance of the vowel percentage (%V) in the utterance and of the interval between two vowel onset points (VtoV) for the perception of language rhythm, this study aims to investigate the relationship between rhythmic variations and speaker age. It also attempts to determine whether %V can be affected by different speech rates. Four young adult and four old speakers of Italian read four sentences of different length, at four different speech rates. The whole corpus was segmented in vocalic and consonantal portions and in VtoV intervals. The analysis results have shown that aged voice accompanies with significant increase in %V, despite speech rate variations. With advancing age, Italian speech tends to shift from the isosyllabic towards the isomoraic rhythmic pattern.", "title": "Age and rhythmic variations: a study on Italian"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/cummins14_interspeech.html", "abstract": "Alterations in speech motor control in depressed individuals have been found to manifest as a reduction in spectral variability. In this paper we present a novel method for measuring acoustic volume \u2014 a model-based measure that is reflective of this decrease in spectral variability \u2014 and assess the ability of features resulting from this measure for indexing a speaker's level of depression. A Monte Carlo approximation that enables the computation of this measure is also outlined in this paper. Results found using the AVEC 2013 Challenge Dataset indicate there is a statistically significant reduction in acoustic variation with increasing levels of speaker depression, and using features designed to capture this change it is possible to outperform a range of conventional spectral measures when predicting a speaker's level of depression.", "title": "Probabilistic acoustic volume analysis for speech affected by depression"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/bozkurt14_interspeech.html", "abstract": "In this paper, we propose a Modulation Spectrum-based manageable feature set for detection of depressed speech. Modulation Spectrum (MS) is obtained from the conventional speech spectrogram by spectral analysis along the temporal trajectories of the acoustic frequency bins. While MS representation of speech provides rich and high-dimensional joint frequency information, extraction of discriminative features from it remains as an open question. We propose a lower dimensional representation, which first employs a Mel-frequency filterbank in the acoustic frequency domain and Discrete Cosine Transform in the modulation frequency domain, and then applies feature selection in both domains. We compare and fuse the proposed feature set with other complementary prosodic and spectral features at the feature and decision levels. In our experiments, we use Support Vector Machines for discriminating the depressed speech in a speaker-independent fashion. Feature-level fusion of the proposed MS-based features with other prosodic and spectral features after dimension reduction provides up to ~9% improvement over the baseline results and also correlates the most with clinical ratings of patients' depression level.", "title": "Exploring modulation spectrum features for speech-based depression level classification"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/honig14_interspeech.html", "abstract": "Depression is an affective disorder characterised by psychomotor retardation; in speech, this shows up in reduction of pitch (variation, range), loudness, and tempo, and in voice qualities different from those of typical modal speech. A similar reduction can be observed in sleepy speech (relaxation). In this paper, we employ a small group of acoustic features modelling prosody and spectrum that have been proven successful in the modelling of sleepy speech, enriched with voice quality features, for the modelling of depressed speech within a regression approach. This knowledge-based approach is complemented by and compared with brute-forcing and automatic feature selection. We further discuss gender differences and the contributions of (groups of) features both for the modelling of depression and across depression and sleepiness.", "title": "Automatic modelling of depressed speech: relevant features and relevance of gender"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/gangamohan14_interspeech.html", "abstract": "Studies on the emotion recognition task indicate that there is confusion in discrimination among higher activation states like `anger' and `happy'. In this study, features related to excitation source of speech are examined for discriminating `anger' and `happy' emotions. The objective is to explore the features which are independent of lexical content, language, channel and speaker. The features like strength of excitation from zero frequency filtering method and spectral band magnitude energies from short-time spectral analysis are used. Experimental results show that these features can discriminate `anger' and `happy' emotion states to a good extent.", "title": "Excitation source features for discrimination of anger and happy emotions"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/wu14_interspeech.html", "abstract": "We present algorithms, implemented as an extension to the OpenFst library, that yield a class of transducers that encode linear models for structured inference tasks like segmentation and tagging. This allows the use of general finite-state operations with such models. For instance, finite-state composition can be used to apply the model to lattice input (or other more general automata) and then the result automaton can be passed to subsequent processing such as general shortest path algorithms. We demonstrate the use of the library extension on grapheme-to-phoneme conversion, encoding multiple varieties of linear models for that task, and achieve solid PER/WER gains over previous best reported results on g2p conversion of a publicly available dataset (CMU).", "title": "Encoding linear models as weighted finite-state transducers"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/kubo14b_interspeech.html", "abstract": "In recent years, structured online discriminative learning methods using second order statistics have been shown to outperform conventional generative and discriminative models in the grapheme-to-phoneme (g2p) conversion task. However, these methods update the parameters by sequentially using N-best hypotheses predicted with the current parameters. Thus, the parameters appearing in early hypotheses are overfitted compared with those in later hypotheses. In this paper, we propose a novel method called structured soft margin confidence weighted learning, which extends multi-class confidence weighted learning to structured learning. The proposed method extends multi-class CW in two ways, allowing for improved robustness to overfitting: (1) regularization inspired by soft margin support vector machines, allowing for margin error, and (2) update using N-best hypotheses simultaneously and interdependently. In an evaluation experiment on the g2p conversion task, the proposed method improved over all other approaches in terms of phoneme error rate with a significant difference.", "title": "Structured soft margin confidence weighted learning for grapheme-to-phoneme conversion"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/zhang14d_interspeech.html", "abstract": "To automatically build from scratch the language processing component for a speech synthesis system in a new language a purified text corpora is needed where any words and phrases from other languages are clearly identified or excluded. When using found data and where there is no inherent linguistic knowledge of the language/languages contained in the data, identifying the pure data is a difficult problem.   We propose an unsupervised language identification approach based on Latent Dirichlet Allocation where we take the raw n-gram count as features without any smoothing, pruning or interpolation. The Latent Dirichlet Allocation topic model is reformulated for the language identification task and Collapsed Gibbs Sampling is used to train an unsupervised language identification model. We show that such a model is highly capable of identifying the primary language in a corpus and filtering out other languages present.To automatically build from scratch the language processing component for a speech synthesis system in a new language a purified text corpora is needed where any words and phrases from other languages are clearly identified or excluded. When using found data and where there is no inherent linguistic knowledge of the language/languages contained in the data, identifying the pure data is a difficult problem.   We propose an unsupervised language identification approach based on Latent Dirichlet Allocation where we take the raw n-gram count as features without any smoothing, pruning or interpolation. The Latent Dirichlet Allocation topic model is reformulated for the language identification task and Collapsed Gibbs Sampling is used to train an unsupervised language identification model. We show that such a model is highly capable of identifying the primary language in a corpus and filtering out other languages present.", "title": "Unsupervised language filtering using the latent dirichlet allocation"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/kolluru14_interspeech.html", "abstract": "Standard grapheme-to-phoneme (G2P) systems are trained using a homogeneous lexicon, for example one associated with a particular accent. In practice, a synthesis system may be required to handle multiple accents. Furthermore, a speaker rarely has a pure accent; accents vary continuously within and between regions of a country. Generating phonetic sequences for each accent is possible, but combining them to yield a single synthesis pronunciation is highly challenging. To address this problem, this paper considers a space of accents. The bases for these spaces are defined by statistical G2P models in the form of graphone models. A linear combination of these models define the accent space. By selecting a point in this continuous space, it is possible to specify the accent for an individual speaker. The performance of this approach is evaluated using an accent space defined by American, Scottish and British English. By moving around the accent space, it is shown that it is possible to synthesize speech from all these accents as well as a range of intermediate points.", "title": "Generating multiple-accent pronunciations for TTS using joint sequence model interpolation"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/mendonca14_interspeech.html", "abstract": "This paper describes the method employed to build a machine-readable pronunciation dictionary for Brazilian Portuguese. The dictionary makes use of a hybrid approach for converting graphemes into phonemes, based on both manual transcription rules and machine learning algorithms. It makes use of a word list compiled from the Portuguese Wikipedia dump. Wikipedia articles were transformed into plain text, tokenized and word types were extracted. A language identification tool was developed to detect loanwords among data. Words' syllable boundaries and stress were identified. The transcription task was carried out in a two-step process: i) words are submitted to a set of transcription rules, in which predictable graphemes (mostly consonants) are transcribed; ii) a machine learning classifier is used to predict the transcription of the remaining graphemes (mostly vowels). The method was evaluated through 5-fold cross-validation; results show a F1-score of 0.98. The dictionary and all the resources used to build it were made publicly available.", "title": "Using a hybrid approach to build a pronunciation dictionary for Brazilian Portuguese"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/aylett14_interspeech.html", "abstract": "Parametric speech synthesis techniques depend on full context acoustic models generated by language front-ends, which analyse linguistic and phonetic structure. HTS, the leading parametric synthesis system, can use a number of different front-ends to generate full context models for synthesis and training. In this paper we explore the use of a new text processing front-end that has been added to the speech recognition toolkit Kaldi as part of an ongoing project to produce a new parametric speech synthesis system, Idlak. The use of XML specification files, a modular design, and modern coding and testing approaches, make the Idlak front-end ideal for adding, altering and experimenting with the contexts used in full context acoustic models. The Idlak front-end was evaluated against the standard Festival front-end in the HTS system. Results from the Idlak front-end compare well with the more mature Festival front-end (Idlak - 2.83 MOS vs Festival - 2.85 MOS), although a slight reduction in naturalness perceived by non-native English speakers can be attributed to Festival's insertion of non-punctuated pauses.", "title": "A flexible front-end for HTS"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/tsukada14_interspeech.html", "abstract": "We examined the perception of Japanese consonant length contrasts (singleton vs geminate) in four groups of listeners: two groups of non-native learners of Japanese, three each at highly-advanced (NNJ1) and upper-intermediate (NNJ2) levels, native speakers of Italian (IT) (n=10) and Australian English (OZ) (n=8) with no knowledge of Japanese. Because Italian, like Japanese, uses consonant length contrastively but Australian English does not, we were interested in whether first language (L1) knowledge of consonant length might have an effect on the ability to perceive short and long Japanese consonants. The NNJ1 learners were more accurate in identifying Japanese singleton and geminate consonants than were the IT listeners who, in turn, were more accurate than the OZ listeners. The NNJ2 learners' results showed similarities and differences to the IT and OZ listeners. Our preliminary results suggest that L1 experience with consonant length may not necessarily guarantee accurate perception of consonant length in an unknown language. However, it may offer some advantage over lack of exposure to consonant length. In addition, the results for learner proficiency demonstrate that non-native learners need much time/practice before they clearly differentiate themselves from na\u00efve listeners, reconfirming previous research that consonant length contrasts are difficult to acquire.", "title": "Cross-language perception of Japanese singleton and geminate consonants: preliminary data from non-native learners of Japanese and native speakers of Italian and australian English"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/alispahic14_interspeech.html", "abstract": "Previous studies have shown that the number of vowels present in one's L1 inventory may affect the ability to learn and discriminate non-native vowel contrasts. Specifically, learners whose L1 contains fewer vowels compared to the target language may find many non-native vowel contrasts novel and have discrimination performance lower than learners whose L1 contains more vowels than the target language for whom most of the non-native vowel contrasts will be familiar. The present study tested monolingual Australian English (AusE) listeners' discrimination of non-native vowels in Dutch, which has fewer vowels compared to AusE. We further compared AusE listeners' performance to that of native monolingual Spanish listeners whose L1 contains fewer vowels than Dutch. AusE listeners were able to discriminate all Dutch vowel contrasts above chance. While there was no main effect of language background, an interaction language background x contrast revealed that AusE listeners more accurately discriminated the /?-?/ contrast compared to Spanish listeners, suggesting some advantage for AusE listeners. The findings are discussed in relation to models of non-native and L2 speech perception together with a comparison of vowel acoustic properties across AusE, Spanish and Dutch.", "title": "Difficulty in discriminating non-native vowels: are Dutch vowels easier for australian English than Spanish listeners?"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/yang14b_interspeech.html", "abstract": "This study investigates L1-L2 interactions in relatively young Mandarin (L1) - English (L2) bilingual children through comparing their static and dynamic vowel acoustic features with those of age-matched corresponding monolingual children. Two groups of sequential bilingual children aged 5\u20136 years (one with low proficiency in English and the other with high proficiency in English) were recorded producing a set of words containing the shared vowels /a/, /i/, /u/ in both languages. Age-matched monolingual children only produced the words in their native language. F1 and F2 values were measured at 5 equidistant time locations. It is found that both groups of bilingual children showed distinctive vowel dispersion patterns and dynamic spectral change patterns from those of monolingual children. Low proficiency bilingual children showed an assimilatory process of L1 on L2 and high proficiency bilingual children showed an assimilatory process of L2 on L1.", "title": "Acoustic properties of shared vowels in bilingual Mandarin-English children"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/lecumberri14_interspeech.html", "abstract": "For most of us, speaking in a non-native language involves deviating to some extent from native pronunciation norms. However, the detailed basis for foreign accent (FA) remains elusive, in part due to methodological challenges in isolating segmental from suprasegmental factors. The current study examines the role of segmental features in conveying FA through the use of a generative approach in which accent is localised to single consonantal segments. Three techniques are evaluated: the first requires a highly-proficiency bilingual to produce words with isolated accented segments; the second uses cross-splicing of context-dependent consonants from the non-native language into native words; the third employs hidden Markov model synthesis to blend voice models for both languages. Using English and Spanish as the native/non-native languages respectively, listener cohorts from both languages identified words and rated their degree of FA. All techniques were capable of generating accented words, but to differing degrees. Naturally-produced speech led to the strongest FA ratings and synthetic speech the weakest, which we interpret as the outcome of over-smoothing. Nevertheless, the flexibility offered by synthesising localised accent encourages further development of the method.", "title": "Generating segmental foreign accent"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/andreeva14_interspeech.html", "abstract": "This study investigates cross-language differences in pitch range and variation in four languages from two language groups: English and German (Germanic) and Bulgarian and Polish (Slavic). The analysis is based on large multi-speaker corpora (48 speakers for Polish, 60 for each of the other three languages). Linear mixed models were computed that include various distributional measures of pitch level, span and variation, revealing characteristic differences across languages and between language groups. A classification experiment based on the relevant parameter measures (span, kurtosis and skewness values for pitch distributions for each speaker) succeeded in separating the language groups.", "title": "Differences of pitch profiles in Germanic and slavic languages"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/avanzi14_interspeech.html", "abstract": "This study provides evidence that the role of the Obligatory Contour Principle (OCP) in substrate languages is reflected in prosodic systems of contact varieties of French. We have compared two contact varieties: Central African French, a variety of French spoken by L1 speakers of Sango a lexical tone language where the constraint is not respected, and Burundi French, which is spoken by L1 speakers of Kirundi, a lexical tone language where the OCP plays an important role in the distribution of tones. Our data indicate that clashes are permitted in the former, but avoided in the latter.", "title": "The obligatory contour principle in african and European varieties of French"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/scheffer14_interspeech.html", "abstract": "This work attempts to tackle the problem of content mismatch for short duration speaker verification. Experiments are run on both text-dependent and text-independent protocols, where a larger amount of enrollment data is available in the latter. We recently proposed a framework based on a deep neural network that explicitly utilizes phonetic information, and showed increased performance on long duration utterances. We show how this new framework can also yield significant improvements for short duration. We then propose an innovative approach to perform content matching, i.e. transforming a text-independent trial into a text-dependent one by mining content from a speaker's enrollment data to match the test utterance. We show how content matching can be effectively done at the statistics level to enable the use of standard verification backends. Experiments \u2014 run on the RSR2015 and NIST SRE 2010 data sets \u2014 show relative improvements of 50% for cases where the content has been said during enrollment. While no significant improvements were observed for the general text-independent case, we believe that this work might pave the way for new research for speaker verification with very short utterances.", "title": "Content matching for short duration speaker recognition"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/larcher14_interspeech.html", "abstract": "Text-dependent speaker verification over degraded radio channel is a challenging task. To better understand the research problem, the Institute for Infocomm Research (I2R) of Singapore has collected a corpus of voice recordings transmitted over marine VHF. Built as an extension of the RSR2015 database, the VHF- RSR2015 consists of recordings from 300 speakers of Part I of the RSR2015 database transmitted over VHF channel. Extending the RSR2015 database, we would like to facilitate the study of the VHF channel effect, therefore, keeping the acoustic environment the same as that of RSR2015. Performance benchmark of a text-dependent speaker verification engine is given as reference on the original RSR2015 database recorded at a sampling frequency of 16kHz, on a sub-sampled version of the same dataset at 8kHz and on the data after transmission through the VHF channel.", "title": "Extended RSR2015 for text-dependent speaker verification over VHF channel"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/fu14_interspeech.html", "abstract": "Although deep learning has been successfully used in acoustic modeling of speech recognition, it has not been thoroughly investigated and widely accepted for speaker verification. This paper describes an investigation of using various types of deep features in a Tandem fashion for text-dependent speaker verification. Three types of networks are used to extract deep features: restricted Boltzmann machine (RBM), phone discriminant and speaker discriminant deep neural network (DNN). Hidden layer outputs from these networks are concatenated with the original acoustic features and used in a GMM-UBM classifier. The systems with Tandem deep feature were evaluated on RSR2015, a short-term text dependent speaker verification task. Experiments showed that the best Tandem deep feature obtained more than 50% relative EER reduction over the traditional feature in a GMM-UBM framework.", "title": "Tandem deep features for text-dependent speaker verification"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/kenny14_interspeech.html", "abstract": "We propose a simple and effective strategy to cope with dataset shifts in text-dependent speaker recognition based on Joint Factor Analysis (JFA). We have previously shown how to compensate for lexical variation in text-dependent JFA by adapting the Universal Background Model (UBM) to individual passphrases. A similar type of adaptation can be used to port a JFA model trained on out-of-domain data to a given text-dependent task domain. On the RSR2015 test set we found that this type of adaptation gave essentially the same results as in-domain JFA training. To explore this idea more fully, we experimented with several types of JFA model on the CSLU speaker recognition dataset. Taking a suitably configured JFA model trained on NIST data and adapting it in the proposed way results in a 22% reduction in error rates compared with the GMM/UBM benchmark. Error rates are still much higher than those that can be achieved on the RSR2015 test set with the same strategy but cheating experiments suggest that if large amounts of in-domain training data are available, then JFA modelling is capable in principle of achieving very low error rates even on hard tasks such as CSLU.", "title": "In-domain versus out-of-domain training for text-dependent JFA"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/aronowitz14_interspeech.html", "abstract": "Recently we have investigated the use of state-of-the-art text-dependent speaker verification algorithms for user authentication and obtained satisfactory results mainly by using a fair amount of text-dependent development data from the target domain. In this work we investigate the ability to build high accuracy text-dependent systems using no data at all from the target domain. Instead of using target domain data, we use resources such as TIMIT, Switchboard, and NIST data. We introduce several techniques addressing both lexical mismatch and channel mismatch. These techniques include synthesizing a universal background model according to lexical content, automatic filtering of irrelevant phonetic content, exploiting information in residual supervectors (usually discarded in the i-vector framework), and inter dataset variability modeling. These techniques reduce verification error significantly, and also improve accuracy when target domain data is available.", "title": "Domain adaptation for text dependent speaker verification"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/miguel14_interspeech.html", "abstract": "Factor analysis is a method for embedding high dimensional data into a lower dimensional factor space. When data are multimodal we use mixtures of factor analyzers (MFA), which assume statistically independent samples. In speaker recognition, samples are not independent because they depend on the speaker in the utterance. In joint factor analysis and i-vectors, the MFA latent factors are tied at different levels. For example, they can be tied for a segment to extract utterance level information. Tied MFA approaches usually present the drawback that computing the exact posterior of the hidden variables (component responsibilities and latent factors) is unfeasible. For JFA, the preferred approximation consists in computing the responsibilities given a speaker independent GMM and they are fixed during the rest of the process. That implies that the estimated responsibilities for a given sample are independent of the rest of the samples of the utterance not taking into account the shared speaker and channel. We present a novel approximation to jointly estimate responsibilities and latent factors based on sampling the latent factor space. This model differs from previous ones in the hidden variables and parameter estimation; and likelihood evaluation. This approach was tested on the RSR2015 database for text-dependent speaker recognition.", "title": "Factor analysis with sampling methods for text dependent speaker recognition"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/berg14_interspeech.html", "abstract": "Pitch detection has important applications in areas of automatic speech recognition such as prosody detection, tonal language transcription, and general feature augmentation. In this paper we describe Pitcher, a new pitch tracking algorithm that correlates spectral information with a dictionary of waveforms each of which is designed to match signals with a given pitch value. We apply dynamic programming techniques on the resulting coefficient matrix to extract a smooth pitch contour while facilitating pitch halving and doubling transitions. We discuss the design of pitch atoms along with the various considerations for the pitch extraction process. We evaluate the performance of Pitcher on the PTDB database and compare its performance with three existing pitch tracking algorithms: YIN, IRAPT, and Swipe'. The performance of Pitcher consistently outperforms the other methods for low-pitched speakers and is comparable in performance to the best of the other three methods for high-pitched speakers.", "title": "Dictionary-based pitch tracking with dynamic programming"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/hu14c_interspeech.html", "abstract": "For applications such as tone modeling and automatic tone recognition, smoothed F0 (pitch) all-voiced pitch tracks are desirable. Three pitch trackers that have been shown to give good accuracy for pitch tracking are YAAPT, YIN, and PRAAT. On tests with English and Japanese databases, for which ground truth pitch tracks are available by other means, we show that YAAPT has lower errors than YIN and PRAAT. We also experimentally compare the effectiveness of the three trackers for automatic classification of Mandarin tones. In addition to F0 tracks, a compact set of low-frequency spectral shape trajectories are used as additional features for automatic tone classification. A combination of pitch trajectories computed with YAAPT and spectral shape trajectories extracted from 800ms intervals for each tone results in tone classification accuracy of nearly 77%, a rate higher than human listeners achieve for isolated tonal syllables, and also higher than that obtained with the other two trackers.", "title": "Acoustic features for robust classification of Mandarin tones"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/karlsson14_interspeech.html", "abstract": "Lexical tones are important for expressing meaning and usually have high priority in tone languages. This can create conflicts with sentence intonation in spoken language and with melodic templates in singing since all of these are transmitted by pitch. The main question in this investigation is whether a language (in our case the Mon-Khmer language Kammu) with a simple two-tone system uses similar strategies for preserving lexical tones in singing and speech. We investigate the realization of lexical tones in a singing genre which can be described as recitation based on a partly predefined, though still flexible, melodic template. The contrast between High and Low tone is preserved, and is realized mainly at the beginning of the vowel. Apparently, the rest of the syllable rhyme serves either for strengthening the lexical contrast or for melodic purposes. Syllables are often reduplicated in singing, and the reduplicant ignores lexical tones. The preservation of lexical tones in Kammu singing, and their early timing close to the vowel onset, is very similar to what we have found for speech.", "title": "Preservation of lexical tones in singing in a tone language"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/yakoumaki14_interspeech.html", "abstract": "Automatic classification of emotional speech is a challenging task with applications in synthesis and recognition. In this paper, an adaptive sinusoidal model (aSM), called the extended adaptive Quasi-Harmonic Model \u2014 eaQHM, is applied on emotional speech analysis for classification purposes. The parameters of the model (amplitude and frequency) are used as features for the classification. Using a well known database of narrowband expressive speech (SUSAS), we develop two separate Vector Quantizers (VQ) for the classification, one for the amplitude and one for the frequency features. It is shown that the eaQHM can outperform the standard Sinusoidal Model in classification scores. However, single feature classification is inappropriate for higher-rate classification. Thus, we suggest a combined amplitude-frequency classification scheme, where the classification scores of each VQ are weighted and ranked, and the decision is made based on the minimum value of this ranking. Experiments show that the proposed scheme achieves higher performance when the features are obtained from eaQHM. Future work can be directed to different classifiers, such as HMMs or GMMs, and ultimately to emotional speech transformations and synthesis.", "title": "Emotional speech classification using adaptive sinusoidal modelling"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/wang14h_interspeech.html", "abstract": "Unauthorized tampering in speech signals has brought serious problems when verifying the originality and integrity of speech signals. Digital watermarking can effectively check if the original signals have been tampered by embedding digital data into them. This paper proposes a tampering detection scheme for speech signals based on formant enhancement-based watermarking. Watermarks are embedded as slight enhancement of formant by symmetrically controlling a pair of linear spectral frequencies (LSFs) of corresponding formant. We evaluated the proposed scheme with objective evaluations concerning three criteria that are required for tampering detection scheme: (i) inaudibility to human auditory system, (ii) robustness against meaningful processing, and (iii) fragility against tampering. The evaluation results showed that the proposed scheme could provide satisfactory performance in all the criteria and had the ability to detect tampering in speech signals.", "title": "Formant enhancement based speech watermarking for tampering detection"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/barker14_interspeech.html", "abstract": "We present a novel method for determining how the perceptual organisation of simple alternating tone sequences is likely to occur in human listeners. By training a tensor model representation using features which incorporate both low-frequency modulation rate and phase, a set of components is learned. Test patterns are modelled using these learned components, and the sum of component activations is used to predict either an `integrated' or `segregated' auditory stream percept. We find that for the basic streaming paradigm tested, our proposed model and method is able to correctly predict either segregation or integration in the majority of cases.", "title": "Modelling primitive streaming of simple tone sequences through factorisation of modulation pattern tensors"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/sarma14_interspeech.html", "abstract": "Vowel onset point (VOP) is defined as the instant at which onset of vowel takes place. Accurate detection of VOP is useful in many applications like syllable unit recognition, end-point detection, speaker verification etc. Manually and automatically locating VOPs accurately in case of voiced aspirated (VA) sounds is found to be difficult and ambiguous. This is due to the complex nature of the speech signal waveform around the VOP. This work addresses this issue and a manual marking approach using electroglottograph (EGG) signal is described which accurately marks the VOPs without any ambiguity. The knowledge derived from this manual analysis is transformed into an automatic method for the detection of VOPs in VA sounds. An automatic method is proposed using both source and vocal tract information. VOP detection accuracy of the proposed method is found to be significantly higher than some of the state of the art techniques.", "title": "Detection of vowel onset points in voiced aspirated sounds of indian languages"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/sasou14_interspeech.html", "abstract": "An Auto-Regressive eXogenous (ARX) model combined with descriptive models of the glottal source waveform has been adopted to more accurately separate the vocal tract and the voicing source. However, these methods cannot be easily applied to the analysis of voices uttered by different speech production methods, such as esophageal voice. We previously proposed the Voicing Source Hidden Markov Model (VS-HMM) and an accompanying parameter estimation method. The states of the VS-HMM were concatenated in a ring topology to represent the periodicity of the glottal source. We refer to the model combining the VS-HMM with an Auto-Regressive (AR) filter as AR-HMM. In this paper, we extend the conventional AR-HMM with its fixed ring topology to automatically generate the optimum topology for the VS-HMM using the Minimum Description Length-based Successive State Splitting (MDL-SSS) algorithm in order to simultaneously and accurately estimate the vocal tract and voicing source based on a voice excited by an unknown, aperiodic voicing source such as an esophageal voice. Experiment results using synthesized pseudo-esophageal voices confirmed that the proposed AR-HMM approach can separate the vocal tract characteristics and the voicing source more accurately than the conventional AR-HMM with its fixed-ring topology or the LP method.", "title": "Accuracy evaluation of esophageal voice analysis based on automatic topology generated-voicing source HMM"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/zhang14e_interspeech.html", "abstract": "An audio watermarking system based on multiple echoes hiding is designed for frequency modulation (FM) broadcasting. The embedded watermarking information of 16 bits can be recovered from the receiver by recording the broadcast for any 1 second. On the premise of guaranteeing the high imperceptibility and robustness, the multiple echoes scheme we proposed has a higher capacity. The proposed system is tested on a semi-physical platform with the real channel transmission and the recovery rate reaches higher than 98%. Some attacks, such as white noise, filtering, re-sampling, adding an echo and re-quantization, are used to test the robustness. The subjective test result shows that the system has a good imperceptibility.", "title": "Audio watermarking based on multiple echoes hiding for FM radio"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/motlicek14_interspeech.html", "abstract": "The development of an Automatic Speech Recognition (ASR) system for the bilingual MediaParl corpus is challenging for several reasons: (1) reverberant recordings, (2) accented speech, and (3) no prior information about the language. In that context, we employ frequency domain linear prediction-based (FDLP) features to reduce the effect of reverberation, exploit bilingual deep neural networks applied in Tandem and hybrid acoustic modeling approaches to significantly improve ASR for accented speech and develop a fully bilingual ASR system using entropy-based decoding-graph selection. Our experiments indicate that the proposed bilingual ASR system performs similar to a language-specific ASR system if approximately five seconds of speech are available.", "title": "Development of bilingual ASR system for MediaParl corpus"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/li14f_interspeech.html", "abstract": "In this work, we investigate cross-lingual BN features in hybrid ASR systems under a two-level DNNs framework. The first-level DNNs are bottleneck feature extractors and the second-level DNNs serve as not only acoustic models but also feature combination modules. Different feature configurations, including the bottleneck dimensionality, the need of delta processing and the necessity of concatenation with standard features of target-language, are first studied. Further experiments are done to evaluate the cross-lingual generalization in a more holistic manner using optimized features. We then analyze the effects of adding more training data on the BN feature extractors. Performance improvement can be obtained when more data available. Finally, two different approaches of utilizing data from non-target languages are experimentally compared. It is shown that these two approaches have similar performance with each other, and the two-level DNNs architecture benefits from either of them.", "title": "Investigation of cross-lingual bottleneck features in hybrid ASR systems"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/giwa14_interspeech.html", "abstract": "Within a multilingual automatic speech recognition (ASR) system, knowledge of the language of origin of unknown words can improve pronunciation modelling accuracy. This is of particular importance for ASR systems required to deal with code-switched speech or proper names of foreign origin. For words that occur in the language model, but do not occur in the pronunciation lexicon, text-based language identification (T-LID) of a single word in isolation may be required. This is a challenging task, especially for short words. We motivate for the importance of accurate T-LID in speech processing systems and introduce a novel way of applying Joint Sequence Models to the T-LID task. We obtain competitive results on a real-world 4-language task: for our best JSM system, an F-measure of 97.2% is obtained, compared to a F-measure of 95.2% obtained with a state-of-the-art Support Vector Machine (SVM).", "title": "Language identification of individual words with joint sequence models"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/anguera14_interspeech.html", "abstract": "In this paper we present our efforts in building a speech recognizer constrained by the availability of very limited resources. We consider that neither proper training databases nor initial acoustic models are available for the target language. Moreover, for the experiments shown here, we use grapheme-based speech recognizers. Most prior work in the area use initial acoustic models, trained on the target or a similar language, to force-align new data and then retrain the models with it. In the proposed approach a speech recognizer is trained from scratch by using audio recordings aligned with (sometimes approximate) text transcripts. All training data has been harvested online (e.g. audiobooks, parliamentary speeches). First, the audio is decoded into a phoneme sequence by an off-the-shelf phonetic recognizer in Hungarian. Phoneme sequences are then aligned to the normalized text transcripts through dynamic programming. Correspondence between phonemes and graphemes is done through a matrix of approximate sound-to-grapheme matching. Finally, the aligned data is split into short audio/text segments and the speech recognizer is trained using Kaldi toolkit. Alignment experiments performed for Catalan and Spanish show the feasibility to obtain accurate alignments that can be used to successfully train a speech recognizer.", "title": "Audio-to-text alignment for speech recognition with very limited resources"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/ngo14_interspeech.html", "abstract": "Transliteration converts words in a source language (e.g., English) into phonetically equivalent words in a target language (e.g., Vietnamese). Transliteration is therefore used to handle out-of-vocabulary (OOV) words adopted from foreign languages in automatic speech recognition and keyword search systems. While statistical transliteration approaches have been widely adopted, they may not always be suitable for under-resourced languages, where training data is scarce. In this work, we present a rule-based Vietnamese transliteration framework suitable for spoken language applications with minimal linguistic resources. We show that the proposed system outperforms statistical baselines by up to 81.70% relative when there is limited training examples (94 word pairs). In addition, we investigate the trade-off between training corpus size and transliteration performance of different methods on two distinct corpora. We also show that the proposed model outperforms statistical baselines up to 36.76% relative in keyword search tasks.", "title": "A minimal-resource transliteration framework for vietnamese"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/adel14b_interspeech.html", "abstract": "In this paper, we present our latest investigations of language modeling for Code-Switching. Since there is only little text material for Code-Switching speech available, we integrate syntactic and semantic features into the language modeling process. In particular, we use part-of-speech tags, language identifiers, Brown word clusters and clusters of open class words. We develop factored language models and convert recurrent neural network language models into backoff language models for an efficient usage during decoding. A detailed error analysis reveals the strengths and weaknesses of the different language models. When we interpolate the models linearly, we reduce the perplexity by 15.6% relative on the SEAME evaluation set. This is even slightly better than the result of the unconverted recurrent neural network. We also combine the language models during decoding and obtain a mixed error rate reduction of 4.4% relative on the SEAME evaluation set.", "title": "Combining recurrent neural networks and factored language models during decoding of code-Switching speech"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/tuske14b_interspeech.html", "abstract": "This paper presents the progress of acoustic models for low-resourced languages (Assamese, Bengali, Haitian Creole, Lao, Zulu) developed within the second evaluation campaign of the IARPA Babel project. This year, the main focus of the project is put on training high-performing automatic speech recognition (ASR) and keyword search (KWS) systems from language resources limited to about 10 hours of transcribed speech data. Optimizing the structure of Multilayer Perceptron (MLP) based feature extraction and switching from the sigmoid activation function to rectified linear units results in about 5% relative improvement over baseline MLP features. Further improvements are obtained when the MLPs are trained on multiple feature streams and by exploiting label preserving data augmentation techniques like vocal tract length perturbation. Systematic application of these methods allows to improve the unilingual systems by 4\u20136% absolute in WER and 0.064\u20130.105 absolute in MTWV. Transfer and adaptation of multilingually trained MLPs lead to additional gains, clearly exceeding the project goal of 0.3 MTWV even when only the limited language pack of the target language is used.", "title": "Data augmentation, feature combination, and multilingual neural networks to improve ASR and KWS performance for low-resource languages"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/masumura14_interspeech.html", "abstract": "This paper introduces a novel language model (LM) adaptation method based on mixture of latent word language models (LWLMs). LMs are often constructed as mixture of n-gram models, whose mixture weights are optimized using target domain data. However, n-gram mixture modeling is not flexible enough for domain adaptation because model merger is conducted on the observed word space. Since the words in out-of-domain LMs often differ from those in the target domain LM, it is hard for out-of domain LMs to offer adequate adaptation performance. Our solution is to carry out model merger in a latent variable space created from LWLMs. The latent variables in the LWLMs are represented as specific words selected from the observed word space, so LWLMs can share a common latent variable space and we can realize mixture modeling with consideration of the latent variable space. Following this change, this paper also describes a method to estimate mixture weights for LWLM mixture modeling. We use a sampling technique based on the Bayesian criterion in place of the conventional expectation maximization algorithm. Our experiments show that the LWLM mixture modeling is more effective than n-gram mixture modeling.", "title": "Mixture of latent words language models for domain adaptation"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/herms14_interspeech.html", "abstract": "This paper introduces a novel language model (LM) adaptation method based on mixture of latent word language models (LWLMs). LMs are often constructed as mixture of n-gram models, whose mixture weights are optimized using target domain data. However, n-gram mixture modeling is not flexible enough for domain adaptation because model merger is conducted on the observed word space. Since the words in out-of-domain LMs often differ from those in the target domain LM, it is hard for out-of domain LMs to offer adequate adaptation performance. Our solution is to carry out model merger in a latent variable space created from LWLMs. The latent variables in the LWLMs are represented as specific words selected from the observed word space, so LWLMs can share a common latent variable space and we can realize mixture modeling with consideration of the latent variable space. Following this change, this paper also describes a method to estimate mixture weights for LWLM mixture modeling. We use a sampling technique based on the Bayesian criterion in place of the conventional expectation maximization algorithm. Our experiments show that the LWLM mixture modeling is more effective than n-gram mixture modeling.", "title": "Improving spoken document retrieval by unsupervised language model adaptation using utterance-based web search"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/chien14_interspeech.html", "abstract": "This paper presents a flexible topic model based on the nested Indian buffet process (nIBP). The flexibility is achieved by relaxing three constraints: (1) number of topics is fixed, (2) topics are independent, and (3) topic hierarchy for a document is limited by a single tree path. Bayesian nonparametric learning is conducted to build a tree model where the number of topics and the topic hierarchies are automatically learnt from the given data. In particular, we propose the nIBP to construct the topic mixture model for representation of heterogeneous documents where the mixture components are flexibly selected from tree nodes or dishes that a document or customer chooses in Indian buffet process. The selection is performed in a nested and hierarchical manner. The experiments on document representation show the benefits of using the proposed nIBP.", "title": "The nested indian buffet process for flexible topic modeling"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/levin14_interspeech.html", "abstract": "The paper describes a hardware-software system for real-time closed captioning of Russian live TV broadcasts. The use of respeaking technology enabled us to create an ASR system with WER not exceeding 5.5%. Editing closed captions in real time further reduces WER down to 0.2%. In the paper we report some advancements in LMs for a highly inflected language and also in using morphological rescoring of the decoder word lattice. We propose a solution of the punctuation problem and effective methods of real-time editing of ASR results. This system was successfully used during paralympic games in Sochi for live web-broadcasting on russiasport.ru. We are reporting work in progress and are planning to achieve even better ASR accuracy in the course of the next year.", "title": "Automated closed captioning for Russian live broadcasting"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/wang14i_interspeech.html", "abstract": "One of the challenges in automatic speech recognition is foreign words recognition. It is observed that a speaker's pronunciation of a foreign word is influenced by his native language knowledge, and such phenomenon is known as the effect of language transfer. This paper focuses on examining the phonetic effect of language transfer in automatic speech recognition. A set of lexical rules is proposed to convert an English word into Mandarin phonetic representation. In this way, a Mandarin lexicon can be augmented by including English words. Hence, the Mandarin ASR system becomes capable to recognize English words without retraining or re-estimation of the acoustic model parameters. Using the lexicon that derived from the proposed rules, the ASR performance of Mandarin English mixed speech is improved without harming the accuracy of Mandarin only speech. The proposed lexical rules are generalized and they can be directly applied to unseen English words.", "title": "Pronunciation modeling of foreign words for Mandarin ASR by considering the effect of language transfer"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/rutherford14_interspeech.html", "abstract": "Obtaining good pronunciations for named-entities poses a challenge for automated speech recognition because named-entities are diverse in nature and origin, and new entities come up every day. In this paper, we investigate the feasibility of learning named-entity pronunciations using crowd-sourcing. By collecting audio samples from non-linguistic-expert speakers with Mechanical Turk and learning from them, we can quickly derive pronunciations that are more accurate in speech recognition tests than manual pronunciations generated by linguistic experts. Compared to traditional approaches of generating pronunciations, this new approach proves to be cheap, fast, and quite accurate.", "title": "Pronunciation learning for named-entities through crowd-sourcing"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/schuppler14_interspeech.html", "abstract": "This paper presents the first large-scale analysis of pronunciation variation in conversational Austrian German. Whereas for the varieties of German spoken in Germany, conversational speech has been given noticeable attention in the fields of linguistics and automatic speech recognition, for conversational Austrian there is a lack in speech resources and tools as well as linguistic and phonetic studies. Based on the recently collected GRASS corpus, we provide (methods for the creation of) a pronunciation dictionary and (tools for the creation of) broad phonetic transcriptions for Austrian German. Subsequently, we present a comparative analysis of the occurrence of phonological and reduction rules in read and conversational speech. We find that whereas some rules are specific for the Austrian Standard variant and thus occur in both speech styles (e.g., the realization of /z/ as [s]), other rules are specific for conversational speech (e.g., the realization of /a:/ as [o:]. Overall, our results show that less words are produced with the citation form for conversational Austrian German (37.8%) than for other languages of the same style (e.g., Dutch conversations: 56%).", "title": "Pronunciation variation in read and conversational austrian German"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/lehr14_interspeech.html", "abstract": "Speech recognizers are typically trained with data from a standard dialect and do not generalize to non-standard dialects. Mismatch mainly occurs in the acoustic realization of words, which is represented by acoustic models and pronunciation lexicon. Standard techniques for addressing this mismatch are generative in nature and include acoustic model adaptation and expansion of lexicon with pronunciation variants, both of which have limited effectiveness. We present a discriminative pronunciation model whose parameters are learned jointly with parameters from the language models. We tease apart the gains from modeling the transitions of canonical phones, the transduction from surface to canonical phones, and the language model. We report experiments on African American Vernacular English (AAVE) using NPR's StoryCorps corpus. Our models improve the performance over the baseline by about 2.1% on AAVE, of which 0.6% can be attributed to the pronunciation model. The model learns the most relevant phonetic transformations for AAVE speech.", "title": "Discriminative pronunciation modeling for dialectal speech recognition"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/pellegrini14_interspeech.html", "abstract": "In this paper, we report on a study with the aim of automatically detecting phoneme-level mispronunciations in 32 French speakers suffering from unilateral facial palsy at four different clinical severity grades. We sought to determine if the Goodness of Pronunciation (GOP) algorithm, which is commonly used in Computer-Assisted Language Learning systems to detect learners' individual errors, could also detect segmental deviances in disordered speech. For this purpose, speech read by the 32 speakers was aligned and GOP scores were computed for each phone realization. The highest scores, which indicate large dissimilarities with standard phone realizations, were obtained for the most severely impaired speakers. The corresponding speech subset was manually transcribed at phone-level. 8.3% of the phones differed from standard pronunciations extracted from our lexicon. The GOP technique allowed to detect 70.2% of mispronunciations with an equal rate of about 30% of false rejections and false acceptances. The phone substitutions detected by the algorithm confirmed that some of the speakers have difficulties to produce bilabial plosives, and showed that other sounds such as sibilants are prone to mispronunciation. Another interesting finding was the fact that speakers diagnosed with a same pathology grade do not necessarily share the same pronunciation issues.", "title": "The goodness of pronunciation algorithm applied to disordered speech"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/metallinou14_interspeech.html", "abstract": "We investigated the use of context-dependent deep neural network hidden Markov models, or CD-DNN-HMMs, to improve speech recognition performance for a better assessment of children English language learners (ELLs). The ELL data used in the present study was obtained from a large language assessment project administered in schools in a U.S. state. Our DNN-based speech recognition system, built using rectified linear units (ReLU), greatly outperformed recognition accuracy of Gaussian mixture models (GMM)-HMMs, even when the latter models were trained with eight times more data. Large improvement was observed for cases of noisy and/or unclear responses, which are common in ELL children speech. We further explored the use of content and manner-of-speaking features, derived from the speech recognizer output, for estimating spoken English proficiency levels. Experimental results show that the DNN-based recognition approach achieved 31% relative WER reduction when compared to GMM-HMMs. This further improved the quality of the extracted features and final spoken English proficiency scores, and increased overall automatic assessment performance to the human performance level, for various open-ended spoken language tasks.", "title": "Using deep neural networks to improve proficiency assessment for children English language learners"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/lu14c_interspeech.html", "abstract": "This paper reports the first known effort to automatically align the spoken utterances in recorded lectures with the content of the slides used. Such technologies will be very useful in Massive Open On-line Courses (MOOCs) and various recorded lectures as well as many other applications. We propose a set of approaches considering the problem that words helpful for such alignment are sparse and noisy, and the assumption that the presentation of a slide is usually smooth and top-down across the slide. This includes utterance clustering, entropy-based word filtering, reliability-propagated word-based matching, and the structured support vector machine (SVM) learning from local and global features. Initial experimental results with the lectures in a course offered in National Taiwan University showed very encouraging results as compared to the baseline approaches.", "title": "Alignment of spoken utterances with slide content for easier learning with recorded lectures using structured support vector machine (SVM)"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/duan14_interspeech.html", "abstract": "Detecting mispronunciations produced by non-native speakers and providing detailed instructive feedbacks are desired in computer assisted pronunciation training system (CAPT), as it is helpful to L2 learners to improve their pronunciation more effectively. In this paper, we present our preliminary study on detecting phonetic segmental mispronunciations on account of the erroneous articulation tendencies, including the place of articulation and the manner of articulation. Through modeling and detecting these error patterns, feedbacks based on articulation-placement and articulation-manner could be given. Moreover, Japanese learners of Chinese are focused on in this study. The experimental results show that the approach can detect the mostly representative pronunciation errors moderately well, achieving a false rejection rate of 8.0% and a false acceptance rate 32.6%. The diagnostic accuracy is 86.0%.", "title": "A preliminary study on ASR-based detection of Chinese mispronunciation by Japanese learners"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/xu14b_interspeech.html", "abstract": "The article proposes a real-time technique for visualizing tongue motion driven by ultrasound image sequences. Local feature description is used to follow characteristic speckle patterns in a set of mid-sagittal contour points in an ultrasound image sequence, which are then used as markers for describing movements of the tongue. A 3D tongue model is subsequently driven by the motion data extracted from the ultrasound image sequences. The \u201cmodal warping\u201d technique is used for real-time tongue deformation visualization. The resulting system will be useful in a variety of domains including speech production study, articulation training, educational scenarios, etc. Some parts of the interface are still being developed; we will show preliminary results in the demonstration.", "title": "3d tongue motion visualization based on ultrasound image sequences"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/derrick14_interspeech.html", "abstract": "Here we introduce Aerotak: A system for audio analysis and perception enhancement that allows speech perceivers to listen with their skin. The current system extracts unvoiced portions of an audio signal representative of turbulent air-flow in speech. It stores the audio signal in the left channel of a stereo audio output, and the air flow signal is stored in the right channel. The stored audio is used to drive a conversion unit that splits the left audio channel into a headphone out (to both ears) and right channel air pump drive signal to a piezoelectric pump that is mounted to the headphones. We have shown, using two-way forced-choice experiments, that the system enhances perception of voiceless stops and voiceless fricatives in noise such that 1 out of every 4 such words that would otherwise be missed will be heard correctly. We are currently conducting experiments on word identification while listening to a short-story, and are completing a stand-alone version of the Aerotak that works with real-time audio and from an embedded system. The short-story research and real-time system will be complete for InterSpeech 2014.", "title": "Listen with your skin: aerotak speech perception enhancement system"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/czap14_interspeech.html", "abstract": "A speech assistant system has been developed at the University of Miskolc in cooperation with the University of Debrecen granted by the European Union. The project aims to help training of deaf and hearing impaired people to speak. The idea of the project has come from a three-dimensional head model for articulation presentation, called \u201ctalking head\u201d developed by the University of Miskolc, and an audio-visual transcoder for sound visualization developed by the University of Debrecen.", "title": "Speech assistant system"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/banchs14_interspeech.html", "abstract": "This Show & Tell demo paper describes a multi-strategy spoken dialogue system for restaurant recommendation and reservation in Singapore. The system uses a three phased dialogue strategy for recommendation, selection and booking. Given its simple architecture, it can be easily adapted to deploy restaurant recommendation services for any city, as far as the required regional data is available.", "title": "Spoken dialogue system for restaurant recommendation and reservation"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/akira14_interspeech.html", "abstract": "We present a prototype interlingual communication system that is being used to collect a corpus of task based dialogues between speakers of different languages. This corpus will be used to assess human reactions to an automated speech-to-speech translation system. In this demonstration we show how the HCRC Map Task can be adapted to support data collection in this interlingual environment, and how we used easily accessible speech and language technology for the rapid prototyping of the system used for data collection. An explanation of the nature and purpose of the data we are collecting is also presented.", "title": "Interlingual map task corpus collection"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/centelles14_interspeech.html", "abstract": "This show and tell paper describes a client mobile application for Chinese-Spanish machine translation. The system combines a standard server-based statistical machine translation (SMT) system, which requires online operation, with different input modalities including text, optical character recognition (OCR) and automatic speech recognition (ASR). It also includes an index-based search engine for supporting off-line translation.", "title": "A client mobile application for Chinese-Spanish statistical machine translation"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/benin14_interspeech.html", "abstract": "In this DEMO we present the first worldwide WebGL implementation of a talking head (LuciaWebGL), and also the first WebGL talking head running on iOS mobile devices (Apple iPhone and iPad).", "title": "LuciawebGL: a new WebGL-Based talking head"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/naderi14_interspeech.html", "abstract": "This paper introduces a novel crowdsourcing platform provided to the community. The platform operates on mobile devices and makes data generation and labeling scenarios available for many related research tracks potentially covering also small and underrepresented languages. Besides the versatile ways for commencing studies using the platform, also active research on crowdsourcing itself becomes feasible. With special focus on speech- and video recordings, the mobility and scalability of the platform is expected to stimulate and foster data-driven studies and insights throughout the community.", "title": "Crowdee: mobile crowdsourcing micro-task platform for celebrating the diversity of languages"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/moore14_interspeech.html", "abstract": "Many educational institutions include a course on speech processing as part of their regular curriculum. Such courses usually cover basic principles in acoustics, phonetics and speech signal processing, and they often incorporate practical lab classes using specialised speech processing toolboxes such as `Praat'. Such toolkits are valuable resources for teaching and learning, but they often involve scripting solutions based on prescribed templates and non-real-time processing, and they do not lend themselves to be used by non-specialists, such as members of the general public or schoolchildren. This paper introduces the Pure Data open-source real-time visual programming language, and presents examples of its use for teaching and public outreach in speech processing. The `Show & Tell' session will present `live' examples as well as handson interactive demonstrations to illustrate its value.", "title": "On the use of the `pure data' programming language for teaching and public outreach in speech processing"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/dubinsky14_interspeech.html", "abstract": "As progressing social mores and new legislation mandate greater levels of accessibility across broadcast television, online video, education, and the workplace, it becomes ever more important to deliver captioned content at lower cost, with higher quality, and more quickly. To reach those goals, attention is shifting toward automation. While many attempts have been made to apply large vocabulary ASR transcription for this task, the results on noisy, post-processed, multi-speaker recordings are insufficient for most real-world applications, to say the least. More viable is the automated synchronization of a human-prepared transcript with the media, and automating the breaking up of the text into well-structured, meaningful captions. SyncWords is a platform that incorporates innovative, originally developed algorithms for both of these tasks, and presents an easy-to-use commercial Web interface for submitting work and receiving captions. In addition, an interactive review tool is available in which user corrections are fed back and cause a re-evaluation of the results in real time.", "title": "Syncwords: a platform for semi-automated closed captioning and subtitles"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/clark14_interspeech.html", "abstract": "The 4 project enables the building of speech synthesis systems automatically from data with little or no expert supervision, thus creating speech synthesisers for new languages or domains in a quick, easy and pain-free way. Whilst substantial effort has been invested over the years into developing novel and progressively more natural sounding speech synthesis techniques, there has always been a high entry barrier to building a speech synthesis system in a new language, because conventional methods require expensive linguistic resources, detailed expert knowledge, and a high degree of skill in system `tuning'. The result is that only a very few of the world's languages have speech synthesis systems. For those less widely-spoken languages that do have a system available, the quality of the synthetic speech is not very good, because of the lack of resources available and the lack of skills to `tune' the system.   The main objectives of 4 are to address this imbalance by providing Open Source tools to: enable the creation of a voice for any domain in any language, provide an end-to-end framework in which these voices can be automatically built and improved, produce natural and expressive speech synthesis, provide feedback-driven learning to improve systems.", "title": "Simple4all"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/chawah14_interspeech.html", "abstract": "This paper presents an early version of an open extendable research and educational platform to support users in learning and mastering the different types of rare-singing. The platform is interfaced with a portable helmet to synchronously capture multiple signals during singing in a non-laboratory environment. Collected signals reflect articulatory movements and induced vibrations. The platform consists of four main modules: i) a capture and recording module, ii) a data replay (post processing) module, iii) an acoustic auto adaptation learning module, iv) and a 3D visualization sensory motor learning module. Our demo will focus on the first two modules. The system has been tested on two rare endangered singing musical styles, the Corsican \u201cCantu in Paghjella\u201d, and the Byzantine hymns from Mount Athos, Greece. The versatility of the approach is further demonstrated by capturing a contemporary singing style known as \u201cHuman Beat Box.\u201d", "title": "An educational platform to capture, visualize and analyze rare singing"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/jeon14_interspeech.html", "abstract": "In this paper, we demonstrate a simulator for real-time speech enhancement based on a non-negative matrix factorization (NMF) technique. In particular, we propose an online noise adaptation method in an NMF framework, which is activated during non-speech intervals and used for adapting noise bases for NMF. Thus, incoming noisy speech is decomposed by using such adapted noise bases and universal speech bases that can be developed through training with examples of speech data. It is shown from the experiments that the proposed method improves speech separation performance and perceptual speech quality.", "title": "Single-channel speech enhancement based on non-negative matrix factorization and online noise adaptation"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/maurer14_interspeech.html", "abstract": "The question whether or not vowel quality can be maintained at F0 of the sounds exceeding statistical F1 of \u201cnormal\u201d speech is still a matter of debate. The present study investigates the perception of long Cantonese vowels /i, y, \u0153, a, \u0254, u/, spoken and sung in (C)V and (C)V:S context by a well-known female Cantonese Opera singer in the range of F0 of c. 560\u2013860Hz. 172 high-pitched syllables or isolated vowel sounds were selected and extracted from a live recording. Corresponding vowel perception was tested in a listening test performed by 26 students of linguistics. All six vowels proved to be identifiable > 80% up to F0 of c. 700Hz, and sounds of /i, a, \u0254, u/ proved to be identifiable > 80% up to a range of F0 of c. 820\u2013860Hz. Confusion matrices are provided in the present paper and spectral illustrations of all sounds are presented at http://is2014.phones-and-phonemes.org.", "title": "Intelligibility of high-pitched vowel sounds in the singing and speaking of a female Cantonese opera singer"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/mowlaee14_interspeech.html", "abstract": "While the state-of-the-art speech enhancement methods are focused on the modification of the noisy spectral amplitude, our recent findings demonstrate positive impact of incorporating the speech phase spectrum in speech enhancement. In this show and tell proposal, we demonstrate the recent progress towards utilizing the phase information in closed-loop iterative manner leading to the joint enhancement of amplitude and phase spectra. In this show and tell, we provide a demo where a recorded speech corrupted with noise is enhanced by an iterative refinement of amplitude and phase. The improvement in speech enhancement in various signal-to-noise ratios is justified by improved perceived speech quality as well as not degrading the speech intelligibility performance.", "title": "Iterative refinement of amplitude and phase in single-channel speech enhancement"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/roekhaut14_interspeech.html", "abstract": "This paper presents eLite-HTS, a web service which generates input files for the training and synthesis stages of a French HMM-based synthesizer using the HTS toolkit.", "title": "elite-HTS: a NLP tool for French HMM-based speech synthesis"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/niculescu14_interspeech.html", "abstract": "In this paper we describe SARA \u2014 Singapore's Automated Responsive Assistance \u2014 an Android mobile phone application for touristic information in Singapore. The application provides information about local attractions, restaurants, sightseeing direction and transportation services. Using a GPS integrated module SARA is able to detect the user's location on a map providing orientation and direction help. Users can use speech, text or scanned QR code to interact with SARA. Input/output modalities for SARA include natural language in form of text or speech. A short video about the main features of our Android application can be seen here: http://vimeo.com/91620644. Currently, SARA supports only English, but we are working towards a multi-lingual support. For test purposes we created a web version of SARA that can be tested for Chinese and English text input at: http://iris.i2r.astar.edu.sg/StatTour/.", "title": "SARA \u2014 singapore's automated responsive assistant for the touristic domain"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/plummer14_interspeech.html", "abstract": "We present updates to the Speech Recognition Virtual Kitchen (SRVK) environment, a repository of pre-configured Virtual Machines (VMs) containing tools and experiments in the speech and language field. SRVK promotes community sharing of research techniques, fosters innovative experimentation, and provides solid reference systems as a tool for education, research, and evaluation. VMs provide a consistent environment for experimentation, without requiring tedious installation of many individual tools, a web-based community platform complements the VMs, allowing users to jointly explore, learn and collaborate using VMs. In this Show&Tell demo, we present the infrastructure to the speech community, along with several example VMs and a set of online error analysis tools. We solicit feedback from the community, in order to further guide development of the kitchen, which we hope to grow into a widely used community resource.", "title": "The speech recognition virtual kitchen: launch party"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/marekspartz14_interspeech.html", "abstract": "SALSA automates cognitive test administration and scoring. These tests are used to characterize cognitive impairment resulting from neurodegenerative disease, traumatic brain injury, and drug toxicity; however, they are currently performed manually, limiting their utility for large clinical populations and longitudinal assessments. We present a fully automated, comprehensive system for collecting spoken test responses with mobile and telephony platforms, using an open-source automatic speech recognition engine (KALDI) to calculate a range of speech characteristics that may be useful in assessment of cognitive function.", "title": "System for automated speech and language analysis (SALSA)"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/masudakatsuse14_interspeech.html", "abstract": "We developed a system with which children who have difficulty correctly pronouncing words to practice their pronunciation. It allows exercises to be individually tailored to each child's pronunciation needs. Three speech evaluation methods were prepared for each type of presented words: automatic speech recognition, phonemic discrimination between the correct and the probable error pronunciation of a consonant period and articulation tests from speech-language-hearing therapists. For 3 or 4 months, we performed practical field tests with nine students in special support education classes in four elementary schools. In the tests, we realized medical-educational-engineering collaboration and the technical support of local-community volunteers.", "title": "Pronunciation practice support system for children who have difficulty correctly pronouncing words"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/driesen14_interspeech.html", "abstract": "Providing subtitling for multimedia content is a highly costly process. Any system aimed at automating at least part of this process may therefore yield significant economic benefits for content providers. In this paper, we present an integrated automatic system capable of automatically subtitling weather forecasts and news broadcasts. In this system, a number of different modules are stringed together, each performing a single processing step in the pipeline. An ASR (Automatic Speech Recognition) module first converts raw audio into an uninterrupted stream of written words. A decision tree classifier then marks sentence boundaries in the resulting word sequence. Finally, a SMT (Statistical Machine Translation) module `translates' the resulting sentences into punctuated true-cased text. The system has been developed in close cooperation with Red Bee Media and will be deployed in their commercial production pipeline.", "title": "Automated production of true-cased punctuated subtitles for weather and news broadcasts"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/dong14_interspeech.html", "abstract": "High-quality natural singing is appealing to everyone. However, singing well is not trivial. Professional singers are trained and practised over the years for various singing skills, such as sense of rhythm, precision in pitch, vocal control and personal styling etc. We, Institute for Infocomm Research (I2R), in Singapore has developed a personalized, real-time singing synthesis technology for the general public. Given an input singing (maybe in mediocre to poor quality), our technology will convert it into a nice piece of singing, perfecting it with correct rhythm and fundamental frequency over the time. Now, you can simply read the lyrics, record in our mobile apps and generate a pleasant and personalized singing with your own vocal characteristics.", "title": "I2r speech2singing perfects everyone's singing"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/henter14_interspeech.html", "abstract": "Acoustic models used for statistical parametric speech synthesis typically incorporate many modelling assumptions. It is an open question to what extent these assumptions limit the naturalness of synthesised speech. To investigate this question, we recorded a speech corpus where each prompt was read aloud multiple times. By combining speech parameter trajectories extracted from different repetitions, we were able to quantify the perceptual effects of certain commonly used modelling assumptions. Subjective listening tests show that taking the source and filter parameters to be conditionally independent, or using diagonal covariance matrices, significantly limits the naturalness that can be achieved. Our experimental results also demonstrate the shortcomings of mean-based parameter generation.", "title": "Measuring the perceptual effects of modelling assumptions in speech synthesis using stimuli constructed from repeated natural speech"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/merritt14_interspeech.html", "abstract": "This paper presents an investigation of the separate perceptual degradations introduced by the modelling of source and filter features in statistical parametric speech synthesis. This is achieved using stimuli in which various permutations of natural, vocoded and modelled source and filter are combined, optionally with the addition of filter modifications (e.g. global variance or modulation spectrum scaling). We also examine the assumption of independence between source and filter parameters. Two complementary perceptual testing paradigms are adopted. In the first, we ask listeners to perform \u201csame or different quality\u201d judgements between pairs of stimuli from different configurations. In the second, we ask listeners to give an opinion score for individual stimuli. Combining the findings from these tests, we draw some conclusions regarding the relative contributions of source and filter to the currently rather limited naturalness of statistical parametric synthetic speech, and test whether current independence assumptions are justified.", "title": "Investigating source and filter contributions, and their interaction, to statistical parametric speech synthesis"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/latorre14b_interspeech.html", "abstract": "This paper proposes a method to modify the expression or emotion in a sample of speech without altering the speaker's identity. The method exploits a statistical speech model that factorises the speaker identity from expressions using linear transforms. For this approach, the set of transforms that best fit the speaker and expression of the input speech sample are learned. They are then combined with the expression transforms of the desired expression taken from another speaker. Since the combined expression transform is factorised and contains information about expression only, it may be applied to the original speech sample to modify its expression to the desired one without altering the identity of the speaker. Notably, this method may be applied universally to any voice without the need for a parallel training corpus.", "title": "Voice expression conversion with factorised HMM-TTS models"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/yanagisawa14_interspeech.html", "abstract": "In practical scenarios for speaker adaptation of speech synthesis systems, the quality of adaptation audio data may be poor. In these situations, it is necessary to make use of the available audio to capture the speaker attributes, whilst aiming to obtain a synthesis voice which does not have any of the low-quality attributes of the audio. One approach to achieving this is to define a sub-space of parametric synthesis parameters in which the adapted system must lie. Though this yields reasonable synthesis quality, target speaker similarity degrades. Quality is also affected in severe noise conditions. This paper describes a smoothing approach that addresses this problem. For a noisy target speaker, first a `similar speaker' is selected from a database of speakers. Statistics from this speaker are then smoothed with those obtained from the target speaker. By appropriately combining the two sources of information, it is possible to balance similarity and quality. Results indicate that both the quality and similarity can be improved by smoothing, especially for severe noise conditions. The similarity performance, however, varies from speaker to speaker, indicating the importance of a reasonable automatic speaker selection method and the coverage of the candidate speaker pool.", "title": "Noise-robust TTS speaker adaptation with statistics smoothing"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/brognaux14_interspeech.html", "abstract": "While current research in speech synthesis focuses on the generation of various speaking styles or emotions, very few studies have addressed the possibility of including phonetic variations according to the communicative situation of the target speech (sports commentaries, TV news, etc.). However, significant phonetic variations have been observed, depending on various communicative factors (e.g. spontaneous/read and media broadcast or not). This study analyzes whether these alternative pronunciations contribute to the plausibility of the message and should therefore be considered in synthesis. To this end, subjective tests are performed on synthesized French sports commentaries. They aim at comparing HMM-based speech synthesis with genuine pronunciation and with neutral NLP-produced phonetization. Results show that the integration of the phonetic variations significantly improves the perceived naturalness of the generated speech. They also highlight the relative importance of the various types of variations and show that schwa elisions, in particular, play a crucial role in that respect.", "title": "Speech synthesis in various communicative situations: impact of pronunciation variations"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/cai14_interspeech.html", "abstract": "This paper presents a statistical parametric speech synthesis method using hidden trajectory model (HTM) for flexibly controlling the formant positions and bandwidths of synthetic speech. In an HTM, hidden formant trajectories are generated by a bidirectional filtering process on the time-aligned and phone-dependent formant targets. The observed cepstral features are constituted by a formant-related component, which is predicted from the hidden formant trajectories using a nonlinear and analytical function, and a residual component, which is modeled by context-dependent Gaussians. In this paper, we apply HTM-based acoustic modeling to speech synthesis. The distribution parameters of the formant targets are manipulated at synthesis time to control the characteristics of synthetic speech. In our implementation, the distributions of residual cepstra are estimated for each quinphone and the question set used in the decision-tree-based model clustering is tailored so as to acquire high controllability for vowels. Experimental results shows that this proposed method can achieve effective controllability on the formant positions and bandwidths while keeping almost the same naturalness as the conventional HMM-based approach.", "title": "Formant-controlled speech synthesis using hidden trajectory model"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/zhang14f_interspeech.html", "abstract": "Voice activity detection (VAD) is an important frontend of many speech processing systems. In this paper, we describe a new VAD algorithm based on boosted deep neural networks (bDNNs). The proposed algorithm first generates multiple base predictions for a single frame from only one DNN and then aggregates the base predictions for a better prediction of the frame. Moreover, we employ a new acoustic feature, multi-resolution cochleagram (MRCG), that concatenates the cochleagram features at multiple spectrotemporal resolutions and shows superior speech separation results over many acoustic features. Experimental results show that bDNN-based VAD with the MRCG feature outperforms state-of-the-art VADs by a considerable margin.", "title": "Boosted deep neural networks and multi-resolution cochleagram features for voice activity detection"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/prasad14_interspeech.html", "abstract": "Real time magnetic resonance imaging (rtMRI) enables direct video capture of the moving vocal tract concurrent with audio signal providing valuable data for speech research. We consider a multimodal approach to voice activity detection (VAD) in the rtMRI recording that uses audio signal as well as MRI image sequence. The degraded quality of the audio recorded in the scanner motivates this multimodal scheme for robust VAD. Optimal regions in the MRI image are selected for performing VAD with a novel algorithm. VAD experiments using rtMRI data of two male and two female subjects show that VAD performance using optimally selected regions from MRI images is comparable to that using only audio signal. The optimal regions turn out to be parts of jaw, velum, glottis and lips. VAD performance using audio signal and MRI image sequence together is found to be significantly better (~14% absolute improvement in VAD accuracy) than that using the audio only when the audio is contaminated with additive noise at low SNR.", "title": "Selection of optimal vocal tract regions using real-time magnetic resonance imaging for robust voice activity detection"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/ziaei14b_interspeech.html", "abstract": "Speech Activity Detection (SAD) is a well researched problem for communication, command and control applications, where audio segments are short duration and solution proposed for noisy as well as clean environments. In this study, we investigate the SAD problem using NASA's Apollo space mission data [1]. Unlike traditional speech corpora, the audio recordings in Apollo are extensive from a longitudinal perspective (i.e., 6\u201312 days each). From SAD perspective, the data offers many challenges: (i) noise distortion with variable SNR, (ii) channel distortion, and (iii) extended periods of non-speech activity. Here, we use the recently proposed Combo-SAD, which has performed remarkably well in DARPA RATS evaluations, as our baseline system [2]. Our analysis reveals that the Combo-SAD performs well when speech-pause durations are balanced in the audio segment, but deteriorates significantly when speech is sparse or absent. In order to mitigate this problem, we propose a simple yet efficient technique which builds an alternative model of speech using data from a separate corpora, and embeds this new information within the Combo-SAD framework. Our experiments show that the proposed approach has a major impact on SAD performance (i.e., +30% absolute), especially in audio segments that contain sparse or no speech information.", "title": "Speech activity detection for NASA apollo space missions: challenges and solutions"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/tu14_interspeech.html", "abstract": "Statistical model based voice activity detection (VAD) is commonly used in various speech related research and applications. In this paper, we try to improve the performance of statistical model based VAD via new feature extraction method. Our main innovation focuses on that we apply Mel-frequency subband coefficients with power-law nonlinearity as feature for statistical model based VAD instead of Discrete Fourier Transform (DFT) coefficients. This proposed feature is then modeled by Gaussian distribution. Performances of this method are comprehensively compared with existing methods. Meanwhile we also test power-law nonlinearity on existing methods. Experimental results prove that with proposed subband coefficients the performance of statistical model based VAD could be improved a lot. Power-law nonlinearity on DFT coefficients could also bring some improvement.", "title": "Towards improving statistical model based voice activity detection"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/mcloughlin14_interspeech.html", "abstract": "An active detection system is developed which uses low-power low-frequency ultrasonic reflection to determine the lip state (i.e. whether open, closed or in between) of a human speaker and hence the presence of vocal activity. In operation, a small loudspeaker or sounder, located within a few centimetres of the lips, produces an excitation signal which is emitted towards the lips. A co-located microphone receives the signal reflected from the lip region. Even simple analysis of the reflected information reveals whether the mouth is open or closed. Given an excitation located above the normal frequency range of human speech, the method is unaffected by speech energy. If the excitation frequency is moved above the normal threshold of human hearing (i.e. an ultrasonic excitation), the method is inaudible. Careful placement of the excitation signal at the extreme low end of the ultrasonic range, allows its generation and analysis to be done with inexpensive off-the-shelf audio hardware.   This paper describes the techniques used, presents experimental details regarding the signals, then implement and evaluates a simple voice activity detector based on the technique.", "title": "The use of low-frequency ultrasound for voice activity detection"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/ma14_interspeech.html", "abstract": "This paper presents the work that we conducted for building the speech activity detection (SAD) systems for the phase 3 evaluation of the RATS program. The work focused on improving the SAD performance with the neural network (NN) approach. The major efforts include reducing the false rejections errors by extensions of speech regions in the training references and use of post-processing NNs, and removing channel variations by design of channel bottleneck features with the deep NN learning approach. With these efforts more 25% relative improvements were achieved over the phase 2 evaluation system. The bigger contribution of the design of the bottleneck features was the enhancement of the SAD system performance on new channels. Our results revealed that the bottleneck features were able to improve SAD performance on new channels significantly.", "title": "Improving the speech activity detection for the DARPA RATS phase-3 evaluation"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/le14_interspeech.html", "abstract": "Patients with aphasia often have impaired speech-language production skills, resulting in tremendous difficulties in tasks that require verbal communication. To facilitate rehabilitation outside of therapy, we are collaborating with the University of Michigan Aphasia Program (UMAP) to develop an automated system capable of providing feedback regarding the patient's verbal output. In this paper we introduce a robust method for extracting rhythm and intonation features from aphasic speech based on template matching. These features, combined with Goodness of Pronunciation (GOP) scores and our previous feature set, help our system achieve human-level performance in classifying the quality of speech produced by patients attending UMAP. The results presented in this work demonstrate the efficacy of our technique and the potential of this system for handling natural speech data recorded in non-ideal conditions as well as the unpredictability in aphasic speech patterns.", "title": "Modeling pronunciation, rhythm, and intonation for automatic assessment of speech quality in aphasia rehabilitation"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/strombergsson14_interspeech.html", "abstract": "Children with speech disorders often present with systematic speech error patterns. In clinical assessments of speech disorders, evaluating the severity of the disorder is central. Current measures of severity have limited sensitivity to factors like the frequency of the target sounds in the child's language and the degree of phonological diversity, which are factors that can be assumed to affect intelligibility. By constructing phonological filters to simulate eight speech error patterns often observed in children, and applying these filters to a phonologically transcribed corpus of 350K words, this study explores three quantitative measures of phonological impact: Percentage of Consonants Correct (PCC), edit distance, and degree of homonymy. These metrics were related to estimated ratings of severity collected from 34 practicing clinicians. The results show an expected high correlation between the PCC and edit distance metrics, but that none of the three metrics align with clinicians' ratings. Although these results do not generate definite answers to what phonological factors contribute the most to (un)intelligibility, this study demonstrates a methodology that allows for large-scale investigations of the interplay between phonological errors and their impact on speech in context, within and across languages.", "title": "Ranking severity of speech errors by their phonological impact in context"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/orozcoarroyave14_interspeech.html", "abstract": "About 90% of the people with Parkinson's disease (PD) develop speech impairments such as monopitch, monoloudness, imprecise articulation, and other symptoms. There are several studies addressing the problem of the automatic detection of PD from speech signals in order to develop computer aided tools for the assessment and monitoring of the patients. Recent works have shown that it is possible to detect PD from speech with accuracies above 90%; however, it is still unclear whether it is possible to make the detection independent of the spoken language. This paper addresses the automatic detection of PD considering speech recordings of three languages: German, Spanish and Czech. According to the results it is possible to classify between speech of people with PD and healthy controls (HC) with accuracies ranging from 84% to 99%, depending on the utterance.", "title": "Automatic detection of parkinson's disease from words uttered in three different languages"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/lilley14b_interspeech.html", "abstract": "There are many research and clinical settings in which accurate objective measures of speech intelligibility are either necessary or highly desirable. Unfortunately, most objective measures of intelligibility require blinded human listeners to classify speech tokens obtained from the talker whose speech intelligibility is to be measured, a process that can be both costly and time-consuming to carry out. In a research setting, it is often possible to justify the time and cost required, but in a clinical setting this is usually infeasible. The current work describes an effort to develop tools that could be of practical use in a clinical setting. An automatic response scoring system based on 32-Gaussian mixture hidden Markov models was trained on responses of both children with normal hearing and those with hearing loss to the Children's Speech Intelligibility Measure (CSIM). The system was able to predict a human listener's response out of 12 choices over 60% of the time. Aggregate CSIM scores computed from the ASR system had a high correlation with scores compiled by human listeners (r2=.83). Automatic scoring of utterances from children could allow the CSIM and similar testing procedures to be used more frequently in research and clinical settings.", "title": "Automating an objective measure of pediatric speech intelligibility"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/shahin14_interspeech.html", "abstract": "This paper introduces a pronunciation verification method to be used in an automatic assessment therapy tool of child disordered speech. The proposed method creates a phone-based search lattice that is flexible enough to cover all probable mispronunciations. This allows us to verify the correctness of the pronunciation and detect the incorrect phonemes produced by the child. We compare between two different acoustic models, the conventional GMM-HMM and the hybrid DNN-HMM. Results show that the hybrid DNN-HMM outperforms the conventional GMM-HMM for all experiments on both normal and disordered speech. The total correctness accuracy of the system at the phoneme level is above 85% when used with disordered speech.", "title": "A comparison of GMM-HMM and DNN-HMM based pronunciation verification techniques for use in the assessment of childhood apraxia of speech"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/berry14_interspeech.html", "abstract": "Broadening our understanding of the components and processes of speech sensorimotor learning is crucial to furthering methods of speech neurorehabilitation. Recent research in limb sensorimotor control has used virtual environments to study learning in novel sensorimotor working spaces. Comparable experimental paradigms have yet to be undertaken to study speech learning. We present acoustic and kinematic data obtained from participants producing vowels in unfamiliar articulatory-acoustic working spaces using a virtual vocal tract. Talkers with dysarthria and healthy controls were asked to produce vowels using an electromagnetic articulograph-driven speech synthesizer for participant-controlled auditory feedback. The aim of the work was to characterize performance within and between groups to generate hypotheses regarding experimental manipulations that may bolster our understanding of speech sensorimotor learning. Results indicate that dysarthric talkers displayed relatively reduced acoustic working spaces and somewhat more variable acoustic targets compared to controls. Kinematic measures of articulatory dynamics, particularly peak speed and movement jerk-cost, were idiosyncratic and did not dissociate talker groups. These findings suggest that individuals with dysarthria and healthy talkers may use idiosyncratic movement strategies in learning to control a virtual vocal tract, but that dysarthric talkers may nonetheless exhibit acoustic limitations that parallel deficits in speech intelligibility.", "title": "Acoustic and kinematic characteristics of vowel production through a virtual vocal tract in dysarthria"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/wand14b_interspeech.html", "abstract": "This article gives an overview of the EMG-UKA corpus, a corpus of electromyographic ( EMG) recordings of articulatory activity enabling speech processing (in particular speech recognition and synthesis) based on EMG signals, with the purpose of building Silent Speech interfaces. Data is available in multiple speaking modes, namely audibly spoken, whispered, and silently articulated speech. Besides the EMG data, synchronous acoustic data was additionally recorded to serve as a reference. The corpus comprises 63 recorded sessions from 8 speakers, the total amount of data is 7:32 hours. A trial subset, consisting of 1:52 hours of data, is freely available for download.", "title": "The EMG-UKA corpus for electromyographic speech processing"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/lee14b_interspeech.html", "abstract": "Whispered speech is a natural mode of speech in which voicing is absent \u2014 its acoustics differ significantly from normally spoken speech or so-called neutral speech, such that it is challenging to use only neutral speech to build speech processing and automatic recognition systems that can deal effectively with whisper. At the same time, humans can naturally produce and perceive whispered speech without explicit training. Tonal languages such as Mandarin present an interesting dilemma \u2014 tone is primarily encoded by pitch tracks which are absent during whispered speech, but humans can still tell tones apart. How humans manage to process whispered speech well without explicit training on it, whereas machine algorithms fail, is presently an unresolved question which could prove fruitful with study. This, however, is hindered by the lack of suitable, systematically collected corpora. We present iWhisper-Mandarin, a 25-hour parallel corpus of neutral and whispered Mandarin, designed to support research in linguistics and speech technology. We demonstrate and verify that earlier techniques applied to whispered speech from non-tonal languages also work with Mandarin, and present some preliminary studies on voice activity detection and whispered Mandarin speech recognition.", "title": "A whispered Mandarin corpus for speech technology applications"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/gretter14_interspeech.html", "abstract": "In this paper we present the first recognition experiments on a multilingual speech corpus, designed for Automatic Speech Recognition (ASR) and Language IDentification (LID) purposes. Data come from the portal Euronews and were acquired both from the Web and from TV. The corpus includes data in 10 languages (Arabic, English, French, German, Italian, Polish, Portuguese, Russian, Spanish and Turkish). For each language, the corpus is composed of about 100 hours of speech for training (60 for Polish) and about 4 hours, manually transcribed, for testing. Training data include the audio, some reference text, the ASR output and their alignment. 10 baselines were prepared \u2014 one for each language \u2014 using only the training data, and performance are evaluated on a subset of the test data. Also a LID system was implemented, capable to recognize words belonging to different languages in a continuous stream. Part of the corpus is freely available, for research purposes only, within the multilingual ASR benchmark for IWSLT 2014.", "title": "Euronews: a multilingual benchmark for ASR and LID"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/tsiami14_interspeech.html", "abstract": "In this paper we present a Greek speech database with real multi-modal data in a smart home two-room environment. In total, 20 speakers were recorded in 240 one-minute long sessions. The recordings include utterances of activation keywords and commands for home automation control, but also phonetically rich sentences and conversational speech. Audio, speaker movements and gestures were captured by 20 condenser microphones installed on the walls and ceiling, 6 MEMS microphones, 2 close-talk microphones and one Kinect camera. The new publicly available database exhibits adverse noise conditions because of background noises and acoustic events performed during the recordings to better approximate a realistic everyday home scenario. Thus, it is suitable for experimentation on voice activity and event detection, source localization, speech enhancement and far-field speech recognition. We present the details of the corpus as well as baseline results on multi-channel voice activity detection and spoken command recognition.", "title": "ATHENA: a Greek multi-sensory database for home automation control uthor: isidoros rodomagoulakis (NTUA, Greece)"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/matassoni14_interspeech.html", "abstract": "Distant speech recognition in real-world environments is still a challenging problem and a particularly interesting topic is the investigation of multi-channel processing in case of distributed microphones in home environments. This paper presents an initiative oriented to address the challenges of such a scenario; an experimental recognition framework comprising a multi-room, multi-channel corpus and the accompanying evaluation tools is made publicly available. The overall goal is to represent a common platform for comparing state-of-the-art algorithms, share ideas of different research communities and integrate several components in a realistic distant-talking recognition chain, e.g., voice activity detection, speech/feature enhancement, channel selection and fusion, model compensation. The recordings include spoken commands (derived from the well-known GRID corpus) mixed with other acoustic events occurring in different rooms of a real apartment. The work provides a detailed description of data, tasks and baseline results, discussing the potential and limits of the approach and highlighting the impact of single modules on recognition performance.", "title": "The DIRHA-GRID corpus: baseline and tools for multi-room distant speech recognition using distributed microphones"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/henriques14_interspeech.html", "abstract": "Query specification for 3D object retrieval still relies on traditional interaction paradigms. The goal of our study was to identify the most natural methods to describe 3D objects, focusing on verbal and gestural expressions. Our case study uses LEGO blocks. We started by collecting a corpus involving ten pairs of subjects, in which one participant requests blocks for building a model from another participant. This small corpus suggests that users prefer to describe 3D objects verbally, rarely resorting to gestures, and using them only as complement. The paper describes this corpus, addressing the challenges that such verbal descriptions create for a speech understanding system, namely the long complex verbal descriptions, involving dimensions, shapes, colors, metaphors, and diminutives. The latter connote small size, endearment or insignificance, and are only very common in informal language. In this corpus, they occurred in one out of seven requests. This experiment was the first step of the development of a prototype for searching LEGO blocks combining speech and stereoscopic 3D. Although the verbal interaction in the first version is limited to relatively simple queries, its combination with immersive visualization allows the user to explore query results in a dataset with virtual blocks.", "title": "Verbal description of LEGO blocks"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/mowlaee14b_interspeech.html", "abstract": "In many speech processing applications, the spectral amplitude is the dominant information while the use of phase spectrum is not so widely spread. In this paper, we present an overview on why speech phase spectrum has been neglected in the conventional techniques used in different applications including: speech separation/enhancement, automatic speech and speaker recognition and speech synthesis. We proceed with giving highlights on the recent progress carried out in demonstrating the importance of phase in different applications and how it impacts on the overall performance. The paper is an introduction to the Interspeech 2014 special session phase importance in speech processing applications.", "title": "Phase importance in speech processing applications"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/cano14_interspeech.html", "abstract": "In this paper, a method for separation of harmonic and percussive elements in music recordings is presented. The proposed method is based on a simple spectral peak detection step followed by a phase expectation analysis that discriminates between harmonic and percussive components. The proposed method was tested on a database of 10 audio tracks and has shown superior results to the reference state-of-the-art approach.", "title": "Phase-based harmonic/percussive separation"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/degottex14_interspeech.html", "abstract": "The representation of the glottal source is of paramount importance for describing para-linguistic information carried through the voice quality (e.g., emotions, mood, attitude). However, some existing representations of the glottal source are based on analytical glottal models, which assume strong a priori constraints on the shape of the glottal pulses. Thus, these representations are restricted to limited number of voices. Recent progresses in the estimation of the glottal models revealed that the Phase Distortion (PD) of the signal carries most of the information about the glottal pulses. This paper introduces a flexible representation of the glottal source - based on the short-term modelling of the phase distortion. This representation is not constrained by a specific analytical model, and thus can be used to describe a larger variety of expressive voices. We address the efficiency of this representation for the recognition of various voice qualities, with comparison to MFCC and standard glottal source representations.", "title": "Phase distortion statistics as a representation of the glottal source: application to the classification of voice qualities"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/degottex14b_interspeech.html", "abstract": "Modern statistical speech processing frameworks require the speech signals to be translated into feature vectors by means of vocoders. While features representing the amplitude envelope already exist (e.g. MFCC, LSF), parametrizing the phase information is far from straightforward, not only because it is a circular data, but also because it shows an irregular behaviour in noisy time-frequency regions. Thus, many vocoders reconstruct speech by using minimum phases and random phases, relying on a previous voicing decision. In this paper, a phase feature is suggested to represent the randomness of the phase across the full time-frequency plan, in both voiced and unvoiced segments, without voicing decision. Resynthesis experiments show that, when integrated into a full-band harmonic vocoder, the suggested randomization feature is slightly better, on average, to STRAIGHT's aperiodicity. In HMM-based synthesis, the results show that the suggested vocoder reduces the complexity of the analysis and statistical modelling by removing the voicing decision, while keeping the perceived quality.", "title": "A measure of phase randomness for the harmonic model in speech synthesis"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/jokinen14_interspeech.html", "abstract": "Post-processing methods can be used in mobile communications to improve the intelligibility of speech in adverse near-end noise conditions. This study proposes a phase spectrum modification method for intelligibility enhancement in mobile devices. The method first modifies the phase spectrum of speech in order to reduce the amplitude range of the signal. The amplitude level is then equalized to that of the unprocessed speech hence resulting in amplification of signal energy. The performance of the proposed method was evaluated in comparison to a known post-processing method with an objective intelligibility metric in multiple noise conditions. Results indicate that the proposed post-processing method improves speech intelligibility over the reference method.", "title": "Enhancement of speech intelligibility in near-end noise conditions with phase modification"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/shanmugam14_interspeech.html", "abstract": "The most popular method for automatic segmentation is embedded reestimation of monophone HMMs after flat start initialization, followed by forced alignment. This method may not yield accurate boundaries. To address this issue, group delay based processing of short-time energy (STE) is performed on the speech signal to obtain syllable boundaries. The syllable boundaries are accurate, but there are a number of spurious insertions as the text transcription is not used during segmentation. The boundaries obtained using group delay segmentation in the vicinity of the HMM syllable boundaries are used as correct boundaries to reestimate the monophone HMM models, where the monophone HMMs are restricted to the syllable boundaries rather than the whole utterance. The reestimated boundaries are again compared with the group delay boundaries and corrected again. Essentially signal processing for detecting boundaries and statistical segmentation for acoustic modelling work in tandem to obtain accurate segmentation at both phoneme and syllable levels. Considering phones and syllables as basic units, HMM based speech synthesis systems (HTS) are built with the proposed segmentation method. Listening tests indicate that there is an improvement in the quality of synthesis.", "title": "A hybrid approach to segmentation of speech using group delay processing and HMM based embedded reestimation"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/koutsogiannaki14_interspeech.html", "abstract": "State of the art objective measures for quantifying voice quality mostly consider estimation of features extracted from the magnitude spectrum. Assuming that speech is obtained by exciting a minimum-phase (vocal tract filter) and a maximum-phase component (glottal source), the amplitude spectrum cannot capture the maximum phase characteristics. Since voice quality is connected to the glottal source, the extracted features should be linked with the maximum-phase component of speech. This work proposes a new metric based on the phase spectrum for characterizing the maximum-phase component of the glottal source. The proposed feature, the Phase Distortion Deviation, reveals the irregularities of the glottal pulses and therefore, can be used for detecting voice disorders. This is evaluated in a ranking problem of speakers with spasmodic dysphonia. Results show that the obtained ranking is highly correlated with the subjective ranking provided by doctors in terms of overall severity, tremor and jitter. The high correlation of the suggested feature with different metrics reveals its ability to capture voice irregularities and highlights the importance of the phase spectrum in voice quality assessment.", "title": "The importance of phase on voice quality assessment"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/vijayan14_interspeech.html", "abstract": "The objective of this work is to study the speaker-specific nature of analytic phase of speech signals. Since computation of analytic phase suffers from phase wrapping problem, we have used its derivative- the instantaneous frequency for feature extraction. The cepstral coefficients extracted from smoothed subband instantaneous frequencies (IFCC) are used as features for speaker verification. The performance of IFCC features is evaluated on NIST-2003 speaker recognition evaluation database and is compared with baseline mel-frequency cepstral coefficients (MFCC). The performance of IFCC features is observed to be comparable with MFCC features in terms of equal error rates and minimum detection cost function values. Different strategies for evaluating the speaker verification performance of IFCC and MFCC are explored and it is found that the evaluation based on cosine similarity delivers better performance than other strategies under consideration.", "title": "Feature extraction from analytic phase of speech signals for speaker verification"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/sanchez14b_interspeech.html", "abstract": "Current speaker verification systems are vulnerable to advanced speech manipulation techniques such as voice conversion and speaker adaptation for TTS systems. Effective anti-spoofing systems that allow the discrimination between human and synthetic impostors have been developed. However, many of them still present two main drawbacks: speaker dependency and, more importantly, counterfeiting technique dependency. Thus, getting a universal synthetic speech detector (SSD) remains an open issue. This paper explores the feasibility of such a system using a statistical classifier for human and synthetic speech. Provided the great diversity of counterfeiting techniques, we have chosen to model a variety of state-of-the-art minimum-phase vocoders, creating imposter synthetic signals by copy-synthesis. Two speech parameter sets are used: MFCCs as a canonical baseline and relative phase shift (RPS) based parameterization. Phase related parameters allow synthetic speech detection based on the presumably different phase structures of the human and synthetic signals due to the fact that most speech synthesis and conversion techniques disregard phase information. The results of the experiments show that speaker independent classifiers perform very well for every vocoder. Cross-vocoder experiments show that the system is highly dependent on the type of vocoder, and that RPS parameterization performs better than MFCC for multi-vocoder models.", "title": "A cross-vocoder study of speaker independent synthetic speech detection using phase information"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/yang14c_interspeech.html", "abstract": "We investigate the use of intrinsic spectral analysis (ISA) for query-by-example spoken term detection (QbE-STD). In the task, spoken queries and test utterances in an audio archive are converted to ISA features, and dynamic time warping is applied to match the feature sequence in each query with those in test utterances. Motivated by manifold learning, ISA has been proposed to recover from untranscribed utterances a set of nonlinear basis functions for the speech manifold, and shown with improved phonetic separability and inherent speaker independence. Due to the coarticulation phenomenon in speech, we propose to use temporal context information to obtain the ISA features. Gaussian posteriorgram, as an efficient acoustic representation usually used in QbE-STD, is considered a baseline feature. Experimental results on the TIMIT speech corpus show that the ISA features can provide a relative 13.5% improvement in mean average precision over the baseline features, when the temporal context information is used.", "title": "Intrinsic spectral analysis based on temporal context features for query-by-example spoken term detection"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/hout14_interspeech.html", "abstract": "We present improvements to a keyword spotting (KWS) system that operates in highly adverse channel conditions with very low signal-to-noise ratio levels. We employ a system combination approach by combining the outputs of multiple large vocabulary continuous speech recognition (LVCSR) systems. These systems are complementary thanks to different design decisions across all levels of information: three speech activity detections systems; a wide range of front-end signal processing features (standard cepstral and filter-bank features, noise-robust features and multi-layer perceptron features); three statistical acoustic model types (Gaussian mixtures models, deep and convolutional neural networks); two keyword search strategies (word-based and phone-based). We explore the scenario where the keywords are known in advance by adding them to the language model and assigning higher weights to n-grams with keywords in them. The scores of each individual system are fused by a logistic-regression based classifier to produce the final system combination output. We present the performance of our system in the Phase III evaluations of DARPAs Robust Automatic Transcription of Speech (RATS) program for Levantine Arabic and Farsi conversational speech corpora.", "title": "Recent improvements in SRI's keyword detection system for noisy audio"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/makino14_interspeech.html", "abstract": "In spoken term detection (STD) systems, approximate subword-level matching of query term and automatically transcribed spoken documents is often employed for its reasonable accuracy and efficiency. However, high out-of-vocabulary (OOV) rate often degrades the subword-level recognition accuracy and affect the STD performance. This paper describes the usage of new expanded acoustic representations of subword sequence for improved scoring between OOV query term and subword-unit transcription. Each subword is expanded in corresponding subword's HMM states and each state is represented as a new acoustic structural feature, a distribution-distance vector (DDV). The proposed DDV representation and scoring is easily combined with two typical baseline STD approaches: a DTW-based approximate matching with subword-level acoustic dissimilarity measure and a lattice-based confidence scoring of subword n-grams. The experimental result showed that the proposed DDV-based scoring method significantly outperforms the simple DTW-scoring baseline with very little increase in the required search time. The combination of the DDV-based scoring with the confidence-based scoring showed the complementary effect and attained the best STD performance compared with the NTCIR-10 SpokenDoc2(SDPWS) submitted results when only the NTCIR reference automatic transcript is used. A preliminary experiment with spoken query terms also showed that the significant improvement for OOV queries.", "title": "Utilizing state-level distance vector representation for improved spoken term detection by text and spoken queries"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/pappagari14_interspeech.html", "abstract": "The objective of this work is to explore a novel unsupervised framework, using Restricted Boltzmann machines, for Spoken Word Retrieval (SWR). In the absence of labelled speech data, SWR is typically performed by matching sequence of feature vectors of query and test utterances using dynamic time warping (DTW). In such a scenario, performance of SWR system critically depends on representation of the speech signal. Typical features, like mel-frequency cepstral coefficients (MFCC), carry significant speaker-specific information, and hence may not be used directly in SWR system. To overcome this issue, we propose to capture the joint density of the acoustic space spanned by MFCCs using Gaussian-Bernoulli restricted Boltzmann machine (GBRBM). In this work, we have used hidden activations of the GBRBM as features for SWR system. Since the GBRBM is trained with speech data collected from large number of speakers, the hidden activations are more robust to the speaker-variability compared to MFCCs. The performance of the proposed features is evaluated on Telugu broadcast news data, and an absolute improvement of 12% was observed compared to MFCCs.", "title": "Unsupervised spoken word retrieval using Gaussian-bernoulli restricted boltzmann machines"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/george14_interspeech.html", "abstract": "The paper proposes an unsupervised framework to address the problem of spotting spoken terms in large speech databases. A two-stage retrieval mechanism is used to perform spoken term detection. A very efficient Bag of Acoustic Words (BoAW) index is created for quick retrieval of relevant documents. Using an N-gram approach, the optimum choice of acoustic dictionary that best describes the document is obtained. Once a quick reduction in search space is achieved in the first phase, the results are fed to the second stage of the retrieval engine. Here, a computationally optimised variant of dynamic programming, called Non-Segmental Dynamic Time Warping (NS-DTW), is used to further prune the results. All the experiments are conducted on MediaEval 2012 dataset. Performance is evaluated at the output of each stage, and the optimum parameters are obtained. We show that the cascade of these two stages helps in reducing the probable search space, which translates to higher search speeds, while ensuring comparable performance. The significance of the indexing framework is proved by its comparison against a random selection system.", "title": "Unsupervised query-by-example spoken term detection using bag of acoustic words and non-segmental dynamic time warping"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/li14g_interspeech.html", "abstract": "As a further step of our previous work, this paper focuses on how to promote the multilingual spoken term detection (STD) system by the use of shared-hidden-layer multilingual DNN (SHL-MDNN). Seven languages namely Arabic, English, German, Japanese, Korean, Mandarin and Spanish are used in our experiments. Compared with our original multilingual STD system, which is based on Subspace GMMs (SGMMs), the resulting system reduces the average equal error rate (EER) on seven languages by 17.2%.   Our STD system is also evaluated under low-resource conditions in this paper. We choose Mandarin and English as two target languages and simulate different degrees of available resources. The experimental results show that with the help of cross-lingual model transfer, our STD system can be elevated a lot in low-resource settings. To further improve the performance, we also attempt to use dropout strategy during the process of cross-lingual model transfer. However, no significant improvement can be observed in our experiments. This indicates the dropout method is not so effective on cross-lingual model transfer task.As a further step of our previous work, this paper focuses on how to promote the multilingual spoken term detection (STD) system by the use of shared-hidden-layer multilingual DNN (SHL-MDNN). Seven languages namely Arabic, English, German, Japanese, Korean, Mandarin and Spanish are used in our experiments. Compared with our original multilingual STD system, which is based on Subspace GMMs (SGMMs), the resulting system reduces the average equal error rate (EER) on seven languages by 17.2%.   Our STD system is also evaluated under low-resource conditions in this paper. We choose Mandarin and English as two target languages and simulate different degrees of available resources. The experimental results show that with the help of cross-lingual model transfer, our STD system can be elevated a lot in low-resource settings. To further improve the performance, we also attempt to use dropout strategy during the process of cross-lingual model transfer. However, no significant improvement can be observed in our experiments. This indicates the dropout method is not so effective on cross-lingual model transfer task.", "title": "An empirical study of multilingual and low-resource spoken term detection using deep neural networks"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/schulam14_interspeech.html", "abstract": "Keyword discovery is an unsupervised technology that can help to process collections of speech and capture repeated patterns. This technology becomes useful and provides solution for unsupervised content analysis tasks, especially when the acoustic and lexical characteristics are not known in advance or there is little or no data to model these characteristics via statistical models. In these situations, keyword discovery can find potentially important words for further analysis using minimal resources. Unfortunately, keyword discovery performance heavily depends on the quality of the features used to characterize the raw signal and the alignment algorithm used to find similar feature subsequences. It is not yet fully understood which features and alignment algorithms work well in different scenarios and for different tasks, and there are very few diagnostic techniques for improving our understanding. In this paper, we present two diagnostic measurements that can be used to directly assess the quality of alignments between sequences of features independently of the intended use of the alignments downstream. We argue that such diagnostic techniques are valuable for intrinsically assessing speech features and alignment algorithms for keyword detection.", "title": "Diagnostic techniques for spoken keyword discovery"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/kawasaki14_interspeech.html", "abstract": "How to deal with speech recognition errors and out-of-vocabulary (OOV) words, which are referred to as false negative errors, are common challenges in spoken document processing. To deal with them in spoken content retrieval (SCR), the SCR method that incorporated spoken term detection (STD) as the pre-process stage (referred to as STD-SCR) has been proposed. However, the STD-SCR tends to increase false positive errors in compensation for reducing false negative errors. In this work, we propose robust retrieval models for false positive errors by using word co-occurrences. The words that co-occur in a given query are semantically related, so that they are likely to co-occur also in the document to be retrieved. On the other hand, if a word in a given query appears alone in a document, it is more like a false positive. We incorporate this idea into two retrieval models commonly used in the literature, i.e. the vector space model and the query likelihood model. Our experimental result showed our proposed extensions on the retrieval models successfully improved the retrieval performance not only for the STD-SCR but also for the conventional SCR method.", "title": "Robust retrieval models for false positive errors in spoken documents"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/liou14_interspeech.html", "abstract": "It is very attractive but technically very challenging if the user can retrieve photos from a huge collection using high-level personal queries (e.q. \u201cUncle Bill's House\u201d). This paper proposes a set of approaches to achieve the goal assuming only 30% of the photos are annotated by sparse and spontaneously spoken descriptions when the photos are taken. We fuse the visual features (visual words plus global visual concepts from Columbia 374 detector) with the sparse speech features and train latent topics for the photos in the collection using non-negative matrix factorization. The retrieved results are then enhanced by two-layer mutually reinforced random walk based on different types of features. In this way it becomes possible to retrieve photos without speech annotation or with sparse annotations regarding different categories of information (e.g. where and who) because of the fused visual/speech features and jointly trained latent topics. Very encouraging results were obtained in initial experiments.", "title": "Semantic retrieval of personal photos using matrix factorization and two-layer random walk fusing sparse speech annotations with visual features"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/gravier14_interspeech.html", "abstract": "The paper presents a system to create audio thumbnails of spoken content, i.e., short audio summaries representative of the entire content, without resorting to a lexical representation. As an alternative to searching for relevant words and phrases in a transcript, unsupervised motif discovery is used to find short, word-like, repeating fragments at the signal level without acoustic models. The output of the word discovery algorithm is exploited via a maximum motif coverage criterion to generate a thumbnail in an extractive manner. A limited number of relevant segments are chosen within the data so as to include the maximum number of motifs while remaining short enough and intelligible. Evaluation is performed on broadcast news reports with a panel of human listeners judging the quality of the thumbnails. Results indicate that motif-based thumbnails stand between random thumbnails and ASR-based keywords, however still far behind thumbnails and keywords humanly authored.", "title": "Audio thumbnails for spoken content without transcription based on a maximum motif coverage criterion"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/garcia14_interspeech.html", "abstract": "In this work, we present an approach for semantically based search for similar segments of a speech task, i.e., the search for audio segments in a row audio repository that are semantically related to the audio segment given by a user. Our approach is based on the lexical representation of segments of words that are enriched by semantic relations. We have studied different distance measures and the lexical/semantic representation of the segments. We present experiments for a task of recorded dialogs between students talking about whatever they want, which is a semantically unbounded task. The results, which are encouraging, indicate the potential advantages of using this approach to address this problem.", "title": "Semantically based search in a social speech task"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/mittal14b_interspeech.html", "abstract": "Laughter in speech has been studied mostly relying upon the spectral representation like formants and harmonics derived from the short-time spectrum. Significant changes appear to take place in the characteristics of glottal source of excitation during the production of laughter, but these changes have not been explored much. In this study, we examine the changes in the glottal vibration characteristics in laughter production, using the electroglottograph (EGG) signals. The excitation source characteristics are also examined from the corresponding acoustic signal, using a modified zero-frequency filtering method. Changes are examined in three categories, namely, normal speech, laughed-speech and nonspeech-laugh. Results using both EGG and acoustic signals are similar. The closed phase quotient in each glottal cycle is observed to decrease more for nonspeech-laugh calls than laughed-speech, with reference to normal speech. Correspondingly, the instantaneous fundamental frequency, hence pitch, increases more for nonspeech-laugh.", "title": "Study of changes in glottal vibration characteristics during laughter"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/ntalampiras14_interspeech.html", "abstract": "This work presents a novel framework for the automatic assessment of the unpleasantness caused by audio events to a human listener which is a relatively new research problem. Mel-frequency cepstral coefficients and temporal modulation parameters were employed to characterize 75 sound stimuli varying from animal calls to baby cries. The final assessment is made by means of a clustering scheme realized by Gaussian mixture models. The proposed framework leads to the best performance in terms of mean squared error and correlation between predicted and measured unpleasantness levels reported so far in the literature.", "title": "On predicting the unpleasantness level of a sound event"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/piot14_interspeech.html", "abstract": "Today, Embodied Conversational Agents (ECAs) are emerging as natural media to interact with machines. Applications are numerous and ECAs can reduce the technological gap between people by providing user-friendly interfaces. Yet, ECAs are still unable to produce social signals appropriately during their interaction with humans, which tends to make the interaction less instinctive. Especially, very little attention has been paid to the use of laughter in human-avatar interactions despite the crucial role played by laughter in human-human interaction. In this paper, a method for predicting the most appropriate moment for laughing for an ECA is proposed. Imitation learning via a structured classification algorithm is used in this purpose and is shown to produce a behavior similar to humans' on a practical application: the yes/no game.", "title": "Predicting when to laugh with structured classification"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/weiss14_interspeech.html", "abstract": "Three-person telephone conferences of unacquainted people are conducted by means of pre-defined scenarios providing individual information and goals. Interlocutors rate the likeability of each other after a training session as well as after the actual conference. The recordings of the conferences are manually annotated concerning speaker's verbal contribution, pauses, and back-channels. Regression analysis reveals likeability ratings after the conference to be dominated by the ratings before the conversation, but simple parameters like number of turns also contribute significantly to a descriptive model. The regression model for ratings averaged for both interlocutors provide a similar fit as the one for pair-wise ratings. Exchanging of the manually obtained parameters by automatically estimated values still results in significant regressions, indicating facilitation for future research.", "title": "Conversational structures affecting auditory likeability"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/avanzi14b_interspeech.html", "abstract": "This paper presents a preliminary study whose main aim is to characterize four distinct speaking styles according to a limited set of prosodic features, including the length of prosodic phrases (AP and IP), the distribution of stressed syllables, pitch register span, the duration of silent pauses, etc. The analysis was performed using semi-automatic procedures on a corpus consisting of 30 minutes of speech per style. The study focuses on four styles, all of which are \u201covertly addressed to a given audience\u201d, but differ as to the nature of the audience (adults vs. children) and the desired impact of the address (\u201cimportance of being understood and convincing, or not\u201d). Data analysis reveals that (a) dictation (addressed to children) and political speeches (addressed to adults) are different to the two other speaking styles (reading of novels and fairy tales) with respect to a specific set of prosodic cues; while (b) the speeches addressed to children differ from the ones addressed to adults, with respect to another set of prosodic cues (especially pitch register span). These results have an interesting practical application: refining the design of pre-processing prosodic modules in a text-to-speech system, in order to improve the expressivity of synthesized speech.", "title": "Towards the adaptation of prosodic models for expressive text-to-speech synthesis"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/matsumiya14_interspeech.html", "abstract": "Most automatic speech recognition systems existing today are still limited to recognizing what is being said, without being concerned with how it is being said. On the other hand, research on emotion recognition from speech has recently gained considerable interest, but how those emotions could be expressed in text-based communication has not been widely investigated. Our long-term goal is to construct expressive speech-to-text systems that conveys all information from acoustic speech, including verbal message, emotional state, speaker condition, and background noise, into unified text-based communication. In this preliminary study, we start with developing a system that can convey emotional speech into text-based communication by way of text balloons. As there exist many possible ways to generate the text balloons, we propose to utilize linguistic and acoustic features based on comic books and anime films. Experimental results reveal that expressive text is more preferable than static text, and the system is able to estimate the shape of text balloons with 87.01% accuracy.", "title": "Data-driven generation of text balloons based on linguistic and acoustic features of a comics-anime corpus"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/tseng14_interspeech.html", "abstract": "We compare the F0 output and chunking size of speech units of L1 English, TW L2 English and L1 Mandarin to see what some of the major intrinsic prosodic difference could be, what prosodic features could account for TW L2 English and in what way prosodic transfer occurs. Results show that the fundamental prosodic difference of the two languages is how in the pitch domain English requires sharper high/low contrast by higher-level prosodic units but less such contrast in accentuating lower-level prosodic units whereas Mandarin patterns are the exact opposite. Explanations are provided regarding how TW L2 English differs from L1, why prosodic transfer merits detailed analysis, and why mastering English prosody is especially difficult.", "title": "Learning L2 prosody is more difficult than you realize \u2014 F0 characteristics and chunking size of L1 English, TW L2 English and TW L1 Mandarin"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/truong14b_interspeech.html", "abstract": "In dialogue, it is not uncommon for people to laugh together. This joint laughter often results in overlapping laughter, consisting of an initiating laugh (the first one), and a responding laugh (the second one). In previous studies, we found that overlapping laughs are acoustically different from non-overlapping ones. So far, we have considered overlapping laughs as one category. Consequently, it is unknown whether there are also acoustic differences between initiating laughs and responding laughs. In this paper, we make a distinction between initiating, responding, and non-overlapping laughs and compare their acoustic characteristics. In particular, we will investigate the prosodic relations between initiating and responding laughs. Do these relations point to a form of accommodation and mimicry? To what extent are initiating and responding laughs paired to each other? The analyses were performed on two speech corpora containing spontaneous conversations between two speakers. Results show indications that initiating and responding laughs share several similar acoustic features that point towards accommodation and mimicry mechanisms.", "title": "Investigating prosodic relations between initiating and responding laughs"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/prylipko14_interspeech.html", "abstract": "To obtain a more human-like interaction with technical systems, those have to be adaptable to the users individual preferences, and current emotional state. In human-human interaction the behaviour of the speaker is characterised by semantic and prosodic cues, given (among other indicators) as short feedback signals. These so called filled pauses minimally convey certain dialogue functions such as attention, understanding, confirmation, or other attitudinal reactions. These signals play a valuable role in the progress and coordination of interaction. Hereby, the first step enabling an automatic system to react on these signals is the detection of them within the users utterances. This is a quite complex task, as the filled pauses are phonetically short, consisting mostly only of one vowel and one consonant. In this paper we present our methods to detect filled pauses in a naturalistic interaction utilising the LAST MINUTE corpus. We used an SVM classifier and improved the results further, by applying a Gaussian filter to infer temporal context information and performing a morphological opening to filter false alarms. We obtained recall of 70%, precision of 55%, and AUC of 0.94.", "title": "Application of image processing methods to filled pauses detection from spontaneous speech"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/kakouros14_interspeech.html", "abstract": "Various studies have examined the acoustic features in infant directed speech (IDS) and adult directed speech (ADS). However, there are few speech corpora with prominence annotation from multiple listeners or analysis of the acoustic properties of the stressed versus unstressed words, most studies and corpora focusing on syllabic stress. In order to fill this gap, the current study analyzes the acoustic properties of sentence stress in a corpus of English IDS. More specifically, the work is one of the first analyzing IDS as perceived by adult listeners, providing inter-annotator agreement ratings and an analysis of the acoustic correlates of sentence stress with regard to the most important prosodic features encountered in the literature: fundamental frequency, intensity, word duration, and spectral tilt. The analysis shows that all of the analyzed features correlate with the perception of stress, indicating that the sentential prominence in IDS is conveyed by similar acoustic characteristics that are known to be relevant for stress perception in ADS.", "title": "Perception of sentence stress in English infant directed speech"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/madzlan14_interspeech.html", "abstract": "This paper reports a study of attitude manifestations in video blogs. We describe the manual annotation of speaker attitudes in a corpus of over 130 video blogs and present an analysis of prosodic and visual cues in relation to attitude states. We use machine learning techniques for the automatic prediction of attitudes from prosodic and visual features in video blogs and compare the performance of prosodic and visual feature sets.", "title": "Automatic recognition of attitudes in video blogs \u2014 prosodic and visual feature analysis"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/katerenchuk14b_interspeech.html", "abstract": "Understanding interpersonal relationships provides important context in understanding spoken communication. In addition to increasing knowledge of the social indicators in spoken communication, the automatic recognition of interpersonal relationships has an application in providing structure to social networks. This paper presents exploratory work on the challenging problem of distinguishing family from friends in spontaneous dialogs drawn from the CALLHOME English corpus. We find both acoustic/prosodic and lexical features useful in classifying these relationships. In binary classification experiments, we achieve accuracy of 10.71% absolute improvement over chance (50%) assignment.", "title": "\u201cwas that your mother on the phone?\u201d: classifying interpersonal relationships between dialog participants with lexical and acoustic properties"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/das14_interspeech.html", "abstract": "Speaker verification using limited data is always a challenge for practical implementation as an application. An analysis on speaker verification studies for an i-vector based method using Mel-Frequency Cepstral Coefficient (MFCC) feature shows that the performance drops drastically as the duration of test data is reduced. This decrease in performance is due to insufficient phonetic coverage when we capture only the vocal tract feature. However the same can be improved if some source characteristics are taken into consideration. This paper attempts to improve the speaker verification performance using source characteristics. A recently proposed characterization of the voice source signal called the discrete cosine transform of the integrated linear prediction residual (DCTILPR) has been found to be useful as a speaker-specific feature. Speaker verification is performed over short test utterances in the NIST 2003 database using both the DCTILPR and MFCC features, and their score-level combination is found to give a significant performance improvement over the system using only the MFCC features.", "title": "Combining source and system information for limited data speaker verification"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/diez14_interspeech.html", "abstract": "Phone Log-Likelihood Ratio (PLLR) features have been recently introduced as an effective way of making use of frame-level phone posteriors in language and speaker recognition systems. In this paper, a deep insight into PLLR features is made and further evidence of the usefulness of these features in spoken language recognition tasks is provided, with a new set of experiments carried out on the NIST 2007 LRE dataset, combining the latest progresses made in optimizing the features. PLLR features are projected into a subspace that enhances the information retrieved by the system. Then, dimensionality reduction is performed on the projected subspace by means of Principal Component Analysis, and shifted deltas are computed on the reduced features to optimize performance. Figures attained are among the best reported so far on the NIST 2007 LRE dataset.", "title": "New insight into the use of phone log-likelihood ratios as features for language recognition"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/ganapathy14_interspeech.html", "abstract": "The language identification (LID) task in the Robust Automatic Transcription of Speech (RATS) program is challenging due to the noisy nature of the audio data collected over highly degraded radio communication channels as well as the use of short duration speech segments for testing. In this paper, we report the recent advances made in the RATS LID task by using bottleneck features from a convolutional neural network (CNN). The CNN, which is trained with labelled data from one of target languages, generates bottleneck features which are used in a Gaussian mixture model (GMM)-ivector LID system. The CNN bottleneck features provide substantial complimentary information to the conventional acoustic features even on languages not seen in its training. Using these bottleneck features in conjunction with acoustic features, we obtain significant improvements (average relative improvements of 25% in terms of equal error rate (EER) compared to the corresponding acoustic system) for the LID task. Furthermore, these improvements are consistent for various choices of acoustic features as well as speech segment durations.", "title": "Robust language identification using convolutional neural network features"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/yu14c_interspeech.html", "abstract": "In state-of-the-art speaker recognition system, universal background model (UBM) plays a role of acoustic space division. Each Gaussian mixture of trained UBM represents one distinct acoustic region. The posterior probabilities of features belonging to each region are further used as core components of Baum-Welch statistics. Therefore, the quality of estimated Baum-Welch statistics depends highly on how acoustic regions are separable with each other. In this paper, we propose to transform the front end acoustical features into a space where the separability of mixtures of trained UBM can be optimized. To achieve this, an UBM was first trained from the acoustical features and a transformation matrix is estimated using linear discriminant analysis (LDA) by treating each mixture of trained UBM as independent class. Therefore, the proposed method named as UBM-based LDA (uLDA) does not require any speaker labels or other supervised information. The obtained transformation matrix is then applied to acoustic features for i-Vector extraction. Experimental results on the male part of core conditions of NIST SRE 2010 dataset confirmed the improved performance using proposed method.", "title": "Acoustic feature transformation using UBM-based LDA for speaker recognition"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/mak14_interspeech.html", "abstract": "This paper proposes a mixture of SNR-dependent PLDA models to provide a wider coverage on the i-vector spaces so that the resulting i-vector/PLDA system can handle test utterances with a wide range of SNR. To maximise the coordination among the PLDA models, they are trained simultaneously via an EM algorithm using utterances contaminated with noise at various levels. The contribution of a training i-vector to individual PLDA models is determined by the posterior probability of the utterance's SNR. Given a test i-vector, the marginal likelihoods from individual PLDA models are linear combined based on the the posterior probabilities of the test utterance and the targetspeaker's utterance. Verification scores are the ratio of the marginal likelihoods. Results based on NIST 2012 SRE suggest that this soft-decision scheme is particularly suitable for the situations where the test utterances exhibit a wide range of SNR.", "title": "SNR-dependent mixture of PLDA for noise robust speaker verification"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/sadjadi14_interspeech.html", "abstract": "With the advent of i-vectors, linear discriminant analysis (LDA) has become an integral part of many state-of-the-art speaker recognition systems. Here, LDA is primarily employed to annihilate the non-speaker related (e.g., channel) directions, thereby maximizing the inter-speaker separation. The traditional approach for computing the LDA transform uses parametric representations for both intra- and inter-speaker scatter matrices that are based on the Gaussian distribution assumption. However, it is known that the actual distribution of i-vectors may not necessarily be Gaussian, and in particular, in the presence of noise and channel distortions. Motivated by this observation, we present an alternative non-parametric discriminant analysis (NDA) technique that measures both the within- and between-speaker variation on a local basis using the nearest neighbor rule. The effectiveness of the NDA method is evaluated in the context of noisy speaker recognition tasks using speech material from the DARPA Robust Automatic Transcription of Speech (RATS) program. Experimental results indicate that the NDA is more effective than the traditional parametric LDA for speaker recognition under noisy and channel degraded conditions.", "title": "Nearest neighbor discriminant analysis for robust speaker recognition"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/liu14e_interspeech.html", "abstract": "Extractive summarization is intended to automatically select a set of representative sentences from a text or spoken document that can concisely express the most important topics of the document. Language modeling (LM) has been proven to be a promising framework for performing extractive summarization in an unsupervised manner. However, there remain two fundamental challenges facing existing LM-based methods. One is how to construct sentence models involved in the LM framework more accurately without resorting to external information sources. The other is how to additionally take into account the sentence-level structural relationships embedded in a document for important sentence selection. To address these two challenges, in this paper we explore a novel approach that generates overlapped clusters to extract sentence relatedness information from the document to be summarized, which can be used not only to enhance the estimation of various sentence models but also to allow for the sentence-level structural relationships for better summarization performance. Further, the utilities of our proposed methods and several state-of-the-art unsupervised methods are analyzed and compared extensively. A series of experiments conducted on a Mandarin broadcast news summarization task demonstrate the effectiveness and viability of our method.", "title": "Enhanced language modeling for extractive speech summarization with sentence relatedness information"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/morchid14b_interspeech.html", "abstract": "The performance of Automatic Speech Recognition (ASR) systems drops dramatically when used in noisy environments. Speech analytics suffer from this poor quality of automatic transcriptions. In this paper, we seek to identify themes from dialogues of telephone conversation services using multiple topic-spaces estimated with a Latent Dirichlet Allocation (LDA) approach. This technique consists in estimating several topic models that offer different views of the document. Unfortunately, such a multi-model approach also introduces additional variabilities due to the model diversity. We propose to extract the useful information from the full model-set by using an i-vector based approach, previously developed in the context of speaker recognition. Experiments are conducted on the DECODA corpus, that contains records from the call center of the Paris Transportation Company. Results show the effectiveness of the proposed representation paradigm, our identification system reaching an accuracy of 84.7%, with a gain of 3.3 points compared to the baseline.", "title": "I-vector based representation of highly imperfect automatic transcriptions"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/lai14b_interspeech.html", "abstract": "This paper investigates how prosodic features can be used to augment lexical features for meeting summarization. Automatic detection of summary-worthy content using non-lexical features, like prosody, has generally focused on features calculated over dialogue acts. However, a salient role of prosody is to distinguish important words within utterances. To examine whether including more fine grained prosodic information can help extractive summarization, we perform experiments incorporating lexical and prosodic features at different levels. For ICSI and AMI meeting corpora, we find that combining prosodic and lexical features at a lower level has better AUROC performance than adding in prosodic features derived over dialogue acts. ROUGE F-scores also show the same pattern for the ICSI data. However, the differences are less clear for the AMI data where the range of scores is much more compressed. In order to understand the relationship between the generated summaries and differences in standard measures, we look at the distribution of extracted content over meeting as well as summary redundancy. We find that summaries based on dialogue act level prosody better reflect the amount of human annotated summary content in meeting segments, while summaries derived from prosodically augmented lexical features exhibit less redundancy.", "title": "Incorporating lexical and prosodic information at different levels for meeting summarization"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/bouallegue14_interspeech.html", "abstract": "The main objective of this paper is to identify themes from dialogues of telephone conversations in a real-life customer care service. In order to capture significant semantic content in spite of high expression variability, features are extracted in a large number of hidden spaces constructed with a Latent Dirichlet Allocation (LDA) approach. Multiple views of a spoke document can then be represented with several hidden topic models. Nonetheless, the model diversity due to the multi-model approach introduces a new type of variability. An approach is proposed based on features extracted in a common homogenous subspace with the purpose of reducing the multi-span representation variability. A Gaussian Mixture Model subspace model, inspired by previous work on speaker identification, is proposed for theme identification. This representation, novel for theme classification, is compared with the direct application of multiple topic-model representations. Experiments are reported using a corpus collected in the call center of the Paris Transportation Service. Results show the effectiveness of the proposed representation paradigm with a theme identification accuracy of 78.8%, showing a significant improvement with respect to previous results on the same corpus.", "title": "Subspace Gaussian mixture models for dialogues classification"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/bouallegue14b_interspeech.html", "abstract": "The main objective of this paper is to identify themes from dialogues of telephone conversations in a real-life customer care service. In this task, the word semantic variability contained in these conversations may impact the classification performance by retaining the noise in their vectorial representation. In this article, we propose an original method to compensate this semantic variability using the Factor Analysis (FA) paradigm, initially designed for speech processing tasks to compensate the acoustic variability, mainly in Speaker Verification (SV) and Automatic Speech Recognition (ASR). In our proposal, we used the FA paradigm to estimate the semantic variability as an additive component located in a subspace of low dimension (with respect to the super-vector space). This additive semantic variability is estimated in Factor Analysis model space. From this estimation, a specific vector transformation is obtained and is applied to vectors of dialogue representation. Experiments are reported using a corpus collected in the call center of the Paris Transportation Service. Results show the effectiveness of the proposed representation paradigm with a theme identification accuracy of 80.0%, showing a significant improvement with respect to previous results on the same corpus.", "title": "Factor analysis based semantic variability compensation for automatic conversation representation"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/bouchekif14_interspeech.html", "abstract": "In this paper, we introduce the notion of speech cohesion for topic segmentation of a spoken content. The aim is to integrate speaker information and lexical information within a single cohesion value. Based on a lexical cohesion system, we propose an approach that directly integrates the speaker distribution when processing the cohesion. A potential boundary is effective if the joint distribution of terms and speakers is different enough from one side of the boundary to the other. Beyond speaker distribution, we also propose to take into account speaker identification and to confront speaker identities to identities mentioned in the spoken content in order to reinforce cohesion of a topic segment. Experiments run on three corpora of various Broadcasts News formats collected from 9 French TV channels, show a significant improvement in the overall topic segmentation process.", "title": "Speech cohesion for topic segmentation of spoken contents"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/huang14d_interspeech.html", "abstract": "We conducted a comparative analytic study on the context-dependent Gaussian mixture hidden Markov model (CD-GMM-HMM) and deep neural network hidden Markov model (CD-DNN-HMM) with respect to the phone discrimination and the robustness performance. We found that the DNN can significantly improve the phone recognition performance for every phoneme with 15.6% to 39.8% relative phone error rate reduction (PERR). It is particularly good at discriminating certain consonants, which are found to be \u201chard\u201d in the GMM. On the robustness side, the DNN outperforms the GMM at all SNR levels, across different devices, and under all speaking rate with nearly uniform improvement. The performance gap with respect to different SNR levels, distinct channels, and varied speaking rate remains large. For example, in CD-DNN-HMM, we observed 1~2% performance degradation per 1dB SNR drop; 20~25% performance gap between the best and least well performed devices; 15~30% relative word error rate increase when the speaking rate speeds up or slows down by 30% from the \u201csweet\u201d spot. Therefore, we conclude the robustness remains to be a major challenge in the deep learning acoustic model. Speech enhancement, channel normalization, and speaking rate compensation are important research areas in order to further improve the DNN model accuracy.", "title": "A comparative analytic study on the Gaussian mixture and context dependent deep neural network hidden Markov models"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/bacchiani14_interspeech.html", "abstract": "We propose an algorithm that allows online training of a context dependent DNN model. It designs a state inventory based on DNN features and jointly optimizes the DNN parameters and alignment of the training data. The process allows flat starting a model from scratch and avoids any dependency on a GMM acoustic model to bootstrap the training process. A 15k state model trained with the proposed algorithm reduced the error rate on a mobile speech task by 24% compared to a system bootstrapped from a CI GMM and by 16% compared to a system bootstrapped from a CD GMM system.", "title": "Asynchronous, online, GMM-free training of a context dependent acoustic model for speech recognition"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/jaitly14_interspeech.html", "abstract": "We describe a simple but effective way of using multi-frame targets to improve the accuracy of Artificial Neural Network-Hidden Markov Model (ANN-HMM) hybrid systems. In this approach a Deep Neural Network (DNN) is trained to predict the forced-alignment state of multiple frames using a separate softmax for each of the frames. This is in contrast to the usual method of training a DNN to predict only the state of the central frame. By itself this is not sufficient to improve accuracy of the system significantly. However, if we average the predictions for each frame \u2014 from the different contexts it is associated with \u2014 we achieve state of the art results on TIMIT using a fully connected Deep Neural Network without convolutional architectures or dropout training. On a 14 hour subset of Wall Street Journal (WSJ) using a context dependent DNN-HMM system it leads to a relative improvement of 6.4% on the dev set ( test-dev93) and 9.3% on test set ( test-eval92).", "title": "Autoregressive product of multi-frame predictions can improve the accuracy of hybrid models"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/li14h_interspeech.html", "abstract": "Deep neural network (DNN) obtains significant accuracy improvements on many speech recognition tasks and its power comes from the deep and wide network structure with a very large number of parameters. It becomes challenging when we deploy DNN on devices which have limited computational and storage resources. The common practice is to train a DNN with a small number of hidden nodes and a small senone set using the standard training process, leading to significant accuracy loss. In this study, we propose to better address these issues by utilizing the DNN output distribution. To learn a DNN with small number of hidden nodes, we minimize the Kullback-Leibler divergence between the output distributions of the small-size DNN and a standard large-size DNN by utilizing a large number of un-transcribed data. For better senone set generation, we cluster the senones in the large set into a small one by directly relating the clustering process to DNN parameters, as opposed to decoupling the senone generation and DNN training process in the standard training. Evaluated on a short message dictation task, the proposed two methods get 5.08% and 1.33% relative word error rate reduction from the standard training method, respectively.", "title": "Learning small-size DNN with output-distribution-based criteria"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/deng14c_interspeech.html", "abstract": "Deep learning systems have dramatically improved the accuracy of speech recognition, and various deep architectures and learning methods have been developed with distinct strengths and weaknesses in recent years. How can ensemble learning be applied to these varying deep learning systems to achieve greater recognition accuracy is the focus of this paper. We develop and report linear and log-linear stacking methods for ensemble learning with applications specifically to speech-class posterior probabilities as computed by the convolutional, recurrent, and fully-connected deep neural networks. Convex optimization problems are formulated and solved, with analytical formulas derived for training the ensemble-learning parameters. Experimental results demonstrate a significant increase in phone recognition accuracy after stacking the deep learning subsystems that use different mechanisms for computing high-level, hierarchical features from the raw acoustic signals in speech.", "title": "Ensemble deep learning for speech recognition"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/zhou14_interspeech.html", "abstract": "The analysis of dialogue act is important for computers to understand natural-language dialogues because the dialogue act of an utterance characterizes the speaker's intention. In this paper, we create a new model that adapts Conditional Random Field (CRF) with efficient hierarchical representations of the raw inputs to solve the dialogue act recognition problem. The proposed model has two advantages. First, CRF can model the statistical dependencies between the utterances which carry important information to determine the dialogue act label of an utterance. Second, the hierarchical representations potentially contain some more abstract concepts with greater predictive power. To verify the effectiveness of our model, we compare it with several baseline methods on a dialogue act classification task. The results of the experiments demonstrate that our model performs much better than all the baseline methods.", "title": "Learning conditional random field with hierarchical representations for dialogue act recognition"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/hsu14b_interspeech.html", "abstract": "Past findings on the perception of emotional prosody by individuals with Autism Spectrum Disorder (ASD) have been incongruent, and a main reason is the lack of clarity about the contributions of specific acoustic features to emotion perception. In this study we test the perception of emotional prosody by adolescents with ASD using a recently developed prosody control method based on a bio-informational dimensions (BID) theory of emotion expressions. We synthesized a Mandarin sentence with different voice qualities using an articulatory synthesizer, and then acoustically manipulated their formant dispersion, median pitch and pitch range. With these utterances we compared the ability to perceive body size, emotion and attitude by high-functioning adolescents with ASD, typically developing adolescents and young adults. Results showed that the three groups made similar perceptual judgements, but the sensitivity of adolescents with ASD to the acoustic manipulations was lower than their typically developing peers, who in turn exhibited less sensitivity than young adults. These findings show that individuals with ASD have a reduced rather than a total lack of ability to perceive emotional prosody, suggesting a delay in their developmental trajectory. The findings also demonstrate the effectiveness of the BID-based method in testing perception of emotional prosody.", "title": "Can adolescents with autism perceive emotional prosody?"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/schmidt14_interspeech.html", "abstract": "This study investigates whether age and/or hearing loss influence the perception of the emotion dimensions arousal (calm vs. aroused) and valence (positive vs. negative attitude) in conversational speech fragments. Specifically, this study focuses on the relationship between participants' ratings of affective speech and acoustic parameters known to be associated with arousal and valence (mean F0, intensity, and articulation rate). Ten normal-hearing younger and ten older adults with varying hearing loss were tested on two rating tasks. Stimuli consisted of short sentences taken from a corpus of conversational affective speech. In both rating tasks, participants estimated the value of the emotion dimension at hand using a 5-point scale. For arousal, higher intensity was generally associated with higher arousal in both age groups. Compared to younger participants, older participants rated the utterances as less aroused, and showed a smaller effect of intensity on their arousal ratings. For valence, higher mean F0 was associated with more negative ratings in both age groups. Generally, age group differences in rating affective utterances may not relate to age group differences in hearing loss, but rather to other differences between the age groups, as older participants' rating patterns were not associated with their individual hearing loss.", "title": "Age, hearing loss and the perception of affective utterances in conversational speech"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/yang14d_interspeech.html", "abstract": "In interpersonal interactions, speech and body gesture channels are internally coordinated towards conveying communicative intentions. The speech-gesture relationship is influenced by the internal emotion state underlying the communication. In this paper, we focus on uncovering the emotional effect on the interrelation between speech and body gestures. We investigate acoustic features describing speech prosody (pitch and energy) and vocal tract configuration (MFCCs), as well as three types of body gestures, viz., head motion, lower and upper body motions. We employ mutual information to measure the coordination between the two communicative channels, and analyze the quantified speech-gesture link with respect to distinct levels of emotion attributes, i.e., activation and valence. The results reveal that the speech-gesture coupling is generally tighter for low-level activation and high-level valence, compared to high-level activation and low-level valence. We further propose a framework for modeling the dynamics of speech-gesture interaction. Experimental studies suggest that such quantified coupling representations can well discriminate different levels of activation and valence, reinforcing that emotions are encoded in the dynamics of the multimodal link. We also verify that the structures of the coupling representations are emotion-dependent using subspace-based analysis.", "title": "Analysis of emotional effect on speech-body gesture interplay"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/chappuis14_interspeech.html", "abstract": "Emotional expressions influence memory by both impairing and enhancing attention and perception at encoding, consolidation and recall. In three studies we compare angry, happy and fearful emotional prosodies using a single-word presentation paradigm, and investigate the effects of emotional vocal expressions on immediate and delayed recall for central emotional items and their periphery. Overall the results support a prosody-induced emotional enhancement of memory, as well as a memory facilitation for the periphery of specific emotions. The fact that emotional enhancement of memory by prosody was replicated in three studies suggests a robust effect, which to our best knowledge has not been observed before with vocal expressions.", "title": "When voices get emotional: a study of emotion-enhanced memory and impairment during emotional prosody exposure"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/zellers14_interspeech.html", "abstract": "In a number of languages, intonational patterns at prosodic boundaries are considered to be relevant for turn transition or turn hold. A perception experiment tested the influence of fundamental frequency (F0) peak height and rising final contours on Swedish listeners' judgment about whether a speaker wanted to hold the turn. While F0 peak height, as has been previously shown, did influence listeners' judgments, the end height of rising pitch tails apparently did not influence listeners' judgments about whether a speaker planned to continue talking, even though they showed sensitivity to the differences in a discrimination task. The differences in responses in the tasks, as well as the difference from results found for other languages, may indicate that listeners used comparative prominence to guide their judgments, rather than intonation playing a direct role in the turn-transition system.", "title": "Perception of pitch tails at potential turn boundaries in Swedish"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/fuchs14_interspeech.html", "abstract": "Previous accounts of speech rhythm focus mainly on duration. For example, the normalised Pairwise Variability Index for vocalic intervals (nPVI-V) quantifies relative duration differences between successive vocalic intervals. Prototypical syllable-timing is characterised by small differences in duration, prototypical stress-timing by large differences. However, differences in F0 between vocalic intervals are thought to influence the perception of duration. This paper (1) quantifies the influence of differences in F0 on perceived duration in a perception experiment, and (2) suggests a modified PVI (nPVI-V(dur*F0)) that takes account of this influence. The new nPVI-V(dur*F0) is then applied to a speech corpus of (stress-timed) British English and (syllable-timed) Indian English. The results are compared to the application of the old nPVI-V, which takes into account duration only, to the same data set.", "title": "Towards a perceptual model of speech rhythm: integrating the influence of f0 on perceived duration"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/chen14h_interspeech.html", "abstract": "In this paper we propose a deep neural network to model the conditional probability of the spectral differences between natural and synthetic speech. This allows us to reconstruct the spectral fine structures in speech generated by HMMs. We compared the new stochastic data-driven postfilter with global variance based parameter generation and modulation spectrum enhancement. Our results confirm that the proposed method significantly improves the segmental quality of synthetic speech compared to the conventional methods.", "title": "DNN-based stochastic postfilter for HMM-based speech synthesis"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/kang14_interspeech.html", "abstract": "This paper presents a weighted multi-distribution deep belief network (wMD-DBN) for context-dependent statistical parametric speech synthesis. We have previously proposed the use of MD-DBN for speech synthesis, which models simultaneously both spectrum and fundamental frequency (F0), and has demonstrated the potential to generate high-dimensional spectra with high quality and to produce natural synthesized speech. However, the model showed only mediocre performance on low-dimensional data, such as the F0 and voiced/unvoiced (V/UV) flag, resulting in a vibrating pitch contour in the synthesized voice. To address this problem, this paper investigates the use of an extra weighting vector on the acoustic output layer of the MD-DBN. It reduces the dimensional imbalance between spectrum and pitch parameters by giving different weighting coefficients to the spectrum, F0 and the V/UV flag in the training procedure. Experimental results show that wMD-DBN can generate smoother pitch contours and improve the naturalness of the synthesized speech.", "title": "Statistical parametric speech synthesis using weighted multi-distribution deep belief network"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/fan14_interspeech.html", "abstract": "Feed-forward, Deep neural networks (DNN)-based text-to-speech (TTS) systems have been recently shown to outperform decision-tree clustered context-dependent HMM TTS systems. However, the long time span contextual effect in a speech utterance is still not easy to accommodate, due to the intrinsic, feed-forward nature in DNN-based modeling. Also, to synthesize a smooth speech trajectory, the dynamic features are commonly used to constrain speech parameter trajectory generation in HMM-based TTS [2]. In this paper, Recurrent Neural Networks (RNNs) with Bidirectional Long Short Term Memory (BLSTM) cells are adopted to capture the correlation or co-occurrence information between any two instants in a speech utterance for parametric TTS synthesis. Experimental results show that a hybrid system of DNN and BLSTM-RNN, i.e., lower hidden layers with a feed-forward structure which is cascaded with upper hidden layers with a bidirectional RNN structure of LSTM, can outperform either the conventional, decision tree-based HMM, or a DNN TTS system, both objectively and subjectively. The speech trajectory generated by the BLSTM-RNN TTS is fairly smooth and no dynamic constraints are needed.", "title": "TTS synthesis with bidirectional LSTM based recurrent neural networks"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/raitio14_interspeech.html", "abstract": "This paper studies a deep neural network (DNN) based voice source modelling method in the synthesis of speech with varying vocal effort. The new trainable voice source model learns a mapping between the acoustic features and the time-domain pitch-synchronous glottal flow waveform using a DNN. The voice source model is trained with various speech material from breathy, normal, and Lombard speech. In synthesis, a normal voice is first adapted to a desired style, and using the flexible DNN-based voice source model, a style-specific excitation waveform is automatically generated based on the adapted acoustic features. The proposed voice source model is compared to a robust and high-quality excitation modelling method based on manually selected mean glottal flow pulses for each vocal effort level and using a spectral matching filter to correctly match the voice source spectrum to a desired style. Subjective evaluations show that the proposed DNN-based method is rated comparable to the baseline method, but avoids the manual selection of the pulses and is computationally faster than a system using a spectral matching filter.", "title": "Deep neural network based trainable voice source model for synthesis of speech with varying vocal effort"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/yu14d_interspeech.html", "abstract": "We introduce the computational network (CN), a generalization of popular machine learning models such as deep neural network, recurrent neural network, convolutional neural network, and log linear model that can be expressed as a series of computation steps. We describe the benefits of such generalization and the key operations on the CN.   We further introduce the computational network toolkit (CNTK), a general purpose C++ implementation of computational networks. We describe its architecture and core functionalities and demonstrate that it can construct and learn models of arbitrary topology, connectivity, and recurrence. The toolkit will be released under a modified Microsoft Research license agreement for non-commercial use.", "title": "An introduction to computational networks and the computational network toolkit (invited talk)"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/fernandez14b_interspeech.html", "abstract": "Deep Neural Networks (DNNs) have been shown to provide state-of-the-art performance over other baseline models in the task of predicting prosodic targets from text in a speech-synthesis system. However, prosody prediction can be affected by an interaction of short- and long-term contextual factors that a static model that depends on a fixed-size context window can fail to properly capture. In this work, we look at a recurrent formulation of neural networks (RNNs) that are deep in time and can store state information from an arbitrarily large input history when making a prediction. We show that RNNs provide improved performance over DNNs of comparable size in terms of various objective metrics for a variety of prosodic streams (notably, a relative reduction of about 6% in F0 mean-square error accompanied by a relative increase of about 14% in F0 variance), as well as in terms of perceptual quality assessed through mean-opinion-score listening tests.", "title": "Prosody contour prediction with long short-term memory, bi-directional, deep recurrent neural networks"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/yin14_interspeech.html", "abstract": "In the conventional HMM-based TTS, the micro structure of F0 contour is modeled at the state level via a (clustered) decision tree. However, the decision tree based state-level modeling is difficult to capture the long term structure of speech prosody, say at intonation phrase level, due to its greedy search nature and usually sparse training data for covering a large, combinatorial number of usually long prosodic contexts in a phrase or sentence. In this study, we adopt a finite number of Discrete Cosine Transform (DCT) coefficients to capture the smoothed trend of F0 patterns of intonation phrases and to normalize the variable duration effects in phrase length. We then use DCT smoothed contours to model phrase intonations with a decision tree or a deep neural network (DNN). The remaining details or the residual F0 is then accommodated by training a state-level model in a Hierarchical Prosody Model (HPM) framework. The internal phrase models are then used to predict the intonation phrase F0 contours and then combine it with the predicted state-level F0 residuals to predict final F0 contours. Either the decision tree based or the DNN based F0 predictors, when working together with the state-level F0 residual predictors, outperform the standard, state-level HMM F0 models.", "title": "Modeling DCT parameterized F0 trajectory at intonation phrase level with DNN or decision tree"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/nakashika14_interspeech.html", "abstract": "This paper presents a voice conversion (VC) method that utilizes recently proposed recurrent temporal restricted Boltzmann machines (RTRBMs) for each speaker, with the goal of capturing high-order temporal dependencies in an acoustic sequence. Our algorithm starts from the separate training of two RTRBMs for a source and target speaker using speaker-dependent training data. Since each RTRBM attempts to discover abstractions at each time step, as well as the temporal dependencies in the training data, we expect that the models represent the speaker-specific latent features in the high-order spaces. In our approach, we run conversion from such speaker-specific-emphasized features of the source speaker to those of the target speaker using a neural network (NN), so that the entire network (the two RTRBMs ant the NN) forms a deep recurrent neural network and can be fine-tuned. Through VC experiments, we confirmed the high performance of our method especially in terms of objective criteria in comparison to conventional VC methods such as Gaussian mixture model (GMM)-based approaches.", "title": "High-order sequence modeling using speaker-dependent recurrent temporal restricted boltzmann machines for voice conversion"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/xie14b_interspeech.html", "abstract": "Neural network (NN) based voice conversion, which employs a nonlinear function to map the features from a source to a target speaker, has been shown to outperform GMM-based voice conversion approach [4\u20137]. However, there are still limitations to be overcome in NN-based voice conversion, e.g. NN is trained on a Frame Error (FE) minimization criterion and the corresponding weights are adjusted to minimize the error squares over the whole source-target, stereo training data set. In this paper, we use the idea of sentence optimization based, minimum generation error (MGE) training in HMM-based TTS synthesis, and modify the FE minimization to Sequence Error (SE) minimization in NN training for voice conversion. The conversion error over a training sentence from a source speaker to a target speaker is minimized via a gradient descent-based, back propagation (BP) procedure. Experimental results show that the speech converted by the NN, which is first trained with frame error minimization and then refined with sequence error minimization, sounds subjectively better than the converted speech by NN trained with frame error minimization only. Scores on both naturalness and similarity to the target speaker are improved.", "title": "Sequence error (SE) minimization training of neural network for voice conversion"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/bocquelet14_interspeech.html", "abstract": "Brain-Computer Interfaces (BCIs) usually propose typing strategies to restore communication for paralyzed and aphasic people. A more natural way would be to use speech BCI directly controlling a speech synthesizer. Toward this goal, a prerequisite is the development a synthesizer that should i) produce intelligible speech, ii) run in real time, iii) depend on as few parameters as possible, and iv) be robust to error fluctuations on the control parameters. In this context, we describe here an articulatory-to-acoustic mapping approach based on deep neural network (DNN) trained on electromagnetic articulography (EMA) data recorded synchronously with produced speech sounds. On this corpus, the DNN-based model provided a speech synthesis quality (as assessed by automatic speech recognition and behavioral testing) comparable to a state-of-the-art Gaussian mixture model (GMM), yet showing higher robustness when noise was added to the EMA coordinates. Moreover, to envision BCI applications, this robustness was also assessed when the space covered by the 12 original articulatory parameters was reduced to 7 parameters using deep auto-encoders (DAE). Given that this method can be implemented in real time, DNN-based articulatory speech synthesis seems a good candidate for speech BCI applications.", "title": "Robust articulatory speech synthesis using deep neural networks for BCI applications"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/xu14c_interspeech.html", "abstract": "This study investigates the acoustic characteristics of /th/ lenition in conversational speech of Brunei Mandarin, a variety of Mandarin Chinese. Based on data from 20 Chinese Bruneians, /th/ lenition was found in the third-person pronoun ta /tha/, which is frequently pronounced as ha [ha]. Perceptual judgments, spectrographic analysis and acoustic measurements were conducted to examine the features of this sound change. In comparison with the perceptual judgments, it was found that the spectrographic inspection yielded 83.6% correct classification of [th] and [h] for female speakers and 77.2% for male speakers, indicating there is reasonably high reliability in identification in terms of spectral properties. Results of the acoustic measurements showed that there is an increase in high frequency intensity after the release of the closure for [th] while there is little change in intensity during the frication for [h]. The results showed that the lack of burst and little increase in intensity are reasonably reliable cues for stop lenition.", "title": "Acoustic investigation of /th/ lenition in brunei Mandarin"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/wang14j_interspeech.html", "abstract": "This study aims to evaluate the importance of phonation cues in the acoustic realization of four vocal emotions (happiness, fear, anger, and sadness) in Mandarin Chinese. Our perception experiment confirmed that native listeners could differentiate these emotions. To explain how listeners accomplished the task in the perception experiment, we investigated the acoustic cues used by speakers. An acoustic analysis revealed that prosodic measures such as F0, intensity, and duration failed to separate certain emotions in the acoustic space, and thus could not explain how native listeners can perceive the different emotions. However, by incorporating phonation-related cues in the multi-dimensional scaling acoustic space, different emotions were separated clearly. Principal components analysis further revealed the specific contribution of each acoustic measure. These results also allow some preliminary conjectures on how these acoustic components might represent the underlying emotion dimensions, namely, arousal, valence, and control.", "title": "Mapping emotions into acoustic space: the role of voice quality"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/mahajan14_interspeech.html", "abstract": "More than two thousand auditory cortical spectro-temporal receptive fields (STRFs) of the ferret were analysed by Principal Component Analysis (PCA) to reveal their dominant properties. Results show that cortical levels of mammalian auditory processing enhance relatively low modulation spectral components of the signal around 3 Hz, using relatively broad spectral processing channels of the order of octave or more, evaluating mean spectral values and local spectral slopes within such frequency channels. These observations are consistent with some engineering techniques shown to be effective in machine recognition of speech.", "title": "Principal components of auditory spectro-temporal receptive fields"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/thlithi14_interspeech.html", "abstract": "As part of a project on indexing ethno-musicological audio recordings, segmentation in singer turns automatically appeared to be essential. In this article, we present the problem of segmentation in singer turns of musical recordings and our first experiments in this direction by exploring a method based on the Bayesian Information Criterion (BIC), which are used in numerous works in audio segmentation, to detect singer turns. The BIC penalty coefficient was shown to vary when determining its value to achieve the best performance for each recording. In order to avoid the decision about which single value is best for all the documents, we propose to combine several segmentations obtained with different values of this parameter. This method consists of taking a posteriori decisions on which segment boundaries are to be kept. A gain of 7.1% in terms of F-measure was obtained compared to a standard coefficient.", "title": "Segmentation in singer turns with the Bayesian information criterion"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/watson14_interspeech.html", "abstract": "This study looks at mappings between vocal tract area functions (obtained from MRI scans), vocal tract resonances, and speech formants for five New Zealand English (NZE) speakers. All eleven NZE monophthongs were investigated, for each speaker. Principal component (PC) analysis on the area functions of both the individual speakers and combined speaker set is performed. In all cases the first two PC account for most the variances in the data. The first two PCs of the vocal tract area accounted for between 71\u201386% of the variance across the data for the individual speakers, and each was highly correlated between speakers. Across the combined speaker set the first two PCs accounted for 60.5% of the variance, and remained related to phonetic height and backness. This can clearly be seen when the transformed area function data is compared to the first two vocal tract resonances and speech formants.", "title": "Mappings between vocal tract area functions, vocal tract resonances and speech formants for multiple speakers"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/arndt14_interspeech.html", "abstract": "For service providers, it is important to know the perceived quality of a transmitted speech signal, as this can be an indicator for choosing one operator over the other. To evaluate the perceived quality, several methods have been introduced in the area of Quality of Experience (QoE), such as subjective, instrumental and physiological methods. In the current study, we expose the test participants towards several speech files, while brain activity was recorded with electroencephalography (EEG). Additionally, a subjective quality judgment is obtained after each presentation which is summarized as the mean opinion score (MOS). To validate the selected stimulus corpus, a quality predication algorithm is used which calculates quality scores based on the speech signal. The recorded EEG data were set in relation to the subjective data and show promising results. In contrast to most previous studies in the domain of QoE using EEG, the study at hand uses a standardized test paradigm proposed by the International Telecommunication Unit (ITU).", "title": "A next step towards measuring perceived quality of speech through physiology"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/chen14i_interspeech.html", "abstract": "Based on the noise-replacement paradigm, recent studies showed that vowels carried more perceptional information for sentence intelligibility than consonants. Considering that vowels contain many important acoustic cues for speech perception, this study further assessed the effect of spectral degradation to the intelligibility of Mandarin vowel sentences. Mandarin sentences were processed to generate three types of spectrally degraded [i.e., fundamental frequency (F0) flattened, sinewave synthesized, and noise-vocoded] stimuli. Noise-replacement paradigm was implemented to preserve different amounts of vowel centers and replace the rest with noise. Listening experiments showed that flattening F0 had a minimal effect on the intelligibility of Mandarin vowel sentences, and the harmonic structure within vowels accounted more for the intelligibility of Mandarin vowel sentences. While deleting vowel edges had little influence on the intelligibility of the unprocessed vowel sentences, it had a significantly negative effect on the intelligibility of vowel sentences with spectral degradation.", "title": "Effect of spectral degradation to the intelligibility of vowel sentences"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/berry14b_interspeech.html", "abstract": "Speech sensorimotor adaptation is the short-term learning of modified articulator movements evoked through sensory-feedback perturbations. A common experimental method manipulates acoustic parameters, such as formant frequencies, using real time resynthesis of the participant's speech to perturb auditory feedback. While some studies have examined phrases comprised of vowels, diphthongs, and semivowels, the bulk of research on auditory feedback-driven sensorimotor adaptation has focused on vowels in neutral contexts (/hVd/). The current study investigates coarticulatory influences of adjacent consonants on sensorimotor adaptation. The purpose is to evaluate differences in the adaptation effects for vowels in consonant environments that vary by place and manner of articulation. In particular, we addressed the hypothesis that contexts with greater intra-articulator coarticulation and more static articulatory postures (alveolars and fricatives) offer greater resistance to vowel adaptation than contexts with primarily inter-articulator coarticulation and more dynamic articulatory patterns (bilabials and stops). Participants completed formant perturbation-driven vowel adaptation experiments for varying CVCs. Results from discrete formant measures at the vowel midpoint were generally consistent with the hypothesis. Analyses of more complete formant trajectories suggest that adaptation can also (or alternatively) influence formant onsets, offsets, and transitions, resulting in complex formant pattern changes that may reflect modifications to consonant articulation.", "title": "Consonant context effects on vowel sensorimotor adaptation"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/bailly14_interspeech.html", "abstract": "This paper focuses on the study of the convergence between characteristics of speech segments \u2014 i.e. spectral characteristics of speech sounds \u2014 during live interactions between speaking dyads. The interaction data has been collected using an original verbal game called `verbal dominoes' that provides a dense sampling of the acoustic spaces of the interlocutors. Two methods for characterizing phonetic convergence are here compared. The first one is based on a fine-grained analysis of the spectra of central frames of vowels (LDA) while the second one uses a more global speaker recognition technique (LLR). We show that convergence rates calculated by the two techniques correlate as the number of dominoes increases and that the LDA method well resists to the decrease of training and test material. We finally comment the impact of several factors on the computed convergence rates, i.e. interlocutors' familiarity and sex pairs.", "title": "Assessing objective characterizations of phonetic convergence"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/mandel14_interspeech.html", "abstract": "Listeners can reliably identify speech in noisy conditions, although it is generally not known what specific features of speech are used to do this. We utilize a recently introduced data-driven framework to identify these features. By analyzing listening-test results involving the same speech utterance mixed with many different noise instances, the framework is able to compute the importance of each time-frequency point in the utterance to its intelligibility. This paper shows that a trained model resulting from this framework can generalize to new conditions, successfully predicting the intelligibility of novel mixtures. First, it can generalize to novel noise instances after being trained on mixtures involving the same speech utterance but different noises. Second, it can generalize to novel talkers after being trained on mixtures involving the same syllables produced by different talkers in different noises. Finally, it can generalize to novel phonemes, after being trained on mixtures involving different consonants produced by the same or different talkers in different noises. Aligning the clean utterances in time and then propagating this alignment to the features used in the intelligibility prediction improves this generalization performance further.", "title": "Generalizing time-frequency importance functions across noises, talkers, and phonemes"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/mahajan14b_interspeech.html", "abstract": "Previous research with young adults has shown that temporal (amplitude modulated, AM) cues are sufficient for recognizing speech in quiet but not for speech in noise. Speech perception in noise is more robust when spectral (frequency modulated, FM) cues are provided in addition to AM ones; visual cues (AV) provide an additional benefit. The elderly typically have problems recognizing speech in noise and it has recently been found that FM discrimination is worse in this group. Given this, it may be that elderly participants will not show an FM speech in noise benefit. To test the relative effectiveness of adding FM cues to AM ones for elderly versus young participants we compared auditory only (AO) speech identification of sentences, vowels and consonants in noise with AM and AM+FM presentation conditions. We also evaluated the relative effectiveness of visual cues for the elderly group. Although the elderly had poorer speech recognition performance overall, they showed a comparable visual benefit to the young group. Moreover, contrary to the prediction of a reduced benefit for FM cues, the FM benefit for the elderly was similar to that of young adults. These results were discussed in relation to speech specific auditory processing.", "title": "Does elderly speech recognition in noise benefit from spectral and visual cues?"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/laskowski14_interspeech.html", "abstract": "Stochastic turn-taking models provide stationary estimates of the probability of a conversant's incipient speech activity, given their own and their interlocutors' recent speech activity. Existing research suggests that such models may be conversant specific, and even conversant-discriminative. The present work establishes this explicitly. It is shown that: (1) the conditioning context can be relaxed to exploit speech activity which need not be attributed to specific interlocutors; (2) the same duration of context can yield better results with a more statistically sound framework; and (3) results further improve asymptotically with the consideration of longer conditioning histories. The findings indicate that inter-conversant variability is a major contributor of variability across stochastic turn-taking models.", "title": "On the conversant-specificity of stochastic turn-taking models"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/sakano14_interspeech.html", "abstract": "We investigated a single-ended speech intelligibility estimation method that does not require clean speech reference signal, using the features defined in the ITU-T standard P.563. We selected two sets of features from the P.563 features; the basic nine feature set, and the extended 31 feature set with 22 additional features for more accurate description of the degraded speech. Four hundred noise samples were added to speech, and about 70% of these samples were used to extract the feature sets to train a support vector regression (SVR) model. The trained models were used to estimate the intelligibility for speech degraded with the remaining 30% of unknown noise samples. The proposed method showed a root mean square error (RMSE) value of about 0.16 and correlation with subjective intelligibility of about 0.84 for speech distorted with unknown noise with either of the feature set. These results were higher than the double-sided estimation using frequency-weighed SNR calculated in critical frequency bands, which require the clean reference signal. We believe this level of accuracy proves the proposed method to be applicable to real-time speech quality monitoring in the field.", "title": "Single-ended estimation of speech intelligibility using the ITU p.563 feature set"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/jokinen14b_interspeech.html", "abstract": "In mobile communications, post-processing methods are used to improve the intelligibility of speech in adverse background noise conditions. In this study, post-processing based on modelling the Lombard effect is investigated. The study focuses on comparing different spectral envelope estimation methods together with Gaussian mixture modelling in order to change the spectral tilt of speech in a post-processing algorithm. Six spectral envelope estimation methods are compared using objective distortion measures as well as subjective word-error rate and quality tests in different near-end noise conditions. Results show that one of the envelope estimation methods, stabilised weighted linear prediction, yielded statistically significant improvement in intelligibility over unprocessed speech.", "title": "Spectral tilt modelling with GMMs for intelligibility enhancement of narrowband telephone speech"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/koster14_interspeech.html", "abstract": "Most telecommunication systems are used for communication between two people which interact during a conversation. In general, the quality of conversational speech is the major indicator for telecommunication-service providers to evaluate their systems. In this context, not only the assessment of the overall quality but also the analysis of the conversational speech quality is essential. We present an initial approach towards analyzing the conversational quality by separating a conversation in phases, and extracting individual corresponding perceptual dimensions of quality, as they are subjectively perceived by the system users. These dimensions can be combined for overall quality estimation and may separately be used to diagnose the technical reasons of quality degradation. For this reason, we review known and identify new dimensions of quality perception on the basis of subjective experiments. This enables to deeply analyze conversational speech quality for diagnosis and optimization of telecommunication systems.", "title": "Analyzing perceptual dimensions of conversational speech quality"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/aubanel14_interspeech.html", "abstract": "It seems plausible that different regions of the speech signal convey different amounts of information. Understanding which aspects of the signal convey information is important for understanding speech perception, particularly when this occurs in noisy environments. The so called cochlea-scaled entropy (CSE) measure is an index of spoken information based on the distribution of spectral energy over consonant/vowel time scales that is defined independently of potential noise corruption. In speech in noise, however, energetic masking distorts information, because it suppresses certain spectro-temporal regions. This study explored the interplay of informational content (defined by CSE) and energetic masking in explaining the listeners ability to understand speech in noise. Using a priming paradigm, mixtures of speech and speech-shape noise were presented to listeners in an identification task. Sentences were preceded by previews consisting of either low or high informational content. Both types yielded a similar performance increase of around 19%. Although the low information preview transmitted less target information it had a greater overlap with the more energetic regions of the target sentence (i.e., those that were less masked). This could explain why both preview types were effective and calls for a consideration of both measures in understanding speech recognition in noise.", "title": "Interplay of informational content and energetic masking in speech perception in noise"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/zorila14_interspeech.html", "abstract": "This paper addresses the problem of increasing speech-in-noise intelligibility under the constraint of energy preservation. Two recently proposed algorithms which have been shown to be very successful in this problem according to two large formal listening tests are reviewed and a hybrid system which combines the properties of the two methods is suggested. The first technique, which is a frequency domain approach, is re-implemented providing clarifications on its energy reallocation strategy. Based on objective measures well correlated with human perception, we show that our implementation performs similarly to the original approach. Moreover, this is combined with a dynamic range compression algorithm from the second method to allow reallocation of energy over time as well. Experiments with speech shaped noise (SSN) and competing speaker (CS) noise maskers at various SNRs indicate that the hybrid system outperforms the individual algorithms in terms of intelligibility scores.", "title": "On spectral and time domain energy reallocation for speech-in-noise intelligibility enhancement"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/chen14j_interspeech.html", "abstract": "While temporal envelope and fine-structure cues are known to be good predictors for speech intelligibility, it is not clear how well they are correlated with subjective quality ratings, particularly those using noise-suppressed speech. The present work evaluated the performance of two objective measures (i.e., NCM and TFSS), which were originally developed with primarily envelope or fine-structure cue as speech intelligibility indices, when they were applied for predicting the subjective quality ratings of noise-suppressed speech along three dimensions of signal distortion, noise distortion and overall quality. We considered a wide range of distortion introduced by four types of real-world noises at two signal-to-noise-ratio levels and by four classes of noise-suppression algorithms. This work finds that the present envelope- and fine-structure-based measures poorly predict the subjective quality ratings of noise-suppressed speech. The PESQ measure is so far the best choice in terms of objectively evaluating both subjective quality ratings and intelligibility scores of noise-suppressed speech.", "title": "Objective quality evaluation of noise-suppressed speech: effects of temporal envelope and fine-structure cues"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/wang14k_interspeech.html", "abstract": "This study proposes a speech enhancement algorithm to improve speech intelligibility for hearing impaired listeners in adverse conditions. The proposed algorithm is based on a long term harmonic model, where the harmonics of target speech are more distinguished from noise spectrum interference. Our method consists of two stages: i) Prominent pitch estimation based on long term harmonic feature analysis and neural network classification. ii) Target speech spectrum estimation with pitch information based on long term noise spectrum extraction. The listening experiment with EAS vocoder speech shows that our algorithm is substantially beneficial for cochlear implant recipients to perceive speech in noisy environment in terms of word recognition rate.", "title": "Noisy speech enhancement based on long term harmonic model to improve speech intelligibility for hearing impaired listeners"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/valentinibotinhao14b_interspeech.html", "abstract": "In order to predict which words in a sentence are harder to understand in noise it is necessary to consider not only audibility but also semantic or linguistic information. This paper focuses on using linguistic predictability to inform an intelligibility enhancement method that uses Lombard-adapted synthetic speech to modify low predictable words in Speech Perception in Noise (SPIN) test sentences. Word intelligibility in the presence of speech-shaped noise was measured using plain, Lombard and a combination of the two synthetic voices. The findings show that the Lombard voice increases intelligibility in noise but the intelligibility gap between words in a high and low predictable context still remains. Using a Lombard voice when a word is unpredictable is a good strategy, but if a word is predictable from its context the Lombard benefit only occurs when other words in the sentence are also modified.", "title": "Using linguistic predictability and the lombard effect to increase the intelligibility of synthetic speech in noise"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/dabel14_interspeech.html", "abstract": "We propose a new approach for optimally pre-enhancing speech signals for given noise conditions. Like others, we optimise the predicted intelligibility of the signal, however, we employ a statistical `microscopic' intelligibility model that encodes information about which spectro-temporal speech regions are most informative. Uniquely, our optimisation strategy aims to maximise the discrimination between the correct interpretation and competing incorrect interpretations of the utterance. We present results from studies that use speech-shaped stationary noise maskers and show the new strategy leads to solutions that are more varied than the simple high frequency emphasis employed in many pre-enhancement systems.", "title": "Speech pre-enhancement using a discriminative microscopic intelligibility model"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/harvilla14_interspeech.html", "abstract": "This paper introduces a novel declipping algorithm based on constrained least-squares minimization. Digital speech signals are often sampled at 16 kHz and classic declipping algorithms fail to accurately reconstruct the signal at this sampling rate due to the scarcity of reliable samples after clipping. The Constrained Blind Amplitude Reconstruction algorithm interpolates missing data points such that the resulting function is smooth while ensuring the inferred data fall in a legitimate range. The inclusion of explicit constraints helps to guide an accurate interpolation. Evaluation of declipping performance is based on automatic speech recognition word error rate and Constrained Blind Amplitude Reconstruction is shown to outperform the current state-of-the-art declipping technology under a variety of conditions. Declipping performance in additive noise is also considered.", "title": "Least squares signal declipping for robust speech recognition"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/xu14d_interspeech.html", "abstract": "In this paper, we investigate semi-supervised training (SST) method in various state-of-the-art acoustic modeling techniques, using bottle-neck and corresponding tandem features. These techniques include subspace GMM, tanh-neuron deep neural network (DNN), and a generalized soft-maxout (p-norm) DNN. We demonstrate that SST may lead up to 2% Word Error Rate (WER) reduction using all these techniques in each case, and the best one comes from tandem feature based p-norm DNN system. In addition to recognition performance, effectiveness of the SST on keyword search performance is also investigated. Results on Actual Term Weighted Value (ATWV) are reported, with an analysis on lattice density. It is shown that SST may not necessarily increase ATWV due to the shrink of lattices size.", "title": "Semi-supervised training for bottle-neck feature based DNN-HMM hybrid systems"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/kapralova14_interspeech.html", "abstract": "Deep neural networks (DNNs) have recently become the state of the art technology in speech recognition systems. In this paper we propose a new approach to constructing large high quality unsupervised sets to train DNN models for large vocabulary speech recognition. The core of our technique consists of two steps. We first redecode speech logged by our production recognizer with a very accurate (and hence too slow for real-time usage) set of speech models to improve the quality of ground truth transcripts used for training alignments. Using confidence scores, transcript length and transcript flattening heuristics designed to cull salient utterances from three decades of speech per language, we then carefully select training data sets consisting of up to 15K hours of speech to be used to train acoustic models without any reliance on manual transcription. We show that this approach yields models with approximately 18K context dependent states that achieve 10% relative improvement in large vocabulary dictation and voice-search systems for Brazilian Portuguese, French, Italian and Russian languages.", "title": "A big data approach to acoustic model training corpus selection"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/cardinal14_interspeech.html", "abstract": "This paper describes a detailed comparison of several state-of-the-art speech recognition techniques applied to a limited Arabic broadcast news dataset. The different approaches were all trained on 50 hours of transcribed audio from the Al-Jazeera news channel. The best results were obtained using i-vector-based speaker adaptation in a training scenario using the Minimum Phone Error (MPE) criteria combined with sequential Deep Neural Network (DNN) training. We report results for two different types of test data: broadcast news reports, with a best word error rate (WER) of 17.86%, and a broadcast conversations with a best WER of 29.85%. The overall WER on this test set is 25.6%.", "title": "Recent advances in ASR applied to an Arabic transcription system for Al-Jazeera"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/sundermeyer14b_interspeech.html", "abstract": "We present a novel toolkit that implements the long short-term memory (LSTM) neural network concept for language modeling. The main goal is to provide a software which is easy to use, and which allows fast training of standard recurrent and LSTM neural network language models.   The toolkit obtains state-of-the-art performance on the standard Treebank corpus. To reduce the training time, BLAS and related libraries are supported, and it is possible to evaluate multiple word sequences in parallel. In addition, arbitrary word classes can be used to speed up the computation in case of large vocabulary sizes.   Finally, the software allows easy integration with SRILM, and it supports direct decoding and rescoring of HTK lattices. The toolkit is available for download under an open source license.", "title": "rwthlm \u2014 the RWTH aachen university neural network language modeling toolkit"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/cheng14_interspeech.html", "abstract": "Sum product networks (SPNs) are a new class of deep probabilistic models. They can contain multiple hidden layers while keeping their inference and training times tractable. An SPN consists of interleaving layers of sum nodes and product nodes. A sum node can be interpreted as a hidden variable, and a product node can be viewed as a feature capturing rich interactions among an SPN's inputs. We show that the ability of SPN to use hidden layers to model complex dependencies among words, and its tractable inference and learning times, make it a suitable framework for a language model. Even though SPNs have been applied to a variety of vision problems, we are the first to use it for language modeling. Our empirical comparisons with six previous language models indicate that our SPN has superior performance.", "title": "Language modeling with sum-product networks"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/cui14b_interspeech.html", "abstract": "This paper is focused on several techniques that improve deep neural network (DNN) acoustic modeling for audio corpus indexing in the context of the IARPA Babel program. Specifically, fundamental frequency variation (FFV) and channel-aware (CA) features and data augmentation based on stochastic feature mapping (SFM) are investigated not only for improved automatic speech recognition (ASR) performance but also for their impact to the final spoken term detection on the pre-indexed audio corpus. Experimental results on development languages of Babel option period one show that the improved DNN acoustic models can reduce word error rates in ASR and also help the keyword search performance compared to already competitive DNN baseline systems.", "title": "Improving deep neural network acoustic modeling for audio corpus indexing under the IARPA babel program"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/chowdhury14_interspeech.html", "abstract": "The development of a natural language speech application requires the process of semantic annotation. Moreover multilingual porting of speech applications increases the cost and complexity of the annotation task. In this paper we address the problem of transferring the semantic annotation of the source language corpus to a low-resource target language via crowdsourcing. The current crowdsourcing approach faces several problems. First, the available crowdsourcing platforms have skewed distribution of language speakers. Second, speech applications require domain-specific knowledge. Third, the lack of reference target language annotation, makes crowdsourcing worker control very difficult. In this paper we address these issues on the task of cross-language transfer of domain-specific semantic annotation from an Italian spoken language corpus to Greek, via targeted crowdsourcing. The issue of domain knowledge transfer is addressed by priming the workers with the source language concepts. The lack of reference annotation is coped with a consensus-based annotation algorithm. The quality of annotation transfer is assessed using source language references and inter-annotator agreement. We demonstrate that the proposed computational methodology is viable and achieves acceptable annotation quality.", "title": "Cross-language transfer of semantic annotation via targeted crowdsourcing"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/hakkanitur14_interspeech.html", "abstract": "Knowledge encoded in semantic graphs such as Freebase has been shown to benefit semantic parsing and interpretation of natural language user utterances. In this paper, we propose new methods to assign weights to semantic graphs that reflect common usage types of the entities and their relations. Such statistical information can improve the disambiguation of entities in natural language utterances. Weights for entity types can be derived from the populated knowledge in the semantic graph, based on the frequency of occurrence of each type. They can also be learned from the usage frequencies in real world natural language text, such as related Wikipedia documents or user queries posed to a search engine. We compare the proposed methods with the unweighted version of the semantic knowledge graph for the relation detection task and show that all weighting methods result in better performance in comparison to using the unweighted version.", "title": "Probabilistic enrichment of knowledge graph entities for relation detection in conversational understanding"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/garner14_interspeech.html", "abstract": "Walliserdeutsch is a Swiss German dialect spoken in the south west of Switzerland. To investigate the potential of automatic speech processing ofWalliserdeutsch, a small database was collected based mainly on broadcast news from a local radio station. Experiments suggest that automatic speech recognition is feasible: use of another (Swiss German) database shows that the small data size lends itself to bootstrapping from other data; use of Kullback-Leibler HMM suggests that phoneme mapping techniques can compensate for a grapheme-based dictionary. Experiments also indicate that statistical machine translation is feasible; the difficulty of small data size is offset by the close proximity to (high) German.", "title": "Automatic speech recognition and translation of a Swiss German dialect: Walliserdeutsch"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/harrat14_interspeech.html", "abstract": "The Algerian Arabic dialects are under-resourced languages, which lack both corpora and Natural Language Processing (NLP) tools, although they are increasingly used in written form, especially on social media and forums. We aim through this paper, and for the first time, to build parallel corpora for Algerian dialects, because our ultimate purpose is to achieve a Machine Translation (MT) for Modern Standard Arabic (MSA) and Algerian dialects (AD), in both directions. We also propose language tools to process these dialects. First, we developed a morphological analysis model of dialects by adapting BAMA, a well-known MSA analyzer. Then we propose a diacritization system, based on a MT process which allows to restore the vowels to dialects corpora. And finally, we propose results on machine translation between MSA and Algerian dialects.", "title": "Building resources for Algerian Arabic dialects"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/ferrer14_interspeech.html", "abstract": "This paper explores in depth a recently proposed approach to spoken language recognition based on the estimated posteriors for a set of senones representing the phonetic space of one or more languages. A neural network (NN) is trained to estimate the posterior probabilities for the senones at a frame level. A feature vector is then derived for every sample using these posteriors. The effect of the language used in training the NN and the number of senones are studied. Speech-activity detection (SAD) and dimensionality reduction approaches are also explored and Gaussian and NN backends are compared. Results are presented on heavily degraded speech data. The proposed system is shown to give over 40% relative gain compared to a state-of-the-art language recognition system at sample durations from 3 to 120 seconds.", "title": "Spoken language recognition based on senone posteriors"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/gonzalezdominguez14_interspeech.html", "abstract": "This work explores the use of Long Short-Term Memory (LSTM) recurrent neural networks (RNNs) for automatic language identification (LID). The use of RNNs is motivated by their better ability in modeling sequences with respect to feed forward networks used in previous works. We show that LSTM RNNs can effectively exploit temporal dependencies in acoustic data, learning relevant features for language discrimination purposes. The proposed approach is compared to baseline i-vector and feed forward Deep Neural Network (DNN) systems in the NIST Language Recognition Evaluation 2009 dataset. We show LSTM RNNs achieve better performance than our best DNN system with an order of magnitude fewer parameters. Further, the combination of the different systems leads to significant performance improvements (up to 28%).", "title": "Automatic language identification using long short-term memory recurrent neural networks"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/desplanques14_interspeech.html", "abstract": "This paper presents a technique to adapt an acoustically based language classifier to the background conditions and speaker accents. This adaptation improves language classification on a broad spectrum of TV broadcasts. The core of the system consists of an iVector-based setup in which language and channel variabilities are modeled separately. The subsequent language classifier (the backend) operates on the language factors, i.e. those features in the extracted iVectors that explain the observed language variability. The proposed technique adapts the language variability model to the background conditions and to the speaker accents present in the audio. The effect of the adaptation is evaluated on a 28 hours corpus composed of documentaries and monolingual as well as multilingual broadcast news shows. Consistent improvements in the automatic identification of Flemish (Belgian Dutch), English and French are demonstrated for all broadcast types.", "title": "Robust language recognition via adaptive language factor extraction"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/behravan14_interspeech.html", "abstract": "We adopt automatic language recognition methods to study dialect levelling \u2014 a phenomenon that leads to reduced structural differences among dialects in a given spoken language. In terms of dialect characterisation, levelling is a nuisance variable that adversely affects recognition accuracy: the more similar two dialects are, the harder it is to set them apart. We address levelling in Finnish regional dialects using a new SAPU (Satakunta in Speech) corpus containing material from Satakunta (South-Western Finland) between 2007 and 2013. To define a compact and universal set of sound units to characterize dialects, we adopt speech attributes features, namely manner and place of articulation. It will be shown that speech attribute distributions can indeed characterise differences among dialects. Experiments with an i-vector system suggest that (1) the attribute features achieve higher dialect recognition accuracy and (2) they are less sensitive against age-related levelling in comparison to traditional spectral approach.", "title": "Dialect levelling in Finnish: a universal speech attribute approach"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/chen14k_interspeech.html", "abstract": "In this paper, we utilize deep neural networks (DNNs) to automatically identify native accents in English and Mandarin when no text, speaker or gender information is available for the speech data. Compared to the Gaussian mixture model (GMM) based conventional methods, the proposed method benefits from two main advantages: first, DNNs are discriminative models which can provide better discrimination on confusion regions of different accents; second, they have the hierarchical nonlinear feature extraction capability which can learn discriminative high-level features for the specified task. In detail, the speech data of all accents is used to train DNNs, and in the testing stage, we first identify the accent label of each frame, then determine the sentence label by the majority voting conducted on the frame labels. The experiments on accented English and Mandarin corpus demonstrate that, compared to the GMM based methods, our proposed method can significantly improve the frame accuracy as well as sentence accuracy on the test set. Moreover, the performance of the proposed method can be further improved by using context information.", "title": "Improving native accent identification using deep neural networks"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/kolly14_interspeech.html", "abstract": "Can the foreign accent of a speaker be recognized based on suprasegmental temporal information? For a perception experiment we created stimuli based on German sentences read by six French and six English speakers. These foreign-accented sentences were manipulated by (1) applying a lowpass filter with a cutoff frequency of 300 Hz and (2) applying the same lowpass filter and monotonizing F0. In a between-subject 2AFC perception experiment we tested the accent recognition ability of 15 Swiss German listeners per signal manipulation condition. The results showed that speakers' native language could be recognized above chance in both conditions. However, listeners obtained significantly lower recognition scores in the monotonized condition. Furthermore, higher recognition scores were obtained for French-accented speech in the monotonized condition, a result that is discussed in light of research on speech rhythm. We further report an effect for speaker within each accent group. The results suggest that suprasegmental temporal information allows for foreign accent recognition to some degree.", "title": "Foreign accent recognition based on temporal information contained in lowpass-filtered speech"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/karanasou14_interspeech.html", "abstract": "The use of deep neural networks (DNNs) in a hybrid configuration is becoming increasingly popular and successful for speech recognition. One issue with these systems is how to efficiently adapt them to reflect an individual speaker or noise condition. Recently speaker i-vectors have been successfully used as an additional input feature for unsupervised speaker adaptation. In this work the use of i-vectors for adaptation is extended to incorporate acoustic factorisation. In particular, separate i-vectors are computed to represent speaker and acoustic environment. By ensuring \u201corthogonality\u201d between the individual factor representations it is possible to represent a wide range of speaker and environment pairs by simply combining i-vectors from a particular speaker and a particular environment. In this paper the i-vectors are viewed as the weights of a cluster adaptive training (CAT) system, where the underlying models are GMMs rather than HMMs. This allows the factorisation approaches developed for CAT to be directly applied. Initial experiments were conducted on a noise distorted version of the WSJ corpus. Compared to standard speaker-based i-vector adaptation, factorised i-vectors showed performance gains.", "title": "Adaptation of deep neural network acoustic models using factorised i-vectors"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/fukuda14_interspeech.html", "abstract": "Model-space adaptation techniques such as MLLR and MAP are often used for porting old acoustic models into new domains. Discriminative schemes for model adaptation based on MMI and MPE objective functions are also utilized. For feature-space adaptations, one extension to the well-known feature-space discriminative training (fMPE) algorithm, feature-space discriminative adaptation, was recently proposed to adapt fMPE transforms. Feature-space discriminative adaptation was shown to work well for some situations when sufficient adaptation data is available. This paper improves the feature-space discriminative adaptation by introducing a regularization term for an indirect differential computation of the fMPE objective function, and also by updating the acoustic models with MAP instead of ML criterion during the adaptation. The proposed method performed favorably for the adaptation conditions from general-purpose LVCSR to automotive environments with small amounts of adaptation data, and yielded 4.4% relative improvement as compared with MAP-adapted system without using the fMPE adaptation.", "title": "Regularized feature-space discriminative adaptation for robust ASR"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/miao14c_interspeech.html", "abstract": "We investigate the concept of speaker adaptive training (SAT) in the context of deep neural network (DNN) acoustic models. Previous studies have shown success of performing speaker adaptation for DNNs in speech recognition. In this paper, we apply SAT to DNNs by learning two types of feature mapping neural networks. Given an initial DNN model, these networks take speaker i-vectors as additional information and project DNN inputs into a speaker-normalized space. The final SAT model is obtained by updating the canonical DNN in the normalized feature space. Experiments on a Switchboard 110-hour setup show that compared with the baseline DNN, the SAT-DNN model brings 7.5% and 6.0% relative improvement when DNN inputs are speaker-independent and speaker-adapted features respectively. Further evaluations on the more challenging BABEL datasets reveal significant word error rate reduction achieved by SAT-DNN.", "title": "Towards speaker adaptive training of deep neural network acoustic models"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/gorin14_interspeech.html", "abstract": "When the speech data are produced by speakers of different age and gender, the acoustic variability of any given phonetic unit becomes large, which degrades speech recognition performance. A way to go beyond the conventional Hidden Markov Model is to explicitly include speaker class information in the modeling. Speaker classes can be obtained by unsupervised clustering of the speech utterances.   This paper introduces a structuring of the Gaussian components of the GMM densities with respect to speaker classes. In a first approach, the structuring of the Gaussian components is combined with speaker class-dependent mixture weights. In a second approach, the structuring is used with mixture transition matrices, which add dependencies between Gaussian components of mixture densities (as in stranded GMMs). The different approaches are evaluated and compared in detail on the TIDIGITS task. Significant improvements are obtained using the proposed approaches based on structured components. Additional results are reported for phonetic decoding on the NEOLOGOS database, a large corpus of French telephone data.When the speech data are produced by speakers of different age and gender, the acoustic variability of any given phonetic unit becomes large, which degrades speech recognition performance. A way to go beyond the conventional Hidden Markov Model is to explicitly include speaker class information in the modeling. Speaker classes can be obtained by unsupervised clustering of the speech utterances.   This paper introduces a structuring of the Gaussian components of the GMM densities with respect to speaker classes. In a first approach, the structuring of the Gaussian components is combined with speaker class-dependent mixture weights. In a second approach, the structuring is used with mixture transition matrices, which add dependencies between Gaussian components of mixture densities (as in stranded GMMs). The different approaches are evaluated and compared in detail on the TIDIGITS task. Significant improvements are obtained using the proposed approaches based on structured components. Additional results are reported for phonetic decoding on the NEOLOGOS database, a large corpus of French telephone data.", "title": "Component structuring and trajectory modeling for speech recognition"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/doddipatla14_interspeech.html", "abstract": "Speaker adaptation of deep neural networks (DNN) is difficult, and most commonly performed by changes to the input of the DNNs. Here we propose to learn discriminative feature transformations to obtain speaker normalised bottleneck (BN) features. This is achieved by interpreting the final two hidden layers as speaker specific matrix transformations. The hidden layer weights are updated with data from a specific speaker to learn speaker-dependent discriminative feature transformations. Such simple implementation lends itself to rapid adaptation and flexibility to be used in Speaker Adaptive Training (SAT) frameworks. The performance of this approach is evaluated on a meeting recognition task, using the official NIST RT'07 and RT'09 evaluation test sets. Supervised adaptation of the BN layer shows similar performance to the application of supervised CMLLR as a global transformation, and the combination of these appears to be additive. In unsupervised mode, CMLLR adaptation only yields 3.4% and 2.5% relative word error rate (WER) improvement, on the RT'07 and RT'09 respectively, where the baselines include speaker based cepstral mean and variance normalisation. The combined CMLLR and BN layer speaker adaptation yields a relative WER gain of 4.5% and 4.2% respectively. SAT style BN layer adaptation is attempted and combined with conventional CMLLR SAT, to show that it provides a relative gain of 1.43% and 2.02% on the RT'07 and RT'09 data sets respectively when compared with CMLLR SAT. While the overall gain from BN layer adaptation is small, the results are found to be statistically significant on both the test sets.", "title": "Speaker dependent bottleneck layer training for speaker adaptation in automatic speech recognition"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/you14_interspeech.html", "abstract": "In the past few years, deep neural networks (DNNs) have achieved great successes in speech recognition. The deep network model can be viewed as a series of feature transforms followed by a log-linear classifier. For input of speeches from different bandwidths, although the hidden layer transform and log-linear classification can be shared, the input layer transforms should be specially designed respectively. So, training DNNs directly on different bandwidth speeches is intractable. In this paper, we treat the problem of training DNNs on mixed bandwidth data as an domain-adaptation problem. Upon our adaptation approach, DNNs trainied on the rich narrowband speech can be adapted effectively to the target wideband domain, and meanwhile shows good performance on the wideband speech. We evaluate this approach on the wideband clean7k and noise360 speech. Experimental results show that the DNNs adaptation approach can reduce character error rate (CER) range from 5% to 15%, relatively, over the baseline DNNs trained only on the limited wideband data.", "title": "Improving wideband acoustic models using mixed-bandwidth training data via DNN adaptation"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/pellegrini14b_interspeech.html", "abstract": "Phone-like acoustic models (AMs) used in large-vocabulary automatic speech recognition (ASR) systems are usually trained with speech collected from young adult speakers. Using such models, ASR performance may decrease by about 10% absolute when transcribing elderly speech. Ageing is known to alter speech production in ways that require ASR systems to be adapted, in particular at the level of acoustic modeling. In this study, we investigated automatic age estimation in order to select age-specific adapted AMs. A large corpus of read speech from European Portuguese speakers aged 60 or over was used. Age estimation (AE) based on i-vectors and support vector regression achieved mean error rates of about 4.2 and 4.5 years for males and females, respectively. Compared with a baseline ASR system with AMs trained using young adult speech and a WER of 13.9%, the selection of five-year-range adapted AMs, based on the estimated age of the speakers, led to a decrease in WER of about 9.3% relative (1.3% absolute). Comparable gains in ASR performance were observed when considering two larger age ranges (60\u201375 and 76\u201390) instead of six five-year ranges, suggesting that it would be sufficient to use the two large ranges only.", "title": "Speaker age estimation for elderly speech recognition in European Portuguese"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/najafian14_interspeech.html", "abstract": "This paper is concerned with automatic speech recognition (ASR) for accented speech. Given a small amount of speech from a new speaker, is it better to apply speaker adaptation to the baseline, or to use accent identification (AID) to identify the speaker's accent and select an accent-dependent acoustic model? Three accent-based model selection methods are investigated: using the `true' accent model, and unsupervised model selection using i-Vector and phonotactic-based AID. All three methods outperform the unadapted baseline. Most significantly, AID-based model selection using 43s of speech performs better than unsupervised speaker adaptation, even if the latter uses five times more adaptation data. Combining unsupervised AID-based model selection and speaker adaptation gives an average relative reduction in ASR error rate of up to 47%.", "title": "Unsupervised model selection for recognition of regional accented speech"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/zhang14g_interspeech.html", "abstract": "The eigenphone based speaker adaptation outperforms the conventional MLLR and eigenvoice methods when the adaptation data is sufficient, but it suffers from severe over-fitting when the adaptation data is limited. In this paper, l1 and nuclear norm regularization are applied simultaneously to obtain a more robust eigenphone estimation, resulting in a sparse and low-rank eigenphone matrix. The sparse constraint can reduce the number of free parameters while the low rank constraint can limit the dimension of phone variation subspace, which are both benefit to the generalization ability. Experimental results show that the proposed method can improve the adaptation performance substantially, especially when the amount of adaptation data is limited.", "title": "Speaker adaptation based on sparse and low-rank eigenphone matrix estimation"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/huang14e_interspeech.html", "abstract": "We propose a multi-accent deep neural network acoustic model with an accent-specific top layer and shared bottom hidden layers. The accent-specific top layer is used to model the distinct accent specific patterns. The shared bottom hidden layers allow maximum knowledge sharing between the native and the accent models. This design is particularly attractive when considering deploying such a system to a live speech service due to its computational efficiency. We applied the KL-divergence (KLD) regularized model adaptation to train the accent-specific top layer. On the mobile short message dictation task (SMD), with 1K, 10K, and 100K British or Indian accent adaptation utterances, the proposed approach achieves 18.1%, 26.0%, and 28.5% or 16.1%, 25.4%, and 30.6% word error rate reduction (WERR) for the British and the Indian accent respectively against a baseline cross entropy (CE) model trained from 400 hour data. On the 100K utterance accent adaptation setup, comparable performance gain can be obtained against a baseline CE model trained with 2000 hour data. We observe smaller yet significant WER reduction on a baseline model trained using the MMI sequence-level criterion.", "title": "Multi-accent deep neural network acoustic model with accent-specific top layer using the KLD-regularized model adaptation"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/shahnawazuddin14_interspeech.html", "abstract": "The work presented in this paper describes a novel on-line adaptation approach for extremely low adaptation data scenario. The proposed approach extends a similar redundant dictionary based approach reported recently in literature. In this work, the orthogonal matching pursuit (OMP) algorithm is used for bases selection instead of the matching pursuit (MP). This helps in avoiding the selection of an atom more than once. Furthermore, this work also explores the use of cluster-specific eigenvoices to capture local acoustic details unlike the conventional eigenvoices technique. These approaches are then combined to reduce the number of weight parameters being estimated for deriving adapted model. Towards this purpose, separate sparse coding of the test data is performed over a set of dictionaries. Those sparse coded supervectors are then scaled and used as the Gaussian mean parameter in the adapted model. Consequently, only a few scaling factors are needed to be estimated. Such a reduction in number of parameters is highly desirable for on-line applications where the latency is a major factor.", "title": "A low complexity model adaptation approach involving sparse coding over multiple dictionaries"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/kubota14_interspeech.html", "abstract": "Accurate and efficient speaker canonicalization is proposed to improve the performance of speaker-independent ASR systems. Vocal tract length normalization (VTLN) is often applied to speaker canonicalization in ASR; however, it requires parallel decoding of speech when estimating the optimal warping parameter. In addition, VTLN provides the same linear spectral transformation in an utterance, although optimal mapping functions differ among phonemes. In this study, we propose a novel speaker canonicalization using multilayer perceptron (MLP) that is trained with a data set of vowels to map an input spectrum to the output spectrum of a standard speaker or a canonical speaker. The proposed speaker canonicalization operates according to the integration of MLP-based mapping and identity mapping that depends on frequency bands and achieves accurate recognition without any tuning of mapping function during run-time. Results of experiments conducted with a continuous digit recognition task showed that the proposed method reduces the intra-class variability in both of the vowel and consonant parts and outperforms VTLN.", "title": "Effect of frequency weighting on MLP-based speaker canonicalization"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/huang14f_interspeech.html", "abstract": "We propose a feature space maximum a posteriori (MAP) linear regression framework to adapt parameters for context dependent deep neural network hidden Markov models (CD-DNN-HMMs). Due to the huge amount of parameters used in DNN acoustic models in large vocabulary continuous speech recognition, the problem of over-fitting can be severe in DNN adaptation, thus often impair the robustness of the adapted DNN model. Linear input network (LIN) as a straight-forward feature space adaptation method for DNN, similar to feature space maximum likelihood linear regression (fMLLR), can potentially suffer from the same robustness situation. The proposed adaptation framework is built based on MAP estimation of the LIN parameters by incorporating prior knowledge into the adaptation process. Experimental results on the Switchboard task show that against the speaker independent CD-DNN-HMM systems, LIN provides 4.28% relative word error rate reduction (WERR) and the proposed fMAPLIN method is able to provide further 1.15% (totally 5.43%) WERR on top of LIN.", "title": "Feature space maximum a posteriori linear regression for adaptation of deep neural networks"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/tomashenko14_interspeech.html", "abstract": "In this paper we propose a novel speaker adaptation method for a context-dependent deep neural network HMM (CD-DNN-HMM) acoustic model. The approach is based on using GMM-derived features as the input to the DNN. The described technique of processing features for DNNs makes it possible to use GMM-HMM adaptation algorithms in the neural network framework. Adaptation to a new speaker can be simply performed by adapting an auxiliary GMM-HMM model used in calculation of GMM-derived features and can be regarded as adaptation in the feature space for a DNN system. In this work, traditional maximum a posteriori adaptation is performed for an auxiliary GMM-HMM model. Experiments show that the proposed adaptation technique can provide, on average, a 5%\u201336% relative word error reduction on different adaptation sets under supervised adaptation setup, compared to speaker independent (SI) CD-DNN-HMM systems. In addition, several multi-stream combination techniques are examined in order to improve the performance of the baseline SI model.", "title": "Speaker adaptation of context dependent deep neural networks based on MAP-adaptation and GMM-derived feature processing"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/karafiat14_interspeech.html", "abstract": "Features based on a hierarchy of neural networks with compressive layers \u2014 Stacked Bottle-Neck (SBN) features \u2014 were recently shown to provide excellent performance in LVCSR systems. This paper summarizes several techniques investigated in our work towards Babel 2014 evaluations: (1) using several versions of fundamental frequency (F0) estimates, (2) semi-supervised training on un-transcribed data and mainly (3) adapting the NN structure at different levels. They are tested on three 2014 Babel languages with full GMM- and DNN-based systems. Separately and in combination, they are shown to outperform the baselines and confirm the usefulness of bottle-neck features in current ASR systems.", "title": "BUT 2014 Babel system: analysis of adaptation in NN based systems"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/rouvier14_interspeech.html", "abstract": "Deep neural networks (DNN) are currently very successful for acoustic modeling in ASR systems. One of the main challenges with DNNs is unsupervised speaker adaptation from an initial speaker clustering, because DNNs have a very large number of parameters. Recently, a method has been proposed to adapt DNNs to speakers by combining speaker-specific information (in the form of i-vectors computed at the speaker-cluster level) with fMLLR-transformed acoustic features. In this paper we try to gain insight on what kind of adaptation is performed on DNNs when stacking i-vectors with acoustic features and what information exactly is carried by i-vectors. We observe on REPERE corpus that DNNs trained on i-vector features concatenated with fMLLR-transformed acoustic features lead to a gain of 0.7 points. The experiments shows that using i-vector stacking in DNN acoustic models is not only performing speaker adaptation, but also adaptation to acoustic conditions.", "title": "Speaker adaptation of DNN-based ASR with i-vectors: does it actually adapt models to speakers?"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/singhal14_interspeech.html", "abstract": "Sparse reconstruction methods have been used extensively for source localization over uniform linear arrays and circular arrays. In this paper a sparse reconstruction method for speech source localization using partial dictionaries over a spherical microphone array is proposed. The source localization method proposed in this work addresses two important research issues. It formulates the source localization problem in the spherical harmonics domain as a sparse reconstruction problem. Subsequently, a low complexity method to estimate the direction of arrival (DOA) of multiple sources is also proposed by using partial elevation angle dictionaries. The use of such dictionaries reduces the complexity of the search involved in the two dimensional DOA estimation. Source localization experiments are conducted at different SNRs and compared with conventional DOA estimation methods like MUSIC and MVDR. The experimental results obtained from the proposed method indicate a reasonable reduction in the localization error.", "title": "A sparse reconstruction method for speech source localization using partial dictionaries over a spherical microphone array"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/cui14c_interspeech.html", "abstract": "Time difference of arrival (TDOA) estimation is one of the key techniques in array signal processing and has wide applications in hands-free speech interface. TDOA is used to derive the time difference of signal propagation from source to the spatially separated microphones. The acoustic transfer functions ratio (ATF-s ratio) method is one of the robust approaches to get TDOA estimate in presence of strong background noise and in reverberant environment. Although the effectiveness is proved by various simulations in related studies, the anti-noise or anti-reverberation capability of ATF-s ratio is limited in real acoustic environment. In this work, a frequency domain partial-whitening ATF-s ratio method is proposed for detecting the time delay in an in-car mobile system. To test the efficacy of the proposed method, the experiments were conducted in a car running at a speed of about 100km/h on the highway, and the results show that the percentage of TDOA estimation anomaly is reduced by 15%~ 25%.", "title": "A robust TDOA estimation method for in-car-noise environments"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/netsch14_interspeech.html", "abstract": "In this paper we address the problem of sound source location using the time difference of arrival (TDOA) technique in an environment containing stationary correlated noise. We present a robust low-complexity method for enhancing estimation of sound direction, augmenting the well-known Generalized Cross-Correlation with Phase Transform (GCC-PHAT) approach. In the proposed method, the estimated cross-spectrum of a correlated background noise is subtracted from the observed spectrum. This effectively removes the phase distortion introduced by the interfering noise and significantly improves the robustness of the sound direction estimate. We test the performance of this approach on data collected and processed with a low-resource embedded platform. Results illustrate substantially enhanced performance over the baseline GCC-PHAT sound localization.", "title": "Robust low-resource sound localization in correlated noise"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/ying14_interspeech.html", "abstract": "This paper presents a novel method to estimate the direction-of-arrivals (DOAs) of multiple speakers based on time-frequency sparsity. The acoustic interferences are first suppressed, and then a concave cost function (CCF) is utilized to estimate bin-wise DOAs. The DOAs of sources are subsequently identified by picking peaks in a histogram of bin-wise azimuths. Three aspects distinguish this method from conventional ones. First, a closed-form solution to bin-wise DOA is given by CCF, which replaces the extensively used grid search and enables high computational efficiency. Second, signal enhancement is employed to suppress acoustic interferences on phase spectra. Last, the time-delay weights mitigate the effect of delay outliers. The proposed method is compared with well-established methods in simulated environments. The experimental results confirmed its superiority in computational efficiency and acoustic robustness.", "title": "Direction-of-arrival estimation of multiple speakers using a planar array"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/xue14_interspeech.html", "abstract": "Besides the undirected environmental noise, the surrounding interference also brings great challenges to the robust DOA estimation of the speech source. As conventional DOA estimation methods always assume an undirected noise model, they usually cannot perform reliably when the strong inference exists. In this paper, we propose a novel interference robust DOA estimation method, which is based on the \u201cweighted spatial bispectrum correlation matrix (WSBCM)\u201d. The WSBCM contains the spatial correlation information of bispectrum phase difference (BPD), and a new DOA estimator is further derived based on the eigenvalue analysis of the WSBCM. By formulating with WSBCM, the proposed method benefits from the redundant DOA-related information provided by the BPD. In addition, the WSBCM enables bispectrum weighting to highlight the pure speech units in the bispectrum, which further helps to improve the performance. In order to compute the bispectrum weights, a decision-directed approach is derived. The effectiveness of the proposed method is demonstrated by experiments conducted under various kinds of interference-existing scenarios.", "title": "Weighted spatial bispectrum correlation matrix for DOA estimation in the presence of interferences"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/bouafif14_interspeech.html", "abstract": "Spatial localization given measurements of the sound field at a microphone array is a computationally challenging task. While big progress has been achieved in Azimuth localization, little attention has been paid to localization in distance. This paper presents a multi-sources localization model based on Time Delay of Arrival (TDOA) using only two microphones. The proposed method requires estimation of mixing parameters, which are informative about speakers' positions. The mixing parameters are estimated by determining the slope of a scatter plot of the separated sound source from the time delayed mixed speech captured by each microphone. In the determined context, experiments on BSS-eval dataset show that the proposed model achieves distance accuracy of more than 92% with a less than 1,8m distance localization error.", "title": "Multi-sources separation for sound source localization"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/zhang14h_interspeech.html", "abstract": "We propose a multi-layer feature extraction framework for speech, capable of providing invariant representations. A set of templates is generated by sampling the result of applying smooth, identity-preserving transformations (such as vocal tract length and tempo variations) to arbitrarily-selected speech signals. Templates are then stored as the weights of \u201cneurons\u201d. We use a cascade of such computational modules to factor out different types of transformation variability in a hierarchy, and show that it improves phone classification over baseline features. In addition, we describe empirical comparisons of a) different transformations which may be responsible for the variability in speech signals and of b) different ways of assembling template sets for training. The proposed layered system is an effort towards explaining the performance of recent deep learning networks and the principles by which the human auditory cortex might reduce the sample complexity of learning in speech recognition. Our theory and experiments suggest that invariant representations are crucial in learning from complex, real-world data like natural speech. Our model is built on basic computational primitives of cortical neurons, thus making an argument about how representations might be learned in the human auditory cortex.", "title": "Phone classification by a hierarchy of invariant representation layers"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/sinclair14_interspeech.html", "abstract": "Speech segmentation is the problem of finding the end points of a speech utterance for passing to an automatic speech recognition (ASR) system. The quality of this segmentation can have a large impact on the accuracy of the ASR system; in this paper we demonstrate that it can have an even larger impact on downstream natural language processing tasks \u2014 in this case, machine translation. We develop a novel semi-Markov model which allows the segmentation of audio streams into speech utterances which are optimised for the desired distribution of sentence lengths for the target domain. We compare this with existing state-of-the-art methods and show that it is able to achieve not only improved ASR performance, but also to yield significant benefits to a speech translation task.", "title": "A semi-Markov model for speech segmentation with an utterance-break prior"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/aneeja14_interspeech.html", "abstract": "Voice activity detection (VAD) uses a representation of speech derived from spectrum analysis, followed by statistical characterization of speech and degrading noise. Features derived using traditional methods may not be adequate for VAD in the case of transient noises. In this paper, we focus on transient noises where most of the VAD systems in literature do not perform well. A high temporal resolution and high frequency resolution representation is used to discriminate the transient noises from speech.   The high temporal and frequency resolution representation is achieved by filtering the signal at several single frequencies. The single frequency filtering approach helps to isolate the regions of transient noise in a signal. A time varying threshold is proposed based on the spectral variance and the temporal variance of the speech signal to detect transient noise. The remaining regions are processed by the spectral variance measure for VAD. The results have been compared to the Adaptive Multi-rate (AMR) methods. The performance of proposed method is consistently better due to the instantaneous feature. The percentage of detection of transient noise is higher for the proposed method than the methods reported in the literature.", "title": "Speech detection in transient noises"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/he14b_interspeech.html", "abstract": "As a promising technique, sparse coding has been widely used for the analysis, representation, compression, denoising and separation of speech. To represent a signal accurately and sparsely, a good dictionary which contains elemental signals is urgently desired and many methods have been proposed to obtain such a dictionary. However, there is a lack of reasonable evaluation methods to judge whether a dictionary is good enough. To solve this problem, we define a group of measures for the evaluation of a dictionary. These measures not only address the sparsity and reconstruction error in representation of a signal, but also consider the denoising and separating performances. Experiments show that the proposed measures can make reasonable evaluations.", "title": "Evaluation of dictionary for sparse coding in speech processing"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/vaz14b_interspeech.html", "abstract": "We propose a joint filtering and factorization algorithm to recover latent structure from noisy speech. We incorporate the minimum variance distortionless response (MVDR) formulation within the non-negative matrix factorization (NMF) framework to derive a single, unified cost function for both filtering and factorization. Minimizing this cost function jointly optimizes three quantities \u2014 a filter that removes noise, a basis matrix that captures latent structure in the data, and an activation matrix that captures how the elements in the basis matrix can be linearly combined to reconstruct input data. Results show that the proposed algorithm recovers the speech basis matrix from noisy speech significantly better than NMF alone or Wiener filtering followed by NMF. Furthermore, PESQ scores show that our algorithm is a viable choice for speech denoising.", "title": "Joint filtering and factorization for recovering latent structure from noisy speech data"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/gallardoantolin14_interspeech.html", "abstract": "Traditional Text-To-Speech (TTS) systems have been developed using especially-designed non-expressive scripted recordings. In order to develop a new generation of expressive TTS systems in the Simple4All project, real recordings from the media should be used for training new voices with a whole new range of speaking styles. However, for processing this more spontaneous material, the new systems must be able to deal with imperfect data (multi-speaker recordings, background and foreground music and noise), filtering out low-quality audio segments and creating mono-speaker clusters. In this paper we compare several architectures for combining speaker diarization and music and noise detection which improve the precision and overall quality of the segmentation.", "title": "A comparison of open-source segmentation architectures for dealing with imperfect data from the media in speech synthesis"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/asami14_interspeech.html", "abstract": "This paper provides a novel method to classify spoken utterances into reading style or spontaneous style. Read/spontaneous speech classification is important for extracting data to train acoustic models for speech recognition from real data in which read speech and spontaneous speech samples are mixed. We analyzed 23,900 reading and 31,988 spontaneous utterances of 30 speakers and found that variance of GMM supervectors in several consecutive utterances can discriminate the reading and spontaneous styles and has less speaker-dependency. Based on this knowledge, our method uses variance of GMM supervectors to classify unknown consecutive utterances into reading style or spontaneous style. Experiments show that our technique can classify 5 consecutive utterances of unknown speakers with over 95% accuracy without any other lexical, phonetic, or prosodic features.", "title": "Read and spontaneous speech classification based on variance of GMM supervectors"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/shokouhi14_interspeech.html", "abstract": "Overlapped-speech is known to degrade performance in automatic speech systems. In this study, a sub-band speech analysis technique is proposed to detect overlapped-speech segments in single-channel multi-speaker scenarios (i.e., co-channel speech). Sub-band signals are obtained by decomposing the input speech using a gammatone filterbank. Filterbank outputs are then used to modulate the frequency argument of a sinusoidal carrier. We show that the spectra of these frequency-modulated signals, namely Gammatone Sub-band Frequency Modulation (GSFM) features, are more disperse in overlapped-speech segments compared to single-speaker regions. We quantify the dispersion rate to obtain a measure for the amount of overlapped speech in a given speech segment. Overlap detection experiments are conducted using the speech separation challenge corpus and GSFM features are compared to commonly used overlap detection features. Detection errors are reduced by a relative 50% across different signal-to-interference values ranging from 0 to 9dB.", "title": "Co-channel speech detection via spectral analysis of frequency modulated sub-bands"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/voinea14_interspeech.html", "abstract": "Extracting discriminant, transformation-invariant features from raw audio signals remains a serious challenge for speech recognition. The issue of speaker variability is central to this problem, as changes in accent, dialect, gender, and age alter the sound waveform of speech units at multiple levels (phonemes, words, or phrases). Approaches for dealing with this variability have typically focused on analyzing the spectral properties of speech at the level of frames, on par with frame-level acoustic modeling usually applied to speech recognition systems. In this paper, we propose a framework for representing speech at the word level and extracting features from the acoustic, temporal domain, without the need for spectral encoding or preprocessing. Leveraging recent work on unsupervised learning of invariant sensory representations, we extract a signature for a word by first projecting its raw waveform onto a set of templates and their transformations, and then forming empirical estimates of the resulting one-dimensional distributions via histograms. The representation and relevant parameters are evaluated for word classification on a series of datasets with increasing speaker-mismatch difficulty, and the results are compared to those of an MFCC-based representation.", "title": "Word-level invariant representations from acoustic waveforms"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/dalsgaard14_interspeech.html", "abstract": "The mathematical theory of closed form functions for calculating LSFs on the basis of generating functions is presented. Exploiting recurrence relationships in the series expansion of Chebyshev polynomials of the first kind makes it possible to bootstrap iterative LSF-search from a set of characteristic polynomial zeros. The theoretical analysis is based on decomposition of sequences into symmetric and anti-symmetric polynomials defined as a series expansion of reduced Chebyshev polynomials of the first kind. Two variants of closed form functions are presented \u2014 each characterised by using a recurrence relationship in Chebyshev polynomials. The first exploits the well known three terms recurrence relationships of Chebyshev polynomials. The second hitherto unused recurrence properties of Chebyshev coefficients defining a set of coefficients and zeros used for bootstrapping calculation of LSFs. The theory is tested using bootstrapped calculation of zeros and by evaluating the complexity of the closed form function. The results of the lower complexity calculations show that real axis zeros are within a given iteration tolerance when compared to results of a standard root-finder.", "title": "On closed form calculation of line spectral frequencies (LSF)"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/ouali14_interspeech.html", "abstract": "In this paper, we present the latest improvements on spectrogram-matrix based fingerprinting system for detecting transformed audio copies. In particular, we experiment with two feature parameters derived using global and local spectrogram averages and show that combining results from these two feature parameters significantly improves performance. We test our system on TRECVID 2010 content-based copy detection dataset. Experimental results show the robustness of our method against various audio distortions. The proposed method reduces the minimal Normalized Detection Cost Rate (min NDCR) by 23% and improves localization accuracy by 24% compared to a state-of-the-art audio fingerprinting system. Our system achieves the lowest min NDCR for MP3 compression transformation with a relative improvement of 60%.", "title": "Robust features for content-based audio copy detection"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/jiang14_interspeech.html", "abstract": "While human listening is robust in complex auditory scenes, current speech segregation algorithms do not perform well in noisy and reverberant environments. This paper addresses the robustness in binaural speech segregation by employing binary classification based on deep neural networks (DNNs). We systematically examine DNN generalization to untrained configurations. Evaluations and comparisons show that DNN based binaural classification produces superior segregation performance in a variety of multisource and reverberant conditions.", "title": "Binaural deep neural network classification for reverberant speech segregation"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/anguera14b_interspeech.html", "abstract": "As part of the MediaEval 2013 benchmark evaluation campaign, the objective of the Spoken Web Search (SWS) task was to perform Query-by-Example Spoken Term Detection (QbE-STD) using audio queries in a low-resource setting. After two successful editions and a continuously growing interest in the scientific community, a special effort was made in SWS 2013 to prepare a challenging database, including speech in 9 different languages with diverse environment and channel conditions. In this paper, first we describe the database and the performance metrics. Then, we briefly review the algorithmic approaches followed by participants and present and discuss the obtained performances, which demonstrate the feasibility of the proposed task, even under such challenging conditions (multiple languages and unconstrained acoustic conditions). Finally, we analyze the fusion of the top-performing systems, which achieved a 30% relative improvement over the best single system in the evaluation, proving that a variety of approaches can be effectively combined to bring complementary information in the search for queries.", "title": "Query-by-example spoken term detection on multilingual unconstrained speech"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/soto14_interspeech.html", "abstract": "We review the performance of a new two-stage cascaded machine learning approach for rescoring keyword search output for low resource languages. In the first stage Confusion Networks (CNs) are rescored for improved Automatic Speech Recognition (ASR) by reranking the arcs of each confusion bin. In the second stage we generate keyword search hypotheses from the rescored ASR output and rescore them using logistic regression classifiers to detect true hits and false alarms. We compare the performance of our system with state of the art rescoring techniques, including probability of false alarm normalization, exponential normalization, rank-normalized posterior scores and sum-to-one normalization and show promising results. Experimental validation is performed using the Term Weighted Value (TWV) metric on four corpora from the IARPA-Babel program for keyword search on low resource languages, including Assamese, Bengali, Lao and Zulu.", "title": "A comparison of multiple methods for rescoring keyword search lists for low resource languages"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/karakos14_interspeech.html", "abstract": "We compare several approaches, separately and together, for spotting of out-of-vocabulary (OOV) keywords, in terms of their ATWV scores. We considered three types of recognition units (whole words, syllables, and subwords of different lengths) and two basic search strategies (whole-unit, fuzzy phonetic search). In all cases, the search was performed by collapsing the recognition lattice into a consensus network, either in terms of the recognized whole units, or by first splitting the recognized units into phonemes. We ran experiments on five languages, for which the language model and vocabulary were derived from only 10 hours of transcriptions (70k-100k words of text), resulting in keyword OOV rates varying from 10% to 63% on new data, depending on the language. Our conclusions were that: 1) In all cases, the fuzzy phonetic search on phoneme-split lattices is better than searching for the whole units, 2) The syllable units are the best of the subword units for OOV keyword detection using fuzzy phonetic search, and 3) These methods combine very well, sometimes resulting in ATWV scores for OOV terms which are not too far below those of IV terms.", "title": "Subword and phonetic search for detecting out-of-vocabulary keywords"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/wang14l_interspeech.html", "abstract": "We compare several approaches, separately and together, for spotting of out-of-vocabulary (OOV) keywords, in terms of their ATWV scores. We considered three types of recognition units (whole words, syllables, and subwords of different lengths) and two basic search strategies (whole-unit, fuzzy phonetic search). In all cases, the search was performed by collapsing the recognition lattice into a consensus network, either in terms of the recognized whole units, or by first splitting the recognized units into phonemes. We ran experiments on five languages, for which the language model and vocabulary were derived from only 10 hours of transcriptions (70k-100k words of text), resulting in keyword OOV rates varying from 10% to 63% on new data, depending on the language. Our conclusions were that: 1) In all cases, the fuzzy phonetic search on phoneme-split lattices is better than searching for the whole units, 2) The syllable units are the best of the subword units for OOV keyword detection using fuzzy phonetic search, and 3) These methods combine very well, sometimes resulting in ATWV scores for OOV terms which are not too far below those of IV terms.", "title": "An in-depth comparison of keyword specific thresholding and sum-to-one score normalization"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/lee14c_interspeech.html", "abstract": "Acoustic feature similarity between search results has been shown to be very helpful for the task of spoken term detection (STD). A graph-based re-ranking approach for STD has been proposed based on the concept that search results, which are acoustically similar to other results with higher confidence scores, should have higher scores themselves. In this approach, the similarity between all search results of a given term are considered as a graph, and the confidence scores of the search results propagate through this graph. Since this approach can improve STD results without any additional labelled data, it is especially suitable for STD on languages with limited amounts of annotated data. However, its performance has not been widely studied on benchmark corpora. In this paper, we investigate the effectiveness of the graph-based re-ranking approach on limited language data from the IARPA Babel program. Experiments on the low-resource languages, Assamese, Bengali and Lao, show that graph-based re-ranking improves STD systems using fuzzy matching, and lattices based on different kinds of units including words, subwords, and hybrids.", "title": "Graph-based re-ranking using acoustic feature similarity between search results for spoken term detection on low-resource languages"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/le14b_interspeech.html", "abstract": "This paper presents recent progress in developing speech-to-text (STT) and keyword spotting (KWS) systems for the 2014 IARPA-Babel evaluation. Systems have been developed for the limited language pack condition for four of the five development languages in this program phase: Assamese, Bengali, Haitian Creole and Zulu. The systems have several novel characteristics that support rapid development of KWS systems. On the STT side different acoustic units are explored based on phonemic or graphemic representations, and system combination is used to improve STT performance. The acoustic models are trained on only 10 hours of speech data with manual transcriptions, completed with unsupervised training on additional untranscribed data. Both word and subword units (morphologically decomposed, syllables, phonemes) are used for KWS. The KWS systems are based on the multi-hypotheses produced by a consensus network decoding or searching word lattices. The word error rates of the individual STT systems are on the order of 50\u201360%, and the KWS systems obtain Maximum Term Weighted Values ranging from 30\u201345% for all keywords (in-vocabulary and out-of-vocabulary (OOV)). Sub-word units are shown to be successful at locating some of the OOV keywords, and system combination improves system performance.", "title": "Developing STT and KWS systems using limited language resources"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/hartmann14_interspeech.html", "abstract": "For languages with limited training resources, out-of-vocabulary (OOV) words are a significant problem, both for transcription and keyword spotting. This paper investigates the use of subword lexical units for keyword spotting. Three strategies for using the sub-word units are explored: 1) converting word-based lattices to subword lattices after decoding, 2) performing a separate decoding for each subword type, and 3) a single decoding using all possible subword units. In these experiments, the best performance is achieved by carrying out a separate decoding for each subword type. Further gains are attained through system combination. We also find that ignoring word boundaries improves the detection of OOV keywords without significantly impacting in-vocabulary keyword detection. Results are presented on four languages from the IARPA Babel Program (Haitian Creole, Assamese, Bengali, and Zulu).", "title": "Comparing decoding strategies for subword-based keyword spotting in low-resourced languages"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/ma14b_interspeech.html", "abstract": "The identification of keyword queries in speech data from low-resources languages poses a challenge for current methods as speech recognition algorithms lack sufficient training data to produce high accuracy transcript. To compensate for these shortcomings, we extract signals from the data that are useful in keyword identification but are not being used by the speech recognizer. These signals take multiple forms \u2014 word burstiness, rescored confusion network posteriors and acoustic/prosodic qualities. The former denotes the tendency for keywords to occur in bursts within a conversational topic. We employ three different strategies to exploit this information: 1) a four-way classification of keyword hypotheses that targets low-scoring correct hits and high-scoring false alarms, 2) ranking algorithms, and 3) a direct adjustment of keyword hit scores based on hypothesized repetition. We find that interpolating the results of these three strategies in an ensemble provides a reliable way to improve the results of keyword search.", "title": "Strategies for rescoring keyword search results using word-burst and acoustic features"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/xu14e_interspeech.html", "abstract": "Two problems make Spoken Term Detection (STD) particularly challenging under low-resource conditions: the low quality of speech recognition hypotheses, and a high number of out-of-vocabulary (OOV) words. In this paper, we propose an intuitive way to handle OOV terms for STD on word-based Confusion Networks using phonetic similarities, and generalize it into a probabilistic and vocabulary-independent retrieval framework. We then reflect on how several heuristics and Machine Learning based methods can be incorporated into this framework to improve retrieval performance. We present experimental results on several low-resource languages from IARPA's Babel program, such as Assamese, Bengali, Haitian, and Lao.", "title": "Word-based probabilistic phonetic retrieval for low-resource spoken term detection"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/chen14l_interspeech.html", "abstract": "We propose a keyword-boosted state-level minimum Bayes risk (sMBR) criterion for training DNN-HMM hybrid keyword search systems by enhancing acoustic detail of a given list of target keyword terms. The rationale behind the proposed discriminative training strategy is to place more acoustic modeling emphasis on states appearing in the given keywords. We observed a relative gain of 1.7~6.1% in actual term weighted value (ATWV) performance with the proposed keyword-boosted sMBR training over the conventional sMBR systems when tested on the IARPA Babel program's Vietnamese limited-language-pack task. A detailed result analysis suggests that the proposed sMBR objective function effectively improves the ATWV scores by boosting the probability of detecting keywords appearing in the system output with an increased correct and insertion rates in the decoded lattices.", "title": "A keyword-boosted sMBR criterion to enhance keyword search performance in deep neural network based acoustic modeling"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/chiu14_interspeech.html", "abstract": "Spoken Term Detection (STD) focuses on finding instances of a particular spoken word or phrase in an audio corpus. Most STD systems have a two-step pipeline, ASR followed by search. Two approaches to search are common, Confusion Network (CN) based search and Finite State Transducer (FST) based search. In this paper, we examine combination of these two different search approaches, using the same ASR output. We find that the CN search performs better on shorter queries, and FST search performs better on longer queries. By combining the different search results from the same ASR decoding, we achieve better performance compared to either search approach on its own. We also find that this improvement is additive to the usual combination of decoder results using different modeling techniques.", "title": "Combination of FST and CN search in spoken term detection"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/liu14f_interspeech.html", "abstract": "The point process model (PPM) for keyword search is a whole-word parametric modeling framework based on the timing of phonetic events rather than the evolution of frame-level phonetic likelihoods. Recent progress in PPM training and decoding algorithms has yielded state-of-the-art phonetic search performance in high-resource settings, both in terms of accuracy and computational efficiency. In this paper, we consider PPM application to low-resource settings where the amount of transcribed speech is severely limited and the pronunciation dictionary is incomplete. By using (i) state-of-the-art deep neural network acoustic models to generate phonetic events and (ii) grapheme-to-phoneme conversion to generate pronunciations for out-of-vocabulary (OOV) keywords, we find the PPM system reaches state-of-the-art OOV search performance at a small computational cost. Moreover, due to their complementary methodologies, combining PPM outputs with the LVCSR baseline produces average relative ATWV improvements of 7% and 50% for in-vocabulary and OOV keywords, respectively (16% overall).", "title": "Low-resource open vocabulary keyword search using point process models"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/ohtani14_interspeech.html", "abstract": "This paper describes a novel GMM-based bandwidth extension (BWE) method based on a sub-band basis spectrum model (SBM), in which each dimensional component represents a specific acoustic space in the frequency domain. The proposed method can achieve the BWE from a speech data with an arbitrary frequency bandwidth while the conventional methods perform the conversion from a fixed narrowband data. In the proposed method, we train a GMM with SBM parameters extracted from wideband spectra in advance. An input signal with a limited frequency band is converted into a wideband signal by estimating high-band SBM components from low-band SBM components of the input signal based on the GMM. The results of some objective and subjective evaluations show that the proposed method extends bandwidth of speech data robustly.", "title": "GMM-based bandwidth extension using sub-band basis spectrum model"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/nakamura14_interspeech.html", "abstract": "In statistical speech synthesis, the quality of the synthesized speech depends on the quality of training data. As the sampling rate of speech is one of the effective factors, speech data has been recently recorded at a high sampling rate. However, the sampling rates of speech data recorded in the past or collected from the Internet were often low. Therefore, to use these speech data effectively for model training, we propose a mel-cepstral analysis technique that restores missing high frequency components from low-sampling-rate speech with a statistical approach. In this technique, high-sampling-rate speech waveforms are modeled directly by integrating feature extraction and modeling processes. This framework makes it possible to optimize whole processes on the basis of an integrated objective function. Then, mel-cepstral coefficients are estimated from the low-sampling-rate speech by using the model as a prior distribution. Experimental results show that the proposed method improved the quality of synthesized speech.", "title": "A mel-cepstral analysis technique restoring high frequency components from low-sampling-rate speech"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/lee14d_interspeech.html", "abstract": "Studies show that professional singing matches well the associated melody and typically exhibits spectra different from speech in resonance tuning and singing formant. Therefore, one of the important topics in speech-to-singing conversion is to characterize the spectral transformation between speech and singing. This paper extends two types of spectral transformation techniques, namely voice conversion and model adaptation, and examines their performance. For the first time, we carry out a comparative study over four singing voice synthesis techniques. The experiments on various data sizes reveal that maximum-likelihood Gaussian mixture model (ML-GMM) of voice conversion always delivers the best performance in terms of spectral estimation accuracy; while model adaptation generates the best singing quality in all cases. When a large dataset is available, both techniques achieve the highest similarity to target singing. With a small dataset, the highest similarity is obtained by ML-GMM. It is also found that the music context-dependent modeling in adaptation, in which detailed partition of transform space is involved, leads to pleasant singing spectra.", "title": "A comparative study of spectral transformation techniques for singing voice synthesis"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/saito14_interspeech.html", "abstract": "This paper describes a novel approach to construct a mapping function between a given speaker pair using probability density functions (PDF) of matrix variate. In voice conversion studies, two important functions should be realized: 1) precise modeling of both the source and target feature spaces, and 2) construction of a proper transform function between these spaces. Voice conversion based on Gaussian mixture model (GMM) is the de facto standard because of their flexibility and easiness in handling. In GMM-based approaches, a joint vector space of the source and target is first constructed, and the joint PDF of the two vectors is modeled as GMM in the joint vector space. The joint vector approach mainly focuses on precise modeling of the `joint' feature space, and does not always construct a proper transform between two feature spaces. In contrast, the proposed method constructs the joint PDF as GMM in a matrix variate space whose row and column respectively correspond to the two functions, and it has potential to precisely model both the characteristics of the feature spaces and the relation between the source and target spaces.", "title": "Application of matrix variate Gaussian mixture model to statistical voice conversion"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/wu14b_interspeech.html", "abstract": "Recently, exemplar-based sparse representation methods have been proposed for voice conversion. These methods reconstruct a target spectrum through a weighted linear combination from a set of basis spectra, called exemplars. To include temporal constraint, multiple-frame exemplars are employed when estimating the linear combination weights, namely activations, by the nonnegative matrix factorization technique with a sparsity constraint. In practice, low-resolution mel-scale filter bank energies rather than high-resolution spectra are employed to estimate the activations in order to reduce computational cost and memory usages. However, the conversion performance degrades due to the loss of the spectral details in the low-resolution representations. In this study, we propose a joint nonnegative matrix factorization technique to estimate the activations using both the low- and high-resolution features simultaneously. In this way, we include temporal information by using multiple-frame low-resolution exemplars for computational efficiency and one-frame high-resolution exemplars to improve spectral details at the same time. The VOICES database was employed to assess the performance of the proposed method. The experiments confirmed the effectiveness of the proposed method over conventional nonnegative matrix factorization method in term of both objective spectral distortion and subjective evaluation.", "title": "Joint nonnegative matrix factorization for exemplar-based voice conversion"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/kobayashi14_interspeech.html", "abstract": "This paper presents a novel statistical singing voice conversion (SVC) technique with direct waveform modification based on the spectrum differential that can convert voice timbre of a source singer into that of a target singer without using a vocoder to generate converted singing voice waveforms. SVC makes it possible to convert singing voice characteristics of an arbitrary source singer into those of an arbitrary target singer. However, speech quality of the converted singing voice is significantly degraded compared to that of a natural singing voice due to various factors, such as analysis and modeling errors in the vocoder-based framework. To alleviate this degradation, we propose a statistical conversion process that directly modifies the signal in the waveform domain by estimating the difference in the spectra of the source and target singers' singing voices. The differential spectral feature is directly estimated using a differential Gaussian mixture model (GMM) that is analytically derived from the traditional GMM used as a conversion model in the conventional SVC. The experimental results demonstrate that the proposed method makes it possible to significantly improve speech quality in the converted singing voice while preserving the conversion accuracy of singer identity compared to the conventional SVC.", "title": "Statistical singing voice conversion with direct waveform modification based on the spectrum differential"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/ellis14_interspeech.html", "abstract": "It is common to be carrying an advanced computational device with a microphone \u2014 a smartphone \u2014 on your person at virtually all times. One application this makes possible is to automatically detect when individuals are in close proximity by detecting the similarity between the acoustic ambience recorded by body-worn mics. This paper investigates two techniques for proximity detection on a database of personal audio recordings made by six participants in a poster presentation session. We show that cross-correlation between 10 s windows is effective for detecting when individuals are close enough to be in conversation, and that using a fingerprinting approach based on acoustic landmarks is comparably accurate for this task, while at the same time being much more efficient, privacy-preserving, and viable for detecting proximity between a large number of body-worn devices.", "title": "Detecting proximity from personal audio recordings"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/phan14_interspeech.html", "abstract": "This paper proposes an approach for the efficient automatic joint detection and localization of single-channel acoustic events using random forest regression. The audio signals are decomposed into multiple densely overlapping superframes annotated with event class labels and their displacements to the temporal starting and ending points of the events. Using the displacement information, a multivariate random forest regression model is learned for each event category to map each superframe to continuous estimates of onset and offset locations of the events. In addition, two classifiers are trained using random forest classification to classify superframes of background and different event categories. On testing, based on the detection of category-specific superframes using the classifiers, the learned regressor provides the estimates of onset and offset locations in time of the corresponding event. While posing event detection and localization as a regression problem is novel, the quantitative evaluation on ITC-Irst database of highly variable acoustic events shows the efficiency and potential of the proposed approach.", "title": "Acoustic event detection and localization with regression forests"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/ferras14b_interspeech.html", "abstract": "Speech activity detection (SAD) is a conceptually simple task that still poses serious challenges for speech processing in a large variety of scenarios. Current energy-based and model-based approaches tend to directly segment speech and non-speech classes, but are not robust enough to non-stationary noise. In this paper, we use a multi-source activity detection (MSAD) approach to SAD by finding the activity levels of speech and a set of non-speech acoustic sources. Public talks such as TED involve a large variety of non-speech audio that is difficult to handle with standard SAD systems. We evaluate the effect of using either the proposed MSAD system versus a tailored version of the popular SHOUT SAD system. We evaluate our approach on a subset of the TED data to show the effectiveness of the technique, with and without a sparsity constraint on the vector of acoustic source activities.", "title": "Multi-source posteriors for speech activity detection on public talks"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/dennis14_interspeech.html", "abstract": "The time-frequency spectrogram representation of an audio signal can be visually analysed by a trained researcher to recognise any underlying sound events in a process called \u201cspectrogram reading\u201d. However, this has not become a popular approach for automatic classification, as the field is driven by Automatic Speech Recognition (ASR) where frame-based features are popular. As opposed to speech, sound events typically have a more distinctive time-frequency representation, with the energy concentrated in a small number of spectral components. This makes them more suitable for classification based on their visual signature, and enables inspiration to be found in techniques from the related field of image processing. Recently, there have been a range of techniques that extract image processing-inspired features from the spectrogram for sound event classification. In this paper, we introduce the idea and structure behind six recent spectrogram image methods and analyse their performance on a large database containing 50 different environmental sounds to give a standardised comparison that is not often available in sound event classification tasks.", "title": "Analysis of spectrogram image methods for sound event classification"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/satt14_interspeech.html", "abstract": "We provide evidence to the potential use of simple spoken tasks for automatic assessment of very early dementia. Timely detection of dementia is required for effective psychological treatment and to enable patients to participate in new drug therapy research. The technology enables automatic, cheap, remote and wide-scale screening of dementia, typically a costly and complex procedure. It can aid clinicians in the diagnosis of very early dementia, as well as assessing the disease progression.   We describe the spoken tasks, and their respective language-independent vocal feature extraction, followed by classification accuracy evaluation. We use recordings from over 60 persons, diagnosed as healthy-control (CTRL) / mild-cognitive-impairment (MCI) / early-stage-Alzheimer-disease and early-mixed-dementia (AD).   We present a new data regularization technique to overcome data sparseness due to the limited data set size. Next, we present a comprehensive statistical analysis, showing that the suggested classifier generalizes, and revealing the role and the statistical importance of the different spoken tasks and their respective vocal features.   We demonstrate classification accuracy of about 80% for CTRL vs. MCI and MCI vs. AD, and 87% for CTRL vs. AD, all shown to generalize. This provides an evidence for potential use for automatic detection of very early dementia.", "title": "Speech-based automatic and robust detection of very early dementia"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/raboshchuk14_interspeech.html", "abstract": "The acoustic environment of a typical neonatal intensive care unit (NICU) is very rich and may contain a large number of different sounds, which come either from the equipment or from the human activities taking place in it. There exists a medical concern about the effect of that acoustical environment on preterm infants, since loud sounds or particular sounds may be harmful for their further neurological development. In this work, first of all, an initial description of the acoustic characteristics of the NICU has been carried out using a set of diverse recordings produced with microphones placed both inside and outside an incubator. Then, the work has focused on detection of the most relevant types of sounds. In this paper, after describing the recorded database and the acoustic environment, preliminary experiments for detection of the acoustic alarms of devices are reported. The proposed detection system is based on Deep Belief Networks (DBN). The experimental results show that the DBN-based system is able to achieve better results than a baseline GMM-based system.", "title": "On the acoustic environment of a neonatal intensive care unit: initial description, and detection of equipment alarms"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/fox14b_interspeech.html", "abstract": "Noisy listening conditions are challenging to non-native listeners who typically perform poorly while attending to several competing talkers. This study examined whether non-native listeners are able to utilize dialect-related cues in the target and in the masking speech, even if they do not reach the proficiency level of the native listeners. 35 Indonesian-English bilinguals residing in the United States were presented with speech stimuli from two American English dialects, General American English and Southern American English, which were systematically varied both in the target sentences and in 2-talker masking babble at three sound-to-noise ratios (SNR). We found that the non-native listeners were (1) sensitive to dialect-specific phonetic details in speech of competing talkers and (2) performed in a manner similar to native listeners despite their apparent deficit. However, their performance differed significantly when the speech levels of the competing talkers were equal (0 dB SNR). The differential sensitivity of non-native listeners may reflect their inability to separate utterances of competing talkers when there is not enough contrast in their voice levels. In turn, the lack of sufficient contrast may reduce their ability to benefit from the phonetic-acoustic details necessary to encode the signal and comprehend a message.", "title": "Non-native perception of regionally accented speech in a multitalker context"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/turco14_interspeech.html", "abstract": "This study compares rising contours produced in the context of contrastive topics by French natives and by low and high proficient learners of French with German as mother tongue. Results show a systematic pattern for French natives who mostly produced a final rise LH*, and hardly ever a bridge accent on the whole phrase. Our results on French natives seem to support earlier claims that tonal patterns with late dip alignments may be recruited for encoding contrast meaning. Results on French learners show a development in the acquisition of the prosody-semantics mapping principles (shifting the accent position from the phrase-initial mon to the phrase-final image) and, not surprisingly, differences in the phonetic implementation of the final rises. Crucially, the impact of phonological and phonetic transfer is more complex than expected: text-to-tune associations are not easy to re-programme when a new accent location has to be learnt. However, once the phonology is learnt, the phonetic implementation starts being problematic.", "title": "A crosslinguistic and acquisitional perspective on intonational rises in French"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/tu14b_interspeech.html", "abstract": "Previous studies on Mandarin tone production indicate that there is no agreement on which tones are most difficult for L2 learners. Much of previous research on L2 learning of Mandarin tones has focused on monosyllables. In modern Mandarin, however, it is disyllabic words that dominate the vocabulary. This research investigates the production of Mandarin disyllabic tones by Japanese learners. In the current study, 25 Japanese learners of Mandarin were requested to produce 80 Mandarin disyllabic words with all tonal combinations (except for the neutral tone). The overall results showed a hierarchy of difficulty: Tone 3 > Tone 2 > Tone 1 = Tone 4. Most errors in the first syllable were found for Tone 2 and Tone 3 when followed by Tone 1 or Tone 4 (both start with a high pitch). In the second syllable, most errors were found for Tone 3 (misproduced as Tone 2). The findings are discussed in terms of the phonetic nature of Mandarin lexical tones and the interference from Japanese phonology.", "title": "Error patterns of Mandarin disyllabic tones by Japanese learners"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/leong14_interspeech.html", "abstract": "Infant-directed speech (IDS) supports language learning via mechanisms that are still not well-understood. Here, we adopt a `temporal sampling' perspective to investigate whether rhythmic enhancements in the temporal structure of IDS could support multi-timescale neuronal oscillatory sampling of the speech signal by the infant brain. We compare natural maternal speech directed to infants at the ages of 7-, 9-, 11- and 19-months, to adult-directed speech (ADS). Speech temporal structure is analysed using a novel multi-timescale Spectral-Amplitude Modulation Phase Hierarchy (S-AMPH) model, which extracts the Stress-rate, Syllable-rate and Phoneme-rate modulations (i.e. temporal patterns). Compared to ADS, we find that IDS shows a `stress-shifted' temporal profile. Stress-rate modulations dominate the modulation spectrum of IDS, whereas Syllable-rate modulations are dominant in ADS. Further, multi-timescale phase-synchronisation measures indicate that in IDS, Syllable-rate modulations are more synchronised to Stress-rate modulations and less synchronised to Phoneme-rate modulations. Thus, when speaking to infants, mothers pattern their syllables more regularly with prosodic stress, while allowing the phonemes within uttered syllables to vary more in timing. Accordingly, we conclude that the temporal structure of (Australian English) IDS is primarily stress-dominant, which could `tune' the infant brain toward stress-based speech segmentation \u2014 an adaptive strategy for boot-strapping early language learning.", "title": "Infant-directed speech enhances temporal rhythmic structure in the envelope"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/wewalaarachchi14_interspeech.html", "abstract": "Language learners have to contend with systematic variation in the input in the form of morpho-phonological change. In particular, tone Sandhi refers to a morphophonemic alternation referring to a license tone substitution under particular conditions. Two cohorts of children (3\u20134 years and 4\u20135 years) were tested on their ability to recognize words under four different conditions. The first three conditions entailed a change in the form of a word due to i) a native phonological rule (Tone 3 Sandhi), ii) a phonologically illegal tone substitution and iii) absence of a substitution when Tone 3 Sandhi was warranted. The fourth condition consisted of the correct, unaltered form of a word, which served as a control condition. Results demonstrated that 3 to 4 year old children were not able to recognize words that were subjected to Sandhi even when Sandhi was licensed by the phonological context, although they were able to correctly identify instances where the Sandhi rule was omitted, resulting in mispronunciations. By 5 years, children were able to recognize words when Sandhi was applied correctly. Results point to a stabilization of word recognition abilities during the preschool years with respect to suprasegmental morphophonemic change.", "title": "Influences of tone sandhi on word recognition in preschool children"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/goh14_interspeech.html", "abstract": "Several previous studies have investigated the extent to which segmental detail is represented in the developing lexicon. However, the majority of previous studies have focused on consonant and vowel representation, with little attention to the representation of lexicon tone in spite of its predominance in languages of the world. The current research provides a direct comparison of vowels, consonants and lexical tone representation using a mispronunciation paradigm in children at two ages (2.5 to 3.5 years and 4 to 5 years). Results point to asynchronous emergence of sensitivity to vowel, consonant and tone variation. Tone sensitivity appeared to be high in the younger cohort relative to vowels and consonants. By contrast, vowel and consonant sensitivity was strong in the older age group, with tone sensitivity appearing relatively weak. Findings point to a close coupling in vowel and consonant sensitivity in the preschool years and a dissociation with lexical tone.", "title": "Lexical representation of consonant, vowels and tones in early childhood"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/francisco14_interspeech.html", "abstract": "Reading is an audiovisual process that requires the learning of systematic links between graphemes and phonemes. It is thus possible that reading impairments reflect an audiovisual processing deficit. In this study, we compared audiovisual processing in adults with developmental dyslexia and adults without reading difficulties. We focused on differences in cross-modal temporal sensitivity both for speech and for non-speech events. When compared to adults without reading difficulties, adults with developmental dyslexia presented a wider temporal window in which unsynchronized speech events were perceived as synchronized. No differences were found between groups for the non-speech events. These results suggests a deficit in dyslexia in the perception of cross-modal temporal synchrony for speech events.", "title": "Audiovisual temporal sensitivity in typical and dyslexic adult readers"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/derrick14b_interspeech.html", "abstract": "We follow up on research demonstrating that aero-tactile information can enhance or interfere with accurate auditory perception among uninformed and untrained perceivers. We computationally extract aperiodic information from auditory recordings of speech, which represents turbulent air-flow produced from the lips. This extracted signal is used to drive a piezoelectric air-pump producing air-flow to the right temple simultaneous with presentation of auditory recordings. Using forced-choice experiments, we replicate previous results with stops, finding enhanced perception of /pa/ in /pa/ vs. /ba/ pairs, and /ta/ in /ta/ vs. /da/ pairs. We also found enhanced perception of /fa/ in /ba/ vs. /fa/ pairs, and /sha/ in /da/ vs. /sha/ pairs, demonstrating that air flow during fricative production contacting the skin can also enhance speech perception. The results show that aero-tactile information can be extracted from the audio signal and used to enhance speech perception of a large class of speech sounds found in many languages of the world.", "title": "Aero-tactile integration in fricatives: converting audio to air flow information for speech perception enhancement"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/mai14_interspeech.html", "abstract": "Previous studies have shown that slowly-varying amplitude modulations (AM) are crucial for speech comprehension. Moreover, recent neurophysiological studies showed low-frequency neural oscillations (< 10 Hz) are taking roles in tracking such critical AM cues which facilitate speech comprehension. However, many of such studies neglected the detailed spectral information (frequency modulations (FM)). The current paper conducted a behavioral experiment to study the relative importance of AM and FM cues for sentence intelligibility based on the hypothesis that such importance is modulated by speaking rate. By measuring the intelligibility of Mandarin sentences with selective removal of AM cues at particular AM rates or replacing FM cues with Gaussian noise, the current study found: (1) at a low speaking rate (4-Hz syllable rate), FM cues and high-rate AM cues made only marginal contributions to speech intelligibility, which is consistent with previous findings; (2) at high speaking rates (6- and 8-Hz syllable rates), however, FM cues made significant contributions even greater than AM cues at the rates suggested to be essential by previous neurophysiological studies. This result thus illustrates the relative importance of AM and FM cues at different speaking rates in Mandarin and implications for the neurophysiological speech processing were further discussed.", "title": "Relative importance of AM and FM cues for speech comprehension: effects of speaking rate and their implications for neurophysiological processing of speech"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/stringer14_interspeech.html", "abstract": "How vowels are organized cortically has previously been studied using auditory evoked potentials (AEPs), one focus of which is to determine whether perceptual distance could be inferred using AEP components. The present study extends this line of research by adopting a machine-learning framework to classify evoked responses to four synthetic mid-vowels differing only in second formant frequency (F2 = 840, 1200, 1680, and 2280 Hz). 6 subjects attended 4 EEG sessions each on separate days. Classifiers were trained using time-domain data in successive time-windows of various sizes. Results were the most accurate when a window of about 80 ms was used. By integrating the scores from individual classifiers, the maximum mean binary classification rates improved to 70% (10 trials) and 77% (20 trials). To assess how well perceptual distances among the vowels were reflected in our results, discriminability indices (d') were computed using both the behavioral results in a screening test and the classification results. It was found that the two set of indices were significantly correlated. The pair that was the most (least) discriminable behaviorally was also the most (least) classifiable neurally. Our results support the use of classification methodology for developing a neural measure of perceptual distance.", "title": "The effect of regional and non-native accents on word recognition processes: a comparison of EEG responses in quiet to speech recognition in noise"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/fong14_interspeech.html", "abstract": "How vowels are organized cortically has previously been studied using auditory evoked potentials (AEPs), one focus of which is to determine whether perceptual distance could be inferred using AEP components. The present study extends this line of research by adopting a machine-learning framework to classify evoked responses to four synthetic mid-vowels differing only in second formant frequency (F2 = 840, 1200, 1680, and 2280 Hz). 6 subjects attended 4 EEG sessions each on separate days. Classifiers were trained using time-domain data in successive time-windows of various sizes. Results were the most accurate when a window of about 80 ms was used. By integrating the scores from individual classifiers, the maximum mean binary classification rates improved to 70% (10 trials) and 77% (20 trials). To assess how well perceptual distances among the vowels were reflected in our results, discriminability indices (d') were computed using both the behavioral results in a screening test and the classification results. It was found that the two set of indices were significantly correlated. The pair that was the most (least) discriminable behaviorally was also the most (least) classifiable neurally. Our results support the use of classification methodology for developing a neural measure of perceptual distance.", "title": "Towards a neural measure of perceptual distance \u2014 classification of electroencephalographic responses to synthetic vowels"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/scharenborg14_interspeech.html", "abstract": "When trying to understand how listeners recognise words, listeners' misperceptions, so-called `slips of the ear', can reveal important aspects of the underlying mechanisms of normal word recognition. Such misperceptions shed light onto how inferences are made by listeners about acoustic details in the speech signal and how these interact with other sound sources in the background. On the other hand, if speech from a particular speaker is more prone to being misperceived than that from another speaker, these misperceptions may also shed light onto speaker characteristics. To study these phenomena, misperceptions that occur consistently are invaluable. Although such confusions are quite rare, within the Marie Curie INSPIRE project, software has been developed to efficiently collect such consistent confusions for different languages. Using this software, we have started to collect Dutch consistent confusions. Single words, embedded in five different types of noise at different SNRs, produced by four speakers were presented to Dutch listeners. In a preliminary analysis, consistent confusions were analysed in terms of phoneme substitutions, insertions, and deletions, reconstructions of words using background noise, and eccentric cases. Moreover, the number and types of consistent confusions obtained in the different noise types and from different speakers are compared.", "title": "Collecting a corpus of Dutch noise-induced `slips of the ear'"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/hanai14_interspeech.html", "abstract": "Arabic has an ambiguous mapping between words and pronunciations, making it a deep orthographic system. This ambiguity can be resolved through diacritics, which if displayed, would compose 30% of characters in a text. We investigate the different dimensions of lexical modeling, covering diacritics, pronunciation rules, and acoustic based pronunciation modeling.   We show the impact of explicitly modeling the different classes of diacritics (short vowels, geminates, nunnations). We further show that a phonetic lexicon, derived by applying simple pronunciation rules to diacritized words, offers the best gains in ASR performance. Finally, deriving pronunciations from acoustics, yields improvements, beyond a canonical lexicon.", "title": "Lexical modeling for Arabic ASR: a systematic approach"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/orosanu14_interspeech.html", "abstract": "This paper analyzes the use of hybrid language models for automatic speech transcription. The goal is to later use such an approach as a support for helping communication with deaf people, and to run it on an embedded decoder on a portable device, which introduces constraints on the model size. The main linguistic units considered for this task are the words and the syllables. Various lexicon sizes are studied by setting thresholds on the word occurrence frequencies in the training data, the less frequent words being therefore syllabified. A recognizer using this kind of language model can output between 62% and 96% of words (with respect to the thresholds on the word occurrence frequencies; the other recognized lexical units are syllables). By setting different thresholds on the confidence measures associated to the recognized words, the most reliable word hypotheses can be identified, and they have correct recognition rates between 70% and 92%.", "title": "Hybrid language models for speech transcription"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/gandhe14_interspeech.html", "abstract": "For resource rich languages, recent works have shown Neural Network based Language Models (NNLMs) to be an effective modeling technique for Automatic Speech Recognition, out performing standard n-gram language models (LMs). For low resource languages, however, the performance of NNLMs has not been well explored. In this paper, we evaluate the effectiveness of NNLMs for low resource languages and show that NNLMs learn better word probabilities than state-of-the-art n-gram models even when the amount of training data is severely limited. We show that interpolated NNLMs obtain a lower WER than standard n-gram models, no mater the amount of training data. Additionally, we observe that with small amounts of data (approx. 100k training tokens), feed-forward NNLMs obtain lower perplexity than recurrent NNLMs, while for the larger data condition (500k\u20131M training tokens), recurrent NNLMs can obtain lower perplexity than feed-forward models.", "title": "Neural network language models for low resource languages"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/gangireddy14_interspeech.html", "abstract": "The recurrent neural network language model (RNNLM) has been demonstrated to consistently reduce perplexities and automatic speech recognition (ASR) word error rates across a variety of domains. In this paper we propose a pre-training method for the RNNLM, by sharing the output weights of a feed forward neural network language model (NNLM) with the RNNLM. This is accomplished by first fine-tuning the weights of the NNLM, which are then used to initialise the output weights of an RNNLM with the same number of hidden units. We have carried out text-based experiments on the Penn Treebank Wall Street Journal data, and ASR experiments on the TED talks data used in the International Workshop on Spoken Language Translation (IWSLT) evaluation campaigns. Across the experiments, we observe small improvements in perplexity and ASR word error rate.", "title": "Feed forward pre-training for recurrent neural network language models"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/roy14_interspeech.html", "abstract": "Natural language is rich and varied, but also highly structured. The rules of grammar are a primary source of linguistic regularity, but there are many other factors that govern patterns of language use. Language models attempt to capture linguistic regularities, typically by modeling the statistics of word use, thereby folding in some aspects of grammar and style. Spoken language is an important and interesting subset of natural language that is temporally and spatially grounded. While time and space may directly contribute to a speaker's choice of words, they may also serve as indicators for communicative intent or other contextual and situational factors.   To investigate the value of spatial and temporal information, we build a series of language models using a large, naturalistic corpus of spatially and temporally coded speech collected from a home environment. We incorporate this extralinguistic information by building spatiotemporal word classifiers that are mixed with traditional unigram and bigram models. Our evaluation shows that both perplexity and word error rate can be significantly improved by incorporating this information in a simple framework. The underlying principles of this work could be applied in a wide range of scenarios in which temporal or spatial information is available.", "title": "Grounding language models in spatiotemporal context"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/jalalvand14_interspeech.html", "abstract": "The usage of Recurrent Neural Network Language Models (RNNLMs) has allowed reaching significant improvements in Automatic Speech Recognition (ASR) tasks. However, to take advantage of their capability for considering long histories, they are usually used to rescore the N-best lists (i.e. it is in practice not possible to use them directly during acoustic trellis search). We propose in this paper a novel method for rescoring directly the hypotheses contained in the word graphs, which are generated in the first pass of ASR decoding. The method, based on the A* stack search, rescores the partial theories of the stack with a log-linear combination of the acoustic model score and a linear combination of multiple language model scores (including RNNLM). We compared, on an ASR task consisting of the automatic transcription of English weather news, the A* based approach with N-best rescoring and iterative confusion network decoding. Using the proposed method, we measured a relative word error rate improvement of about 6%, on the given task, with respect to using the baseline system. The latter improvement is comparable with that obtained with N-best list based rescoring method.", "title": "Direct word graph rescoring using a* search and RNNLM"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/chelba14_interspeech.html", "abstract": "We propose a new benchmark corpus to be used for measuring progress in statistical language modeling. With almost one billion words of training data, we hope this benchmark will be useful to quickly evaluate novel language modeling techniques, and to compare their contribution when combined with other advanced techniques. We show performance of several well-known types of language models, with the best results achieved with a recurrent neural network based language model. The baseline unpruned Kneser-Ney 5-gram model achieves perplexity 67.6. A combination of techniques leads to 35% reduction in perplexity, or 10% reduction in cross-entropy (bits), over that baseline.   The benchmark is available as a code.google.com project; besides the scripts needed to rebuild the training/held-out data, it also makes available log-probability values for each word in each of ten held-out data sets, for each of the baseline N-gram models.", "title": "One billion word benchmark for measuring progress in statistical language modeling"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/schnall14_interspeech.html", "abstract": "Modifying the articulatory parameters to raise the prominence of a segment of an utterance (hyperarticulating) is usually accompanied by a reduction of these parameters (hypoarticulation) for the neighboring segments. In this paper we investigate different approaches for the automatic labeling of the prominence of words. In particular, we investigate how the information in the sequence can be used. During the recording of the underlying audio-visual database, the subjects were asked to make corrections for a misunderstanding of a single word of the system by using prosodic cues only. We extracted an extensive range of features from the audio and visual channel. For the classification of word prominence we compare two algorithms. On the one hand SVM, a local classifier, on the other hand a classifier based on a sequential model, linear chain Conditional Random Fields (CRF). Both were trained on different context regions. For the CRF the whole sentence is used as a word sequence for training and testing. Overall we show that sequence models such as CRF, which performs best in our experiment, are suited for prominence detection and, furthermore, that the neighboring words contain information which further improves the detection.", "title": "Integrating sequence information in the audio-visual detection of word prominence in a human-machine interaction scenario"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/biadsy14_interspeech.html", "abstract": "Maximum Entropy (MaxEnt) language models are linear models that are typically regularized via well-known L1 or L2 terms in the likelihood objective, hence avoiding the need for the kinds of backoff or mixture weights used in smoothed n-gram language models using Katz backoff and similar techniques. Even though backoff cost is not required to regularize the model, we investigate the use of backoff features in MaxEnt models, as well as some backoff-inspired variants. These features are shown to improve model quality substantially, as shown in perplexity and word-error rate reductions, even in very large scale training scenarios of tens or hundreds of billions of words and hundreds of millions of features.", "title": "Backoff inspired features for maximum entropy language models"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/telaar14_interspeech.html", "abstract": "We introduce BioKIT, a new Hidden Markov Model based toolkit to preprocess, model and interpret biosignals such as speech, motion, muscle and brain activities. The focus of this toolkit is to enable researchers from various communities to pursue their experiments and integrate real-time biosignal interpretation into their applications. BioKIT boosts a flexible two-layer structure with a modular C++ core that interfaces with a Python scripting layer, to facilitate development of new applications. BioKIT employs sequence-level parallelization and memory sharing across threads. Additionally, a fully integrated error blaming component facilitates in-depth analysis. A generic terminology keeps the barrier to entry for researchers from multiple fields to a minimum. We describe our onlinecapable dynamic decoder and report on initial experiments on three different tasks. The presented speech recognition experiments employ Kaldi [1] trained deep neural networks with the results set in relation to the real time factor needed to obtain them.", "title": "BioKIT \u2014 real-time decoder for biosignal processing"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/harwath14b_interspeech.html", "abstract": "Modern speech recognizers rely on three core components: an acoustic model, a language model, and a pronunciation lexicon. In order to expand speech recognition capability to low-resource languages and domains, techniques to peel away the expert knowledge required to craft these three components have been growing in popularity. In this paper, we present a method for automatically learning a weighted pronunciation lexicon in a data-driven fashion without assuming the existence of any phonetic lexicon whatsoever. Given an initial grapheme acoustic model, our method utilizes a novel technique for semi-constrained acoustic unit decoding, which is used to help train a letter to sound (L2S) model. The L2S model is then used in conjunction with a Pronunciation Mixture Model (PMM) to infer a pronunciation lexicon. We evaluate our method on English as well as Lao and Haitian, two low-resource languages featured in the IARPA Babel program.", "title": "Speech recognition without a lexicon \u2014 bridging the gap between graphemic and phonetic systems"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/zhao14b_interspeech.html", "abstract": "In this paper, we propose a new auxiliary-vector (AV) algorithm using the conjugate orthogonality for speech enhancement. When only a limited data record is available, the AV algorithm is the state-of-the-art for obtaining the minimum-variance-distortionless (MVDR) filter. However, the current AV algorithms suffer from convergence problems when applied to the speech enhancement. Based on the conjugate Gram-Schmidt process, we develop new auxiliary vectors that are conjugate orthogonal and apply them to the AV algorithm. The proposed conjugate AV algorithm converges to the optimal MVDR solution within finite steps no greater than the filter dimension. Theoretical analysis establishes formal convergence of the proposed conjugate AV algorithm. Our experiments using the synthetic and real speech data show favorites of the new proposal over the state-of-the-art approaches.", "title": "A new auxiliary-vector algorithm with conjugate orthogonality for speech enhancement"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/jathar14_interspeech.html", "abstract": "Motivated by the potential of speech modification for the enhancement of intelligibility in noisy environments, we study the acoustic characteristics of speech produced in the context of critical announcements made in noisy listening situations. A corpus of 3 speakers producing 20 Marathi train station announcements is analysed for articulatory-acoustic and prosodic differences between speech in noise and in quiet. It is observed that apart from the global changes that are characteristic of increased vocal effort, the intonation associated with phrase and word accents is consistently modified. Listening tests with modified speech suggest that spectral shaping and F0 modifications that are linguistically valid contribute constructively to increased intelligibility in noise, as measured in an information extraction task.", "title": "Acoustic characteristics of critical message utterances in noise applied to speech intelligibility enhancement"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/xu14f_interspeech.html", "abstract": "We propose three algorithms to address the mismatch problem in deep neural network (DNN) based speech enhancement. First, we investigate noise aware training by incorporating noise information in the test utterance with an ideal binary mask based dynamic noise estimation approach to improve DNN's speech separation ability from the noisy signal. Next, a set of more than 100 noise types is adopted to enrich the generalization capabilities of the DNN to unseen and non-stationary noise conditions. Finally, the quality of the enhanced speech can further be improved by global variance equalization. Empirical results show that each of the three proposed techniques contributes to the performance improvement. Compared to the conventional logarithmic minimum mean squared error speech enhancement method, our DNN system achieves 0.32 PESQ (perceptual evaluation of speech quality) improvement across six signal-to-noise ratio levels ranging from -5dB to 20dB on a test set with unknown noise types. We also observe that the combined strategies can well suppress highly non-stationary noise better than all the competing state-of-the-art techniques we have evaluated.", "title": "Dynamic noise aware training for speech enhancement based on deep neural networks"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/pertila14_interspeech.html", "abstract": "High level of noise reduces the perceptual quality and intelligibility of speech. Therefore, enhancing the captured speech signal is important in everyday applications such as telephony and teleconferencing. Microphone arrays are typically placed at a distance from a speaker and require processing to enhance the captured signal. Beamforming provides directional gain towards the source of interest and attenuation of interference. It is often followed by a single channel post-filter to further enhance the signal. Non-linear spatial post-filters are capable of providing high noise suppression but can produce unwanted musical noise that lowers the perceptual quality of the output. This work proposes an artificial neural network (ANN) to learn the structure of naturally occurring post-filters to enhance speech from interfering noise. The ANN uses phase-based features obtained from a multichannel array as an input. Simulations are used to train the ANN in a supervised manner. The performance is measured with objective scores from speech recorded in an office environment. The post-filters predicted by the ANN are found to improve the perceptual quality over delay-and-sum beamforming while maintaining high suppression of noise characteristic to spatial post-filters.", "title": "Microphone array post-filtering using supervised machine learning for speech enhancement"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/mani14_interspeech.html", "abstract": "In this paper, we propose a real-time method for duration modification of speech for packet based communication system. While there is rich literature available on duration modification, it fails to clearly address the issues in real-time implementation of the same. Most of the duration modification methods rely on accurate estimation of pitch marks, which is not feasible in a real-time scenario. The proposed method modifies the duration of Linear Prediction residual of individual frames without using any look-ahead delay and knowledge of pitch marks. In this method, multiples of pitch period is repeated or removed from a frame depending on a scheduling algorithm. The subjective quality of the proposed method was found to be better than waveform similarity overlap and add (WSOLA) technique as well as Linear Prediction Pitch Synchronous Overlap and Add (LP-PSOLA) technique.", "title": "Novel speech duration modifier for packet based communication system"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/liu14g_interspeech.html", "abstract": "In this paper we present some experiments using a deep learning model for speech denoising. We propose a very lightweight procedure that can predict clean speech spectra when presented with noisy speech inputs, and we show how various parameter choices impact the quality of the denoised signal. Through our experiments we conclude that such a structure can perform better than some comparable single-channel approaches and that it is able to generalize well across various speakers, noise types and signal-to-noise ratios.", "title": "Experiments on deep learning for speech denoising"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/mohammadiha14_interspeech.html", "abstract": "This paper proposes an exemplar-based speech enhancement method based on high-resolution STFT magnitude spectrograms, where a selection of the nonnegative training data is used as the dictionary to provide a holistic nonnegative representation of the test data. We discuss how this exemplar-based model ensures that the enhanced speech signal falls on the speech manifold, which improves the quality of the enhanced speech signal. To exploit the temporal continuity, a vector autoregressive model is used to model the activations where the model parameters are learned using a new NMF-based approach. Results from several supervised and semi-supervised speech enhancement experiments indicate that the proposed exemplar-based method outperforms the considered supervised and unsupervised denoising algorithms in terms of both segmental SNR and PESQ at different input SNRs.", "title": "Single-channel dynamic exemplar-based speech enhancement"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/kato14_interspeech.html", "abstract": "This work presents an approach to speech enhancement that operates using a speech production model to reconstruct a clean speech signal from a set of speech parameters that are estimated from the noisy speech. The motivation is to remove the distortion and residual and musical noises that are associated with conventional filtering-based methods of speech enhancement. The STRAIGHT vocoder forms the model for speech reconstruction and requires a time-frequency surface and fundamental frequency information. Hidden Markov model synthesis is used to create an estimate of the time-frequency surface and this is combined with the noisy surface using a perceptually motivated signal-to-noise ratio weighting. Experimental results compare the proposed reconstruction-based method to conventional filtering-based approaches of speech enhancement.", "title": "Using hidden Markov models for speech enhancement"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/pfeifenberger14_interspeech.html", "abstract": "In many hands-free applications, we encounter a speaker located in the near-field embedded in diffuse far-field noise. In this paper, we contribute an algorithm to estimate the speech and noise power spectral density (PSD) based on a direction-dependent SNR (DD-SNR). The only prior knowledge needed is a model of the diffuse noise sound field. The enhanced speech signal is obtained by a parametric multi-channel Wiener filter (PMWF), which is constructed without any speech presence or absence probabilities, or smoothing in frequency. We achieve high speech quality and sufficient noise reduction by iteratively improving the speech PSD estimate using the output of the PMWF. The performance of our algorithm is demonstrated by using the PESQ and PEASS measures.", "title": "Blind source extraction based on a direction-dependent a-priori SNR"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/chacon14_interspeech.html", "abstract": "Estimating the phase of sinusoids in noise in contrast to amplitude and frequency components has been less addressed in previous studies. In this paper, we derive the least squares phase estimator (LSPE) solution to recover the phase of an underlying signal observed in noise. Through Monte-Carlo simulations, we demonstrate the robustness of the proposed phase estimator against the modeling error. The proposed phase estimator is further evaluated in the speech enhancement setup to asses how much improvement is obtained by replacing the noisy phase with the LSPE when reconstructing the enhanced speech signal. Significant improvement in speech quality and speech intelligibility is obtained by replacing the noisy phase with the estimated phase provided by the proposed LSPE once the ambiguity in the phase candidates is removed.", "title": "Least squares phase estimation of mixed signals"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/ming14_interspeech.html", "abstract": "This paper presents a new approach to single-channel speech enhancement involving both noise and channel distortion (i.e., convolutional noise). The approach is based on finding longest matching segments (LMS) from a corpus of clean, wideband speech. The approach adds three novel developments to our previous LMS research. First, we address the problem of channel distortion as well as additive noise. Second, we present an improved method for modeling noise. Third, we present an iterative algorithm for improved speech estimates. In experiments using speech recognition as a test with the Aurora 4 database, the use of our enhancement approach as a preprocessor for feature extraction significantly improved the performance of a baseline recognition system. In another comparison against conventional enhancement algorithms, both the PESQ and the segmental SNR ratings of the LMS algorithm were superior to the other methods for noisy speech enhancement.", "title": "Speech enhancement from additive noise and channel distortion \u2014 a corpus-based approach"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/zhou14b_interspeech.html", "abstract": "A novel multi-channel speech enhancement technique is proposed in the present paper. We focus on the local sparsities of speech signals in contrast to the conventional beamforming and blind source seperation methods. The technique utilizes the difference of local structures in temporary-frequency domain between the target speech and interfering signals for enhancing the target speech. We first estimate the local structures of the speech and noise signals at each time-frequency bin to form a local dictionary, and then recover the clean speech via sparse coding. The proposed algorithm is simple to implement and requires no prior knowledge of speech and noise. Our experimental evaluations demonstrate that the proposed method can suppress interferer and meantime preserve target speech more than some conventional methods.", "title": "Multi-channel speech enhancement using sparse coding on local time-frequency structures"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/mirsamadi14_interspeech.html", "abstract": "Room reverberation is a primary cause of failure in distant speech recognition (DSR) systems. In this study, we present a multichannel spectrum enhancement method for reverberant speech recognition, which is an extension of a single-channel dereverberation algorithm based on convolutive nonnegative matrix factorization (NMF). The generalization to a multichannel scenario is shown to be a special case of convolutive nonnegative tensor factorization (NTF). The presented algorithm integrates information from across different channels in the magnitude short time Fourier transform (STFT) domain. By doing so, it eliminates any limitations on the array geometry or a need for information concerning the source location, making the algorithm particularly suitable for distributed microphone arrays. Experiments are performed on speech data using actual room impulse responses from AIR database. Relative WER improvements using a clean-trained ASR system vary from +7.1% to +30.1% based on the number of channels and the source to microphone distances (1 to 3 meters)", "title": "Multichannel speech dereverberation based on convolutive nonnegative tensor factorization for ASR applications"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/chen14m_interspeech.html", "abstract": "A successful speech enhancement system requires strong models for both speech and noise to decompose a mixture into the most likely combination. However, if the noise encountered differs significantly from the system's assumptions, performance will suffer. In previous work, we proposed a speech enhancement framework based on decomposing the noisy spectrogram into low rank background noise and a sparse activation of pre-learned templates, which requires few assumptions about the noise and showed promising results. However, when the noise is highly non-stationary or has large amplitude, the local SNR of the noisy speech can change drastically, resulting in less accurate decompositions between foreground speech and background noise. In this work, we extend the previous model by changing the modeling of the speech part to be the convolution of a sparse activation and pre-learned template patches, which enforces continuous structure within the speech and leads to better results in highly corrupted noisy mixtures.", "title": "Speech enhancement by low-rank and convolutive dictionary spectrogram decomposition"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/jaureguiberry14_interspeech.html", "abstract": "Amongst the speech enhancement techniques, statistical models based on Non-negative Matrix Factorization (NMF) have received great attention. In a single channel configuration, NMF is used to describe the spectral content of both the speech and noise sources. As the number of components can have a crucial influence on separation quality, we here propose to investigate model order selection based on the variational Bayesian approximation to the marginal likelihood of models of different orders. To go further, we propose to use model averaging to combine several single-order NMFs and we show that a straightforward application of model averaging principles is inefficient as it turned out to be equivalent to model selection. We thus introduce a parameter to control the entropy of the model order distribution which makes the averaging effective. We also show that our probabilistic model nicely extends to a multiple-order NMF model where several NMFs are jointly estimated and averaged. Experiments are conducted on real data from the CHiME challenge and give an interesting insight on the entropic parameter and model order priors. Separation results are also promising as model averaging outperforms single-order model selection. Finally, our multiple-order NMF shows an interesting gain in computation time.", "title": "Multiple-order non-negative matrix factorization for speech enhancement"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/kang14b_interspeech.html", "abstract": "Recently, lots of algorithms using machine learning approaches have been proposed in the speech enhancement area. One of the most well-known approaches is the non-negative matrix factorization (NMF) -based one which analyzes noisy speech with speech and noise bases. However, NMF-based algorithms have difficulties in estimating speech and noise encoding vectors when their subspaces overlap. In this paper, we propose a novel speech enhancement algorithm which uses deep neural network (DNN) to improve the encoding vector estimation of the NMF-based technique. A DNN is trained to represent the mapping from noisy speech to corresponding encoding vectors. The quality of the enhanced speech from the proposed NMF-based scheme adopting DNN-based encoding vector estimation is compared with that from the conventional NMF-based technique. The experimental results showed that the proposed speech enhancement algorithm outperformed the conventional NMF-based speech enhancement technique.", "title": "NMF-based speech enhancement incorporating deep neural network"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/sonowal14_interspeech.html", "abstract": "This paper presents a novel data-driven approach to single channel speech enhancement employing Gaussian process (GP). Our approach is based on applying GP regression to estimate the residual gain with the input features being the a priori and a posteriori signal-to-noise ratios (SNRs). The residual gain is defined as the difference between the optimal gain and that obtained from the minimum mean-square error log-spectral amplitude (MMSE-LSA) estimator. Our proposed approach involves a cascaded structure consisting of two stages. At the first stage, the gain of the MMSE-LSA estimator is calculated in conjunction with the SNR features. In the second stage, the residual gains are estimated through GP and they are used to further enhance the output of the MMSE-LSA module. Experimental results show that the proposed approach produced better speech quality than not only the MMSE-LSA enhancement module but also the other data-driven technique.", "title": "A data-driven approach to speech enhancement using Gaussian process"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/backstrom14_interspeech.html", "abstract": "Modern speech codecs based on Code Excited Linear Prediction (CELP) employ an analysis-by-synthesis optimization loop to find the best quantization of the source model parameters. With this approach, optimal quantization can be achieved only with an exhaustive search. Instead, we propose to use matrix factorization to decorrelate the objective function of the optimization problem, whereby the computationally expensive iteration can be avoided and optimal performance is guaranteed. We compare two factorizations of the autocorrelation matrix, the eigenvalue decomposition and Vandermonde factorization. Our experiments show that decorrelation improves perceptual SNR and gives a large reduction in computational complexity, mostly without significant impact on subjective quality.", "title": "Decorrelated innovative codebooks for ACELP using factorization of autocorrelation matrix"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/cernak14_interspeech.html", "abstract": "In this paper, we propose a solution to reconstruct stress and accent contextual factors at the receiver of a very low bitrate speech codec built on recognition/synthesis architecture. In speech synthesis, accent and stress symbols are predicted from the text, which is not available at the receiver side of the speech codec. Therefore, speech signal-based symbols, generated as syllable-level log average F0 and energy acoustic measures, quantized using a scalar quantization, are used instead of accentual and stress symbols for HMM-based speech synthesis. Results from incremental real-time speech synthesis confirmed, that a combination of F0 and energy signal-based symbols can replace their counterparts of text-based binary accent and stress symbols developed for text-to-speech systems. The estimated transmission bit-rate overhead is about 14 bits/second per acoustic measure.", "title": "Stress and accent transmission in HMM-based syllable-context very low bit rate speech coding"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/pulakka14_interspeech.html", "abstract": "Artificial bandwidth extension (ABE) methods have been developed to improve the quality and intelligibility of telephone speech. In many previous studies, however, the evaluation of ABE has not fully reflected the use of ABE in mobile communication (e.g., evaluation with clean speech without coding). In this study, the subjective quality of ABE was evaluated with absolute category rating (ACR) tests involving both clean and noisy speech, two cutoff frequencies of highpass filtering, and input encoded at different bit rates. Three ABE methods were evaluated, two for narrowband-to-wideband extension and one for wideband-to-superwideband extension. Several speech codecs with different audio bandwidths were included in the tests. Narrowband-to-wideband ABE methods were found to significantly improve the speech quality when no background noise was present, and the mean quality scores were slightly but not significantly increased for noisy speech. Wideband-to-superwideband ABE also showed significant improvement in certain conditions with no background noise. ABE did not cause significant decrease of the mean scores in any of the tests.", "title": "Subjective voice quality evaluation of artificial bandwidth extension: comparing different audio bandwidths and speech codecs"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/fu14b_interspeech.html", "abstract": "This paper proposes a stereo acoustic echo suppression method for duplex stereo teleconferencing. Firstly the stereo signals are combined in the frequency domain to build a single-channel signal. Secondly, with the single-channel signal model, an optimal Linearly Constrained Minimum Variance (LCMV) filter based on the Widely Linear theory is derived, which is applied on microphone signal directly to maximumly suppress the acoustic echo and preserve the quality of near-end signal. To estimate the normalized correlation vectors required in the LCMV filter, we employ an initial guess method, which doesn't require double-talk detector and inter-channel pre-decorrelation. The experimental results verify that this method can preserve the near-end speech quality as well as the spatial information, and suppresses echo greatly.", "title": "Stereo acoustic echo suppression using widely linear filtering in the frequency domain"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/lee14e_interspeech.html", "abstract": "In this paper, we propose an enhanced adaptive muting method using a sigmoid function, which is based on a parameter tracking technique for the packet loss concealment algorithm of ITUT G.722 speech codec. The packet loss concealment algorithm performs an adaptive muting to prevent the generation of unnecessary noises or clicks during packet loss recovery. While a conventional muting method applies the sigmoid function to the muting curve and the principal parameters of the sigmoid function are obtained by using a grid search-based training method, in the proposed muting algorithm, the parameters are substantially obtained from the previous good frames using the steepest descent algorithm, which minimizes the error between the desired signal and the reconstructed signal. From experimental results, the proposed adaptive muting method turns out to improve the performance of the conventional muting method under various experimental conditions.", "title": "Enhanced muting method in packet loss concealment of ITU-t g.722 using sigmoid function with on-line optimized parameters"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/wu14c_interspeech.html", "abstract": "The presence of near-end interferences and echo path changes make it essential for an adaptive filter to vary its learning rate according to corresponding conditions. In this paper, a robust step-size control algorithm which is based on the optimization of the square of the bin-wise a posteriori error is proposed. To prevent the adaptive filter from diverging in the presence of interferences, constraints on the filter update are applied. The learning rate expression is derived and then we extend the method to multidelay block frequency domain adaptive filter (MDF) so as to meet the demand of low delay in practical application. An updating strategy for the constraints is proposed as well. Experiments are carried out to demonstrate the superiority of the proposed approach, especially in double-talk and echo path change situations.", "title": "A robust step-size control algorithm for frequency domain acoustic echo cancellation"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/byambakhishig14_interspeech.html", "abstract": "In this paper, we focus on the problems associated with error correction of automatic speech recognition (ASR) based on confusion networks. The problems discussed are the availability of corpus in terms of calculating the semantic score and performance degradation for error correction using N-gram due to the null transitions in the confusion networks. In attempt to solve these problems, first, we employ Normalized Web Distance as a measure for semantic similarity between words that are located far from each other. The advantage of Normalized Web Distance is that it may use the Internet and so on for learning semantic similarity, which might solve the problem of corpus availability. Secondly, an error correction model without null nodes in confusion networks is trained using conditional random fields in order to improve the performance of error correction using N-grams.", "title": "Error correction of automatic speech recognition based on normalized web distance"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/dikici14_interspeech.html", "abstract": "Discriminative language modeling (DLM) aims to choose the most accurate word sequence by reranking the alternatives output by the automatic speech recognizer (ASR). The conventional (supervised) way of training a DLM requires a large amount of acoustic recordings together with their manual reference transcriptions. These transcriptions are used to determine the target ranks of the ASR outputs, but may be hard to obtain. Previous studies make use of the existing transcribed data to build a confusion model which boosts the training set by generating artificial data: a process known as semi-supervised training. In this study we concentrate on the unsupervised setting where no manual transcriptions are available at all. We propose three ways to determine a sequence that could serve as the missing reference text and two approaches which use this information to (i) determine the ranks of the ASR outputs in order to train the discriminative model directly, and (ii) build a confusion model in order to generate artificial training examples. We compare our techniques with the supervised and the semi-supervised setups. Using the reranking variant of the WER-sensitive perceptron algorithm, we obtain word error rate improvements up to half of those of the supervised case.", "title": "Unsupervised training methods for discriminative language modeling"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/qin14_interspeech.html", "abstract": "This paper presents initial studies on building a vocabulary self-learning speech recognition system that can automatically learn unknown words and expand its recognition vocabulary. Our recognizer can detect and recover out-of-vocabulary (OOV) words in speech, then incorporate OOV words into its lexicon and language model (LM). As a result, these unknown words can be correctly recognized when encountered by the recognizer in future. Specifically, we apply the word-fragment hybrid system framework to detect the presence of OOV words. We propose a better phoneme-to-grapheme (P2G) model so as to correctly recover the written form for more OOV words. Furthermore, we estimate LM scores for OOV words using their syntactic and semantic properties. The experimental results show that more than 40% OOV words are successfully learned from the development data, and about 60% learned OOV words are recognized in the testing data.", "title": "Building a vocabulary self-learning speech recognition system"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/schlippe14_interspeech.html", "abstract": "In this paper we propose efficient methods which contribute to a rapid and economic semi-automatic pronunciation dictionary development and evaluate them on English, German, Spanish, Vietnamese, Swahili, and Haitian Creole. First we determine optimal strategies for the word selection and the period for the grapheme-to-phoneme model retraining. In addition to the traditional concatenation of single phonemes most commonly associated with each grapheme, we show that web-derived pronunciations and cross-lingual grapheme-to-phoneme models can help to reduce the initial editing effort. Furthermore we show that our phoneme-level combination of the output of multiple grapheme-to-phoneme converters reduces the editing effort more than the best single converters. Totally, we report on average 15% relative editing effort reduction with our phoneme-level combination compared to conventional methods. An additional reduction of 6% relative was possible by including initial pronunciations from Wiktionary for English, German, and Spanish.", "title": "Methods for efficient semi-automatic pronunciation dictionary bootstrapping"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/akbacak14_interspeech.html", "abstract": "For domain-specific speech recognition tasks, it is best if the statistical language model component is trained with text data that is content-wise and style-wise similar to the targeted domain for which the application is built. For state-of-the-art language modeling techniques that can be used in real-time within speech recognition engines during first-pass decoding (e.g., N-gram models), the above constraints have to be fulfilled in the training data. However collecting such data, even through crowd sourcing, is expensive and time consuming, and can still be not representative of how a much larger user population would interact with the recognition system. In this paper, we address this problem by employing several semantic web sources that already contain the domain-specific knowledge, such as query click logs and knowledge graphs. We build statistical language models that meet the requirements listed above for domain-specific recognition tasks where natural language is used and the user queries are about name entities in a specific domain. As a case study, in the movies domain where users' voice queries are movie related, compared to a generic web language model, a language model trained with the above resources not only yields significant perplexity and word-error-rate improvements, but also presents an approach where such language models can be rapidly developed for other domains.", "title": "Rapidly building domain-specific entity-centric language models using semantic web knowledge sources"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/lee14f_interspeech.html", "abstract": "A Computer-Assisted Pronunciation Training (CAPT) system can provide greater benefit to language learners if it provides not only scoring but also corrective feedback. However, the process of deriving pronunciation error patterns usually requires linguistic knowledge, or large quantities of expensive, annotated, corpora from nonnative speakers. In this paper we explore the possibility of deriving context-dependent error patterns with limited human annotations. A two-stage labeling mechanism is proposed, which first selects a set of templates for human annotation, and then propagates the labels. To deal with the imbalanced number of correct and incorrect phone-level pronunciations in nonnative speech, pronunciation patterns on an individual learner-level are first summarized, and then corpus-level clustering is done for template selection. The concept of contextual similarity based on a phonemic broad class definition is also proposed for label propagation. For evaluation, we view the task as an information retrieval task, and take advantage of metrics that consider both the importance and the ranking of an error type. Experimental results on a Chinese University of Hong Kong (CUHK) nonnative corpus show that the proposed framework can effectively discover prominent error patterns.", "title": "Context-dependent pronunciation error pattern discovery with limited annotations"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/sapru14_interspeech.html", "abstract": "Accessing and browsing archives of multiparty conversations can be significantly facilitated by labeling them in terms of high level information. In this paper, we investigate automatic labeling of speaker roles and topic changes in professional meetings. Using the framework of unsupervised topic modeling we express speaker utterances as mixture of latent variables, each of which is governed by a multinomial distribution. The generated latent topic distributions are then used as features for predicting role and topic changes. Experiments performed on several hours of meeting data selected from AMI corpus reveal that latent topic features are effective predictors of speaker roles and topic changes. Furthermore, experiments also reveal an improvement in performance when latent topic information is combined with other multistream features.", "title": "Detecting speaker roles and topic changes in multiparty conversations using latent topic models"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/xu14g_interspeech.html", "abstract": "This paper presents a deep neural network (DNN) approach to sentence boundary detection in broadcast news. We extract prosodic and lexical features at each inter-word position in the transcripts and learn a sequential classifier to label these positions as either boundary or non-boundary. This work is realized by a hybrid DNN-CRF (conditional random field) architecture. The DNN accepts prosodic feature inputs and non-linearly maps them into boundary/non-boundary posterior probability outputs. Subsequently, the posterior probabilities are combined with lexical features and the integrated features are modeled by a linear-chain CRF. The CRF finally labels the inter-word positions as boundary or non-boundary by Viterbi decoding. Experiments show that, as compared with the state-of-the-art DTCRF approach, the proposed DNN-CRF approach achieves 16.7% and 4.1% reduction in NIST boundary detection error in reference and speech recognition transcripts, respectively.", "title": "A deep neural network approach for sentence boundary detection in broadcast news"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/gupta14b_interspeech.html", "abstract": "Natural conversations often involve disfluencies in the form of revisions, repetitions, interjections, filled pauses and such. This paper focuses on word/phrase repetitions and revisions that are lexically well formed. These are generally captured by an ASR but pose problems to downstream processing such as spoken language translation (SLT). We describe a system to identify such word level disfluencies with a goal towards removing them in real time from the automatic recognition (ASR) system output. We use a span based training system to utilize the contextual information while tagging disfluencies. We design our system on the oracle transcripts and test them on both reference and ASR transcripts. We achieve an area under the receiver operating characteristics (ROC) curve for word level disfluency detection of .93 and .87 for the reference and the ASR transcripts respectively.", "title": "Variable Span disfluency detection in ASR transcripts"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/dutrey14_interspeech.html", "abstract": "In this paper, we present a Conditional Random Field based approach for automatic detection of edit disfluencies in a conversational telephone corpus in French. We define disfluency patterns using both linguistic and acoustic features to perform disfluency detection. Two related tasks are considered: the first task aims at detecting the disfluent speech portion proper or reparandum, i.e. the portion to be removed if we want to improve the readability of transcribed data ; in the second task, we aim at identifying also the corrected portion or repair which can be useful in follow-up discourse and dialogue analyses or in opinion mining. For these two tasks, we present comparative results as a function of the involved type of features (acoustic and/or linguistic). Generally speaking, best results are obtained by CRF models combining both acoustic and linguistic features.", "title": "A CRF-based approach to automatic disfluency detection in a French call-centre corpus"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/hasan14_interspeech.html", "abstract": "Making speech recognition output readable is an important task. The first step here is automatic sentence end detection (SED). We introduce novel F0 derivative-based features and sentence end distance features for SED that yield significant improvements in slot error rate (SER) in a multi-pass framework. Three different SED approaches are compared on a spoken lecture task: hidden event language models, boosting, and conditional random fields (CRFs). Experiments on reference transcripts show that CRF-based models give best results. Inclusion of pause duration features yields an improvement of 11.1% in SER. The addition of the F0-derivative features gives a further reduction of 3.0% absolute, and an additional 0.5% is gained by use of backward distance features. In the absence of audio, the use of backward features alone yields 2.2% absolute reduction in SER.", "title": "Multi-pass sentence-end detection of lecture speech"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/zayats14_interspeech.html", "abstract": "This paper investigates automatic detection of different types of self-repairs in spontaneous speech under different social contexts, from casual conversations to government hearings. The work shows that a simple CRF-based model is effective for cross-domain training, which is important for contexts where annotated data is not available. The approach explicitly represents common types of disfluencies observed in multi-domain data both in the model state space and the features extracted. In addition, the model incorporates an expanded state space for recognizing the repair structure, unlike prior work that annotates only the reparandum.", "title": "Multi-domain disfluency and repair detection"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/jiang14b_interspeech.html", "abstract": "Recently, deep bottleneck features (DBF) extracted from a deep neural network (DNN) containing a narrow bottleneck layer, have been applied for language identification (LID), and yield significant performance improvement over state-of-the-art methods on NIST LRE 2009. However, the DNN is trained using a large corpus of specific language which is not directly related to the LID task. More recently, lattice based discriminative training methods for extracting more targeted DBF were proposed for ASR. Inspired by this, this paper proposes to tune the post-trained DNN parameters using an LID-specific training corpus, which may make the resulting DBF, termed a Discriminative DBF (D2BF), more discriminative and task-aware. Specifically, the maximum mutual information (MMI) criterion, with gradient descent, is applied to update the DNN parameters of the bottleneck layer in an iterative fashion. We evaluate the performance of the proposed D2BF using different back-end models, including GMM-MMI and ivector, over the most confused 6-languages selected from NIST LRE 2009. The results show that the proposed D2BF is more appropriate and effective than the original DBF.", "title": "Task-aware deep bottleneck features for spoken language identification"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/tong14_interspeech.html", "abstract": "One challenge in spoken language recognition is the availability of training data. In this paper, we propose a virtual example construction method to derive artificial training examples from the existing training data. Using the proposed method, both target virtual examples and non-target virtual examples can be derived from the available training samples. An iterative virtual example selection method is proposed to select those virtual examples that may provide extra discriminative information for language separation. By incorporating virtual examples in language classifier training, the language recognition performances are improved for both closed-set and open-set tasks. Specifically, for LRE 2009 evaluation data of three durations: 30-seconds, 10-seconds and 3-seconds, the language recognition performance improved by 3.67%, 11.98%, 6.42% respectively in closed-set conditions, and 10.14%, 10.55%, 5.75% respectively in open-set conditions.", "title": "Virtual example for phonotactic language recognition"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/liu14h_interspeech.html", "abstract": "Phonotactic method for spoken language recognition (SLR) deals with permissible phone patterns and their frequencies of occurrence in a specific language. Phone recognizers followed by vector space models (PR-VSM) system is a state-of-the-art phonotactic language identification system, in which any utterance can be mapped into a supervector filled with likelihood scores of the n-gram tokens (bag-of-n-gram). However, the bag-of-n-gram language model is not good at capture the long-context co-occurrence relations due to the restriction match of the n-gram phonemes and vulnerable to the insert and delete errors induced by the frontend phone recognizer. We propose a novel approach to fill the gaps based on the use of time-gap-weighted lattice kernel (TGWLK) in this paper. The kernel is an inner product in the feature space generated by all contiguous and uncontiguous subsequences in variety length in the lattice, which are weighted by an exponentially decaying factor produced by their time gap length. The results of experiments on the NIST 2009 LRE corpus demonstrate that the proposed TGWLK shows a reduction in equal error rate (EER) than baseline system.", "title": "Phonotactic language recognition based on time-gap-weighted lattice kernels"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/segbroeck14b_interspeech.html", "abstract": "This paper proposes Universal Background Model (UBM) fusion in the framework of total variability or i-vector modeling with the application to language identification (LID). The total variability subspace which is typically exploited to discriminate between the language classes of different speech recordings, is trained by combining the normalized Baum-Welch statistics of multiple UBMs. When the UBMs model a diverse set of feature representations, the method yields an i-vector representation which is more discriminant between the classes of interest. This approach is particularly useful when applied to short-duration utterances, and is a computationally less complex alternative to performance boosting as compared to system level fusion. We assess the performance of UBM fused total variability modeling on the task of robust language identification on short-duration utterances, as part of Phase-III of the DARPA RATS (Robust Automatic Transcription of Speech) program.", "title": "UBM fused total variability modeling for language identification"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/diez14b_interspeech.html", "abstract": "Previous works have shown that remarkable performance improvements can be attained in speaker and language recognition tasks by combining several heterogeneous systems that provide complementary information. In this work, the complementarity of several i-vector language recognition systems, using Mel-Frequency Cepstral-Coefficient (MFCC) features computed on Short-Time Fourier Analysis windows of different sizes, is studied. Language recognition experiments carried out on the NIST 2007 and 2009 LRE datasets reveal relative performance gains of up to 33% when fusing the systems, with regard to the best single system. Results suggest that combining acoustic systems based on analysis windows of different sizes may allow to get advantage from both the sharper characterization of short events provided by short windows and the better frequency resolution of stationary events provided by long windows.", "title": "On the complementarity of short-time fourier analysis windows of different lengths for improved language recognition"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/travadi14_interspeech.html", "abstract": "In this paper, we address the problem of Language Identification (LID) on short duration segments. Current state-of-the-art LID systems typically employ total variability i-Vector modeling for obtaining fixed length representation of utterances. However, when the utterances are short, only a small amount of data is available, and the estimated i-Vector representation will consequently exhibit significant variability, making the identification problem challenging. In this paper, we propose novel techniques to modify the standard normal prior distribution of the i-Vectors, to obtain a more discriminative i-Vector extraction given the small amount of available utterance data. Improved performance was observed by using the proposed i-Vector estimation techniques on short segments of the DARPA RATS corpora, with lengths as small as 3 seconds.", "title": "Modified-prior i-vector estimation for language identification of short duration utterances"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/dharo14_interspeech.html", "abstract": "A new language recognition technique based on the application of the philosophy of the Shifted Delta Coefficients (SDC) to phone log-likelihood ratio features (PLLR) is described. The new methodology allows the incorporation of long-span phonetic information at a frame-by-frame level while dealing with the temporal length of each phone unit. The proposed features are used to train an i-vector based system and tested on the Albayzin LRE 2012 dataset. The results show a relative improvement of 33.3% in Cavg in comparison with different state-of-the-art acoustic i-vector based systems. On the other hand, the integration of parallel phone ASR systems where each one is used to generate multiple PLLR coefficients which are stacked together and then projected into a reduced dimension are also presented. Finally, the paper shows how the incorporation of state information from the phone ASR contributes to provide additional improvements and how the fusion with the other acoustic and phonotactic systems provides an important improvement of 25.8% over the system presented during the competition.", "title": "Language recognition using phonotactic-based shifted delta coefficients and multiple phone recognizers"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/plchot14_interspeech.html", "abstract": "In this paper, we study the use of features based on frame-by-frame phone posteriors (PLLRs) for language recognition. The results are reported on the datasets developed for the DARPA RATS (Robust Automatic Transcription of Speech) program, which seeks to advance state of the art detection capabilities on audio from highly degraded communication channels. We show that systems based on the PLLRs outperform the standard acoustic system based on PLP2 features. By experimenting with the system combinations, we also demonstrate that the PLLR-based systems contain complementary information with respect to the PLP2 system. Finally we make a comparison between the PLLR and phonotactic systems with the outcome favorable to the PLLR.", "title": "PLLR features in language recognition system for RATS"}, {"url": "https://www.isca-speech.org/archive/interspeech_2014/yeong14_interspeech.html", "abstract": "Language identification (LID) is a process to identify the languages used in a text or speech. Code switching is the switching of a language in a sentence or speech utterance. This paper focuses on LID of words in code switching sentences. Code switching can occur intersentential or intrasentential. The reasons why a writer switches from one language to another due to various reasons and among them are the inability to express opinion in a particular target language, to attract attention, to address different audience, habitual expressions and so on. The difficulty in identifying the languages of each word in a code switching sentence is because the languages have the same character set. In addition, code switching can happen in a sentence as short as a word or as long as a sentence. In this paper, we propose an automatic LID for words in code switching sentences by using multi structural word information (MUSWI) such as grapheme, syllable and word structure and calculate by using n-gram statistical model. The proposed MUSWI approach achieves 96.36% in term of accuracy on the code switching sentences, 99.07% on the multilingual sentences (non-code switching) which are under-resourced and closely related languages.", "title": "Language identification of code Switching sentences and multilingual sentences of under-resourced languages by using multi structural word information"}, {"url": null, "abstract": null, "title": "Keynote"}, {"url": null, "abstract": null, "title": "Multi-Lingual ASR"}, {"url": null, "abstract": null, "title": "Prosody Processing"}, {"url": null, "abstract": null, "title": "Speaker Recognition \u2014 Applications"}, {"url": null, "abstract": null, "title": "Phonetics and Phonology 1, 2"}, {"url": null, "abstract": null, "title": "Open Domain Situated Conversational Interaction (Special Session)"}, {"url": null, "abstract": null, "title": "Speech Production: Models and Acoustics"}, {"url": null, "abstract": null, "title": "Extraction of Para-Linguistic Information"}, {"url": null, "abstract": null, "title": "Spoken Language Understanding"}, {"url": null, "abstract": null, "title": "Spoken Dialogue Systems"}, {"url": null, "abstract": null, "title": "DNN Architectures and Robust Recognition"}, {"url": null, "abstract": null, "title": "Speaker Recognition \u2014 Evaluation and Forensics"}, {"url": null, "abstract": null, "title": "Speech Production I, II"}, {"url": null, "abstract": null, "title": "INTERSPEECH 2014 Computational Paralinguistics ChallengE (ComParE)"}, {"url": null, "abstract": null, "title": "Hearing and Perception"}, {"url": null, "abstract": null, "title": "Cross-Linguistic Studies"}, {"url": null, "abstract": null, "title": "Speaker Diarization"}, {"url": null, "abstract": null, "title": "Robust ASR 1, 2"}, {"url": null, "abstract": null, "title": "Implementation of Language Model Algorithms"}, {"url": null, "abstract": null, "title": "Speaker Recognition \u2014 Noise and Channel Robustness"}, {"url": null, "abstract": null, "title": "Speech Synthesis I-III"}, {"url": null, "abstract": null, "title": "Multi-Lingual Cross-Lingual and Low-Resource ASR"}, {"url": null, "abstract": null, "title": "Speech Estimation and Sound Source Separation"}, {"url": null, "abstract": null, "title": "Feature Extraction and Modeling for ASR 1, 2"}, {"url": null, "abstract": null, "title": "Speech Analysis I, II"}, {"url": null, "abstract": null, "title": "Speech Technologies and Applications"}, {"url": null, "abstract": null, "title": "Source Separation and Computational Auditory Scene Analysis"}, {"url": null, "abstract": null, "title": "Speech Technologies for Ambient Assisted Living (Special Session)"}, {"url": null, "abstract": null, "title": "DNN for ASR"}, {"url": null, "abstract": null, "title": "Speaker Recognition \u2014 General Topics"}, {"url": null, "abstract": null, "title": "Speech Processing with Multi-Modalities"}, {"url": null, "abstract": null, "title": "Normalization and Discriminative Training Methods"}, {"url": null, "abstract": null, "title": "Paralinguistic and Extralinguistic Information"}, {"url": null, "abstract": null, "title": "Text Processing for Speech Synthesis"}, {"url": null, "abstract": null, "title": "Cross-language Perception and Production"}, {"url": null, "abstract": null, "title": "Text-Dependent Speaker Verification With Short Utterances (Special"}, {"url": null, "abstract": null, "title": "Speech and Audio Analysis"}, {"url": null, "abstract": null, "title": "Cross-Lingual and Adaptive Language Modeling"}, {"url": null, "abstract": null, "title": "Pronunciation Modeling and Learning"}, {"url": null, "abstract": null, "title": "Show and Tell Session 1, 1"}, {"url": null, "abstract": null, "title": "Statistical Parametric Speech Synthesis"}, {"url": null, "abstract": null, "title": "Voice Activity Detection"}, {"url": null, "abstract": null, "title": "Disordered Speech"}, {"url": null, "abstract": null, "title": "Speech and Multimodal Resources"}, {"url": null, "abstract": null, "title": "Phase Importance in Speech Processing Applications (Special Session)"}, {"url": null, "abstract": null, "title": "Spoken Term Detection and Document Retrieval"}, {"url": null, "abstract": null, "title": "Prosody and Paralinguistic Information"}, {"url": null, "abstract": null, "title": "Features and Robustness in Speaker and Language Recognition"}, {"url": null, "abstract": null, "title": "Topic Spotting and Summarization of Spoken Documents"}, {"url": null, "abstract": null, "title": "DNN Learning"}, {"url": null, "abstract": null, "title": "Perception of Emotion and Prosody"}, {"url": null, "abstract": null, "title": "Deep Neural Networks for Speech Generation and Synthesis (Special"}, {"url": null, "abstract": null, "title": "Speech Analysis and Perception"}, {"url": null, "abstract": null, "title": "Intelligibility Enhancement and Predictive Measures"}, {"url": null, "abstract": null, "title": "Speech and Language Processing \u2014 General Topics"}, {"url": null, "abstract": null, "title": "Language, Dialect and Accent Recognition"}, {"url": null, "abstract": null, "title": "Adaptation 1, 2"}, {"url": null, "abstract": null, "title": "Speaker Localization"}, {"url": null, "abstract": null, "title": "Speech Representation, Detection and Classification"}, {"url": null, "abstract": null, "title": "Spoken Term Detection for Low-Resource Languages I, II"}, {"url": null, "abstract": null, "title": "Voice Conversion"}, {"url": null, "abstract": null, "title": "Speech and Audio Segmentation and Classification"}, {"url": null, "abstract": null, "title": "Language Acquisition"}, {"url": null, "abstract": null, "title": "Speech Perception"}, {"url": null, "abstract": null, "title": "Language and Lexical Modeling"}, {"url": null, "abstract": null, "title": "Speech Enhancement (Single- and Multi-Channel) 1, 2"}, {"url": null, "abstract": null, "title": "Speech Coding and Transmission"}, {"url": null, "abstract": null, "title": "Unsupervised or Corrective Lexical Modeling"}, {"url": null, "abstract": null, "title": "Meta Data"}, {"url": null, "abstract": null, "title": "Language Recognition"}]